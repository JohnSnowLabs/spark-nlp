<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-08-04T09:17:20+00:00</updated><id>/feed.xml</id><title type="html">Spark NLP</title><subtitle>High Performance NLP with Apache Spark
</subtitle><author><name>{&quot;type&quot;=&gt;nil, &quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;googleplus&quot;=&gt;nil, &quot;telegram&quot;=&gt;nil, &quot;medium&quot;=&gt;nil, &quot;zhihu&quot;=&gt;nil, &quot;douban&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;npm&quot;=&gt;nil}</name></author><entry><title type="html">ALBERT Embeddings (Base Uncase)</title><link href="/2023/08/02/albert_base_uncased_en.html" rel="alternate" type="text/html" title="ALBERT Embeddings (Base Uncase)" /><published>2023-08-02T00:00:00+00:00</published><updated>2023-08-02T00:00:00+00:00</updated><id>/2023/08/02/albert_base_uncased_en</id><content type="html" xml:base="/2023/08/02/albert_base_uncased_en.html">## Description

ALBERT is &quot;A Lite&quot; version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper &quot;[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.](https://arxiv.org/abs/1909.11942)&quot;

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/albert_base_uncased_en_5.0.2_3.0_1690935260361.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/albert_base_uncased_en_5.0.2_3.0_1690935260361.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```

{:.nlu-block}
```python
import nlu

text = [&quot;I love NLP&quot;]
embeddings_df = nlu.load('en.embed.albert.base_uncased').predict(text, output_level='token')
embeddings_df
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|albert_base_uncased|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|42.0 MB|
|Case sensitive:|false|

## References

[https://huggingface.co/albert-base-v2](https://huggingface.co/albert-base-v2)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="en" /><category term="english" /><category term="embeddings" /><category term="albert" /><category term="onnx" /><summary type="html">Description ALBERT is “A Lite” version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper “ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.” Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu text = [&quot;I love NLP&quot;] embeddings_df = nlu.load('en.embed.albert.base_uncased').predict(text, output_level='token') embeddings_df Model Information Model Name: albert_base_uncased Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 42.0 MB Case sensitive: false References https://huggingface.co/albert-base-v2</summary></entry><entry><title type="html">ALBERT Embeddings (Base Uncase)</title><link href="/2023/08/02/albert_base_uncased_opt_en.html" rel="alternate" type="text/html" title="ALBERT Embeddings (Base Uncase)" /><published>2023-08-02T00:00:00+00:00</published><updated>2023-08-02T00:00:00+00:00</updated><id>/2023/08/02/albert_base_uncased_opt_en</id><content type="html" xml:base="/2023/08/02/albert_base_uncased_opt_en.html">## Description

ALBERT is &quot;A Lite&quot; version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper &quot;[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.](https://arxiv.org/abs/1909.11942)&quot;

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/albert_base_uncased_opt_en_5.0.2_3.0_1690935304465.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/albert_base_uncased_opt_en_5.0.2_3.0_1690935304465.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```

{:.nlu-block}
```python
import nlu

text = [&quot;I love NLP&quot;]
embeddings_df = nlu.load('en.embed.albert.base_uncased').predict(text, output_level='token')
embeddings_df
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|albert_base_uncased_opt|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|115.0 MB|
|Case sensitive:|false|

## References

[https://huggingface.co/albert-base-v2](https://huggingface.co/albert-base-v2)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="en" /><category term="english" /><category term="embeddings" /><category term="albert" /><category term="onnx" /><summary type="html">Description ALBERT is “A Lite” version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper “ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.” Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu text = [&quot;I love NLP&quot;] embeddings_df = nlu.load('en.embed.albert.base_uncased').predict(text, output_level='token') embeddings_df Model Information Model Name: albert_base_uncased_opt Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 115.0 MB Case sensitive: false References https://huggingface.co/albert-base-v2</summary></entry><entry><title type="html">ALBERT Embeddings (Base Uncase)</title><link href="/2023/08/02/albert_base_uncased_quantized_en.html" rel="alternate" type="text/html" title="ALBERT Embeddings (Base Uncase)" /><published>2023-08-02T00:00:00+00:00</published><updated>2023-08-02T00:00:00+00:00</updated><id>/2023/08/02/albert_base_uncased_quantized_en</id><content type="html" xml:base="/2023/08/02/albert_base_uncased_quantized_en.html">## Description

ALBERT is &quot;A Lite&quot; version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper &quot;[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.](https://arxiv.org/abs/1909.11942)&quot;

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/albert_base_uncased_quantized_en_5.0.2_3.0_1690935326685.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/albert_base_uncased_quantized_en_5.0.2_3.0_1690935326685.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```

{:.nlu-block}
```python
import nlu

text = [&quot;I love NLP&quot;]
embeddings_df = nlu.load('en.embed.albert.base_uncased').predict(text, output_level='token')
embeddings_df
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|albert_base_uncased_quantized|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|46.0 MB|
|Case sensitive:|false|

## References

[https://huggingface.co/albert-base-v2](https://huggingface.co/albert-base-v2)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="en" /><category term="english" /><category term="embeddings" /><category term="albert" /><category term="onnx" /><summary type="html">Description ALBERT is “A Lite” version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper “ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.” Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu text = [&quot;I love NLP&quot;] embeddings_df = nlu.load('en.embed.albert.base_uncased').predict(text, output_level='token') embeddings_df Model Information Model Name: albert_base_uncased_quantized Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 46.0 MB Case sensitive: false References https://huggingface.co/albert-base-v2</summary></entry><entry><title type="html">ALBERT Embeddings (Large Uncase)</title><link href="/2023/08/02/albert_large_uncased_en.html" rel="alternate" type="text/html" title="ALBERT Embeddings (Large Uncase)" /><published>2023-08-02T00:00:00+00:00</published><updated>2023-08-02T00:00:00+00:00</updated><id>/2023/08/02/albert_large_uncased_en</id><content type="html" xml:base="/2023/08/02/albert_large_uncased_en.html">## Description

ALBERT is &quot;A Lite&quot; version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper &quot;[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.](https://arxiv.org/abs/1909.11942)&quot;

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/albert_large_uncased_en_5.0.2_3.0_1690935781574.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/albert_large_uncased_en_5.0.2_3.0_1690935781574.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = AlbertEmbeddings.pretrained(&quot;albert_large_uncased&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = AlbertEmbeddings.pretrained(&quot;albert_large_uncased&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```

{:.nlu-block}
```python
import nlu

text = [&quot;I love NLP&quot;]
embeddings_df = nlu.load('en.embed.albert.large_uncased').predict(text, output_level='token')
embeddings_df
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|albert_large_uncased|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|62.7 MB|
|Case sensitive:|false|

## References

[https://huggingface.co/albert-large-v2](https://huggingface.co/albert-large-v2)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="en" /><category term="english" /><category term="embeddings" /><category term="albert" /><category term="large" /><category term="onnx" /><summary type="html">Description ALBERT is “A Lite” version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper “ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.” Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = AlbertEmbeddings.pretrained(&quot;albert_large_uncased&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = AlbertEmbeddings.pretrained(&quot;albert_large_uncased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu text = [&quot;I love NLP&quot;] embeddings_df = nlu.load('en.embed.albert.large_uncased').predict(text, output_level='token') embeddings_df Model Information Model Name: albert_large_uncased Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 62.7 MB Case sensitive: false References https://huggingface.co/albert-large-v2</summary></entry><entry><title type="html">ALBERT Embeddings (Large Uncase)</title><link href="/2023/08/02/albert_large_uncased_opt_en.html" rel="alternate" type="text/html" title="ALBERT Embeddings (Large Uncase)" /><published>2023-08-02T00:00:00+00:00</published><updated>2023-08-02T00:00:00+00:00</updated><id>/2023/08/02/albert_large_uncased_opt_en</id><content type="html" xml:base="/2023/08/02/albert_large_uncased_opt_en.html">## Description

ALBERT is &quot;A Lite&quot; version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper &quot;[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.](https://arxiv.org/abs/1909.11942)&quot;

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/albert_large_uncased_opt_en_5.0.2_3.0_1690935934328.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/albert_large_uncased_opt_en_5.0.2_3.0_1690935934328.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = AlbertEmbeddings.pretrained(&quot;albert_large_uncased&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = AlbertEmbeddings.pretrained(&quot;albert_large_uncased&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```

{:.nlu-block}
```python
import nlu

text = [&quot;I love NLP&quot;]
embeddings_df = nlu.load('en.embed.albert.large_uncased').predict(text, output_level='token')
embeddings_df
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|albert_large_uncased_opt|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|333.8 MB|
|Case sensitive:|false|

## References

[https://huggingface.co/albert-large-v2](https://huggingface.co/albert-large-v2)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="en" /><category term="english" /><category term="embeddings" /><category term="albert" /><category term="large" /><category term="onnx" /><summary type="html">Description ALBERT is “A Lite” version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper “ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.” Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = AlbertEmbeddings.pretrained(&quot;albert_large_uncased&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = AlbertEmbeddings.pretrained(&quot;albert_large_uncased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu text = [&quot;I love NLP&quot;] embeddings_df = nlu.load('en.embed.albert.large_uncased').predict(text, output_level='token') embeddings_df Model Information Model Name: albert_large_uncased_opt Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 333.8 MB Case sensitive: false References https://huggingface.co/albert-large-v2</summary></entry><entry><title type="html">ALBERT Embeddings (Large Uncase)</title><link href="/2023/08/02/albert_large_uncased_quantized_en.html" rel="alternate" type="text/html" title="ALBERT Embeddings (Large Uncase)" /><published>2023-08-02T00:00:00+00:00</published><updated>2023-08-02T00:00:00+00:00</updated><id>/2023/08/02/albert_large_uncased_quantized_en</id><content type="html" xml:base="/2023/08/02/albert_large_uncased_quantized_en.html">## Description

ALBERT is &quot;A Lite&quot; version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper &quot;[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.](https://arxiv.org/abs/1909.11942)&quot;

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/albert_large_uncased_quantized_en_5.0.2_3.0_1690935970820.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/albert_large_uncased_quantized_en_5.0.2_3.0_1690935970820.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = AlbertEmbeddings.pretrained(&quot;albert_large_uncased&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = AlbertEmbeddings.pretrained(&quot;albert_large_uncased&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```

{:.nlu-block}
```python
import nlu

text = [&quot;I love NLP&quot;]
embeddings_df = nlu.load('en.embed.albert.large_uncased').predict(text, output_level='token')
embeddings_df
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|albert_large_uncased_quantized|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|71.4 MB|
|Case sensitive:|false|

## References

[https://huggingface.co/albert-large-v2](https://huggingface.co/albert-large-v2)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="en" /><category term="english" /><category term="embeddings" /><category term="albert" /><category term="large" /><category term="onnx" /><summary type="html">Description ALBERT is “A Lite” version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper “ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.” Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = AlbertEmbeddings.pretrained(&quot;albert_large_uncased&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = AlbertEmbeddings.pretrained(&quot;albert_large_uncased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu text = [&quot;I love NLP&quot;] embeddings_df = nlu.load('en.embed.albert.large_uncased').predict(text, output_level='token') embeddings_df Model Information Model Name: albert_large_uncased_quantized Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 71.4 MB Case sensitive: false References https://huggingface.co/albert-large-v2</summary></entry><entry><title type="html">CamemBERT Subsample of CCNet</title><link href="/2023/08/01/camembert_base_ccnet_4gb_fr.html" rel="alternate" type="text/html" title="CamemBERT Subsample of CCNet" /><published>2023-08-01T00:00:00+00:00</published><updated>2023-08-01T00:00:00+00:00</updated><id>/2023/08/01/camembert_base_ccnet_4gb_fr</id><content type="html" xml:base="/2023/08/01/camembert_base_ccnet_4gb_fr.html">## Description

[CamemBERT](https://arxiv.org/abs/1911.03894) is a state-of-the-art language model for French based on the RoBERTa model.
For further information or requests, please go to [Camembert Website](https://camembert-model.fr/)

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/camembert_base_ccnet_4gb_fr_5.0.2_3.0_1690928369186.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/camembert_base_ccnet_4gb_fr_5.0.2_3.0_1690928369186.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = CamemBertEmbeddings.pretrained(&quot;camembert_base_ccnet_4gb&quot;, &quot;fr&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = CamemBertEmbeddings.pretrained(&quot;camembert_base_ccnet_4gb&quot;, &quot;fr&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```

{:.nlu-block}
```python
import nlu
nlu.load(&quot;fr.embed.camembert_ccnet4g&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|camembert_base_ccnet_4gb|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|fr|
|Size:|263.4 MB|
|Case sensitive:|true|

## Benchmarking

```bash

| Model                          | #params                        | Arch. | Training data                     |
|--------------------------------|--------------------------------|-------|-----------------------------------|
| `camembert-base` | 110M   | Base  | OSCAR (138 GB of text)            |
| `camembert/camembert-large`              | 335M    | Large | CCNet (135 GB of text)            |
| `camembert/camembert-base-ccnet`         | 110M    | Base  | CCNet (135 GB of text)            |
| `camembert/camembert-base-wikipedia-4gb` | 110M    | Base  | Wikipedia (4 GB of text)          |
| `camembert/camembert-base-oscar-4gb`     | 110M    | Base  | Subsample of OSCAR (4 GB of text) |
| `camembert/camembert-base-ccnet-4gb`     | 110M    | Base  | Subsample of CCNet (4 GB of text) |
```</content><author><name>John Snow Labs</name></author><category term="fr" /><category term="french" /><category term="embeddings" /><category term="camembert" /><category term="ccnet" /><category term="open_source" /><category term="onnx" /><summary type="html">Description CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. For further information or requests, please go to Camembert Website Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = CamemBertEmbeddings.pretrained(&quot;camembert_base_ccnet_4gb&quot;, &quot;fr&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = CamemBertEmbeddings.pretrained(&quot;camembert_base_ccnet_4gb&quot;, &quot;fr&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu nlu.load(&quot;fr.embed.camembert_ccnet4g&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;) Model Information Model Name: camembert_base_ccnet_4gb Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: fr Size: 263.4 MB Case sensitive: true Benchmarking | Model | #params | Arch. | Training data | |--------------------------------|--------------------------------|-------|-----------------------------------| | `camembert-base` | 110M | Base | OSCAR (138 GB of text) | | `camembert/camembert-large` | 335M | Large | CCNet (135 GB of text) | | `camembert/camembert-base-ccnet` | 110M | Base | CCNet (135 GB of text) | | `camembert/camembert-base-wikipedia-4gb` | 110M | Base | Wikipedia (4 GB of text) | | `camembert/camembert-base-oscar-4gb` | 110M | Base | Subsample of OSCAR (4 GB of text) | | `camembert/camembert-base-ccnet-4gb` | 110M | Base | Subsample of CCNet (4 GB of text) |</summary></entry><entry><title type="html">CamemBERT Base CCNet</title><link href="/2023/08/01/camembert_base_ccnet_fr.html" rel="alternate" type="text/html" title="CamemBERT Base CCNet" /><published>2023-08-01T00:00:00+00:00</published><updated>2023-08-01T00:00:00+00:00</updated><id>/2023/08/01/camembert_base_ccnet_fr</id><content type="html" xml:base="/2023/08/01/camembert_base_ccnet_fr.html">## Description

[CamemBERT](https://arxiv.org/abs/1911.03894) is a state-of-the-art language model for French based on the RoBERTa model.
For further information or requests, please go to [Camembert Website](https://camembert-model.fr/)

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/camembert_base_ccnet_fr_5.0.2_3.0_1690927305918.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/camembert_base_ccnet_fr_5.0.2_3.0_1690927305918.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = CamemBertEmbeddings.pretrained(&quot;camembert_base_ccnet&quot;, &quot;fr&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = CamemBertEmbeddings.pretrained(&quot;camembert_base_ccnet&quot;, &quot;fr&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```

{:.nlu-block}
```python
import nlu
nlu.load(&quot;fr.embed.camembert_base_ccnet&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|camembert_base_ccnet|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|fr|
|Size:|263.6 MB|
|Case sensitive:|true|

## Benchmarking

```bash

| Model                          | #params                        | Arch. | Training data                     |
|--------------------------------|--------------------------------|-------|-----------------------------------|
| `camembert-base` | 110M   | Base  | OSCAR (138 GB of text)            |
| `camembert/camembert-large`              | 335M    | Large | CCNet (135 GB of text)            |
| `camembert/camembert-base-ccnet`         | 110M    | Base  | CCNet (135 GB of text)            |
| `camembert/camembert-base-wikipedia-4gb` | 110M    | Base  | Wikipedia (4 GB of text)          |
| `camembert/camembert-base-oscar-4gb`     | 110M    | Base  | Subsample of OSCAR (4 GB of text) |
| `camembert/camembert-base-ccnet-4gb`     | 110M    | Base  | Subsample of CCNet (4 GB of text) |
```</content><author><name>John Snow Labs</name></author><category term="fr" /><category term="french" /><category term="embeddings" /><category term="camembert" /><category term="ccnet" /><category term="open_source" /><category term="onnx" /><summary type="html">Description CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. For further information or requests, please go to Camembert Website Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = CamemBertEmbeddings.pretrained(&quot;camembert_base_ccnet&quot;, &quot;fr&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = CamemBertEmbeddings.pretrained(&quot;camembert_base_ccnet&quot;, &quot;fr&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu nlu.load(&quot;fr.embed.camembert_base_ccnet&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;) Model Information Model Name: camembert_base_ccnet Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: fr Size: 263.6 MB Case sensitive: true Benchmarking | Model | #params | Arch. | Training data | |--------------------------------|--------------------------------|-------|-----------------------------------| | `camembert-base` | 110M | Base | OSCAR (138 GB of text) | | `camembert/camembert-large` | 335M | Large | CCNet (135 GB of text) | | `camembert/camembert-base-ccnet` | 110M | Base | CCNet (135 GB of text) | | `camembert/camembert-base-wikipedia-4gb` | 110M | Base | Wikipedia (4 GB of text) | | `camembert/camembert-base-oscar-4gb` | 110M | Base | Subsample of OSCAR (4 GB of text) | | `camembert/camembert-base-ccnet-4gb` | 110M | Base | Subsample of CCNet (4 GB of text) |</summary></entry><entry><title type="html">CamemBERT Base Model</title><link href="/2023/08/01/camembert_base_fr.html" rel="alternate" type="text/html" title="CamemBERT Base Model" /><published>2023-08-01T00:00:00+00:00</published><updated>2023-08-01T00:00:00+00:00</updated><id>/2023/08/01/camembert_base_fr</id><content type="html" xml:base="/2023/08/01/camembert_base_fr.html">## Description

[CamemBERT](https://arxiv.org/abs/1911.03894) is a state-of-the-art language model for French based on the RoBERTa model.
For further information or requests, please go to [Camembert Website](https://camembert-model.fr/)

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/camembert_base_fr_5.0.2_3.0_1690933576243.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/camembert_base_fr_5.0.2_3.0_1690933576243.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = CamemBertEmbeddings.pretrained(&quot;camembert_base&quot;, &quot;fr&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = CamemBertEmbeddings.pretrained(&quot;camembert_base&quot;, &quot;fr&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```

{:.nlu-block}
```python
import nlu
nlu.load(&quot;fr.embed.camembert_base&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|camembert_base|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|fr|
|Size:|264.0 MB|
|Case sensitive:|true|

## Benchmarking

```bash

| Model                          | #params                        | Arch. | Training data                     |
|--------------------------------|--------------------------------|-------|-----------------------------------|
| `camembert-base` | 110M   | Base  | OSCAR (138 GB of text)            |
| `camembert/camembert-large`              | 335M    | Large | CCNet (135 GB of text)            |
| `camembert/camembert-base-ccnet`         | 110M    | Base  | CCNet (135 GB of text)            |
| `camembert/camembert-base-wikipedia-4gb` | 110M    | Base  | Wikipedia (4 GB of text)          |
| `camembert/camembert-base-oscar-4gb`     | 110M    | Base  | Subsample of OSCAR (4 GB of text) |
| `camembert/camembert-base-ccnet-4gb`     | 110M    | Base  | Subsample of CCNet (4 GB of text) |
```</content><author><name>John Snow Labs</name></author><category term="fr" /><category term="french" /><category term="embeddings" /><category term="camembert" /><category term="base" /><category term="open_source" /><category term="onnx" /><summary type="html">Description CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. For further information or requests, please go to Camembert Website Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = CamemBertEmbeddings.pretrained(&quot;camembert_base&quot;, &quot;fr&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = CamemBertEmbeddings.pretrained(&quot;camembert_base&quot;, &quot;fr&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu nlu.load(&quot;fr.embed.camembert_base&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;) Model Information Model Name: camembert_base Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: fr Size: 264.0 MB Case sensitive: true Benchmarking | Model | #params | Arch. | Training data | |--------------------------------|--------------------------------|-------|-----------------------------------| | `camembert-base` | 110M | Base | OSCAR (138 GB of text) | | `camembert/camembert-large` | 335M | Large | CCNet (135 GB of text) | | `camembert/camembert-base-ccnet` | 110M | Base | CCNet (135 GB of text) | | `camembert/camembert-base-wikipedia-4gb` | 110M | Base | Wikipedia (4 GB of text) | | `camembert/camembert-base-oscar-4gb` | 110M | Base | Subsample of OSCAR (4 GB of text) | | `camembert/camembert-base-ccnet-4gb` | 110M | Base | Subsample of CCNet (4 GB of text) |</summary></entry><entry><title type="html">CamemBERT Base Model</title><link href="/2023/08/01/camembert_base_opt_fr.html" rel="alternate" type="text/html" title="CamemBERT Base Model" /><published>2023-08-01T00:00:00+00:00</published><updated>2023-08-01T00:00:00+00:00</updated><id>/2023/08/01/camembert_base_opt_fr</id><content type="html" xml:base="/2023/08/01/camembert_base_opt_fr.html">## Description

[CamemBERT](https://arxiv.org/abs/1911.03894) is a state-of-the-art language model for French based on the RoBERTa model.
For further information or requests, please go to [Camembert Website](https://camembert-model.fr/)

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/camembert_base_opt_fr_5.0.2_3.0_1690933783384.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/camembert_base_opt_fr_5.0.2_3.0_1690933783384.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = CamemBertEmbeddings.pretrained(&quot;camembert_base&quot;, &quot;fr&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = CamemBertEmbeddings.pretrained(&quot;camembert_base&quot;, &quot;fr&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```

{:.nlu-block}
```python
import nlu
nlu.load(&quot;fr.embed.camembert_base&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|camembert_base_opt|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|fr|
|Size:|264.3 MB|
|Case sensitive:|true|

## Benchmarking

```bash

| Model                          | #params                        | Arch. | Training data                     |
|--------------------------------|--------------------------------|-------|-----------------------------------|
| `camembert-base` | 110M   | Base  | OSCAR (138 GB of text)            |
| `camembert/camembert-large`              | 335M    | Large | CCNet (135 GB of text)            |
| `camembert/camembert-base-ccnet`         | 110M    | Base  | CCNet (135 GB of text)            |
| `camembert/camembert-base-wikipedia-4gb` | 110M    | Base  | Wikipedia (4 GB of text)          |
| `camembert/camembert-base-oscar-4gb`     | 110M    | Base  | Subsample of OSCAR (4 GB of text) |
| `camembert/camembert-base-ccnet-4gb`     | 110M    | Base  | Subsample of CCNet (4 GB of text) |
```</content><author><name>John Snow Labs</name></author><category term="fr" /><category term="french" /><category term="embeddings" /><category term="camembert" /><category term="base" /><category term="open_source" /><category term="onnx" /><summary type="html">Description CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. For further information or requests, please go to Camembert Website Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = CamemBertEmbeddings.pretrained(&quot;camembert_base&quot;, &quot;fr&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = CamemBertEmbeddings.pretrained(&quot;camembert_base&quot;, &quot;fr&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu nlu.load(&quot;fr.embed.camembert_base&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;) Model Information Model Name: camembert_base_opt Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: fr Size: 264.3 MB Case sensitive: true Benchmarking | Model | #params | Arch. | Training data | |--------------------------------|--------------------------------|-------|-----------------------------------| | `camembert-base` | 110M | Base | OSCAR (138 GB of text) | | `camembert/camembert-large` | 335M | Large | CCNet (135 GB of text) | | `camembert/camembert-base-ccnet` | 110M | Base | CCNet (135 GB of text) | | `camembert/camembert-base-wikipedia-4gb` | 110M | Base | Wikipedia (4 GB of text) | | `camembert/camembert-base-oscar-4gb` | 110M | Base | Subsample of OSCAR (4 GB of text) | | `camembert/camembert-base-ccnet-4gb` | 110M | Base | Subsample of CCNet (4 GB of text) |</summary></entry></feed>