<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-01-28T13:20:31+00:00</updated><id>/feed.xml</id><title type="html">Spark NLP</title><subtitle>High Performance NLP with Apache Spark
</subtitle><author><name>{&quot;type&quot;=&gt;nil, &quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;googleplus&quot;=&gt;nil, &quot;telegram&quot;=&gt;nil, &quot;medium&quot;=&gt;nil, &quot;zhihu&quot;=&gt;nil, &quot;douban&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;npm&quot;=&gt;nil}</name></author><entry><title type="html">Hocr for table recognition</title><link href="/2023/01/23/hocr_table_recognition_en_3_2.html" rel="alternate" type="text/html" title="Hocr for table recognition" /><published>2023-01-23T00:00:00+00:00</published><updated>2023-01-23T00:00:00+00:00</updated><id>/2023/01/23/hocr_table_recognition_en_3_2</id><content type="html" xml:base="/2023/01/23/hocr_table_recognition_en_3_2.html">## Description

Table structure recognition based on hocr with Tesseract architecture. 

Tesseract has been trained on a variety of datasets to improve its recognition capabilities. These datasets include images of text in various languages and scripts, as well as images with different font styles, sizes, and orientations. The training process involves feeding the engine with a large number of images and their corresponding text, allowing the engine to learn the patterns and characteristics of different text styles. One of the most important datasets used in training Tesseract is the UNLV dataset, which contains over 400,000 images of text in different languages, scripts, and font styles. This dataset is widely used in the OCR community and has been instrumental in improving the accuracy of Tesseract. Other datasets that have been used in training Tesseract include the ICDAR dataset, the IIIT-HWS dataset, and the RRC-GV-WS dataset.

In addition to these datasets, Tesseract also uses a technique called adaptive training, where the engine can continuously improve its recognition capabilities by learning from new images and text. This allows Tesseract to adapt to new text styles and languages, and improve its overall accuracy.


## Predicted Entities

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
[Open in Colab](https://github.com/JohnSnowLabs/spark-ocr-workshop/tree/master/jupyter/Cards/SparkOcrImageTableRecognitionWHOCR.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Download&lt;/button&gt;


## How to use

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
binary_to_image = BinaryToImage() \
    .setInputCol(&quot;content&quot;)  \
    .setOutputCol(&quot;image&quot;) 

table_detector = ImageTableDetector.pretrained(&quot;general_model_table_detection_v2&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) \
    .setInputCol(&quot;image&quot;) \
    .setOutputCol(&quot;table_regions&quot;)

splitter = ImageSplitRegions() \
    .setInputCol(&quot;image&quot;) \
    .setInputRegionsCol(&quot;table_regions&quot;) \
    .setOutputCol(&quot;table_image&quot;) \
    .setDropCols(&quot;image&quot;) \
    .setImageType(ImageType.TYPE_BYTE_GRAY) \
    .setExplodeCols([])

text_detector = ImageTextDetectorV2.pretrained(&quot;image_text_detector_v2&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) \
    .setInputCol(&quot;image&quot;) \
    .setOutputCol(&quot;text_regions&quot;) \
    .setWithRefiner(True)

draw_regions = ImageDrawRegions() \
    .setInputCol(&quot;image&quot;) \
    .setInputRegionsCol(&quot;text_regions&quot;) \
    .setOutputCol(&quot;image_with_regions&quot;) \
    .setRectColor(Color.green) \
    .setRotated(True)

img_to_hocr = ImageToTextV2().pretrained(&quot;ocr_small_printed&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) \
    .setInputCols([&quot;image&quot;, &quot;text_regions&quot;]) \
    .setUsePandasUdf(False) \
    .setOutputFormat(OcrOutputFormat.HOCR) \
    .setOutputCol(&quot;hocr&quot;) \
    .setGroupImages(False)

hocr_to_table = HocrToTextTable() \
    .setInputCol(&quot;hocr&quot;) \
    .setRegionCol(&quot;table_regions&quot;) \
    .setOutputCol(&quot;tables&quot;)

pipeline = PipelineModel(stages=[
    binary_to_image,
    table_detector,
    splitter,
    text_detector,
    draw_regions,
    img_to_hocr,
    hocr_to_table
])

imagePath = &quot;data/tab_images_hocr_1/table4_1.jpg&quot;
image_df= spark.read.format(&quot;binaryFile&quot;).load(imagePath)

result = pipeline.transform(image_df).cache()
```
```scala
val binary_to_image = new BinaryToImage() 
    .setInputCol(&quot;content&quot;)  
    .setOutputCol(&quot;image&quot;) 

val table_detector = new ImageTableDetector
    .pretrained(&quot;general_model_table_detection_v2&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) 
    .setInputCol(&quot;image&quot;) 
    .setOutputCol(&quot;table_regions&quot;)

val splitter = new ImageSplitRegions() 
    .setInputCol(&quot;image&quot;) 
    .setInputRegionsCol(&quot;table_regions&quot;) 
    .setOutputCol(&quot;table_image&quot;) 
    .setDropCols(&quot;image&quot;) 
    .setImageType(ImageType.TYPE_BYTE_GRAY) 
    .setExplodeCols(Array())

val text_detector = new ImageTextDetectorV2
    .pretrained(&quot;image_text_detector_v2&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) 
    .setInputCol(&quot;image&quot;) 
    .setOutputCol(&quot;text_regions&quot;) 
    .setWithRefiner(True)

val draw_regions = new ImageDrawRegions() 
    .setInputCol(&quot;image&quot;) 
    .setInputRegionsCol(&quot;text_regions&quot;) 
    .setOutputCol(&quot;image_with_regions&quot;) 
    .setRectColor(Color.green) 
    .setRotated(True)

img_to_hocr = ImageToTextV2()
    .pretrained(&quot;ocr_small_printed&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) 
    .setInputCols(Array(&quot;image&quot;, &quot;text_regions&quot;)) 
    .setUsePandasUdf(False) 
    .setOutputFormat(OcrOutputFormat.HOCR) 
    .setOutputCol(&quot;hocr&quot;) 
    .setGroupImages(False)

val hocr_to_table = new HocrToTextTable() 
    .setInputCol(&quot;hocr&quot;) 
    .setRegionCol(&quot;table_regions&quot;) 
    .setOutputCol(&quot;tables&quot;)

val pipeline = new PipelineModel().setStages(Array(
    binary_to_image, 
    table_detector, 
    splitter, 
    text_detector, 
    draw_regions, 
    img_to_hocr, 
    hocr_to_table))

val imagePath = &quot;data/tab_images_hocr_1/table4_1.jpg&quot;
val image_df= spark.read.format(&quot;binaryFile&quot;).load(imagePath)

val result = pipeline.transform(image_df).cache()
```
&lt;/div&gt;

## Example

### Input:
![Screenshot](../../_examples_ocr/image13.png)

### Output:
![Screenshot](../../_examples_ocr/image13_out.png)

```bash
text_regions	table_image	pagenum	modificationTime	path	table_regions	length	image	image_with_regions	hocr	tables	exception	table_index
[{0, 0, 566.32025...	{file:/content/ta...	0	2023-01-23 08:21:...	file:/content/tab...	{0, 0, 40.0, 0.0,...	172124	{file:/content/ta...	{file:/content/ta...	&lt;?xml version=&quot;1....	{0, 0, 0.0, 0.0,...	null	0
```

```bash
Filename: table4_1.jpg
Page:     0
Table:    0
4
col0	col1	col2	col3
0		MATERIAL	LABOR	TOTAL
1	SURFACE FACILITIES	None	None	None
2	BUILDINGS AND STRUCTURES	29,380	33,640	63,020
3	MAJOR EQUIPMENT	46,350	4,570	50,920
4	BULK MATERIAL	29,040	16,410	45,450
5	SITE DEVELOPMENT	7,570	4,730	12,300
6	SHAFTS AND HOISTS	None	None	None
7	MAJOR EQUIPMENT	24,500	8,300	32,800
8	SHAFTS AND LINING	58,100	31,400	89,500
9	UNDERGROUND FACILITIES	None	None	None
10	EXCAVATIONS AND STRUCTURES	2,510	4,510	7,020
11	MAJOR EQUIPMENT	3,170	220	3,390
12	BULK MATERIAL	1,960	1,470	3,430
13	MINING	None	None	None
14	MAJOR EQUIPMENT	64,700		64,700
15	MINE CONSTRUCTION	582,330	655,640	1,237,970
16	BACKFULLING	None	None	None
17	MINE BACKFILLING	102,300	116,000	218,300
18	SHAFT SEALING	90	710	200
19	TOTAL FIELD COSTS	952.000	877.000	1,829,000
20	ARCHITECT-ENGINEER SERVICES			53,000
21	OWNER'S COSTS			218,000
22	CONTINGENCY			534.0001
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="licensed" /><summary type="html">Description Table structure recognition based on hocr with Tesseract architecture. Tesseract has been trained on a variety of datasets to improve its recognition capabilities. These datasets include images of text in various languages and scripts, as well as images with different font styles, sizes, and orientations. The training process involves feeding the engine with a large number of images and their corresponding text, allowing the engine to learn the patterns and characteristics of different text styles. One of the most important datasets used in training Tesseract is the UNLV dataset, which contains over 400,000 images of text in different languages, scripts, and font styles. This dataset is widely used in the OCR community and has been instrumental in improving the accuracy of Tesseract. Other datasets that have been used in training Tesseract include the ICDAR dataset, the IIIT-HWS dataset, and the RRC-GV-WS dataset. In addition to these datasets, Tesseract also uses a technique called adaptive training, where the engine can continuously improve its recognition capabilities by learning from new images and text. This allows Tesseract to adapt to new text styles and languages, and improve its overall accuracy. Predicted Entities Live Demo Open in Colab Download How to use PythonScalaNLU binary_to_image = BinaryToImage() \ .setInputCol(&quot;content&quot;) \ .setOutputCol(&quot;image&quot;) table_detector = ImageTableDetector.pretrained(&quot;general_model_table_detection_v2&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) \ .setInputCol(&quot;image&quot;) \ .setOutputCol(&quot;table_regions&quot;) splitter = ImageSplitRegions() \ .setInputCol(&quot;image&quot;) \ .setInputRegionsCol(&quot;table_regions&quot;) \ .setOutputCol(&quot;table_image&quot;) \ .setDropCols(&quot;image&quot;) \ .setImageType(ImageType.TYPE_BYTE_GRAY) \ .setExplodeCols([]) text_detector = ImageTextDetectorV2.pretrained(&quot;image_text_detector_v2&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) \ .setInputCol(&quot;image&quot;) \ .setOutputCol(&quot;text_regions&quot;) \ .setWithRefiner(True) draw_regions = ImageDrawRegions() \ .setInputCol(&quot;image&quot;) \ .setInputRegionsCol(&quot;text_regions&quot;) \ .setOutputCol(&quot;image_with_regions&quot;) \ .setRectColor(Color.green) \ .setRotated(True) img_to_hocr = ImageToTextV2().pretrained(&quot;ocr_small_printed&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) \ .setInputCols([&quot;image&quot;, &quot;text_regions&quot;]) \ .setUsePandasUdf(False) \ .setOutputFormat(OcrOutputFormat.HOCR) \ .setOutputCol(&quot;hocr&quot;) \ .setGroupImages(False) hocr_to_table = HocrToTextTable() \ .setInputCol(&quot;hocr&quot;) \ .setRegionCol(&quot;table_regions&quot;) \ .setOutputCol(&quot;tables&quot;) pipeline = PipelineModel(stages=[ binary_to_image, table_detector, splitter, text_detector, draw_regions, img_to_hocr, hocr_to_table ]) imagePath = &quot;data/tab_images_hocr_1/table4_1.jpg&quot; image_df= spark.read.format(&quot;binaryFile&quot;).load(imagePath) result = pipeline.transform(image_df).cache() val binary_to_image = new BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) val table_detector = new ImageTableDetector .pretrained(&quot;general_model_table_detection_v2&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setInputCol(&quot;image&quot;) .setOutputCol(&quot;table_regions&quot;) val splitter = new ImageSplitRegions() .setInputCol(&quot;image&quot;) .setInputRegionsCol(&quot;table_regions&quot;) .setOutputCol(&quot;table_image&quot;) .setDropCols(&quot;image&quot;) .setImageType(ImageType.TYPE_BYTE_GRAY) .setExplodeCols(Array()) val text_detector = new ImageTextDetectorV2 .pretrained(&quot;image_text_detector_v2&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text_regions&quot;) .setWithRefiner(True) val draw_regions = new ImageDrawRegions() .setInputCol(&quot;image&quot;) .setInputRegionsCol(&quot;text_regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) .setRectColor(Color.green) .setRotated(True) img_to_hocr = ImageToTextV2() .pretrained(&quot;ocr_small_printed&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setInputCols(Array(&quot;image&quot;, &quot;text_regions&quot;)) .setUsePandasUdf(False) .setOutputFormat(OcrOutputFormat.HOCR) .setOutputCol(&quot;hocr&quot;) .setGroupImages(False) val hocr_to_table = new HocrToTextTable() .setInputCol(&quot;hocr&quot;) .setRegionCol(&quot;table_regions&quot;) .setOutputCol(&quot;tables&quot;) val pipeline = new PipelineModel().setStages(Array( binary_to_image, table_detector, splitter, text_detector, draw_regions, img_to_hocr, hocr_to_table)) val imagePath = &quot;data/tab_images_hocr_1/table4_1.jpg&quot; val image_df= spark.read.format(&quot;binaryFile&quot;).load(imagePath) val result = pipeline.transform(image_df).cache() Example Input: Output: text_regions table_image pagenum modificationTime path table_regions length image image_with_regions hocr tables exception table_index [{0, 0, 566.32025... {file:/content/ta... 0 2023-01-23 08:21:... file:/content/tab... {0, 0, 40.0, 0.0,... 172124 {file:/content/ta... {file:/content/ta... &amp;lt;?xml version=&quot;1.... {0, 0, 0.0, 0.0,... null 0 Filename: table4_1.jpg Page: 0 Table: 0 4 col0 col1 col2 col3 0 MATERIAL LABOR TOTAL 1 SURFACE FACILITIES None None None 2 BUILDINGS AND STRUCTURES 29,380 33,640 63,020 3 MAJOR EQUIPMENT 46,350 4,570 50,920 4 BULK MATERIAL 29,040 16,410 45,450 5 SITE DEVELOPMENT 7,570 4,730 12,300 6 SHAFTS AND HOISTS None None None 7 MAJOR EQUIPMENT 24,500 8,300 32,800 8 SHAFTS AND LINING 58,100 31,400 89,500 9 UNDERGROUND FACILITIES None None None 10 EXCAVATIONS AND STRUCTURES 2,510 4,510 7,020 11 MAJOR EQUIPMENT 3,170 220 3,390 12 BULK MATERIAL 1,960 1,470 3,430 13 MINING None None None 14 MAJOR EQUIPMENT 64,700 64,700 15 MINE CONSTRUCTION 582,330 655,640 1,237,970 16 BACKFULLING None None None 17 MINE BACKFILLING 102,300 116,000 218,300 18 SHAFT SEALING 90 710 200 19 TOTAL FIELD COSTS 952.000 877.000 1,829,000 20 ARCHITECT-ENGINEER SERVICES 53,000 21 OWNER'S COSTS 218,000 22 CONTINGENCY 534.0001</summary></entry><entry><title type="html">Hocr for table recognition pdf</title><link href="/2023/01/23/hocr_table_recognition_pdf_en_3_2.html" rel="alternate" type="text/html" title="Hocr for table recognition pdf" /><published>2023-01-23T00:00:00+00:00</published><updated>2023-01-23T00:00:00+00:00</updated><id>/2023/01/23/hocr_table_recognition_pdf_en_3_2</id><content type="html" xml:base="/2023/01/23/hocr_table_recognition_pdf_en_3_2.html">## Description

Table structure recognition based on hocr with Tesseract architecture, for PDF documents. 

Tesseract has been trained on a variety of datasets to improve its recognition capabilities. These datasets include images of text in various languages and scripts, as well as images with different font styles, sizes, and orientations. The training process involves feeding the engine with a large number of images and their corresponding text, allowing the engine to learn the patterns and characteristics of different text styles. One of the most important datasets used in training Tesseract is the UNLV dataset, which contains over 400,000 images of text in different languages, scripts, and font styles. This dataset is widely used in the OCR community and has been instrumental in improving the accuracy of Tesseract. Other datasets that have been used in training Tesseract include the ICDAR dataset, the IIIT-HWS dataset, and the RRC-GV-WS dataset.

In addition to these datasets, Tesseract also uses a technique called adaptive training, where the engine can continuously improve its recognition capabilities by learning from new images and text. This allows Tesseract to adapt to new text styles and languages, and improve its overall accuracy.


## Predicted Entities

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
[Open in Colab](https://github.com/JohnSnowLabs/spark-ocr-workshop/tree/master/jupyter/Cards/SparkOCRPdfToTable.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Download&lt;/button&gt;


## How to use

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
pdf_to_hocr = PdfToHocr() \
        .setInputCol(&quot;content&quot;) \
        .setOutputCol(&quot;hocr&quot;)

tokenizer = HocrTokenizer() \
        .setInputCol(&quot;hocr&quot;) \
        .setOutputCol(&quot;token&quot;) \

pdf_to_image = PdfToImage() \
        .setInputCol(&quot;content&quot;) \
        .setOutputCol(&quot;image&quot;) \
        .setPageNumCol(&quot;tmp_pagenum&quot;) \
        .setImageType(ImageType.TYPE_3BYTE_BGR)

table_detector = ImageTableDetector \
        .pretrained(&quot;general_model_table_detection_v2&quot;, &quot;en&quot;, &quot;public/ocr/models&quot;) \
        .setInputCol(&quot;image&quot;) \
        .setOutputCol(&quot;table_regions&quot;) \
        .setScoreThreshold(0.9) \
        .setApplyCorrection(True) \
        .setScaleWidthToCol(&quot;width_dimension&quot;) \
        .setScaleHeightToCol(&quot;height_dimension&quot;)

image_scaler = ImageScaler() \
        .setWidthCol(&quot;width_dimension&quot;) \
        .setHeightCol(&quot;height_dimension&quot;)

hocr_to_table = HocrToTextTable() \
        .setInputCol(&quot;hocr&quot;) \
        .setRegionCol(&quot;table_regions&quot;) \
        .setOutputCol(&quot;tables&quot;)

draw_annotations = ImageDrawAnnotations() \
        .setInputCol(&quot;scaled_image&quot;) \
        .setInputChunksCol(&quot;tables&quot;) \
        .setOutputCol(&quot;image_with_annotations&quot;) \
        .setFilledRect(False) \
        .setFontSize(5) \
        .setRectColor(Color.red)

draw_regions = ImageDrawRegions() \
        .setInputCol(&quot;scaled_image&quot;) \
        .setInputRegionsCol(&quot;table_regions&quot;) \
        .setOutputCol(&quot;image_with_regions&quot;) \
        .setRectColor(Color.red)

pipeline1 = PipelineModel(stages=[
        pdf_to_hocr,
        tokenizer,
        pdf_to_image,
        table_detector,
        image_scaler,
        draw_regions,
        hocr_to_table
])

test_image_path = &quot;data/pdfs/f1120.pdf&quot;
bin_df = spark.read.format(&quot;binaryFile&quot;).load(test_image_path)

result = pipeline1.transform(bin_df).cache().drop(&quot;tmp_pagenum&quot;)
result = result.filter(result.pagenum == 1)
```
```scala
val pdf_to_hocr = new PdfToHocr() 
        .setInputCol(&quot;content&quot;) 
        .setOutputCol(&quot;hocr&quot;)

val tokenizer = new HocrTokenizer() 
        .setInputCol(&quot;hocr&quot;) 
        .setOutputCol(&quot;token&quot;) 

val pdf_to_image = new PdfToImage() 
        .setInputCol(&quot;content&quot;) 
        .setOutputCol(&quot;image&quot;) 
        .setPageNumCol(&quot;tmp_pagenum&quot;) 
        .setImageType(ImageType.TYPE_3BYTE_BGR)

val table_detector = ImageTableDetector 
        .pretrained(&quot;general_model_table_detection_v2&quot;, &quot;en&quot;, &quot;public/ocr/models&quot;) 
        .setInputCol(&quot;image&quot;) 
        .setOutputCol(&quot;table_regions&quot;) 
        .setScoreThreshold(0.9) 
        .setApplyCorrection(True) 
        .setScaleWidthToCol(&quot;width_dimension&quot;) 
        .setScaleHeightToCol(&quot;height_dimension&quot;)

val image_scaler = new ImageScaler() 
        .setWidthCol(&quot;width_dimension&quot;) 
        .setHeightCol(&quot;height_dimension&quot;)

val hocr_to_table = new HocrToTextTable() 
        .setInputCol(&quot;hocr&quot;) 
        .setRegionCol(&quot;table_regions&quot;) 
        .setOutputCol(&quot;tables&quot;)

val draw_annotations = new ImageDrawAnnotations() 
        .setInputCol(&quot;scaled_image&quot;) 
        .setInputChunksCol(&quot;tables&quot;) 
        .setOutputCol(&quot;image_with_annotations&quot;) 
        .setFilledRect(False) 
        .setFontSize(5) 
        .setRectColor(Color.red)

val draw_regions = new ImageDrawRegions() 
        .setInputCol(&quot;scaled_image&quot;) 
        .setInputRegionsCol(&quot;table_regions&quot;) 
        .setOutputCol(&quot;image_with_regions&quot;) 
        .setRectColor(Color.red)

val pipeline1 = new PipelineModel().setStages(Array(
         pdf_to_hocr, 
         tokenizer, 
         pdf_to_image, 
         table_detector, 
         image_scaler, 
         draw_regions, 
         hocr_to_table))
        
val test_image_path = &quot;data/pdfs/f1120.pdf&quot;
val bin_df = spark.read.format(&quot;binaryFile&quot;).load(test_image_path)

val result = pipeline1.transform(bin_df).cache().drop(&quot;tmp_pagenum&quot;)
result = result.filter(result.pagenum == 1)
```
&lt;/div&gt;

## Example

### Input:
![Screenshot](../../_examples_ocr/image14.png)

### Output:
![Screenshot](../../_examples_ocr/image14_out.png)

```bash
path	modificationTime	length	hocr	height_dimension	width_dimension	pagenum	token	image	total_pages	documentnum	table_regions	scaled_image	image_with_regions	tables	exception	table_index
file:/content/f11...	2023-01-23 08:16:...	3471478	&lt;div title=&quot;bbox ...	791	611	1	[{token, 0, 2, 23...	{file:/content/f1...	1	0	{0, 0, 32.839153,...	{file:/content/f1...	{file:/content/f1...	{0, 0, 0.0, 0.0,...	null	0
```

```bash
Filename: f1120.pdf
Page:     1
Table:    0
5
col0	col1	col2	col3	col4
0	Schedule C	Dividends instructions , ) Inclusions , and Sp...	( a ) Dividends inclusions and	( b ) %	( c ) Special ( a ) × deductions ( b )
1	1	Dividends from less - than - 20 % - owned dome...	234	50	None
2	2	Dividends from 20 % - or - more - owned domest...	324123	65	None
3	3	Dividends on certain debt - financed stock of ...	324	instructions see	None
4	4	Dividends on certain preferred stock of less -...	234	23 . 3	None
5	5	Dividends on certain preferred stock of 20 % -...	42134	26 . 7	None
6	6	Dividends from less - than - 20 % - owned fore...	4234	50	None
7	7	Dividends from 20 % - or - more - owned foreig...	4234	65	None
8	8	Dividends from wholly owned foreign subsidiaries	42348987	100	None
9	9	Subtotal . Add lines 1 through 8 . See instruc...	987	instructions see	None
10	10	Dividends from domestic corporations received ...	9786	100	None
11	11	Dividends from affiliated group members .	789	100	None
12	12	Dividends from certain FSCs	0.00	100	None
13	13	Foreign - source portion of dividends received...	421.34	100	None
14	14	Dividends from foreign corporations not includ...	2341.23	None	None
15	15	Section 965 ( a ) inclusion .	1234.14	instructions see	None
16	16a	Subpart F inclusions derived from the sale by ...	46.54	100	None
17	b	Subpart F inclusions derived from hybrid divid...	6453.65	None	None
18	c	Other ( attach inclusions Form ( s ) 5471 from...	985.76	None	None
19	17	Global Intangible Low - Taxed Income ( GILTI )...	23.41	None	None
20	18	Gross - up for foreign taxes deemed paid	1.01	None	None
21	19	IC - DISC and former DISC dividends not includ...	3123.91	None	None
22	20	Other dividends	12.23	None	None
23	21	Deduction for dividends paid on certain prefer...			1.23
24	22	Section 250 deduction ( attach Form 8993 )			3.41
25	23	Total dividends and inclusions . Add column ( ...	2341.23	None	None
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="licensed" /><summary type="html">Description Table structure recognition based on hocr with Tesseract architecture, for PDF documents. Tesseract has been trained on a variety of datasets to improve its recognition capabilities. These datasets include images of text in various languages and scripts, as well as images with different font styles, sizes, and orientations. The training process involves feeding the engine with a large number of images and their corresponding text, allowing the engine to learn the patterns and characteristics of different text styles. One of the most important datasets used in training Tesseract is the UNLV dataset, which contains over 400,000 images of text in different languages, scripts, and font styles. This dataset is widely used in the OCR community and has been instrumental in improving the accuracy of Tesseract. Other datasets that have been used in training Tesseract include the ICDAR dataset, the IIIT-HWS dataset, and the RRC-GV-WS dataset. In addition to these datasets, Tesseract also uses a technique called adaptive training, where the engine can continuously improve its recognition capabilities by learning from new images and text. This allows Tesseract to adapt to new text styles and languages, and improve its overall accuracy. Predicted Entities Live Demo Open in Colab Download How to use PythonScalaNLU pdf_to_hocr = PdfToHocr() \ .setInputCol(&quot;content&quot;) \ .setOutputCol(&quot;hocr&quot;) tokenizer = HocrTokenizer() \ .setInputCol(&quot;hocr&quot;) \ .setOutputCol(&quot;token&quot;) \ pdf_to_image = PdfToImage() \ .setInputCol(&quot;content&quot;) \ .setOutputCol(&quot;image&quot;) \ .setPageNumCol(&quot;tmp_pagenum&quot;) \ .setImageType(ImageType.TYPE_3BYTE_BGR) table_detector = ImageTableDetector \ .pretrained(&quot;general_model_table_detection_v2&quot;, &quot;en&quot;, &quot;public/ocr/models&quot;) \ .setInputCol(&quot;image&quot;) \ .setOutputCol(&quot;table_regions&quot;) \ .setScoreThreshold(0.9) \ .setApplyCorrection(True) \ .setScaleWidthToCol(&quot;width_dimension&quot;) \ .setScaleHeightToCol(&quot;height_dimension&quot;) image_scaler = ImageScaler() \ .setWidthCol(&quot;width_dimension&quot;) \ .setHeightCol(&quot;height_dimension&quot;) hocr_to_table = HocrToTextTable() \ .setInputCol(&quot;hocr&quot;) \ .setRegionCol(&quot;table_regions&quot;) \ .setOutputCol(&quot;tables&quot;) draw_annotations = ImageDrawAnnotations() \ .setInputCol(&quot;scaled_image&quot;) \ .setInputChunksCol(&quot;tables&quot;) \ .setOutputCol(&quot;image_with_annotations&quot;) \ .setFilledRect(False) \ .setFontSize(5) \ .setRectColor(Color.red) draw_regions = ImageDrawRegions() \ .setInputCol(&quot;scaled_image&quot;) \ .setInputRegionsCol(&quot;table_regions&quot;) \ .setOutputCol(&quot;image_with_regions&quot;) \ .setRectColor(Color.red) pipeline1 = PipelineModel(stages=[ pdf_to_hocr, tokenizer, pdf_to_image, table_detector, image_scaler, draw_regions, hocr_to_table ]) test_image_path = &quot;data/pdfs/f1120.pdf&quot; bin_df = spark.read.format(&quot;binaryFile&quot;).load(test_image_path) result = pipeline1.transform(bin_df).cache().drop(&quot;tmp_pagenum&quot;) result = result.filter(result.pagenum == 1) val pdf_to_hocr = new PdfToHocr() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;hocr&quot;) val tokenizer = new HocrTokenizer() .setInputCol(&quot;hocr&quot;) .setOutputCol(&quot;token&quot;) val pdf_to_image = new PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setPageNumCol(&quot;tmp_pagenum&quot;) .setImageType(ImageType.TYPE_3BYTE_BGR) val table_detector = ImageTableDetector .pretrained(&quot;general_model_table_detection_v2&quot;, &quot;en&quot;, &quot;public/ocr/models&quot;) .setInputCol(&quot;image&quot;) .setOutputCol(&quot;table_regions&quot;) .setScoreThreshold(0.9) .setApplyCorrection(True) .setScaleWidthToCol(&quot;width_dimension&quot;) .setScaleHeightToCol(&quot;height_dimension&quot;) val image_scaler = new ImageScaler() .setWidthCol(&quot;width_dimension&quot;) .setHeightCol(&quot;height_dimension&quot;) val hocr_to_table = new HocrToTextTable() .setInputCol(&quot;hocr&quot;) .setRegionCol(&quot;table_regions&quot;) .setOutputCol(&quot;tables&quot;) val draw_annotations = new ImageDrawAnnotations() .setInputCol(&quot;scaled_image&quot;) .setInputChunksCol(&quot;tables&quot;) .setOutputCol(&quot;image_with_annotations&quot;) .setFilledRect(False) .setFontSize(5) .setRectColor(Color.red) val draw_regions = new ImageDrawRegions() .setInputCol(&quot;scaled_image&quot;) .setInputRegionsCol(&quot;table_regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) .setRectColor(Color.red) val pipeline1 = new PipelineModel().setStages(Array( pdf_to_hocr, tokenizer, pdf_to_image, table_detector, image_scaler, draw_regions, hocr_to_table)) val test_image_path = &quot;data/pdfs/f1120.pdf&quot; val bin_df = spark.read.format(&quot;binaryFile&quot;).load(test_image_path) val result = pipeline1.transform(bin_df).cache().drop(&quot;tmp_pagenum&quot;) result = result.filter(result.pagenum == 1) Example Input: Output: path modificationTime length hocr height_dimension width_dimension pagenum token image total_pages documentnum table_regions scaled_image image_with_regions tables exception table_index file:/content/f11... 2023-01-23 08:16:... 3471478 &amp;lt;div title=&quot;bbox ... 791 611 1 [{token, 0, 2, 23... {file:/content/f1... 1 0 {0, 0, 32.839153,... {file:/content/f1... {file:/content/f1... {0, 0, 0.0, 0.0,... null 0 Filename: f1120.pdf Page: 1 Table: 0 5 col0 col1 col2 col3 col4 0 Schedule C Dividends instructions , ) Inclusions , and Sp... ( a ) Dividends inclusions and ( b ) % ( c ) Special ( a ) × deductions ( b ) 1 1 Dividends from less - than - 20 % - owned dome... 234 50 None 2 2 Dividends from 20 % - or - more - owned domest... 324123 65 None 3 3 Dividends on certain debt - financed stock of ... 324 instructions see None 4 4 Dividends on certain preferred stock of less -... 234 23 . 3 None 5 5 Dividends on certain preferred stock of 20 % -... 42134 26 . 7 None 6 6 Dividends from less - than - 20 % - owned fore... 4234 50 None 7 7 Dividends from 20 % - or - more - owned foreig... 4234 65 None 8 8 Dividends from wholly owned foreign subsidiaries 42348987 100 None 9 9 Subtotal . Add lines 1 through 8 . See instruc... 987 instructions see None 10 10 Dividends from domestic corporations received ... 9786 100 None 11 11 Dividends from affiliated group members . 789 100 None 12 12 Dividends from certain FSCs 0.00 100 None 13 13 Foreign - source portion of dividends received... 421.34 100 None 14 14 Dividends from foreign corporations not includ... 2341.23 None None 15 15 Section 965 ( a ) inclusion . 1234.14 instructions see None 16 16a Subpart F inclusions derived from the sale by ... 46.54 100 None 17 b Subpart F inclusions derived from hybrid divid... 6453.65 None None 18 c Other ( attach inclusions Form ( s ) 5471 from... 985.76 None None 19 17 Global Intangible Low - Taxed Income ( GILTI )... 23.41 None None 20 18 Gross - up for foreign taxes deemed paid 1.01 None None 21 19 IC - DISC and former DISC dividends not includ... 3123.91 None None 22 20 Other dividends 12.23 None None 23 21 Deduction for dividends paid on certain prefer... 1.23 24 22 Section 250 deduction ( attach Form 8993 ) 3.41 25 23 Total dividends and inclusions . Add column ( ... 2341.23 None None</summary></entry><entry><title type="html">Legal NER (Parties, Dates, Alias, Former names, Document Type - lg)</title><link href="/2023/01/21/legner_contract_doc_parties_lg_en.html" rel="alternate" type="text/html" title="Legal NER (Parties, Dates, Alias, Former names, Document Type - lg)" /><published>2023-01-21T00:00:00+00:00</published><updated>2023-01-21T00:00:00+00:00</updated><id>/2023/01/21/legner_contract_doc_parties_lg_en</id><content type="html" xml:base="/2023/01/21/legner_contract_doc_parties_lg_en.html">## Description

MPORTANT: Don't run this model on the whole legal agreement. Instead:
- Split by paragraphs. You can use [notebook 1](https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/tutorials/Certification_Trainings_JSL) in Finance or Legal as inspiration;
- Use the `legclf_introduction_clause` Text Classifier to select only these paragraphs; 

This is a Legal NER Model, aimed to process the first page of the agreements when information can be found about:
- Parties of the contract/agreement;
- Their former names;
- Aliases of those parties, or how those parties will be called further on in the document;
- Document Type;
- Effective Date of the agreement;
- Other organizations;

This model can be used all along with its Relation Extraction model to retrieve the relations between these entities, called `legre_contract_doc_parties`

## Predicted Entities

`PARTY`, `EFFDATE`, `DOC`, `ALIAS`, `ORG`, `FORMER_NAME`

{:.btn-box}
[Live Demo](https://demo.johnsnowlabs.com/finance/LEGALNER_PARTIES/){:.button.button-orange}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/legal/models/legner_contract_doc_parties_lg_en_1.0.0_3.0_1674321394808.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/legal/models/legner_contract_doc_parties_lg_en_1.0.0_3.0_1674321394808.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
documentAssembler = nlp.DocumentAssembler()\
        .setInputCol(&quot;text&quot;)\
        .setOutputCol(&quot;document&quot;)
        
sentenceDetector = nlp.SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;)\
        .setInputCols([&quot;document&quot;])\
        .setOutputCol(&quot;sentence&quot;)

tokenizer = nlp.Tokenizer()\
        .setInputCols([&quot;sentence&quot;])\
        .setOutputCol(&quot;token&quot;)

embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;, &quot;en&quot;) \
        .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
        .setOutputCol(&quot;embeddings&quot;)\

ner_model = legal.NerModel.pretrained('legner_contract_doc_parties_lg', 'en', 'legal/models')\
        .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;])\
        .setOutputCol(&quot;ner&quot;)

ner_converter = nlp.NerConverter()\
        .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;])\
        .setOutputCol(&quot;ner_chunk&quot;)

nlpPipeline = nlp.Pipeline(stages=[
        documentAssembler,
        sentenceDetector,
        tokenizer,
        embeddings,
        ner_model,
        ner_converter])

empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)

model = nlpPipeline.fit(empty_data)

text = [&quot;&quot;&quot;
INTELLECTUAL PROPERTY AGREEMENT

This INTELLECTUAL PROPERTY AGREEMENT (this &quot;Agreement&quot;), dated as of December 31, 2018 (the &quot;Effective Date&quot;) is entered into by and between Armstrong Flooring, Inc., a Delaware corporation (&quot;Seller&quot;) and AFI Licensing LLC, a Delaware limited liability company (&quot;Licensing&quot; and together with Seller, &quot;Arizona&quot;) and AHF Holding, Inc. (formerly known as Tarzan HoldCo, Inc.), a Delaware corporation (&quot;Buyer&quot;) and Armstrong Hardwood Flooring Company, a Tennessee corporation (the &quot;Company&quot; and together with Buyer the &quot;Buyer Entities&quot;) (each of Arizona on the one hand and the Buyer Entities on the other hand, a &quot;Party&quot; and collectively, the &quot;Parties&quot;).
&quot;&quot;&quot;]

res = model.transform(spark.createDataFrame([text]).toDF(&quot;text&quot;))
```

&lt;/div&gt;

## Results

```bash
+------------+---------+
|       token|ner_label|
+------------+---------+
|INTELLECTUAL|    B-DOC|
|    PROPERTY|    I-DOC|
|   AGREEMENT|    I-DOC|
|        This|        O|
|INTELLECTUAL|    B-DOC|
|    PROPERTY|    I-DOC|
|   AGREEMENT|    I-DOC|
|           (|        O|
|        this|        O|
|           &quot;|        O|
|   Agreement|        O|
|         &quot;),|        O|
|       dated|        O|
|          as|        O|
|          of|        O|
|    December|B-EFFDATE|
|          31|I-EFFDATE|
|           ,|I-EFFDATE|
|        2018|I-EFFDATE|
|           (|        O|
|         the|        O|
|           &quot;|        O|
|   Effective|        O|
|        Date|        O|
|          &quot;)|        O|
|          is|        O|
|     entered|        O|
|        into|        O|
|          by|        O|
|         and|        O|
|     between|        O|
|   Armstrong|  B-PARTY|
|    Flooring|  I-PARTY|
|           ,|  I-PARTY|
|         Inc|  I-PARTY|
|          .,|        O|
|           a|        O|
|    Delaware|        O|
| corporation|        O|
|          (&quot;|        O|
|      Seller|  B-ALIAS|
|          &quot;)|        O|
|         and|        O|
|         AFI|  B-PARTY|
|   Licensing|  I-PARTY|
|         LLC|  I-PARTY|
|           ,|        O|
|           a|        O|
|    Delaware|        O|
|     limited|        O|
|   liability|        O|
|     company|        O|
|          (&quot;|        O|
|   Licensing|  B-ALIAS|
|           &quot;|        O|
|         and|        O|
|    together|        O|
|        with|        O|
|      Seller|  B-ALIAS|
|           ,|        O|
|           &quot;|        O|
|     Arizona|  B-ALIAS|
|          &quot;)|        O|
|         and|        O|
|         AHF|  B-PARTY|
|     Holding|  I-PARTY|
|           ,|  I-PARTY|
|         Inc|  I-PARTY|
|           .|        O|
|           (|        O|
|    formerly|        O|
|       known|        O|
|          as|        O|
|      Tarzan|        O|
|      HoldCo|        O|
|           ,|        O|
|         Inc|        O|
|         .),|        O|
|           a|        O|
|    Delaware|        O|
| corporation|        O|
|          (&quot;|        O|
|       Buyer|  B-ALIAS|
|          &quot;)|        O|
|         and|        O|
|   Armstrong|  B-PARTY|
|    Hardwood|  I-PARTY|
|    Flooring|  I-PARTY|
|     Company|  I-PARTY|
------------------------
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|legner_contract_doc_parties_lg|
|Compatibility:|Legal NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[sentence, token, embeddings]|
|Output Labels:|[ner]|
|Language:|en|
|Size:|16.3 MB|

## References

Manual annotations on CUAD dataset

## Benchmarking

```bash
label	 tp	 fp	 fn	 prec	 rec	 f1
I-PARTY	 513	 58	 85	 0.8984238	 0.85785955	 0.87767327
B-EFFDATE	 57	 5	 7	 0.91935486	 0.890625	 0.90476185
I-ORG	 208	 32	 49	 0.8666667	 0.8093385	 0.8370222
B-DOC	 80	 9	 21	 0.8988764	 0.7920792	 0.8421053
B-FORMER_NAME	 5	 0	 0	 1.0	 1.0	 1.0
I-EFFDATE	 214	 11	 6	 0.95111114	 0.9727273	 0.96179783
I-FORMER_NAME	 7	 1	 0	 0.875	 1.0	 0.93333334
I-ALIAS	 14	 5	 13	 0.7368421	 0.5185185	 0.6086956
I-DOC	 166	 16	 52	 0.9120879	 0.7614679	 0.83
B-ORG	 131	 28	 42	 0.8238994	 0.75722545	 0.7891567
B-PARTY	 174	 40	 57	 0.8130841	 0.7532467	 0.7820225
B-ALIAS	 170	 17	 13	 0.90909094	 0.92896175	 0.9189189
Macro-average	 1739 222 345 0.8837032 0.8368375 0.859632
Micro-average	 1739 222 345 0.8867925 0.834453 0.859827
```</content><author><name>John Snow Labs</name></author><category term="document" /><category term="contract" /><category term="agreement" /><category term="type" /><category term="parties" /><category term="aliases" /><category term="former" /><category term="names" /><category term="effective" /><category term="dates" /><category term="en" /><category term="licensed" /><summary type="html">Description MPORTANT: Don’t run this model on the whole legal agreement. Instead: Split by paragraphs. You can use notebook 1 in Finance or Legal as inspiration; Use the legclf_introduction_clause Text Classifier to select only these paragraphs; This is a Legal NER Model, aimed to process the first page of the agreements when information can be found about: Parties of the contract/agreement; Their former names; Aliases of those parties, or how those parties will be called further on in the document; Document Type; Effective Date of the agreement; Other organizations; This model can be used all along with its Relation Extraction model to retrieve the relations between these entities, called legre_contract_doc_parties Predicted Entities PARTY, EFFDATE, DOC, ALIAS, ORG, FORMER_NAME Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU documentAssembler = nlp.DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;)\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer()\ .setInputCols([&quot;sentence&quot;])\ .setOutputCol(&quot;token&quot;) embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;)\ ner_model = legal.NerModel.pretrained('legner_contract_doc_parties_lg', 'en', 'legal/models')\ .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;])\ .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter()\ .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;])\ .setOutputCol(&quot;ner_chunk&quot;) nlpPipeline = nlp.Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, ner_converter]) empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) model = nlpPipeline.fit(empty_data) text = [&quot;&quot;&quot; INTELLECTUAL PROPERTY AGREEMENT This INTELLECTUAL PROPERTY AGREEMENT (this &quot;Agreement&quot;), dated as of December 31, 2018 (the &quot;Effective Date&quot;) is entered into by and between Armstrong Flooring, Inc., a Delaware corporation (&quot;Seller&quot;) and AFI Licensing LLC, a Delaware limited liability company (&quot;Licensing&quot; and together with Seller, &quot;Arizona&quot;) and AHF Holding, Inc. (formerly known as Tarzan HoldCo, Inc.), a Delaware corporation (&quot;Buyer&quot;) and Armstrong Hardwood Flooring Company, a Tennessee corporation (the &quot;Company&quot; and together with Buyer the &quot;Buyer Entities&quot;) (each of Arizona on the one hand and the Buyer Entities on the other hand, a &quot;Party&quot; and collectively, the &quot;Parties&quot;). &quot;&quot;&quot;] res = model.transform(spark.createDataFrame([text]).toDF(&quot;text&quot;)) Results +------------+---------+ | token|ner_label| +------------+---------+ |INTELLECTUAL| B-DOC| | PROPERTY| I-DOC| | AGREEMENT| I-DOC| | This| O| |INTELLECTUAL| B-DOC| | PROPERTY| I-DOC| | AGREEMENT| I-DOC| | (| O| | this| O| | &quot;| O| | Agreement| O| | &quot;),| O| | dated| O| | as| O| | of| O| | December|B-EFFDATE| | 31|I-EFFDATE| | ,|I-EFFDATE| | 2018|I-EFFDATE| | (| O| | the| O| | &quot;| O| | Effective| O| | Date| O| | &quot;)| O| | is| O| | entered| O| | into| O| | by| O| | and| O| | between| O| | Armstrong| B-PARTY| | Flooring| I-PARTY| | ,| I-PARTY| | Inc| I-PARTY| | .,| O| | a| O| | Delaware| O| | corporation| O| | (&quot;| O| | Seller| B-ALIAS| | &quot;)| O| | and| O| | AFI| B-PARTY| | Licensing| I-PARTY| | LLC| I-PARTY| | ,| O| | a| O| | Delaware| O| | limited| O| | liability| O| | company| O| | (&quot;| O| | Licensing| B-ALIAS| | &quot;| O| | and| O| | together| O| | with| O| | Seller| B-ALIAS| | ,| O| | &quot;| O| | Arizona| B-ALIAS| | &quot;)| O| | and| O| | AHF| B-PARTY| | Holding| I-PARTY| | ,| I-PARTY| | Inc| I-PARTY| | .| O| | (| O| | formerly| O| | known| O| | as| O| | Tarzan| O| | HoldCo| O| | ,| O| | Inc| O| | .),| O| | a| O| | Delaware| O| | corporation| O| | (&quot;| O| | Buyer| B-ALIAS| | &quot;)| O| | and| O| | Armstrong| B-PARTY| | Hardwood| I-PARTY| | Flooring| I-PARTY| | Company| I-PARTY| ------------------------ Model Information Model Name: legner_contract_doc_parties_lg Compatibility: Legal NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [sentence, token, embeddings] Output Labels: [ner] Language: en Size: 16.3 MB References Manual annotations on CUAD dataset Benchmarking label tp fp fn prec rec f1 I-PARTY 513 58 85 0.8984238 0.85785955 0.87767327 B-EFFDATE 57 5 7 0.91935486 0.890625 0.90476185 I-ORG 208 32 49 0.8666667 0.8093385 0.8370222 B-DOC 80 9 21 0.8988764 0.7920792 0.8421053 B-FORMER_NAME 5 0 0 1.0 1.0 1.0 I-EFFDATE 214 11 6 0.95111114 0.9727273 0.96179783 I-FORMER_NAME 7 1 0 0.875 1.0 0.93333334 I-ALIAS 14 5 13 0.7368421 0.5185185 0.6086956 I-DOC 166 16 52 0.9120879 0.7614679 0.83 B-ORG 131 28 42 0.8238994 0.75722545 0.7891567 B-PARTY 174 40 57 0.8130841 0.7532467 0.7820225 B-ALIAS 170 17 13 0.90909094 0.92896175 0.9189189 Macro-average 1739 222 345 0.8837032 0.8368375 0.859632 Micro-average 1739 222 345 0.8867925 0.834453 0.859827</summary></entry><entry><title type="html">Finance Pipeline (Headers / Subheaders)</title><link href="/2023/01/20/finpipe_header_subheader_en.html" rel="alternate" type="text/html" title="Finance Pipeline (Headers / Subheaders)" /><published>2023-01-20T00:00:00+00:00</published><updated>2023-01-20T00:00:00+00:00</updated><id>/2023/01/20/finpipe_header_subheader_en</id><content type="html" xml:base="/2023/01/20/finpipe_header_subheader_en.html">## Description

This is a finance pretrained pipeline that will help you split long financial documents into smaller sections. To do that, it detects Headers and Subheaders of different sections. You can then use the beginning and end information in the metadata to retrieve the text between those headers.

PART I, PART II, etc are HEADERS
Item 1, Item 2, etc are also HEADERS
Item 1A, 2B, etc are SUBHEADERS
1., 2., 2.1, etc. are SUBHEADERS

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/finance/models/finpipe_header_subheader_en_1.0.0_3.0_1674243435691.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/finance/models/finpipe_header_subheader_en_1.0.0_3.0_1674243435691.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
finance_pipeline = nlp.PretrainedPipeline(&quot;finpipe_header_subheader&quot;, &quot;en&quot;, &quot;finance/models&quot;)

text = [&quot;&quot;&quot;
Item 2. Definitions. 
For purposes of this Agreement, the following terms have the meanings ascribed thereto in this Section 1. 2. Appointment as Reseller.

Item 2A. Appointment. 
The Company hereby [***]. Allscripts may also disclose Company's pricing information relating to its Merchant Processing Services and facilitate procurement of Merchant Processing Services on behalf of Sublicensed Customers, including, without limitation by references to such pricing information and Merchant Processing Services in Customer Agreements. 6

Item 2B. Customer Agreements.&quot;&quot;&quot;]

result = finance_pipeline.annotate(text)
```

&lt;/div&gt;

## Results

```bash
|                        chunks | begin | end |  entities |
|------------------------------:|------:|----:|----------:|
|          Item 2. Definitions. |     1 |  21 |    HEADER |
|         Item 2A. Appointment. |   158 | 179 | SUBHEADER |
| Item 2B. Customer Agreements. |   538 | 566 | SUBHEADER |
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|finpipe_header_subheader|
|Type:|pipeline|
|Compatibility:|Finance NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Language:|en|
|Size:|23.6 KB|

## Included Models

- DocumentAssembler
- TokenizerModel
- ContextualParserModel
- ContextualParserModel
- ChunkMergeModel</content><author><name>John Snow Labs</name></author><category term="en" /><category term="finance" /><category term="ner" /><category term="licensed" /><category term="contextual_parser" /><summary type="html">Description This is a finance pretrained pipeline that will help you split long financial documents into smaller sections. To do that, it detects Headers and Subheaders of different sections. You can then use the beginning and end information in the metadata to retrieve the text between those headers. PART I, PART II, etc are HEADERS Item 1, Item 2, etc are also HEADERS Item 1A, 2B, etc are SUBHEADERS 1., 2., 2.1, etc. are SUBHEADERS Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU finance_pipeline = nlp.PretrainedPipeline(&quot;finpipe_header_subheader&quot;, &quot;en&quot;, &quot;finance/models&quot;) text = [&quot;&quot;&quot; Item 2. Definitions. For purposes of this Agreement, the following terms have the meanings ascribed thereto in this Section 1. 2. Appointment as Reseller. Item 2A. Appointment. The Company hereby [***]. Allscripts may also disclose Company's pricing information relating to its Merchant Processing Services and facilitate procurement of Merchant Processing Services on behalf of Sublicensed Customers, including, without limitation by references to such pricing information and Merchant Processing Services in Customer Agreements. 6 Item 2B. Customer Agreements.&quot;&quot;&quot;] result = finance_pipeline.annotate(text) Results | chunks | begin | end | entities | |------------------------------:|------:|----:|----------:| | Item 2. Definitions. | 1 | 21 | HEADER | | Item 2A. Appointment. | 158 | 179 | SUBHEADER | | Item 2B. Customer Agreements. | 538 | 566 | SUBHEADER | Model Information Model Name: finpipe_header_subheader Type: pipeline Compatibility: Finance NLP 1.0.0+ License: Licensed Edition: Official Language: en Size: 23.6 KB Included Models DocumentAssembler TokenizerModel ContextualParserModel ContextualParserModel ChunkMergeModel</summary></entry><entry><title type="html">Legal Pipeline (Headers / Subheaders)</title><link href="/2023/01/20/legpipe_header_subheader_en.html" rel="alternate" type="text/html" title="Legal Pipeline (Headers / Subheaders)" /><published>2023-01-20T00:00:00+00:00</published><updated>2023-01-20T00:00:00+00:00</updated><id>/2023/01/20/legpipe_header_subheader_en</id><content type="html" xml:base="/2023/01/20/legpipe_header_subheader_en.html">## Description

This is a Legal pretrained pipeline, aimed to carry out Section Splitting by using the Headers and Subheaders entities, detected in the document.

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/legal/models/legpipe_header_subheader_en_1.0.0_3.0_1674244247295.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/legal/models/legpipe_header_subheader_en_1.0.0_3.0_1674244247295.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
legal_pipeline = nlp.PretrainedPipeline(&quot;legpipe_header_subheader&quot;, &quot;en&quot;, &quot;legal/models&quot;)

text = [&quot;&quot;&quot;2. DEFINITION. 
For purposes of this Agreement, the following terms have the meanings ascribed thereto in this Section 1 and 2 Appointment as Reseller.
2.1 Appointment. 
The Company hereby [***]. Allscripts may also disclose Company's pricing information relating to its Merchant Processing Services and facilitate procurement of Merchant Processing Services on behalf of Sublicensed Customers, including, without limitation by references to such pricing information and Merchant Processing Services in Customer Agreements. 6
2.2 Customer Agreements.&quot;&quot;&quot;]

result = legal_pipeline.annotate(text)
```

&lt;/div&gt;

## Results

```bash
|                  chunks | begin | end |  entities |
|------------------------:|------:|----:|----------:|
|           2. DEFINITION |     0 |  12 |    HEADER |
|         2.1 Appointment |   154 | 168 | SUBHEADER |
| 2.2 Customer Agreements |   530 | 552 | SUBHEADER |
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|legpipe_header_subheader|
|Type:|pipeline|
|Compatibility:|Legal NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Language:|en|
|Size:|23.6 KB|

## Included Models

- DocumentAssembler
- TokenizerModel
- ContextualParserModel
- ContextualParserModel
- ChunkMergeModel</content><author><name>John Snow Labs</name></author><category term="en" /><category term="licensed" /><category term="legal" /><category term="ner" /><category term="contextual_parser" /><summary type="html">Description This is a Legal pretrained pipeline, aimed to carry out Section Splitting by using the Headers and Subheaders entities, detected in the document. Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU legal_pipeline = nlp.PretrainedPipeline(&quot;legpipe_header_subheader&quot;, &quot;en&quot;, &quot;legal/models&quot;) text = [&quot;&quot;&quot;2. DEFINITION. For purposes of this Agreement, the following terms have the meanings ascribed thereto in this Section 1 and 2 Appointment as Reseller. 2.1 Appointment. The Company hereby [***]. Allscripts may also disclose Company's pricing information relating to its Merchant Processing Services and facilitate procurement of Merchant Processing Services on behalf of Sublicensed Customers, including, without limitation by references to such pricing information and Merchant Processing Services in Customer Agreements. 6 2.2 Customer Agreements.&quot;&quot;&quot;] result = legal_pipeline.annotate(text) Results | chunks | begin | end | entities | |------------------------:|------:|----:|----------:| | 2. DEFINITION | 0 | 12 | HEADER | | 2.1 Appointment | 154 | 168 | SUBHEADER | | 2.2 Customer Agreements | 530 | 552 | SUBHEADER | Model Information Model Name: legpipe_header_subheader Type: pipeline Compatibility: Legal NLP 1.0.0+ License: Licensed Edition: Official Language: en Size: 23.6 KB Included Models DocumentAssembler TokenizerModel ContextualParserModel ContextualParserModel ChunkMergeModel</summary></entry><entry><title type="html">Company Name Normalization using Nasdaq Stock Screener</title><link href="/2023/01/20/finel_nasdaq_company_name_stock_screener_en.html" rel="alternate" type="text/html" title="Company Name Normalization using Nasdaq Stock Screener" /><published>2023-01-20T00:00:00+00:00</published><updated>2023-01-20T00:00:00+00:00</updated><id>/2023/01/20/finel_nasdaq_company_name_stock_screener_en</id><content type="html" xml:base="/2023/01/20/finel_nasdaq_company_name_stock_screener_en.html">## Description

This is a Financial Entity Resolver model, trained to obtain normalized versions of Company Names, registered in NASDAQ Stock Screener. You can use this model after extracting a company name using any NER, and you will obtain the official name of the company as per NASDAQ Stock Screener.

After this, you can use `finmapper_nasdaq_company_name_stock_screener` to augment and obtain more information about a company using NASDAQ Stock Screener, including Ticker, Sector, Country, etc.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/finance/models/finel_nasdaq_company_name_stock_screener_en_1.0.0_3.0_1674233034536.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/finance/models/finel_nasdaq_company_name_stock_screener_en_1.0.0_3.0_1674233034536.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
documentAssembler = nlp.DocumentAssembler()\
    .setInputCol(&quot;text&quot;)\
    .setOutputCol(&quot;document&quot;)

tokenizer = nlp.Tokenizer()\
    .setInputCols([&quot;document&quot;])\
    .setOutputCol(&quot;token&quot;)

embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) \
    .setInputCols([&quot;document&quot;, &quot;token&quot;]) \
    .setOutputCol(&quot;embeddings&quot;)

ner_model = finance.NerModel.pretrained(&quot;finner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;finance/models&quot;)\
    .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;embeddings&quot;])\
    .setOutputCol(&quot;ner&quot;)

ner_converter = nlp.NerConverter()\
    .setInputCols([&quot;document&quot;,&quot;token&quot;,&quot;ner&quot;])\
    .setOutputCol(&quot;ner_chunk&quot;)

chunkToDoc = nlp.Chunk2Doc()\
    .setInputCols(&quot;ner_chunk&quot;)\
    .setOutputCol(&quot;ner_chunk_doc&quot;)

chunk_embeddings = nlp.UniversalSentenceEncoder.pretrained(&quot;tfhub_use&quot;, &quot;en&quot;) \
    .setInputCols(&quot;ner_chunk_doc&quot;) \
    .setOutputCol(&quot;sentence_embeddings&quot;)

use_er_model = finance.SentenceEntityResolverModel.pretrained(&quot;finel_nasdaq_company_name_stock_screener&quot;, &quot;en&quot;, &quot;finance/models&quot;)\
    .setInputCols([&quot;sentence_embeddings&quot;])\
    .setOutputCol(&quot;normalized&quot;)\
    .setDistanceFunction(&quot;EUCLIDEAN&quot;)

nlpPipeline = nlp.Pipeline(stages=[
     documentAssembler,
     tokenizer,
     embeddings,
     ner_model,
     ner_converter,
     chunkToDoc,
     chunk_embeddings,
     use_er_model
])

text = &quot;&quot;&quot;NIKE is an American multinational corporation that is engaged in the design, development, manufacturing, and worldwide marketing and sales of footwear, apparel, equipment, accessories, and services.&quot;&quot;&quot;

test_data = spark.createDataFrame([[text]]).toDF(&quot;text&quot;)

model = nlpPipeline.fit(test_data)

lp = nlp.LightPipeline(model)

result = lp.annotate(text)

result[&quot;normalized&quot;]
```

&lt;/div&gt;

## Results

```bash
['Nike Inc. Common Stock']
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|finel_nasdaq_company_name_stock_screener|
|Compatibility:|Finance NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[sentence_embeddings]|
|Output Labels:|[normalized]|
|Language:|en|
|Size:|54.7 MB|
|Case sensitive:|false|

## References

https://www.nasdaq.com/market-activity/stocks/screener</content><author><name>John Snow Labs</name></author><category term="en" /><category term="finance" /><category term="licensed" /><category term="nasdaq" /><category term="company" /><summary type="html">Description This is a Financial Entity Resolver model, trained to obtain normalized versions of Company Names, registered in NASDAQ Stock Screener. You can use this model after extracting a company name using any NER, and you will obtain the official name of the company as per NASDAQ Stock Screener. After this, you can use finmapper_nasdaq_company_name_stock_screener to augment and obtain more information about a company using NASDAQ Stock Screener, including Ticker, Sector, Country, etc. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU documentAssembler = nlp.DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) tokenizer = nlp.Tokenizer()\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;token&quot;) embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) \ .setInputCols([&quot;document&quot;, &quot;token&quot;]) \ .setOutputCol(&quot;embeddings&quot;) ner_model = finance.NerModel.pretrained(&quot;finner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;finance/models&quot;)\ .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;embeddings&quot;])\ .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter()\ .setInputCols([&quot;document&quot;,&quot;token&quot;,&quot;ner&quot;])\ .setOutputCol(&quot;ner_chunk&quot;) chunkToDoc = nlp.Chunk2Doc()\ .setInputCols(&quot;ner_chunk&quot;)\ .setOutputCol(&quot;ner_chunk_doc&quot;) chunk_embeddings = nlp.UniversalSentenceEncoder.pretrained(&quot;tfhub_use&quot;, &quot;en&quot;) \ .setInputCols(&quot;ner_chunk_doc&quot;) \ .setOutputCol(&quot;sentence_embeddings&quot;) use_er_model = finance.SentenceEntityResolverModel.pretrained(&quot;finel_nasdaq_company_name_stock_screener&quot;, &quot;en&quot;, &quot;finance/models&quot;)\ .setInputCols([&quot;sentence_embeddings&quot;])\ .setOutputCol(&quot;normalized&quot;)\ .setDistanceFunction(&quot;EUCLIDEAN&quot;) nlpPipeline = nlp.Pipeline(stages=[ documentAssembler, tokenizer, embeddings, ner_model, ner_converter, chunkToDoc, chunk_embeddings, use_er_model ]) text = &quot;&quot;&quot;NIKE is an American multinational corporation that is engaged in the design, development, manufacturing, and worldwide marketing and sales of footwear, apparel, equipment, accessories, and services.&quot;&quot;&quot; test_data = spark.createDataFrame([[text]]).toDF(&quot;text&quot;) model = nlpPipeline.fit(test_data) lp = nlp.LightPipeline(model) result = lp.annotate(text) result[&quot;normalized&quot;] Results ['Nike Inc. Common Stock'] Model Information Model Name: finel_nasdaq_company_name_stock_screener Compatibility: Finance NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [sentence_embeddings] Output Labels: [normalized] Language: en Size: 54.7 MB Case sensitive: false References https://www.nasdaq.com/market-activity/stocks/screener</summary></entry><entry><title type="html">Resolver Company Names to Tickers using Nasdaq Stock Screener</title><link href="/2023/01/20/finel_nasdaq_ticker_stock_screener_en.html" rel="alternate" type="text/html" title="Resolver Company Names to Tickers using Nasdaq Stock Screener" /><published>2023-01-20T00:00:00+00:00</published><updated>2023-01-20T00:00:00+00:00</updated><id>/2023/01/20/finel_nasdaq_ticker_stock_screener_en</id><content type="html" xml:base="/2023/01/20/finel_nasdaq_ticker_stock_screener_en.html">## Description

This is an Entity Resolution / Entity Linking model, which is able to provide Ticker / Trading Symbols using a Company Name as an input. You can use any NER which extracts Organizations / Companies / Parties to then send the input to `finel_nasdaq_company_name_stock_screener` model to get normalized company name. Finally, this Entity Linking model get the Ticker / Trading Symbol (given the company has one).

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/finance/models/finel_nasdaq_ticker_stock_screener_en_1.0.0_3.0_1674236954508.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/finance/models/finel_nasdaq_ticker_stock_screener_en_1.0.0_3.0_1674236954508.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
document_assembler = nlp.DocumentAssembler()\
    .setInputCol('text')\
    .setOutputCol('document')

tokenizer = nlp.Tokenizer()\
    .setInputCols(&quot;document&quot;)\
    .setOutputCol(&quot;token&quot;)

ner_embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;)\
    .setInputCols([&quot;document&quot;, &quot;token&quot;])\
    .setOutputCol(&quot;embeddings&quot;)

ner_model = finance.NerModel.pretrained('finner_orgs_prods_alias', 'en', 'finance/models')\
    .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;embeddings&quot;])\
    .setOutputCol(&quot;ner&quot;)

ner_converter = nlp.NerConverter()\
    .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;])\
    .setOutputCol(&quot;ner_chunk&quot;)

chunkToDoc = nlp.Chunk2Doc()\
    .setInputCols(&quot;ner_chunk&quot;)\
    .setOutputCol(&quot;ner_chunk_doc&quot;) 

ticker_embeddings = nlp.UniversalSentenceEncoder.pretrained(&quot;tfhub_use&quot;, &quot;en&quot;)\
    .setInputCols(&quot;ner_chunk_doc&quot;)\
    .setOutputCol(&quot;ticker_embeddings&quot;)

er_ticker_model = finance.SentenceEntityResolverModel.pretrained('finel_nasdaq_ticker_stock_screener', 'en', 'finance/model')\
    .setInputCols([&quot;ticker_embeddings&quot;])\
    .setOutputCol(&quot;ticker&quot;)\
    .setAuxLabelCol(&quot;company_name&quot;)

pipeline = nlp.Pipeline().setStages([document_assembler,
                              tokenizer, 
                              ner_embeddings,
                              ner_model, 
                              ner_converter,
                              chunkToDoc,
                              ticker_embeddings,
                              er_ticker_model])

empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)

model = pipeline.fit(empty_data)

lp = nlp.LightPipeline(model)

text = &quot;&quot;&quot;Nike is an American multinational association that is involved in the design, development, manufacturing and worldwide marketing and sales of apparel, footwear, accessories, equipment and services.&quot;&quot;&quot;

result = lp.annotate(text)

result[&quot;ticker&quot;]
```

&lt;/div&gt;

## Results

```bash
['NKE']
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|finel_nasdaq_ticker_stock_screener|
|Compatibility:|Finance NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[sentence_embeddings]|
|Output Labels:|[normalized]|
|Language:|en|
|Size:|54.6 MB|
|Case sensitive:|false|

## References

https://www.nasdaq.com/market-activity/stocks/screener</content><author><name>John Snow Labs</name></author><category term="en" /><category term="licensed" /><category term="finance" /><category term="nasdaq" /><category term="ticker" /><summary type="html">Description This is an Entity Resolution / Entity Linking model, which is able to provide Ticker / Trading Symbols using a Company Name as an input. You can use any NER which extracts Organizations / Companies / Parties to then send the input to finel_nasdaq_company_name_stock_screener model to get normalized company name. Finally, this Entity Linking model get the Ticker / Trading Symbol (given the company has one). Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler()\ .setInputCol('text')\ .setOutputCol('document') tokenizer = nlp.Tokenizer()\ .setInputCols(&quot;document&quot;)\ .setOutputCol(&quot;token&quot;) ner_embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;)\ .setInputCols([&quot;document&quot;, &quot;token&quot;])\ .setOutputCol(&quot;embeddings&quot;) ner_model = finance.NerModel.pretrained('finner_orgs_prods_alias', 'en', 'finance/models')\ .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;embeddings&quot;])\ .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter()\ .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;])\ .setOutputCol(&quot;ner_chunk&quot;) chunkToDoc = nlp.Chunk2Doc()\ .setInputCols(&quot;ner_chunk&quot;)\ .setOutputCol(&quot;ner_chunk_doc&quot;) ticker_embeddings = nlp.UniversalSentenceEncoder.pretrained(&quot;tfhub_use&quot;, &quot;en&quot;)\ .setInputCols(&quot;ner_chunk_doc&quot;)\ .setOutputCol(&quot;ticker_embeddings&quot;) er_ticker_model = finance.SentenceEntityResolverModel.pretrained('finel_nasdaq_ticker_stock_screener', 'en', 'finance/model')\ .setInputCols([&quot;ticker_embeddings&quot;])\ .setOutputCol(&quot;ticker&quot;)\ .setAuxLabelCol(&quot;company_name&quot;) pipeline = nlp.Pipeline().setStages([document_assembler, tokenizer, ner_embeddings, ner_model, ner_converter, chunkToDoc, ticker_embeddings, er_ticker_model]) empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) model = pipeline.fit(empty_data) lp = nlp.LightPipeline(model) text = &quot;&quot;&quot;Nike is an American multinational association that is involved in the design, development, manufacturing and worldwide marketing and sales of apparel, footwear, accessories, equipment and services.&quot;&quot;&quot; result = lp.annotate(text) result[&quot;ticker&quot;] Results ['NKE'] Model Information Model Name: finel_nasdaq_ticker_stock_screener Compatibility: Finance NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [sentence_embeddings] Output Labels: [normalized] Language: en Size: 54.6 MB Case sensitive: false References https://www.nasdaq.com/market-activity/stocks/screener</summary></entry><entry><title type="html">Mapping Companies to NASDAQ Stock Screener by Company Name</title><link href="/2023/01/19/finmapper_nasdaq_company_name_stock_screener_en.html" rel="alternate" type="text/html" title="Mapping Companies to NASDAQ Stock Screener by Company Name" /><published>2023-01-19T00:00:00+00:00</published><updated>2023-01-19T00:00:00+00:00</updated><id>/2023/01/19/finmapper_nasdaq_company_name_stock_screener_en</id><content type="html" xml:base="/2023/01/19/finmapper_nasdaq_company_name_stock_screener_en.html">## Description

This model allows you to, given an extracted name of a company, get following information about that company from Nasdaq Stock Screener:

  - Country
  - IPO_Year
  - Industry
  - Last_Sale
  - Market_Cap
  - Name
  - Net_Change
  - Percent_Change
  - Sector
  - Ticker
  - Volume

It can be optionally combined with Entity Resolution to normalize first the name of the company.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/finance/models/finmapper_nasdaq_company_name_stock_screener_en_1.0.0_3.0_1674161310624.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/finance/models/finmapper_nasdaq_company_name_stock_screener_en_1.0.0_3.0_1674161310624.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
document_assembler = nlp.DocumentAssembler()\
    .setInputCol('text')\
    .setOutputCol('document')

tokenizer = nlp.Tokenizer()\
    .setInputCols(&quot;document&quot;)\
    .setOutputCol(&quot;token&quot;)

embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) \
    .setInputCols([&quot;document&quot;, &quot;token&quot;]) \
    .setOutputCol(&quot;embeddings&quot;)

ner_model = finance.NerModel.pretrained('finner_orgs_prods_alias', 'en', 'finance/models')\
    .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;embeddings&quot;])\
    .setOutputCol(&quot;ner&quot;)

ner_converter = nlp.NerConverter()\
    .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;])\
    .setOutputCol(&quot;ner_chunk&quot;)

# Optional: To normalize the ORG name using NASDAQ data before the mapping
##########################################################################
chunkToDoc = nlp.Chunk2Doc()\
    .setInputCols(&quot;ner_chunk&quot;)\
    .setOutputCol(&quot;ner_chunk_doc&quot;)

chunk_embeddings = nlp.UniversalSentenceEncoder.pretrained(&quot;tfhub_use&quot;, &quot;en&quot;)\
    .setInputCols([&quot;ner_chunk_doc&quot;])\
    .setOutputCol(&quot;chunk_embeddings&quot;)

use_er_model = finance.SentenceEntityResolverModel.pretrained('finel_nasdaq_company_name_stock_screener', 'en', 'finance/models')\
    .setInputCols(&quot;chunk_embeddings&quot;)\
    .setOutputCol('normalized')\
    .setDistanceFunction(&quot;EUCLIDEAN&quot;)  
##########################################################################

CM = finance.ChunkMapperModel.pretrained('finmapper_nasdaq_company_name_stock_screener', 'en', 'finance/models')\
    .setInputCols([&quot;normalized&quot;])\
    .setOutputCol(&quot;mappings&quot;)

pipeline = nlp.Pipeline().setStages([document_assembler,
                                 tokenizer, 
                                 embeddings,
                                 ner_model, 
                                 ner_converter,
                                 chunkToDoc, # Optional for normalization
                                 chunk_embeddings, # Optional for normalization
                                 use_er_model, # Optional for normalization
                                 CM])

empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)

model = pipeline.fit(empty_data)

lp = nlp.LightPipeline(model)

text = &quot;&quot;&quot;Nike is an American multinational association that is involved in the design, development, manufacturing and worldwide marketing and sales of apparel, footwear, accessories, equipment and services.&quot;&quot;&quot;

result = lp.fullAnnotate(text)
```

&lt;/div&gt;

## Results

```bash
&quot;Country&quot;: &quot;United States&quot;,
&quot;IPO_Year&quot;: &quot;0&quot;,
&quot;Industry&quot;: &quot;Shoe Manufacturing&quot;,
&quot;Last_Sale&quot;: &quot;$128.85&quot;,
&quot;Market_Cap&quot;: &quot;1.9979004036E11&quot;,
&quot;Name&quot;: &quot;Nike Inc. Common Stock&quot;,
&quot;Net_Change&quot;: &quot;0.96&quot;,
&quot;Percent_Change&quot;: &quot;0.751%&quot;,
&quot;Sector&quot;: &quot;Consumer Discretionary&quot;,
&quot;Symbol&quot;: &quot;NKE&quot;,
&quot;Volume&quot;: &quot;4854668&quot;
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|finmapper_nasdaq_company_name_stock_screener|
|Compatibility:|Finance NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[ner_chunk]|
|Output Labels:|[mappings]|
|Language:|en|
|Size:|599.1 KB|

## References

https://www.nasdaq.com/market-activity/stocks/screener</content><author><name>John Snow Labs</name></author><category term="en" /><category term="finance" /><category term="licensed" /><category term="nasdaq" /><category term="company" /><summary type="html">Description This model allows you to, given an extracted name of a company, get following information about that company from Nasdaq Stock Screener: Country IPO_Year Industry Last_Sale Market_Cap Name Net_Change Percent_Change Sector Ticker Volume It can be optionally combined with Entity Resolution to normalize first the name of the company. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler()\ .setInputCol('text')\ .setOutputCol('document') tokenizer = nlp.Tokenizer()\ .setInputCols(&quot;document&quot;)\ .setOutputCol(&quot;token&quot;) embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) \ .setInputCols([&quot;document&quot;, &quot;token&quot;]) \ .setOutputCol(&quot;embeddings&quot;) ner_model = finance.NerModel.pretrained('finner_orgs_prods_alias', 'en', 'finance/models')\ .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;embeddings&quot;])\ .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter()\ .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;])\ .setOutputCol(&quot;ner_chunk&quot;) # Optional: To normalize the ORG name using NASDAQ data before the mapping ########################################################################## chunkToDoc = nlp.Chunk2Doc()\ .setInputCols(&quot;ner_chunk&quot;)\ .setOutputCol(&quot;ner_chunk_doc&quot;) chunk_embeddings = nlp.UniversalSentenceEncoder.pretrained(&quot;tfhub_use&quot;, &quot;en&quot;)\ .setInputCols([&quot;ner_chunk_doc&quot;])\ .setOutputCol(&quot;chunk_embeddings&quot;) use_er_model = finance.SentenceEntityResolverModel.pretrained('finel_nasdaq_company_name_stock_screener', 'en', 'finance/models')\ .setInputCols(&quot;chunk_embeddings&quot;)\ .setOutputCol('normalized')\ .setDistanceFunction(&quot;EUCLIDEAN&quot;) ########################################################################## CM = finance.ChunkMapperModel.pretrained('finmapper_nasdaq_company_name_stock_screener', 'en', 'finance/models')\ .setInputCols([&quot;normalized&quot;])\ .setOutputCol(&quot;mappings&quot;) pipeline = nlp.Pipeline().setStages([document_assembler, tokenizer, embeddings, ner_model, ner_converter, chunkToDoc, # Optional for normalization chunk_embeddings, # Optional for normalization use_er_model, # Optional for normalization CM]) empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) model = pipeline.fit(empty_data) lp = nlp.LightPipeline(model) text = &quot;&quot;&quot;Nike is an American multinational association that is involved in the design, development, manufacturing and worldwide marketing and sales of apparel, footwear, accessories, equipment and services.&quot;&quot;&quot; result = lp.fullAnnotate(text) Results &quot;Country&quot;: &quot;United States&quot;, &quot;IPO_Year&quot;: &quot;0&quot;, &quot;Industry&quot;: &quot;Shoe Manufacturing&quot;, &quot;Last_Sale&quot;: &quot;$128.85&quot;, &quot;Market_Cap&quot;: &quot;1.9979004036E11&quot;, &quot;Name&quot;: &quot;Nike Inc. Common Stock&quot;, &quot;Net_Change&quot;: &quot;0.96&quot;, &quot;Percent_Change&quot;: &quot;0.751%&quot;, &quot;Sector&quot;: &quot;Consumer Discretionary&quot;, &quot;Symbol&quot;: &quot;NKE&quot;, &quot;Volume&quot;: &quot;4854668&quot; Model Information Model Name: finmapper_nasdaq_company_name_stock_screener Compatibility: Finance NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [ner_chunk] Output Labels: [mappings] Language: en Size: 599.1 KB References https://www.nasdaq.com/market-activity/stocks/screener</summary></entry><entry><title type="html">Mapping Companies to NASDAQ Stock Screener by Ticker</title><link href="/2023/01/19/finmapper_nasdaq_ticker_stock_screener_en.html" rel="alternate" type="text/html" title="Mapping Companies to NASDAQ Stock Screener by Ticker" /><published>2023-01-19T00:00:00+00:00</published><updated>2023-01-19T00:00:00+00:00</updated><id>/2023/01/19/finmapper_nasdaq_ticker_stock_screener_en</id><content type="html" xml:base="/2023/01/19/finmapper_nasdaq_ticker_stock_screener_en.html">## Description

This model allows you to, given a Ticker, get the following information about a company at Nasdaq Stock Screener:

 - Country
 - IPO_Year
 - Industry
 - Last_Sale
 - Market_Cap
 - Name
 - Net_Change
 - Percent_Change
 - Sector
 - Ticker
 - Volume

Firstly, you should get the TICKER symbol from the finance text with the `finner_ticker` model, then you can get detailed information about the company with the ChunkMapper model.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/finance/models/finmapper_nasdaq_ticker_stock_screener_en_1.0.0_3.0_1674157233652.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/finance/models/finmapper_nasdaq_ticker_stock_screener_en_1.0.0_3.0_1674157233652.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
document_assembler = nlp.DocumentAssembler()\
    .setInputCol('text')\
    .setOutputCol('document')

tokenizer = nlp.Tokenizer()\
    .setInputCols(&quot;document&quot;)\
    .setOutputCol(&quot;token&quot;)

embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) \
    .setInputCols([&quot;document&quot;, &quot;token&quot;]) \
    .setOutputCol(&quot;embeddings&quot;)

ner_model = finance.NerModel.pretrained(&quot;finner_ticker&quot;, &quot;en&quot;, &quot;finance/models&quot;)\
    .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;embeddings&quot;])\
    .setOutputCol(&quot;ner&quot;)

ner_converter = nlp.NerConverter()\
    .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;])\
    .setOutputCol(&quot;ner_chunk&quot;)

CM = finance.ChunkMapperModel.pretrained('finmapper_nasdaq_ticker_stock_screener', 'en', 'finance/models')\
    .setInputCols([&quot;ner_chunk&quot;])\
    .setOutputCol(&quot;mappings&quot;)

pipeline = nlp.Pipeline().setStages([document_assembler,
                                 tokenizer, 
                                 embeddings,
                                 ner_model, 
                                 ner_converter, 
                                 CM])
                                 
text = [&quot;&quot;&quot;There are some serious purchases and sales of AMZN stock today.&quot;&quot;&quot;]

test_data = spark.createDataFrame([text]).toDF(&quot;text&quot;)

model = pipeline.fit(test_data)

result = model.transform(test_data).select('mappings').collect()
```

&lt;/div&gt;

## Results

```bash
&quot;Country&quot;: &quot;United States&quot;,
&quot;IPO_Year&quot;: &quot;1997&quot;,
&quot;Industry&quot;: &quot;Catalog/Specialty Distribution&quot;,
&quot;Last_Sale&quot;: &quot;$98.12&quot;,
&quot;Market_Cap&quot;: &quot;9.98556270184E11&quot;,
&quot;Name&quot;: &quot;Amazon.com Inc. Common Stock&quot;,
&quot;Net_Change&quot;: &quot;2.85&quot;,
&quot;Percent_Change&quot;: &quot;2.991%&quot;,
&quot;Sector&quot;: &quot;Consumer Discretionary&quot;,
&quot;Ticker&quot;: &quot;AMZN&quot;,
&quot;Volume&quot;: &quot;85412563&quot;
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|finmapper_nasdaq_ticker_stock_screener|
|Compatibility:|Finance NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[ner_chunk]|
|Output Labels:|[mappings]|
|Language:|en|
|Size:|584.5 KB|

## References

https://www.nasdaq.com/market-activity/stocks/screener</content><author><name>John Snow Labs</name></author><category term="en" /><category term="finance" /><category term="licensed" /><category term="nasdaq" /><category term="ticker" /><summary type="html">Description This model allows you to, given a Ticker, get the following information about a company at Nasdaq Stock Screener: Country IPO_Year Industry Last_Sale Market_Cap Name Net_Change Percent_Change Sector Ticker Volume Firstly, you should get the TICKER symbol from the finance text with the finner_ticker model, then you can get detailed information about the company with the ChunkMapper model. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler()\ .setInputCol('text')\ .setOutputCol('document') tokenizer = nlp.Tokenizer()\ .setInputCols(&quot;document&quot;)\ .setOutputCol(&quot;token&quot;) embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) \ .setInputCols([&quot;document&quot;, &quot;token&quot;]) \ .setOutputCol(&quot;embeddings&quot;) ner_model = finance.NerModel.pretrained(&quot;finner_ticker&quot;, &quot;en&quot;, &quot;finance/models&quot;)\ .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;embeddings&quot;])\ .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter()\ .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;])\ .setOutputCol(&quot;ner_chunk&quot;) CM = finance.ChunkMapperModel.pretrained('finmapper_nasdaq_ticker_stock_screener', 'en', 'finance/models')\ .setInputCols([&quot;ner_chunk&quot;])\ .setOutputCol(&quot;mappings&quot;) pipeline = nlp.Pipeline().setStages([document_assembler, tokenizer, embeddings, ner_model, ner_converter, CM]) text = [&quot;&quot;&quot;There are some serious purchases and sales of AMZN stock today.&quot;&quot;&quot;] test_data = spark.createDataFrame([text]).toDF(&quot;text&quot;) model = pipeline.fit(test_data) result = model.transform(test_data).select('mappings').collect() Results &quot;Country&quot;: &quot;United States&quot;, &quot;IPO_Year&quot;: &quot;1997&quot;, &quot;Industry&quot;: &quot;Catalog/Specialty Distribution&quot;, &quot;Last_Sale&quot;: &quot;$98.12&quot;, &quot;Market_Cap&quot;: &quot;9.98556270184E11&quot;, &quot;Name&quot;: &quot;Amazon.com Inc. Common Stock&quot;, &quot;Net_Change&quot;: &quot;2.85&quot;, &quot;Percent_Change&quot;: &quot;2.991%&quot;, &quot;Sector&quot;: &quot;Consumer Discretionary&quot;, &quot;Ticker&quot;: &quot;AMZN&quot;, &quot;Volume&quot;: &quot;85412563&quot; Model Information Model Name: finmapper_nasdaq_ticker_stock_screener Compatibility: Finance NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [ner_chunk] Output Labels: [mappings] Language: en Size: 584.5 KB References https://www.nasdaq.com/market-activity/stocks/screener</summary></entry><entry><title type="html">Normalize Parent Companies Names using Wikidata</title><link href="/2023/01/18/finel_wiki_parentorgs_en.html" rel="alternate" type="text/html" title="Normalize Parent Companies Names using Wikidata" /><published>2023-01-18T00:00:00+00:00</published><updated>2023-01-18T00:00:00+00:00</updated><id>/2023/01/18/finel_wiki_parentorgs_en</id><content type="html" xml:base="/2023/01/18/finel_wiki_parentorgs_en.html">## Description

This is an Entity Resolution model, aimed to normalize a previously extracted ORG entity, using its reference name in WIkidata. This is useful to then use `finel_wiki_parentorgs` Chunk Mapping model and get information of the subsidiaries, countries, stock exchange, etc.

It also retrieves the TICKER, which can be retrieved from `aux_label` column in metadata.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/finance/models/finel_wiki_parentorgs_en_1.0.0_3.0_1674038525188.zip){:.button.button-orange.button-orange-trans.arr.button-icon}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
documentAssembler = nlp.DocumentAssembler()\
      .setInputCol(&quot;text&quot;)\
      .setOutputCol(&quot;ner_chunk&quot;)

embeddings = nlp.UniversalSentenceEncoder.pretrained(&quot;tfhub_use&quot;, &quot;en&quot;) \
      .setInputCols(&quot;ner_chunk&quot;) \
      .setOutputCol(&quot;sentence_embeddings&quot;)
    
resolver = finance.SentenceEntityResolverModel.pretrained(&quot;finel_wiki_parentorgs&quot;, &quot;en&quot;, &quot;finance/models&quot;)\
      .setInputCols([&quot;sentence_embeddings&quot;]) \
      .setOutputCol(&quot;normalized_name&quot;)\
      .setDistanceFunction(&quot;EUCLIDEAN&quot;)

pipelineModel = PipelineModel(
      stages = [
          documentAssembler,
          embeddings,
          resolver
      ])

lp = nlp.LightPipeline(pipelineModel)
test_pred = lp.fullAnnotate('ALPHABET')
print(test_pred[0]['normalized_name'][0].result)
print(test_pred[0]['normalized_name'][0].metadata['all_k_aux_labels'].split(':::')[0])
```

&lt;/div&gt;

## Results

```bash
Alphabet Inc.
Aux data: GOOGL
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|finel_wiki_parentorgs|
|Compatibility:|Finance NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[sentence_embeddings]|
|Output Labels:|[original_company_name]|
|Language:|en|
|Size:|2.8 MB|
|Case sensitive:|false|

## References

Wikidata dump about company holdings using SparQL</content><author><name>John Snow Labs</name></author><category term="parent" /><category term="wikipedia" /><category term="wikidata" /><category term="en" /><category term="licensed" /><summary type="html">Description This is an Entity Resolution model, aimed to normalize a previously extracted ORG entity, using its reference name in WIkidata. This is useful to then use finel_wiki_parentorgs Chunk Mapping model and get information of the subsidiaries, countries, stock exchange, etc. It also retrieves the TICKER, which can be retrieved from aux_label column in metadata. Predicted Entities Live Demo Open in Colab Download How to use PythonScalaNLU documentAssembler = nlp.DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;ner_chunk&quot;) embeddings = nlp.UniversalSentenceEncoder.pretrained(&quot;tfhub_use&quot;, &quot;en&quot;) \ .setInputCols(&quot;ner_chunk&quot;) \ .setOutputCol(&quot;sentence_embeddings&quot;) resolver = finance.SentenceEntityResolverModel.pretrained(&quot;finel_wiki_parentorgs&quot;, &quot;en&quot;, &quot;finance/models&quot;)\ .setInputCols([&quot;sentence_embeddings&quot;]) \ .setOutputCol(&quot;normalized_name&quot;)\ .setDistanceFunction(&quot;EUCLIDEAN&quot;) pipelineModel = PipelineModel( stages = [ documentAssembler, embeddings, resolver ]) lp = nlp.LightPipeline(pipelineModel) test_pred = lp.fullAnnotate('ALPHABET') print(test_pred[0]['normalized_name'][0].result) print(test_pred[0]['normalized_name'][0].metadata['all_k_aux_labels'].split(':::')[0]) Results Alphabet Inc. Aux data: GOOGL Model Information Model Name: finel_wiki_parentorgs Compatibility: Finance NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [sentence_embeddings] Output Labels: [original_company_name] Language: en Size: 2.8 MB Case sensitive: false References Wikidata dump about company holdings using SparQL</summary></entry></feed>