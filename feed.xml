<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-07-18T11:34:09+00:00</updated><id>/feed.xml</id><title type="html">Spark NLP</title><subtitle>High Performance NLP with Apache Spark
</subtitle><author><name>{&quot;type&quot;=&gt;nil, &quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;googleplus&quot;=&gt;nil, &quot;telegram&quot;=&gt;nil, &quot;medium&quot;=&gt;nil, &quot;zhihu&quot;=&gt;nil, &quot;douban&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;npm&quot;=&gt;nil}</name></author><entry><title type="html">Embeddings For Similarity Search</title><link href="/2023/07/06/quora_distilbert_multilingual_en.html" rel="alternate" type="text/html" title="Embeddings For Similarity Search" /><published>2023-07-06T00:00:00+00:00</published><updated>2023-07-06T00:00:00+00:00</updated><id>/2023/07/06/quora_distilbert_multilingual_en</id><content type="html" xml:base="/2023/07/06/quora_distilbert_multilingual_en.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;Copy of https://huggingface.co/sentence-transformers/quora-distilbert-multilingual&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/community.johnsnowlabs.com/purulalwani/quora_distilbert_multilingual_en_5.0.0_3.2_1688648417016.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;
&lt;a href=&quot;s3://community.johnsnowlabs.com/purulalwani/quora_distilbert_multilingual_en_5.0.0_3.2_1688648417016.zip&quot; class=&quot;button button-orange button-orange-trans button-icon button-copy-s3&quot;&gt;Copy S3 URI&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;tabs-model-aproach-head&quot;&gt;&lt;button class=&quot;tab-li-model-aproach tabheader_active&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li-model-aproach&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li-model-aproach&quot;&gt;NLU&lt;/button&gt;&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;See&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;https&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;huggingface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;co&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quora&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distilbert&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multilingual&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;quora_distilbert_multilingual&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 5.0.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Community&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[sentence, token]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[embeddings]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;en&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Size:&lt;/td&gt;
      &lt;td&gt;506.5 MB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>purulalwani</name></author><category term="en" /><category term="open_source" /><category term="tensorflow" /><summary type="html">Description Copy of https://huggingface.co/sentence-transformers/quora-distilbert-multilingual Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU See -&amp;gt; https://huggingface.co/sentence-transformers/quora-distilbert-multilingual Model Information Model Name: quora_distilbert_multilingual Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Community Input Labels: [sentence, token] Output Labels: [embeddings] Language: en Size: 506.5 MB Case sensitive: false</summary></entry><entry><title type="html">ConvNextForImageClassification - image_classifier_convnext_tiny_224_local</title><link href="/2023/07/05/image_classifier_convnext_tiny_224_local_en.html" rel="alternate" type="text/html" title="ConvNextForImageClassification - image_classifier_convnext_tiny_224_local" /><published>2023-07-05T00:00:00+00:00</published><updated>2023-07-05T00:00:00+00:00</updated><id>/2023/07/05/image_classifier_convnext_tiny_224_local_en</id><content type="html" xml:base="/2023/07/05/image_classifier_convnext_tiny_224_local_en.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;Pretrained ConvNext model for Image Classification, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP.&lt;/p&gt;

&lt;p&gt;The ConvNeXT model was proposed in A ConvNet for the 2020s by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/image_classifier_convnext_tiny_224_local_en_5.0.0_3.0_1688564243397.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;
&lt;a href=&quot;s3://auxdata.johnsnowlabs.com/public/models/image_classifier_convnext_tiny_224_local_en_5.0.0_3.0_1688564243397.zip&quot; class=&quot;button button-orange button-orange-trans button-icon button-copy-s3&quot;&gt;Copy S3 URI&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;tabs-model-aproach-head&quot;&gt;&lt;button class=&quot;tab-li-model-aproach tabheader_active&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li-model-aproach&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li-model-aproach&quot;&gt;NLU&lt;/button&gt;&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;image_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ImageAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;\
  &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\
  &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;image_assembler&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;imageClassifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ConvNextForImageClassification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;image_classifier_convnext_tiny_224_local&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\
  &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;image_assembler&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\
  &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;class&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;image_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;imageClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipelineModel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imageDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipelineDF&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipelineModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imageDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;image_classifier_convnext_tiny_224_local&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 5.0.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[image_assembler]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[class]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;en&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Size:&lt;/td&gt;
      &lt;td&gt;107.6 MB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>John Snow Labs</name></author><category term="convnext" /><category term="en" /><category term="engligh" /><category term="image_classification" /><category term="imagenet" /><category term="convolution" /><category term="open_source" /><category term="tensorflow" /><summary type="html">Description Pretrained ConvNext model for Image Classification, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP. The ConvNeXT model was proposed in A ConvNet for the 2020s by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU image_assembler = ImageAssembler()\ .setInputCol(&quot;image&quot;)\ .setOutputCol(&quot;image_assembler&quot;) imageClassifier = ConvNextForImageClassification.pretrained(&quot;image_classifier_convnext_tiny_224_local&quot;, &quot;en&quot;)\ .setInputCols(&quot;image_assembler&quot;)\ .setOutputCol(&quot;class&quot;) pipeline = Pipeline(stages=[ image_assembler, imageClassifier, ]) pipelineModel = pipeline.fit(imageDF) pipelineDF = pipelineModel.transform(imageDF) Model Information Model Name: image_classifier_convnext_tiny_224_local Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Official Input Labels: [image_assembler] Output Labels: [class] Language: en Size: 107.6 MB</summary></entry><entry><title type="html">Multilingual XLMRoBerta Embeddings Cased Model</title><link href="/2023/06/29/xlmroberta_embeddings_paraphrase_mpnet_base_v2_xx.html" rel="alternate" type="text/html" title="Multilingual XLMRoBerta Embeddings Cased Model" /><published>2023-06-29T00:00:00+00:00</published><updated>2023-06-29T00:00:00+00:00</updated><id>/2023/06/29/xlmroberta_embeddings_paraphrase_mpnet_base_v2_xx</id><content type="html" xml:base="/2023/06/29/xlmroberta_embeddings_paraphrase_mpnet_base_v2_xx.html">## Description

Pretrained XLMRoberta Embeddings model is a multilingual embedding model adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/xlmroberta_embeddings_paraphrase_mpnet_base_v2_xx_4.4.4_3.0_1688073546075.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/xlmroberta_embeddings_paraphrase_mpnet_base_v2_xx_4.4.4_3.0_1688073546075.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
documentAssembler = DocumentAssembler() \
    .setInputCol(&quot;text&quot;) \
    .setOutputCol(&quot;document&quot;)

tokenizer = Tokenizer() \
    .setInputCols(&quot;document&quot;) \
    .setOutputCol(&quot;token&quot;)

embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlmroberta_embeddings_paraphrase_mpnet_base_v2&quot;,&quot;xx&quot;) \
    .setInputCols([&quot;document&quot;, &quot;token&quot;]) \
    .setOutputCol(&quot;embeddings&quot;) \
    .setCaseSensitive(True)

pipeline = Pipeline(stages=[documentAssembler, 
                            tokenizer, 
                            embeddings])

data = spark.createDataFrame([[&quot;I love Spark NLP&quot;]]).toDF(&quot;text&quot;)
result = pipeline.fit(data).transform(data)
```
```scala
val documentAssembler = new DocumentAssembler() 
      .setInputCol(&quot;text&quot;) 
      .setOutputCol(&quot;document&quot;)
 
val tokenizer = new Tokenizer() 
    .setInputCols(&quot;document&quot;)
    .setOutputCol(&quot;token&quot;)

val embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlmroberta_embeddings_paraphrase_mpnet_base_v2&quot;, &quot;xx&quot;) 
    .setInputCols(Array(&quot;document&quot;, &quot;token&quot;)) 
    .setOutputCol(&quot;embeddings&quot;)

val pipeline = new Pipeline().setStages(Array(documentAssembler, 
                                              tokenizer, 
                                              embeddings))

val data = Seq(&quot;I love Spark NLP&quot;).toDS.toDF(&quot;text&quot;)
val result = pipeline.fit(data).transform(data)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|xlmroberta_embeddings_paraphrase_mpnet_base_v2|
|Compatibility:|Spark NLP 4.4.4+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[sentence, token]|
|Output Labels:|[embeddings]|
|Language:|xx|
|Size:|1.0 GB|
|Case sensitive:|true|

## References

https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2</content><author><name>John Snow Labs</name></author><category term="xx" /><category term="embeddings" /><category term="xlmroberta" /><category term="open_source" /><category term="transformer" /><category term="tensorflow" /><summary type="html">Description Pretrained XLMRoberta Embeddings model is a multilingual embedding model adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU documentAssembler = DocumentAssembler() \ .setInputCol(&quot;text&quot;) \ .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() \ .setInputCols(&quot;document&quot;) \ .setOutputCol(&quot;token&quot;) embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlmroberta_embeddings_paraphrase_mpnet_base_v2&quot;,&quot;xx&quot;) \ .setInputCols([&quot;document&quot;, &quot;token&quot;]) \ .setOutputCol(&quot;embeddings&quot;) \ .setCaseSensitive(True) pipeline = Pipeline(stages=[documentAssembler, tokenizer, embeddings]) data = spark.createDataFrame([[&quot;I love Spark NLP&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlmroberta_embeddings_paraphrase_mpnet_base_v2&quot;, &quot;xx&quot;) .setInputCols(Array(&quot;document&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline().setStages(Array(documentAssembler, tokenizer, embeddings)) val data = Seq(&quot;I love Spark NLP&quot;).toDS.toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) Model Information Model Name: xlmroberta_embeddings_paraphrase_mpnet_base_v2 Compatibility: Spark NLP 4.4.4+ License: Open Source Edition: Official Input Labels: [sentence, token] Output Labels: [embeddings] Language: xx Size: 1.0 GB Case sensitive: true References https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2</summary></entry><entry><title type="html">BERT Embeddings (Base Cased)</title><link href="/2023/06/29/bert_base_cased_en.html" rel="alternate" type="text/html" title="BERT Embeddings (Base Cased)" /><published>2023-06-29T00:00:00+00:00</published><updated>2023-06-29T00:00:00+00:00</updated><id>/2023/06/29/bert_base_cased_en</id><content type="html" xml:base="/2023/06/29/bert_base_cased_en.html">## Description

This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper &quot;[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)&quot;.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_base_cased_en_5.0.0_3.0_1688044252396.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/bert_base_cased_en_5.0.0_3.0_1688044252396.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;

{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
...
embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings])
pipeline_model = nlp_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;))
result = pipeline_model.transform(spark.createDataFrame([['I love NLP']], [&quot;text&quot;]))
```

```scala
...
val embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings))
val data = Seq(&quot;I love NLP&quot;).toDF(&quot;text&quot;)
val result = pipeline.fit(data).transform(data)
```

{:.nlu-block}
```python
import nlu

text = [&quot;I love NLP&quot;]
embeddings_df = nlu.load('en.embed.bert.base_cased').predict(text, output_level='token')
embeddings_df
```

&lt;/div&gt;

{:.h2_title}

## Results

```bash
Results

	token	en_embed_bert_base_cased_embeddings

	I	[0.43879568576812744, -0.40160006284713745, 0....
	love	[0.21737590432167053, -0.3865768313407898, -0....
	NLP	[-0.16226479411125183, -0.053727392107248306, ...



{:.model-param}
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|bert_base_cased|
|Compatibility:|Spark NLP 5.0.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[sentence, token]|
|Output Labels:|[bert]|
|Language:|en|
|Size:|403.6 MB|
|Case sensitive:|true|

## References

The model is imported from [https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1](https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="embeddings" /><category term="en" /><category term="onnx" /><summary type="html">Description This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU ... embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings]) pipeline_model = nlp_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) result = pipeline_model.transform(spark.createDataFrame([['I love NLP']], [&quot;text&quot;])) ... val embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings)) val data = Seq(&quot;I love NLP&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) import nlu text = [&quot;I love NLP&quot;] embeddings_df = nlu.load('en.embed.bert.base_cased').predict(text, output_level='token') embeddings_df Results Results token en_embed_bert_base_cased_embeddings I [0.43879568576812744, -0.40160006284713745, 0.... love [0.21737590432167053, -0.3865768313407898, -0.... NLP [-0.16226479411125183, -0.053727392107248306, ... {:.model-param} Model Information Model Name: bert_base_cased Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Official Input Labels: [sentence, token] Output Labels: [bert] Language: en Size: 403.6 MB Case sensitive: true References The model is imported from https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1</summary></entry><entry><title type="html">BERT Embeddings (Base Cased) Optimized</title><link href="/2023/06/29/bert_base_cased_opt_en.html" rel="alternate" type="text/html" title="BERT Embeddings (Base Cased) Optimized" /><published>2023-06-29T00:00:00+00:00</published><updated>2023-06-29T00:00:00+00:00</updated><id>/2023/06/29/bert_base_cased_opt_en</id><content type="html" xml:base="/2023/06/29/bert_base_cased_opt_en.html">## Description

This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper &quot;[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)&quot;.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_base_cased_opt_en_5.0.0_3.0_1688044364323.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/bert_base_cased_opt_en_5.0.0_3.0_1688044364323.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;

{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
...
embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased_opt&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings])
pipeline_model = nlp_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;))
result = pipeline_model.transform(spark.createDataFrame([['I love NLP']], [&quot;text&quot;]))
```

```scala
...
val embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased_opt&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings))
val data = Seq(&quot;I love NLP&quot;).toDF(&quot;text&quot;)
val result = pipeline.fit(data).transform(data)
```

{:.nlu-block}
```python
import nlu

text = [&quot;I love NLP&quot;]
embeddings_df = nlu.load('en.embed.bert.base_cased').predict(text, output_level='token')
embeddings_df
```

&lt;/div&gt;

{:.h2_title}

## Results

```bash
Results

	token	en_embed_bert_base_cased_embeddings

	I	[0.43879568576812744, -0.40160006284713745, 0....
	love	[0.21737590432167053, -0.3865768313407898, -0....
	NLP	[-0.16226479411125183, -0.053727392107248306, ...



{:.model-param}
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|bert_base_cased_opt|
|Compatibility:|Spark NLP 5.0.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[sentence, token]|
|Output Labels:|[bert]|
|Language:|en|
|Size:|403.7 MB|
|Case sensitive:|true|

## References

The model is imported from [https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1](https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="embeddings" /><category term="en" /><category term="onnx" /><summary type="html">Description This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU ... embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased_opt&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings]) pipeline_model = nlp_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) result = pipeline_model.transform(spark.createDataFrame([['I love NLP']], [&quot;text&quot;])) ... val embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased_opt&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings)) val data = Seq(&quot;I love NLP&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) import nlu text = [&quot;I love NLP&quot;] embeddings_df = nlu.load('en.embed.bert.base_cased').predict(text, output_level='token') embeddings_df Results Results token en_embed_bert_base_cased_embeddings I [0.43879568576812744, -0.40160006284713745, 0.... love [0.21737590432167053, -0.3865768313407898, -0.... NLP [-0.16226479411125183, -0.053727392107248306, ... {:.model-param} Model Information Model Name: bert_base_cased_opt Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Official Input Labels: [sentence, token] Output Labels: [bert] Language: en Size: 403.7 MB Case sensitive: true References The model is imported from https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1</summary></entry><entry><title type="html">BERT Embeddings (Base Cased) Quantized</title><link href="/2023/06/29/bert_base_cased_quantized_en.html" rel="alternate" type="text/html" title="BERT Embeddings (Base Cased) Quantized" /><published>2023-06-29T00:00:00+00:00</published><updated>2023-06-29T00:00:00+00:00</updated><id>/2023/06/29/bert_base_cased_quantized_en</id><content type="html" xml:base="/2023/06/29/bert_base_cased_quantized_en.html">## Description

This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper &quot;[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)&quot;.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_base_cased_quantized_en_5.0.0_3.0_1688044431004.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/bert_base_cased_quantized_en_5.0.0_3.0_1688044431004.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;

{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
...
embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased_quantized&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings])
pipeline_model = nlp_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;))
result = pipeline_model.transform(spark.createDataFrame([['I love NLP']], [&quot;text&quot;]))
```

```scala
...
val embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased_quantized&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings))
val data = Seq(&quot;I love NLP&quot;).toDF(&quot;text&quot;)
val result = pipeline.fit(data).transform(data)
```

{:.nlu-block}
```python
import nlu

text = [&quot;I love NLP&quot;]
embeddings_df = nlu.load('en.embed.bert.base_cased').predict(text, output_level='token')
embeddings_df
```

&lt;/div&gt;

{:.h2_title}

## Results

```bash
Results

	token	en_embed_bert_base_cased_embeddings

	I	[0.43879568576812744, -0.40160006284713745, 0....
	love	[0.21737590432167053, -0.3865768313407898, -0....
	NLP	[-0.16226479411125183, -0.053727392107248306, ...



{:.model-param}
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|bert_base_cased_quantized|
|Compatibility:|Spark NLP 5.0.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[sentence, token]|
|Output Labels:|[bert]|
|Language:|en|
|Size:|139.5 MB|
|Case sensitive:|true|

## References

The model is imported from [https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1](https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="embeddings" /><category term="en" /><category term="onnx" /><summary type="html">Description This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU ... embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased_quantized&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings]) pipeline_model = nlp_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) result = pipeline_model.transform(spark.createDataFrame([['I love NLP']], [&quot;text&quot;])) ... val embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased_quantized&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings)) val data = Seq(&quot;I love NLP&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) import nlu text = [&quot;I love NLP&quot;] embeddings_df = nlu.load('en.embed.bert.base_cased').predict(text, output_level='token') embeddings_df Results Results token en_embed_bert_base_cased_embeddings I [0.43879568576812744, -0.40160006284713745, 0.... love [0.21737590432167053, -0.3865768313407898, -0.... NLP [-0.16226479411125183, -0.053727392107248306, ... {:.model-param} Model Information Model Name: bert_base_cased_quantized Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Official Input Labels: [sentence, token] Output Labels: [bert] Language: en Size: 139.5 MB Case sensitive: true References The model is imported from https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1</summary></entry><entry><title type="html">DeBERTa base model</title><link href="/2023/06/28/deberta_v3_base_en.html" rel="alternate" type="text/html" title="DeBERTa base model" /><published>2023-06-28T00:00:00+00:00</published><updated>2023-06-28T00:00:00+00:00</updated><id>/2023/06/28/deberta_v3_base_en</id><content type="html" xml:base="/2023/06/28/deberta_v3_base_en.html">## Description

The DeBERTa model was proposed in [[https://arxiv.org/abs/2006.03654 DeBERTa: Decoding-enhanced BERT with Disentangled Attention]] by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google’s BERT model released in 2018 and Facebook’s RoBERTa model released in 2019. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/deberta_v3_base_en_5.0.0_3.0_1687957496351.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/deberta_v3_base_en_5.0.0_3.0_1687957496351.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```


{:.nlu-block}
```python
import nlu
nlu.load(&quot;en.embed.deberta_v3_base&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;)
```

&lt;/div&gt;


{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|deberta_v3_base|
|Compatibility:|Spark NLP 5.0.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|435.2 MB|
|Case sensitive:|true|
|Max sentence length:|128|

## Benchmarking

```bash
Benchmarking
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="english" /><category term="open_source" /><category term="embeddings" /><category term="deberta" /><category term="v3" /><category term="base" /><category term="onnx" /><summary type="html">Description The DeBERTa model was proposed in [[https://arxiv.org/abs/2006.03654 DeBERTa: Decoding-enhanced BERT with Disentangled Attention]] by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google’s BERT model released in 2018 and Facebook’s RoBERTa model released in 2019. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu nlu.load(&quot;en.embed.deberta_v3_base&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;) Model Information Model Name: deberta_v3_base Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 435.2 MB Case sensitive: true Max sentence length: 128 Benchmarking Benchmarking</summary></entry><entry><title type="html">DeBERTa base model optimized for inference</title><link href="/2023/06/28/deberta_v3_base_opt_en.html" rel="alternate" type="text/html" title="DeBERTa base model optimized for inference" /><published>2023-06-28T00:00:00+00:00</published><updated>2023-06-28T00:00:00+00:00</updated><id>/2023/06/28/deberta_v3_base_opt_en</id><content type="html" xml:base="/2023/06/28/deberta_v3_base_opt_en.html">## Description

The DeBERTa model was proposed in [[https://arxiv.org/abs/2006.03654 DeBERTa: Decoding-enhanced BERT with Disentangled Attention]] by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google’s BERT model released in 2018 and Facebook’s RoBERTa model released in 2019. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/deberta_v3_base_opt_en_5.0.0_3.0_1687958380723.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/deberta_v3_base_opt_en_5.0.0_3.0_1687958380723.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base_opt&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base_opt&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```


{:.nlu-block}
```python
import nlu
nlu.load(&quot;en.embed.deberta_v3_base&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;)
```

&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|deberta_v3_base_opt|
|Compatibility:|Spark NLP 5.0.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|469.3 MB|
|Case sensitive:|true|
|Max sentence length:|128|

## Benchmarking

```bash
Benchmarking
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="english" /><category term="open_source" /><category term="embeddings" /><category term="deberta" /><category term="v3" /><category term="base" /><category term="onnx" /><summary type="html">Description The DeBERTa model was proposed in [[https://arxiv.org/abs/2006.03654 DeBERTa: Decoding-enhanced BERT with Disentangled Attention]] by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google’s BERT model released in 2018 and Facebook’s RoBERTa model released in 2019. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base_opt&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base_opt&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu nlu.load(&quot;en.embed.deberta_v3_base&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;) Model Information Model Name: deberta_v3_base_opt Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 469.3 MB Case sensitive: true Max sentence length: 128 Benchmarking Benchmarking</summary></entry><entry><title type="html">DeBERTa base model quantized</title><link href="/2023/06/28/deberta_v3_base_quantized_en.html" rel="alternate" type="text/html" title="DeBERTa base model quantized" /><published>2023-06-28T00:00:00+00:00</published><updated>2023-06-28T00:00:00+00:00</updated><id>/2023/06/28/deberta_v3_base_quantized_en</id><content type="html" xml:base="/2023/06/28/deberta_v3_base_quantized_en.html">## Description

The DeBERTa model was proposed in [[https://arxiv.org/abs/2006.03654 DeBERTa: Decoding-enhanced BERT with Disentangled Attention]] by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google’s BERT model released in 2018 and Facebook’s RoBERTa model released in 2019. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/deberta_v3_base_quantized_en_5.0.0_3.0_1687958846162.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/deberta_v3_base_quantized_en_5.0.0_3.0_1687958846162.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base_quantized&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base_quantized&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```


{:.nlu-block}
```python
import nlu
nlu.load(&quot;en.embed.deberta_v3_base&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;)
```

&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|deberta_v3_base_quantized|
|Compatibility:|Spark NLP 5.0.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|310.7 MB|
|Case sensitive:|true|
|Max sentence length:|128|

## Benchmarking

```bash
Benchmarking
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="english" /><category term="open_source" /><category term="embeddings" /><category term="deberta" /><category term="v3" /><category term="base" /><category term="onnx" /><summary type="html">Description The DeBERTa model was proposed in [[https://arxiv.org/abs/2006.03654 DeBERTa: Decoding-enhanced BERT with Disentangled Attention]] by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google’s BERT model released in 2018 and Facebook’s RoBERTa model released in 2019. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base_quantized&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base_quantized&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu nlu.load(&quot;en.embed.deberta_v3_base&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;) Model Information Model Name: deberta_v3_base_quantized Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 310.7 MB Case sensitive: true Max sentence length: 128 Benchmarking Benchmarking</summary></entry><entry><title type="html">DistilBERT base model (cased)</title><link href="/2023/06/28/distilbert_base_cased_en.html" rel="alternate" type="text/html" title="DistilBERT base model (cased)" /><published>2023-06-28T00:00:00+00:00</published><updated>2023-06-28T00:00:00+00:00</updated><id>/2023/06/28/distilbert_base_cased_en</id><content type="html" xml:base="/2023/06/28/distilbert_base_cased_en.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;This model is a distilled version of the &lt;a href=&quot;https://huggingface.co/bert-base-cased&quot;&gt;BERT base model&lt;/a&gt;. It was introduced in &lt;a href=&quot;https://arxiv.org/abs/1910.01108&quot;&gt;this paper&lt;/a&gt;. The code for the distillation process can be found &lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/examples/research_projects/distillation&quot;&gt;here&lt;/a&gt;. This model is cased: it does make a difference between english and English.&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/distilbert_base_cased_en_5.0.0_3.0_1687955596708.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;
&lt;a href=&quot;s3://auxdata.johnsnowlabs.com/public/models/distilbert_base_cased_en_5.0.0_3.0_1687955596708.zip&quot; class=&quot;button button-orange button-orange-trans button-icon button-copy-s3&quot;&gt;Copy S3 URI&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box model-param&quot;&gt;
  &lt;div class=&quot;tabs-model-aproach-head&quot;&gt;&lt;button class=&quot;tab-li-model-aproach tabheader_active&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li-model-aproach&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li-model-aproach&quot;&gt;NLU&lt;/button&gt;&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DistilBertEmbeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;distilbert_base_cased&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nlp_pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentence_detector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;DistilBertEmbeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;distilbert_base_cased&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentence_detector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

  &lt;div class=&quot;language-python nlu-block highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nlu&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nlu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;en.embed.distilbert&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;distilbert_base_cased&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 5.0.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, sentence]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[embeddings]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;en&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Size:&lt;/td&gt;
      &lt;td&gt;243.8 MB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://huggingface.co/distilbert-base-cased&quot;&gt;https://huggingface.co/distilbert-base-cased&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Benchmarking


When fine-tuned on downstream tasks, this model achieves the following results:

Glue &lt;span class=&quot;nb&quot;&gt;test &lt;/span&gt;results:

| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |
|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|
|      | 81.5 | 87.8 | 88.2 | 90.4  | 47.2 | 85.5  | 85.6 | 60.6 |


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="distilbert" /><category term="en" /><category term="english" /><category term="open_source" /><category term="embeddings" /><category term="onnx" /><summary type="html">Description This model is a distilled version of the BERT base model. It was introduced in this paper. The code for the distillation process can be found here. This model is cased: it does make a difference between english and English. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings]) val embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings)) import nlu nlu.load(&quot;en.embed.distilbert&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;) Model Information Model Name: distilbert_base_cased Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 243.8 MB Case sensitive: true References https://huggingface.co/distilbert-base-cased Benchmarking Benchmarking When fine-tuned on downstream tasks, this model achieves the following results: Glue test results: | Task | MNLI | QQP | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE | |:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:| | | 81.5 | 87.8 | 88.2 | 90.4 | 47.2 | 85.5 | 85.6 | 60.6 |</summary></entry></feed>