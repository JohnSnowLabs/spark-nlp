<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-10-04T07:34:14+00:00</updated><id>/feed.xml</id><title type="html">Spark NLP</title><subtitle>High Performance NLP with Apache Spark
</subtitle><author><name>{&quot;type&quot;=&gt;nil, &quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;googleplus&quot;=&gt;nil, &quot;telegram&quot;=&gt;nil, &quot;medium&quot;=&gt;nil, &quot;zhihu&quot;=&gt;nil, &quot;douban&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;npm&quot;=&gt;nil}</name></author><entry><title type="html">XLM-RoBERTa Base for Amharic (xlm_roberta_base_finetuned_amharic)</title><link href="/2021/09/29/xlm_roberta_base_finetuned_amharic_am.html" rel="alternate" type="text/html" title="XLM-RoBERTa Base for Amharic (xlm_roberta_base_finetuned_amharic)" /><published>2021-09-29T00:00:00+00:00</published><updated>2021-09-29T00:00:00+00:00</updated><id>/2021/09/29/xlm_roberta_base_finetuned_amharic_am</id><content type="html" xml:base="/2021/09/29/xlm_roberta_base_finetuned_amharic_am.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;xlm_roberta_base_finetuned_amharic&lt;/strong&gt; is a &lt;strong&gt;Amharic RoBERTa&lt;/strong&gt; model obtained by fine-tuning &lt;strong&gt;xlm-roberta-base&lt;/strong&gt; model on Amharic language texts. It provides &lt;strong&gt;better performance&lt;/strong&gt; than the XLM-RoBERTa on named entity recognition datasets.&lt;/p&gt;

&lt;p&gt;Specifically, this model is an &lt;em&gt;xlm-roberta-base&lt;/em&gt; model that was fine-tuned on the Amharic corpus.&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/xlm_roberta_base_finetuned_amharic_am_3.3.0_3.0_1632912241061.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XlmRoBertaEmbeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;xlm_roberta_base_finetuned_amharic&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;am&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;XlmRoBertaEmbeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;xlm_roberta_base_finetuned_amharic&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;am&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;xlm_roberta_base_finetuned_amharic&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.3.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, sentence]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[embeddings]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;am&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://huggingface.co/Davlan/xlm-roberta-base-finetuned-amharic&quot;&gt;https://huggingface.co/Davlan/xlm-roberta-base-finetuned-amharic&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## Eval results on the Test set (F-score, average over 5 runs)&lt;/span&gt;

Dataset| XLM-R F1 | am_roberta F1
-|-|-
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;MasakhaNER]&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;https://github.com/masakhane-io/masakhane-ner&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; | 70.96 | 77.97
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="xlm_roberta" /><category term="embeddings" /><category term="am" /><category term="amharic" /><summary type="html">Description xlm_roberta_base_finetuned_amharic is a Amharic RoBERTa model obtained by fine-tuning xlm-roberta-base model on Amharic language texts. It provides better performance than the XLM-RoBERTa on named entity recognition datasets. Specifically, this model is an xlm-roberta-base model that was fine-tuned on the Amharic corpus. Predicted Entities Live Demo Open in Colab Download How to use PythonScalaNLU embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlm_roberta_base_finetuned_amharic&quot;, &quot;am&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlm_roberta_base_finetuned_amharic&quot;, &quot;am&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) Model Information Model Name: xlm_roberta_base_finetuned_amharic Compatibility: Spark NLP 3.3.0+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: am Case sensitive: true Data Source https://huggingface.co/Davlan/xlm-roberta-base-finetuned-amharic Benchmarking ## Eval results on the Test set (F-score, average over 5 runs) Dataset| XLM-R F1 | am_roberta F1 -|-|- [MasakhaNER](https://github.com/masakhane-io/masakhane-ner) | 70.96 | 77.97</summary></entry><entry><title type="html">XLM-RoBERTa Base for Hausa (xlm_roberta_base_finetuned_hausa)</title><link href="/2021/09/29/xlm_roberta_base_finetuned_hausa_ha.html" rel="alternate" type="text/html" title="XLM-RoBERTa Base for Hausa (xlm_roberta_base_finetuned_hausa)" /><published>2021-09-29T00:00:00+00:00</published><updated>2021-09-29T00:00:00+00:00</updated><id>/2021/09/29/xlm_roberta_base_finetuned_hausa_ha</id><content type="html" xml:base="/2021/09/29/xlm_roberta_base_finetuned_hausa_ha.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;xlm_roberta_base_finetuned_hausa&lt;/strong&gt; is a ** Hausa RoBERTa** model obtained by fine-tuning &lt;strong&gt;xlm-roberta-base&lt;/strong&gt; model on Hausa language texts. It provides &lt;strong&gt;better performance&lt;/strong&gt; than the XLM-RoBERTa on named entity recognition datasets.&lt;/p&gt;

&lt;p&gt;Specifically, this model is an &lt;em&gt;xlm-roberta-base&lt;/em&gt; model that was fine-tuned on the Hausa corpus.&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/xlm_roberta_base_finetuned_hausa_ha_3.3.0_3.0_1632912619448.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XlmRoBertaEmbeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;xlm_roberta_base_finetuned_hausa&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ha&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;XlmRoBertaEmbeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;xlm_roberta_base_finetuned_hausa&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ha)
      .setInputCols(&quot;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentence&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;, &quot;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;)
      .setOutputCol(&quot;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;xlm_roberta_base_finetuned_hausa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.3.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, sentence]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[embeddings]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;ha&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://huggingface.co/Davlan/xlm-roberta-base-finetuned-hausa&quot;&gt;https://huggingface.co/Davlan/xlm-roberta-base-finetuned-hausa&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## Eval results on the Test set (F-score, average over 5 runs)&lt;/span&gt;

Dataset| XLM-R F1 | ha_roberta F1
-|-|-
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;MasakhaNER]&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;https://github.com/masakhane-io/masakhane-ner&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; | 86.10 | 91.47
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;VOA Hausa Textclass]&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;https://huggingface.co/datasets/hausa_voa_topics&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; | | 

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="embeddings" /><category term="xlm_roberta" /><category term="open_source" /><category term="ha" /><category term="hausa" /><summary type="html">Description xlm_roberta_base_finetuned_hausa is a ** Hausa RoBERTa** model obtained by fine-tuning xlm-roberta-base model on Hausa language texts. It provides better performance than the XLM-RoBERTa on named entity recognition datasets. Specifically, this model is an xlm-roberta-base model that was fine-tuned on the Hausa corpus. Predicted Entities Live Demo Open in Colab Download How to use PythonScalaNLU embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlm_roberta_base_finetuned_hausa&quot;, &quot;ha&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlm_roberta_base_finetuned_hausa&quot;, &quot;ha) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) Model Information Model Name: xlm_roberta_base_finetuned_hausa Compatibility: Spark NLP 3.3.0+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: ha Case sensitive: true Data Source https://huggingface.co/Davlan/xlm-roberta-base-finetuned-hausa Benchmarking ## Eval results on the Test set (F-score, average over 5 runs) Dataset| XLM-R F1 | ha_roberta F1 -|-|- [MasakhaNER](https://github.com/masakhane-io/masakhane-ner) | 86.10 | 91.47 [VOA Hausa Textclass](https://huggingface.co/datasets/hausa_voa_topics) | |</summary></entry><entry><title type="html">XLM-RoBERTa Base for Igbo (xlm_roberta_base_finetuned_igbo)</title><link href="/2021/09/29/xlm_roberta_base_finetuned_igbo_ig.html" rel="alternate" type="text/html" title="XLM-RoBERTa Base for Igbo (xlm_roberta_base_finetuned_igbo)" /><published>2021-09-29T00:00:00+00:00</published><updated>2021-09-29T00:00:00+00:00</updated><id>/2021/09/29/xlm_roberta_base_finetuned_igbo_ig</id><content type="html" xml:base="/2021/09/29/xlm_roberta_base_finetuned_igbo_ig.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;xlm_roberta_base_finetuned_igbo&lt;/strong&gt; is a &lt;strong&gt;Igbo RoBERTa&lt;/strong&gt; model obtained by fine-tuning &lt;strong&gt;xlm-roberta-base&lt;/strong&gt; model on Igbo language texts. It provides &lt;strong&gt;better performance&lt;/strong&gt; than the XLM-RoBERTa on named entity recognition datasets.&lt;/p&gt;

&lt;p&gt;Specifically, this model is an &lt;em&gt;xlm-roberta-base&lt;/em&gt; model that was fine-tuned on the Igbo corpus.&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/xlm_roberta_base_finetuned_igbo_ig_3.3.0_3.0_1632913008762.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XlmRoBertaEmbeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;xlm_roberta_base_finetuned_igbo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ig&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;XlmRoBertaEmbeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;xlm_roberta_base_finetuned_igbo&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ig&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;xlm_roberta_base_finetuned_igbo&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.3.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, sentence]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[embeddings]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;ig&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://huggingface.co/Davlan/xlm-roberta-base-finetuned-igbo&quot;&gt;https://huggingface.co/Davlan/xlm-roberta-base-finetuned-igbo&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## Eval results on Test set (F-score, average over 5 runs)&lt;/span&gt;

Dataset| XLM-R F1 | ig_roberta F1
-|-|-
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;MasakhaNER]&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;https://github.com/masakhane-io/masakhane-ner&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; | 84.51 | 87.74

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="ig" /><category term="igbo" /><category term="embeddings" /><category term="xlm_roberta" /><category term="open_source" /><summary type="html">Description xlm_roberta_base_finetuned_igbo is a Igbo RoBERTa model obtained by fine-tuning xlm-roberta-base model on Igbo language texts. It provides better performance than the XLM-RoBERTa on named entity recognition datasets. Specifically, this model is an xlm-roberta-base model that was fine-tuned on the Igbo corpus. Predicted Entities Live Demo Open in Colab Download How to use PythonScalaNLU embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlm_roberta_base_finetuned_igbo&quot;, &quot;ig&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlm_roberta_base_finetuned_igbo&quot;, &quot;ig&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) Model Information Model Name: xlm_roberta_base_finetuned_igbo Compatibility: Spark NLP 3.3.0+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: ig Case sensitive: true Data Source https://huggingface.co/Davlan/xlm-roberta-base-finetuned-igbo Benchmarking ## Eval results on Test set (F-score, average over 5 runs) Dataset| XLM-R F1 | ig_roberta F1 -|-|- [MasakhaNER](https://github.com/masakhane-io/masakhane-ner) | 84.51 | 87.74</summary></entry><entry><title type="html">XLM-RoBERTa Base for Kinyarwanda (xlm_roberta_base_finetuned_kinyarwanda)</title><link href="/2021/09/29/xlm_roberta_base_finetuned_kinyarwanda_rw.html" rel="alternate" type="text/html" title="XLM-RoBERTa Base for Kinyarwanda (xlm_roberta_base_finetuned_kinyarwanda)" /><published>2021-09-29T00:00:00+00:00</published><updated>2021-09-29T00:00:00+00:00</updated><id>/2021/09/29/xlm_roberta_base_finetuned_kinyarwanda_rw</id><content type="html" xml:base="/2021/09/29/xlm_roberta_base_finetuned_kinyarwanda_rw.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;xlm_roberta_base_finetuned_kinyarwanda&lt;/strong&gt; is a ** Kinyarwanda RoBERTa** model obtained by fine-tuning &lt;strong&gt;xlm-roberta-base&lt;/strong&gt; model on Kinyarwanda language texts. It provides &lt;strong&gt;better performance&lt;/strong&gt; than the XLM-RoBERTa on named entity recognition datasets.&lt;/p&gt;

&lt;p&gt;Specifically, this model is an &lt;em&gt;xlm-roberta-base&lt;/em&gt; model that was fine-tuned on the Kinyarwanda corpus.&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/xlm_roberta_base_finetuned_kinyarwanda_rw_3.3.0_3.0_1632913275913.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XlmRoBertaEmbeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;xlm_roberta_base_finetuned_kinyarwanda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;rw&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;XlmRoBertaEmbeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;xlm_roberta_base_finetuned_kinyarwanda&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;rw&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;xlm_roberta_base_finetuned_kinyarwanda&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.3.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, sentence]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[embeddings]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;rw&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://huggingface.co/Davlan/xlm-roberta-base-finetuned-kinyarwanda&quot;&gt;https://huggingface.co/Davlan/xlm-roberta-base-finetuned-kinyarwanda&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## Eval results on Test set (F-score, average over 5 runs)&lt;/span&gt;

Dataset| XLM-R F1 | rw_roberta F1
-|-|-
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;MasakhaNER]&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;https://github.com/masakhane-io/masakhane-ner&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; | 73.22 | 77.76

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="embeddings" /><category term="kinyarwanda" /><category term="rw" /><category term="xlm_roberta" /><category term="open_source" /><summary type="html">Description xlm_roberta_base_finetuned_kinyarwanda is a ** Kinyarwanda RoBERTa** model obtained by fine-tuning xlm-roberta-base model on Kinyarwanda language texts. It provides better performance than the XLM-RoBERTa on named entity recognition datasets. Specifically, this model is an xlm-roberta-base model that was fine-tuned on the Kinyarwanda corpus. Predicted Entities Live Demo Open in Colab Download How to use PythonScalaNLU embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlm_roberta_base_finetuned_kinyarwanda&quot;, &quot;rw&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlm_roberta_base_finetuned_kinyarwanda&quot;, &quot;rw&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) Model Information Model Name: xlm_roberta_base_finetuned_kinyarwanda Compatibility: Spark NLP 3.3.0+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: rw Case sensitive: true Data Source https://huggingface.co/Davlan/xlm-roberta-base-finetuned-kinyarwanda Benchmarking ## Eval results on Test set (F-score, average over 5 runs) Dataset| XLM-R F1 | rw_roberta F1 -|-|- [MasakhaNER](https://github.com/masakhane-io/masakhane-ner) | 73.22 | 77.76</summary></entry><entry><title type="html">XLM-RoBERTa Base for Luganda (xlm_roberta_base_finetuned_luganda)</title><link href="/2021/09/29/xlm_roberta_base_finetuned_luganda_lg.html" rel="alternate" type="text/html" title="XLM-RoBERTa Base for Luganda (xlm_roberta_base_finetuned_luganda)" /><published>2021-09-29T00:00:00+00:00</published><updated>2021-09-29T00:00:00+00:00</updated><id>/2021/09/29/xlm_roberta_base_finetuned_luganda_lg</id><content type="html" xml:base="/2021/09/29/xlm_roberta_base_finetuned_luganda_lg.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;xlm_roberta_base_finetuned_luganda&lt;/strong&gt; is a ** Luganda RoBERTa** model obtained by fine-tuning &lt;strong&gt;xlm-roberta-base&lt;/strong&gt; model on Luganda language texts. It provides &lt;strong&gt;better performance&lt;/strong&gt; than the XLM-RoBERTa on named entity recognition datasets.&lt;/p&gt;

&lt;p&gt;Specifically, this model is an &lt;em&gt;xlm-roberta-base&lt;/em&gt; model that was fine-tuned on the Luganda corpus.&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/xlm_roberta_base_finetuned_luganda_lg_3.3.0_3.0_1632913890011.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XlmRoBertaEmbeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;xlm_roberta_base_finetuned_luganda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;lg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;XlmRoBertaEmbeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;xlm_roberta_base_finetuned_luganda&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;lg&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;xlm_roberta_base_finetuned_luganda&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.3.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, sentence]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[embeddings]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;lg&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://huggingface.co/Davlan/xlm-roberta-base-finetuned-luganda&quot;&gt;https://huggingface.co/Davlan/xlm-roberta-base-finetuned-luganda&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## Eval results on Test set (F-score, average over 5 runs)&lt;/span&gt;

Dataset| XLM-R F1 | lg_roberta F1
-|-|-
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;MasakhaNER]&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;https://github.com/masakhane-io/masakhane-ner&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; | 79.69 | 84.70

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="lg" /><category term="luganda" /><category term="embeddings" /><category term="xlm_roberta" /><category term="open_source" /><summary type="html">Description xlm_roberta_base_finetuned_luganda is a ** Luganda RoBERTa** model obtained by fine-tuning xlm-roberta-base model on Luganda language texts. It provides better performance than the XLM-RoBERTa on named entity recognition datasets. Specifically, this model is an xlm-roberta-base model that was fine-tuned on the Luganda corpus. Predicted Entities Live Demo Open in Colab Download How to use PythonScalaNLU embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlm_roberta_base_finetuned_luganda&quot;, &quot;lg&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlm_roberta_base_finetuned_luganda&quot;, &quot;lg&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) Model Information Model Name: xlm_roberta_base_finetuned_luganda Compatibility: Spark NLP 3.3.0+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: lg Case sensitive: true Data Source https://huggingface.co/Davlan/xlm-roberta-base-finetuned-luganda Benchmarking ## Eval results on Test set (F-score, average over 5 runs) Dataset| XLM-R F1 | lg_roberta F1 -|-|- [MasakhaNER](https://github.com/masakhane-io/masakhane-ner) | 79.69 | 84.70</summary></entry><entry><title type="html">XLM-RoBERTa Large (xlm_roberta_large)</title><link href="/2021/09/29/xlm_roberta_large_xx.html" rel="alternate" type="text/html" title="XLM-RoBERTa Large (xlm_roberta_large)" /><published>2021-09-29T00:00:00+00:00</published><updated>2021-09-29T00:00:00+00:00</updated><id>/2021/09/29/xlm_roberta_large_xx</id><content type="html" xml:base="/2021/09/29/xlm_roberta_large_xx.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://ai.facebook.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/&quot;&gt;XLM-RoBERTa&lt;/a&gt; is a scaled cross-lingual sentence encoder. It is trained on 2.5T of data across 100 languages data filtered from Common Crawl. XLM-R achieves state-of-the-arts results on multiple cross-lingual benchmarks.&lt;/p&gt;

&lt;p&gt;The XLM-RoBERTa model was proposed in &lt;a href=&quot;https://arxiv.org/abs/1911.02116&quot;&gt;Unsupervised Cross-lingual Representation Learning at Scale&lt;/a&gt; by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.&lt;/p&gt;

&lt;p&gt;It is based on Facebook’s RoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data.&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/xlm_roberta_large_xx_3.3.0_3.0_1632906808175.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XlmRoBertaEmbeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;xlm_roberta_large&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;XlmRoBertaEmbeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;xlm_roberta_large&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xx&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentence_detector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;xlm_roberta_large&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.3.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, sentence]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[embeddings]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;xx&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://huggingface.co/xlm-roberta-large&quot;&gt;https://huggingface.co/xlm-roberta-large&lt;/a&gt;&lt;/p&gt;</content><author><name>John Snow Labs</name></author><category term="xlm_roberta" /><category term="embeddings" /><category term="large" /><category term="open_source" /><category term="multilingual" /><category term="xx" /><summary type="html">Description XLM-RoBERTa is a scaled cross-lingual sentence encoder. It is trained on 2.5T of data across 100 languages data filtered from Common Crawl. XLM-R achieves state-of-the-arts results on multiple cross-lingual benchmarks. The XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. It is based on Facebook’s RoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data. Predicted Entities Live Demo Open in Colab Download How to use PythonScalaNLU embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlm_roberta_large&quot;, &quot;xx&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlm_roberta_large&quot;, &quot;xx&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings)) Model Information Model Name: xlm_roberta_large Compatibility: Spark NLP 3.3.0+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: xx Case sensitive: true Data Source https://huggingface.co/xlm-roberta-large</summary></entry><entry><title type="html">Sentence Entity Resolver for HCPCS Codes</title><link href="/2021/09/29/sbiobertresolve_hcpcs_en.html" rel="alternate" type="text/html" title="Sentence Entity Resolver for HCPCS Codes" /><published>2021-09-29T00:00:00+00:00</published><updated>2021-09-29T00:00:00+00:00</updated><id>/2021/09/29/sbiobertresolve_hcpcs_en</id><content type="html" xml:base="/2021/09/29/sbiobertresolve_hcpcs_en.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;This model maps extracted medical entities to &lt;a href=&quot;https://www.nlm.nih.gov/research/umls/sourcereleasedocs/current/HCPCS/index.html#:~:text=The%20Healthcare%20Common%20Procedure%20Coding,%2C%20supplies%2C%20products%20and%20services.&quot;&gt;Healthcare Common Procedure Coding System (HCPCS)&lt;/a&gt;
 codes using ‘sbiobert_base_cased_mli’ sentence embeddings. It also returns the domain information of the codes in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all_k_aux_labels&lt;/code&gt; parameter in the metadata of the result.&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCPCS Codes&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;a href=&quot;https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/24.Improved_Entity_Resolvers_in_SparkNLP_with_sBert.ipynb&quot; class=&quot;button button-orange button-orange-trans co button-icon&quot;&gt;Open in Colab&lt;/a&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/sbiobertresolve_hcpcs_en_3.2.3_2.4_1632909577033.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;documentAssembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner_chunk&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sbert_embedder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertSentenceEmbeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sbiobert_base_cased_mli'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'clinical/models'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner_chunk&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence_embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
&lt;span class=&quot;n&quot;&gt;hcpcs_resolver&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SentenceEntityResolverModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sbiobertresolve_hcpcs&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;clinical/models&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner_chunk&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;sentence_embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hcpcs_code&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setDistanceFunction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;EUCLIDEAN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;hcpcs_pipelineModel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PipelineModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;documentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sbert_embedder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;hcpcs_resolver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hcpcs_pipelineModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, unilateral, any size, any type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;documentAssembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()\&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)\&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner_chunk&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;sbert_embedder&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;BertSentenceEmbeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;'sbiobert_base_cased_ml&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;',&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;'e&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;','&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clinical&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;')\&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;ner_chunk&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;])\&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence_embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;hcpcs_resolver&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;SentenceEntityResolverModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sbiobertresolve_hcpcs&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;clinical/models&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;\&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;ner_chunk&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;, &lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;sentence_embeddings&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;\&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hcpcs_code&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)\&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setDistanceFunction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;EUCLIDEAN&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;hcpcs_pipelineModel&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;PipelineModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;documentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sbert_embedder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hcpcs_resolver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, unilateral, any size, any type&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;    
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;hcpcs_pipelineModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+--+---------------------------------------------------------------------------------------------------------+----------+----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+
|  |ner_chunk                                                                                                |hcpcs_code|all_codes                               |resolutions                                                                                                                                                                                                                                                                                                                                                                                                     |domain                                     |
+--+---------------------------------------------------------------------------------------------------------+----------+----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+
|0 |Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, unilateral, any size, any &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;|L8001     |[L8001, L8002, L8000, L8033, L8032, ...]|&lt;span class=&quot;s1&quot;&gt;'Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, unilateral, any size, any type'&lt;/span&gt;, &lt;span class=&quot;s1&quot;&gt;'Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, bilateral, any size, any type'&lt;/span&gt;, &lt;span class=&quot;s1&quot;&gt;'Breast prosthesis, mastectomy bra, without integrated breast prosthesis form, any size, any type'&lt;/span&gt;, &lt;span class=&quot;s1&quot;&gt;'Nipple prosthesis, custom fabricated, reusable, any material, any type, each'&lt;/span&gt;, ...|Device, Device, Device, Device, Device, ...|
+--+---------------------------------------------------------------------------------------------------------+----------+----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;sbiobertresolve_hcpcs&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP for Healthcare 3.2.3+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Licensed&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[sentence_embeddings]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[hcpcs_code]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;en&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>John Snow Labs</name></author><category term="entity_resolution" /><category term="hcpcs" /><category term="licensed" /><category term="en" /><category term="clinical" /><summary type="html">Description This model maps extracted medical entities to Healthcare Common Procedure Coding System (HCPCS) codes using ‘sbiobert_base_cased_mli’ sentence embeddings. It also returns the domain information of the codes in the all_k_aux_labels parameter in the metadata of the result. Predicted Entities HCPCS Codes Live Demo Open in Colab Download How to use PythonScalaNLU documentAssembler = DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained('sbiobert_base_cased_mli', 'en','clinical/models')\ .setInputCols([&quot;ner_chunk&quot;])\ .setOutputCol(&quot;sentence_embeddings&quot;) hcpcs_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_hcpcs&quot;, &quot;en&quot;, &quot;clinical/models&quot;) \ .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) \ .setOutputCol(&quot;hcpcs_code&quot;)\ .setDistanceFunction(&quot;EUCLIDEAN&quot;) hcpcs_pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, hcpcs_resolver]) res = hcpcs_pipelineModel.transform(spark.createDataFrame([[&quot;Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, unilateral, any size, any type&quot;]]).toDF(&quot;text&quot;)) val documentAssembler = DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;ner_chunk&quot;) val sbert_embedder = BertSentenceEmbeddings.pretrained('sbiobert_base_cased_mli', 'en','clinical/models')\ .setInputCols([&quot;ner_chunk&quot;])\ .setOutputCol(&quot;sentence_embeddings&quot;) val hcpcs_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_hcpcs&quot;, &quot;en&quot;, &quot;clinical/models&quot;) \ .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) \ .setOutputCol(&quot;hcpcs_code&quot;)\ .setDistanceFunction(&quot;EUCLIDEAN&quot;) val hcpcs_pipelineModel = new PipelineModel().setStages(Array(documentAssembler, sbert_embedder, hcpcs_resolver)) val data = Seq(&quot;Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, unilateral, any size, any type&quot;).toDF(&quot;text&quot;) val res = hcpcs_pipelineModel.transform(data) Results +--+---------------------------------------------------------------------------------------------------------+----------+----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+ | |ner_chunk |hcpcs_code|all_codes |resolutions |domain | +--+---------------------------------------------------------------------------------------------------------+----------+----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+ |0 |Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, unilateral, any size, any type|L8001 |[L8001, L8002, L8000, L8033, L8032, ...]|'Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, unilateral, any size, any type', 'Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, bilateral, any size, any type', 'Breast prosthesis, mastectomy bra, without integrated breast prosthesis form, any size, any type', 'Nipple prosthesis, custom fabricated, reusable, any material, any type, each', ...|Device, Device, Device, Device, Device, ...| +--+---------------------------------------------------------------------------------------------------------+----------+----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Model Information Model Name: sbiobertresolve_hcpcs Compatibility: Spark NLP for Healthcare 3.2.3+ License: Licensed Edition: Official Input Labels: [sentence_embeddings] Output Labels: [hcpcs_code] Language: en Case sensitive: false</summary></entry><entry><title type="html">RoBERTa Token Classification For Persian (roberta_token_classifier_zwnj_base_ner)</title><link href="/2021/09/28/roberta_token_classifier_zwnj_base_ner_fa.html" rel="alternate" type="text/html" title="RoBERTa Token Classification For Persian (roberta_token_classifier_zwnj_base_ner)" /><published>2021-09-28T00:00:00+00:00</published><updated>2021-09-28T00:00:00+00:00</updated><id>/2021/09/28/roberta_token_classifier_zwnj_base_ner_fa</id><content type="html" xml:base="/2021/09/28/roberta_token_classifier_zwnj_base_ner_fa.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;h1 id=&quot;robertaner&quot;&gt;RobertaNER&lt;/h1&gt;

&lt;p&gt;This model fine-tuned for the Named Entity Recognition (NER) task on a mixed NER dataset collected from &lt;a href=&quot;https://github.com/HaniehP/PersianNER&quot;&gt;ARMAN&lt;/a&gt;, &lt;a href=&quot;http://nsurl.org/2019-2/tasks/task-7-named-entity-recognition-ner-for-farsi/&quot;&gt;PEYMA&lt;/a&gt;, and &lt;a href=&quot;https://elisa-ie.github.io/wikiann/&quot;&gt;WikiANN&lt;/a&gt; that covered ten types of entities:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Date (DAT)&lt;/li&gt;
  &lt;li&gt;Event (EVE)&lt;/li&gt;
  &lt;li&gt;Facility (FAC)&lt;/li&gt;
  &lt;li&gt;Location (LOC)&lt;/li&gt;
  &lt;li&gt;Money (MON)&lt;/li&gt;
  &lt;li&gt;Organization (ORG)&lt;/li&gt;
  &lt;li&gt;Percent (PCT)&lt;/li&gt;
  &lt;li&gt;Person (PER)&lt;/li&gt;
  &lt;li&gt;Product (PRO)&lt;/li&gt;
  &lt;li&gt;Time (TIM)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataset-information&quot;&gt;Dataset Information&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Records&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;B-DAT&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;B-EVE&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;B-FAC&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;B-LOC&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;B-MON&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;B-ORG&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;B-PCT&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;B-PER&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;B-PRO&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;B-TIM&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;I-DAT&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;I-EVE&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;I-FAC&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;I-LOC&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;I-MON&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;I-ORG&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;I-PCT&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;I-PER&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;I-PRO&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;I-TIM&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Train&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;29133&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1423&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1487&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1400&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;13919&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;417&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;15926&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;355&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12347&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1855&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;150&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1947&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5018&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2421&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4118&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1059&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;19579&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;573&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7699&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1914&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;332&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Valid&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5142&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;267&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;253&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;250&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2362&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;100&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2651&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;64&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2173&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;317&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;19&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;373&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;799&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;387&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;717&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;270&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3260&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;101&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1382&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;303&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;35&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Test&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6049&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;407&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;256&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;248&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2886&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;98&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3216&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;94&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2646&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;318&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;43&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;568&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;888&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;408&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;858&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;263&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3967&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;141&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1707&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;296&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;78&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DAT&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EVE&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FAC&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LOC&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MON&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ORG&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PCT&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PER&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PRO&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TIM&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/roberta_token_classifier_zwnj_base_ner_fa_3.3.0_3.0_1632835301097.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RoBertaForTokenClassification&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'roberta_token_classifier_zwnj_base_ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'fa'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# since output column is IOB/IOB2 style, NerConverter can extract entities
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entities'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'در سال ۲۰۱۳ درگذشت و آندرتیکر و کین برای او مراسم یادبود گرفتند.'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;RoBertaForTokenClassification&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;roberta_token_classifier_zwnj_base_ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;fa&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// since output column is IOB/IOB2 style, NerConverter can extract entities&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;در&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;سال&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;۲۰۱۳&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;درگذشت&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;و&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;آندرتیکر&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;و&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;کین&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;برای&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;او&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;مراسم&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;یادبود&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;گرفتند.&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;roberta_token_classifier_zwnj_base_ner&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.3.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, document]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;fa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max sentense length:&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://huggingface.co/HooshvareLab/roberta-fa-zwnj-base-ner&quot;&gt;https://huggingface.co/HooshvareLab/roberta-fa-zwnj-base-ner&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## Evaluation&lt;/span&gt;

The following tables summarize the scores obtained by model overall and per each class.

&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;Overall&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;

|    Model   | accuracy | precision |  recall  |    f1    |
|:----------:|:--------:|:---------:|:--------:|:--------:|
|   Roberta  | 0.994849 |  0.949816 | 0.960235 | 0.954997 |

&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;Per entities&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;

|     	| number 	| precision 	|  recall  	|    f1    	|
|:---:	|:------:	|:---------:	|:--------:	|:--------:	|
| DAT 	|   407  	|  0.844869 	| 0.869779 	| 0.857143 	|
| EVE 	|   256  	|  0.948148 	| 1.000000 	| 0.973384 	|
| FAC 	|   248  	|  0.957529 	| 1.000000 	| 0.978304 	|
| LOC 	|  2884  	|  0.965422 	| 0.968100 	| 0.966759 	|
| MON 	|   98   	|  0.937500 	| 0.918367 	| 0.927835 	|
| ORG 	|  3216  	|  0.943662 	| 0.958333 	| 0.950941 	|
| PCT 	|   94   	|  1.000000 	| 0.968085 	| 0.983784 	|
| PER 	|  2646  	|  0.957030 	| 0.959562 	| 0.958294 	|
| PRO 	|   318  	|  0.963636 	| 1.000000 	| 0.981481 	|
| TIM 	|   43   	|  0.739130 	| 0.790698 	| 0.764045 	|

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="fa" /><category term="persian" /><category term="farsi" /><category term="ner" /><category term="roberta" /><category term="token_classification" /><category term="open_source" /><summary type="html">Description RobertaNER This model fine-tuned for the Named Entity Recognition (NER) task on a mixed NER dataset collected from ARMAN, PEYMA, and WikiANN that covered ten types of entities: Date (DAT) Event (EVE) Facility (FAC) Location (LOC) Money (MON) Organization (ORG) Percent (PCT) Person (PER) Product (PRO) Time (TIM) Dataset Information   Records B-DAT B-EVE B-FAC B-LOC B-MON B-ORG B-PCT B-PER B-PRO B-TIM I-DAT I-EVE I-FAC I-LOC I-MON I-ORG I-PCT I-PER I-PRO I-TIM Train 29133 1423 1487 1400 13919 417 15926 355 12347 1855 150 1947 5018 2421 4118 1059 19579 573 7699 1914 332 Valid 5142 267 253 250 2362 100 2651 64 2173 317 19 373 799 387 717 270 3260 101 1382 303 35 Test 6049 407 256 248 2886 98 3216 94 2646 318 43 568 888 408 858 263 3967 141 1707 296 78 Predicted Entities DAT, EVE, FAC, LOC, MON, ORG, PCT, PER, PRO, TIM Live Demo Open in Colab Download How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') tokenClassifier = RoBertaForTokenClassification \ .pretrained('roberta_token_classifier_zwnj_base_ner', 'fa') \ .setInputCols(['token', 'document']) \ .setOutputCol('ner') \ .setCaseSensitive(True) \ .setMaxSentenceLength(512) # since output column is IOB/IOB2 style, NerConverter can extract entities ner_converter = NerConverter() \ .setInputCols(['document', 'token', 'ner']) \ .setOutputCol('entities') pipeline = Pipeline(stages=[ document_assembler, tokenizer, tokenClassifier, ner_converter ]) example = spark.createDataFrame([['در سال ۲۰۱۳ درگذشت و آندرتیکر و کین برای او مراسم یادبود گرفتند.']]).toDF(&quot;text&quot;) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val tokenClassifier = RoBertaForTokenClassification.pretrained(&quot;roberta_token_classifier_zwnj_base_ner&quot;, &quot;fa&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(true) .setMaxSentenceLength(512) // since output column is IOB/IOB2 style, NerConverter can extract entities val ner_converter = NerConverter() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, tokenClassifier, ner_converter)) val example = Seq.empty[&quot;در سال ۲۰۱۳ درگذشت و آندرتیکر و کین برای او مراسم یادبود گرفتند.&quot;].toDS.toDF(&quot;text&quot;) val result = pipeline.fit(example).transform(example) Model Information Model Name: roberta_token_classifier_zwnj_base_ner Compatibility: Spark NLP 3.3.0+ License: Open Source Edition: Official Input Labels: [token, document] Output Labels: [ner] Language: fa Case sensitive: true Max sentense length: 512 Data Source https://huggingface.co/HooshvareLab/roberta-fa-zwnj-base-ner Benchmarking ## Evaluation The following tables summarize the scores obtained by model overall and per each class. **Overall** | Model | accuracy | precision | recall | f1 | |:----------:|:--------:|:---------:|:--------:|:--------:| | Roberta | 0.994849 | 0.949816 | 0.960235 | 0.954997 | **Per entities** | | number | precision | recall | f1 | |:---: |:------: |:---------: |:--------: |:--------: | | DAT | 407 | 0.844869 | 0.869779 | 0.857143 | | EVE | 256 | 0.948148 | 1.000000 | 0.973384 | | FAC | 248 | 0.957529 | 1.000000 | 0.978304 | | LOC | 2884 | 0.965422 | 0.968100 | 0.966759 | | MON | 98 | 0.937500 | 0.918367 | 0.927835 | | ORG | 3216 | 0.943662 | 0.958333 | 0.950941 | | PCT | 94 | 1.000000 | 0.968085 | 0.983784 | | PER | 2646 | 0.957030 | 0.959562 | 0.958294 | | PRO | 318 | 0.963636 | 1.000000 | 0.981481 | | TIM | 43 | 0.739130 | 0.790698 | 0.764045 |</summary></entry><entry><title type="html">XLM-RoBERTa Token Classification Base - NER XTREME (xlm_roberta_token_classifier_ner_40_lang)</title><link href="/2021/09/28/xlm_roberta_token_classifier_ner_40_lang_xx.html" rel="alternate" type="text/html" title="XLM-RoBERTa Token Classification Base - NER XTREME (xlm_roberta_token_classifier_ner_40_lang)" /><published>2021-09-28T00:00:00+00:00</published><updated>2021-09-28T00:00:00+00:00</updated><id>/2021/09/28/xlm_roberta_token_classifier_ner_40_lang_xx</id><content type="html" xml:base="/2021/09/28/xlm_roberta_token_classifier_ner_40_lang_xx.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;h1 id=&quot;xlm-r--ner&quot;&gt;XLM-R + NER&lt;/h1&gt;

&lt;p&gt;This model is a fine-tuned  &lt;a href=&quot;https://arxiv.org/abs/1911.02116&quot;&gt;XLM-Roberta-base&lt;/a&gt; over the 40 languages proposed in &lt;a href=&quot;https://github.com/google-research/xtreme&quot;&gt;XTREME&lt;/a&gt; from &lt;a href=&quot;https://aclweb.org/anthology/P17-1178&quot;&gt;Wikiann&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The covered labels are:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;LOC
ORG
PER
O
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LOC&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ORG&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PER&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/xlm_roberta_token_classifier_ner_40_lang_xx_3.3.0_3.0_1632835906778.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XlmRoBertaForTokenClassification&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xlm_roberta_token_classifier_ner_40_lang'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'xx'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# since output column is IOB/IOB2 style, NerConverter can extract entities
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entities'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'My name is John!'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;XlmRoBertaForTokenClassification&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;xlm_roberta_token_classifier_ner_40_lang&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xx&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// since output column is IOB/IOB2 style, NerConverter can extract entities&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;My&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;John!&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;xlm_roberta_token_classifier_ner_40_lang&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.3.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, document]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;xx&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max sentense length:&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://huggingface.co/jplu/tf-xlm-r-ner-40-lang&quot;&gt;https://huggingface.co/jplu/tf-xlm-r-ner-40-lang&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## Metrics on evaluation set:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### Average over the 40 languages&lt;/span&gt;
Number of documents: 262300

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.81      0.81      0.81    102452
  PER       0.90      0.91      0.91    108978
  LOC       0.86      0.89      0.87    121868
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.86      0.87      0.87    333298
macro avg       0.86      0.87      0.87    333298&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Afrikaans
Number of documents: 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.89      0.88      0.88       582
  PER       0.89      0.97      0.93       369
  LOC       0.84      0.90      0.86       518
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.87      0.91      0.89      1469
macro avg       0.87      0.91      0.89      1469&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Arabic
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.83      0.84      0.84      3507
  PER       0.90      0.91      0.91      3643
  LOC       0.88      0.89      0.88      3604
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.87      0.88      0.88     10754
macro avg       0.87      0.88      0.88     10754&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Basque
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  LOC       0.88      0.93      0.91      5228
  ORG       0.86      0.81      0.83      3654
  PER       0.91      0.91      0.91      4072
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.89      0.89      0.89     12954
macro avg       0.89      0.89      0.89     12954&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Bengali
Number of documents: 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.86      0.89      0.87       325
  LOC       0.91      0.91      0.91       406
  PER       0.96      0.95      0.95       364
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.91      0.92      0.91      1095
macro avg       0.91      0.92      0.91      1095&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Bulgarian
Number of documents: 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.86      0.83      0.84      3661
  PER       0.92      0.95      0.94      4006
  LOC       0.92      0.95      0.94      6449
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.91      0.92      0.91     14116
macro avg       0.91      0.92      0.91     14116&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Burmese
Number of documents: 100
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  LOC       0.60      0.86      0.71        37
  ORG       0.68      0.63      0.66        30
  PER       0.44      0.44      0.44        36
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.57      0.65      0.61       103
macro avg       0.57      0.65      0.60       103&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Chinese
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.70      0.69      0.70      4022
  LOC       0.76      0.81      0.78      3830
  PER       0.84      0.84      0.84      3706
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.76      0.78      0.77     11558
macro avg       0.76      0.78      0.77     11558&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Dutch
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.87      0.87      0.87      3930
  PER       0.95      0.95      0.95      4377
  LOC       0.91      0.92      0.91      4813
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.91      0.92      0.91     13120
macro avg       0.91      0.92      0.91     13120&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### English
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  LOC       0.83      0.84      0.84      4781
  PER       0.89      0.90      0.89      4559
  ORG       0.75      0.75      0.75      4633
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.82      0.83      0.83     13973
macro avg       0.82      0.83      0.83     13973&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Estonian
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  LOC       0.89      0.92      0.91      5654
  ORG       0.85      0.85      0.85      3878
  PER       0.94      0.94      0.94      4026
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.90      0.91      0.90     13558
macro avg       0.90      0.91      0.90     13558&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Finnish
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.84      0.83      0.84      4104
  LOC       0.88      0.90      0.89      5307
  PER       0.95      0.94      0.94      4519
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.89      0.89      0.89     13930
macro avg       0.89      0.89      0.89     13930&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### French
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  LOC       0.90      0.89      0.89      4808
  ORG       0.84      0.87      0.85      3876
  PER       0.94      0.93      0.94      4249
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.89      0.90      0.90     12933
macro avg       0.89      0.90      0.90     12933&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Georgian
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  PER       0.90      0.91      0.90      3964
  ORG       0.83      0.77      0.80      3757
  LOC       0.82      0.88      0.85      4894
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.84      0.86      0.85     12615
macro avg       0.84      0.86      0.85     12615&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### German
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  LOC       0.85      0.90      0.87      4939
  PER       0.94      0.91      0.92      4452
  ORG       0.79      0.78      0.79      4247
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.86      0.86      0.86     13638
macro avg       0.86      0.86      0.86     13638&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Greek
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.86      0.85      0.85      3771
  LOC       0.88      0.91      0.90      4436
  PER       0.91      0.93      0.92      3894
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.88      0.90      0.89     12101
macro avg       0.88      0.90      0.89     12101&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Hebrew
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  PER       0.87      0.88      0.87      4206
  ORG       0.76      0.75      0.76      4190
  LOC       0.85      0.85      0.85      4538
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.83      0.83      0.83     12934
macro avg       0.82      0.83      0.83     12934&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Hindi
Number of documents: 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.78      0.81      0.79       362
  LOC       0.83      0.85      0.84       422
  PER       0.90      0.95      0.92       427
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.84      0.87      0.85      1211
macro avg       0.84      0.87      0.85      1211&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Hungarian
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  PER       0.95      0.95      0.95      4347
  ORG       0.87      0.88      0.87      3988
  LOC       0.90      0.92      0.91      5544
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.91      0.92      0.91     13879
macro avg       0.91      0.92      0.91     13879&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Indonesian
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.88      0.89      0.88      3735
  LOC       0.93      0.95      0.94      3694
  PER       0.93      0.93      0.93      3947
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.91      0.92      0.92     11376
macro avg       0.91      0.92      0.92     11376&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Italian
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  LOC       0.88      0.88      0.88      4592
  ORG       0.86      0.86      0.86      4088
  PER       0.96      0.96      0.96      4732
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.90      0.90      0.90     13412
macro avg       0.90      0.90      0.90     13412&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Japanese
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.62      0.61      0.62      4184
  PER       0.76      0.81      0.78      3812
  LOC       0.68      0.74      0.71      4281
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.69      0.72      0.70     12277
macro avg       0.69      0.72      0.70     12277&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Javanese
Number of documents: 100
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.79      0.80      0.80        46
  PER       0.81      0.96      0.88        26
  LOC       0.75      0.75      0.75        40
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.78      0.82      0.80       112
macro avg       0.78      0.82      0.80       112&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Kazakh
Number of documents: 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.76      0.61      0.68       307
  LOC       0.78      0.90      0.84       461
  PER       0.87      0.91      0.89       367
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.81      0.83      0.82      1135
macro avg       0.81      0.83      0.81      1135&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Korean
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  LOC       0.86      0.89      0.88      5097
  ORG       0.79      0.74      0.77      4218
  PER       0.83      0.86      0.84      4014
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.83      0.83      0.83     13329
macro avg       0.83      0.83      0.83     13329&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Malay
Number of documents: 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.87      0.89      0.88       368
  PER       0.92      0.91      0.91       366
  LOC       0.94      0.95      0.95       354
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.91      0.92      0.91      1088
macro avg       0.91      0.92      0.91      1088&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Malayalam
Number of documents: 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.75      0.74      0.75       347
  PER       0.84      0.89      0.86       417
  LOC       0.74      0.75      0.75       391
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.78      0.80      0.79      1155
macro avg       0.78      0.80      0.79      1155&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Marathi
Number of documents: 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  PER       0.89      0.94      0.92       394
  LOC       0.82      0.84      0.83       457
  ORG       0.84      0.78      0.81       339
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.85      0.86      0.85      1190
macro avg       0.85      0.86      0.85      1190&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Persian
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  PER       0.93      0.92      0.93      3540
  LOC       0.93      0.93      0.93      3584
  ORG       0.89      0.92      0.90      3370
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.92      0.92      0.92     10494
macro avg       0.92      0.92      0.92     10494&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Portuguese
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  LOC       0.90      0.91      0.91      4819
  PER       0.94      0.92      0.93      4184
  ORG       0.84      0.88      0.86      3670
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.89      0.91      0.90     12673
macro avg       0.90      0.91      0.90     12673&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Russian
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  PER       0.93      0.96      0.95      3574
  LOC       0.87      0.89      0.88      4619
  ORG       0.82      0.80      0.81      3858
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.87      0.88      0.88     12051
macro avg       0.87      0.88      0.88     12051&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Spanish
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  PER       0.95      0.93      0.94      3891
  ORG       0.86      0.88      0.87      3709
  LOC       0.89      0.91      0.90      4553
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.90      0.91      0.90     12153
macro avg       0.90      0.91      0.90     12153&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Swahili
Number of documents: 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.82      0.85      0.83       349
  PER       0.95      0.92      0.94       403
  LOC       0.86      0.89      0.88       450
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.88      0.89      0.88      1202
macro avg       0.88      0.89      0.88      1202&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Tagalog
Number of documents: 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  LOC       0.90      0.91      0.90       338
  ORG       0.83      0.91      0.87       339
  PER       0.96      0.93      0.95       350
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.90      0.92      0.91      1027
macro avg       0.90      0.92      0.91      1027&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Tamil
Number of documents: 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  PER       0.90      0.92      0.91       392
  ORG       0.77      0.76      0.76       370
  LOC       0.78      0.81      0.79       421
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.82      0.83      0.82      1183
macro avg       0.82      0.83      0.82      1183&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Telugu
Number of documents: 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.67      0.55      0.61       347
  LOC       0.78      0.87      0.82       453
  PER       0.73      0.86      0.79       393
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.74      0.77      0.76      1193
macro avg       0.73      0.77      0.75      1193&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Thai
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  LOC       0.63      0.76      0.69      3928
  PER       0.78      0.83      0.80      6537
  ORG       0.59      0.59      0.59      4257
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.68      0.74      0.71     14722
macro avg       0.68      0.74      0.71     14722&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Turkish
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  PER       0.94      0.94      0.94      4337
  ORG       0.88      0.89      0.88      4094
  LOC       0.90      0.92      0.91      4929
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.90      0.92      0.91     13360
macro avg       0.91      0.92      0.91     13360&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Urdu
Number of documents: 1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  LOC       0.90      0.95      0.93       352
  PER       0.96      0.96      0.96       333
  ORG       0.91      0.90      0.90       326
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.92      0.94      0.93      1011
macro avg       0.92      0.94      0.93      1011&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Vietnamese
Number of documents: 10000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  ORG       0.86      0.87      0.86      3579
  LOC       0.88      0.91      0.90      3811
  PER       0.92      0.93      0.93      3717
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.89      0.90      0.90     11107
macro avg       0.89      0.90      0.90     11107&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
### Yoruba
Number of documents: 100
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       precision    recall  f1-score   support

  LOC       0.54      0.72      0.62        36
  ORG       0.58      0.31      0.41        35
  PER       0.77      1.00      0.87        36
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;micro avg       0.64      0.68      0.66       107
macro avg       0.63      0.68      0.63       107&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;</content><author><name>John Snow Labs</name></author><category term="xlm_roberta" /><category term="multilingual" /><category term="xtreme" /><category term="ner" /><category term="token_classification" /><category term="xx" /><category term="open_source" /><summary type="html">Description XLM-R + NER This model is a fine-tuned XLM-Roberta-base over the 40 languages proposed in XTREME from Wikiann. The covered labels are: LOC ORG PER O Predicted Entities LOC, ORG, PER, O Live Demo Open in Colab Download How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') tokenClassifier = XlmRoBertaForTokenClassification \ .pretrained('xlm_roberta_token_classifier_ner_40_lang', 'xx') \ .setInputCols(['token', 'document']) \ .setOutputCol('ner') \ .setCaseSensitive(True) \ .setMaxSentenceLength(512) # since output column is IOB/IOB2 style, NerConverter can extract entities ner_converter = NerConverter() \ .setInputCols(['document', 'token', 'ner']) \ .setOutputCol('entities') pipeline = Pipeline(stages=[ document_assembler, tokenizer, tokenClassifier, ner_converter ]) example = spark.createDataFrame([['My name is John!']]).toDF(&quot;text&quot;) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val tokenClassifier = XlmRoBertaForTokenClassification.pretrained(&quot;xlm_roberta_token_classifier_ner_40_lang&quot;, &quot;xx&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(true) .setMaxSentenceLength(512) // since output column is IOB/IOB2 style, NerConverter can extract entities val ner_converter = NerConverter() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, tokenClassifier, ner_converter)) val example = Seq.empty[&quot;My name is John!&quot;].toDS.toDF(&quot;text&quot;) val result = pipeline.fit(example).transform(example) Model Information Model Name: xlm_roberta_token_classifier_ner_40_lang Compatibility: Spark NLP 3.3.0+ License: Open Source Edition: Official Input Labels: [token, document] Output Labels: [ner] Language: xx Case sensitive: true Max sentense length: 512 Data Source https://huggingface.co/jplu/tf-xlm-r-ner-40-lang Benchmarking ## Metrics on evaluation set: ### Average over the 40 languages Number of documents: 262300 precision recall f1-score support ORG 0.81 0.81 0.81 102452 PER 0.90 0.91 0.91 108978 LOC 0.86 0.89 0.87 121868 micro avg 0.86 0.87 0.87 333298 macro avg 0.86 0.87 0.87 333298 ### Afrikaans Number of documents: 1000 precision recall f1-score support ORG 0.89 0.88 0.88 582 PER 0.89 0.97 0.93 369 LOC 0.84 0.90 0.86 518 micro avg 0.87 0.91 0.89 1469 macro avg 0.87 0.91 0.89 1469 ### Arabic Number of documents: 10000 precision recall f1-score support ORG 0.83 0.84 0.84 3507 PER 0.90 0.91 0.91 3643 LOC 0.88 0.89 0.88 3604 micro avg 0.87 0.88 0.88 10754 macro avg 0.87 0.88 0.88 10754 ### Basque Number of documents: 10000 precision recall f1-score support LOC 0.88 0.93 0.91 5228 ORG 0.86 0.81 0.83 3654 PER 0.91 0.91 0.91 4072 micro avg 0.89 0.89 0.89 12954 macro avg 0.89 0.89 0.89 12954 ### Bengali Number of documents: 1000 precision recall f1-score support ORG 0.86 0.89 0.87 325 LOC 0.91 0.91 0.91 406 PER 0.96 0.95 0.95 364 micro avg 0.91 0.92 0.91 1095 macro avg 0.91 0.92 0.91 1095 ### Bulgarian Number of documents: 1000 precision recall f1-score support ORG 0.86 0.83 0.84 3661 PER 0.92 0.95 0.94 4006 LOC 0.92 0.95 0.94 6449 micro avg 0.91 0.92 0.91 14116 macro avg 0.91 0.92 0.91 14116 ### Burmese Number of documents: 100 precision recall f1-score support LOC 0.60 0.86 0.71 37 ORG 0.68 0.63 0.66 30 PER 0.44 0.44 0.44 36 micro avg 0.57 0.65 0.61 103 macro avg 0.57 0.65 0.60 103 ### Chinese Number of documents: 10000 precision recall f1-score support ORG 0.70 0.69 0.70 4022 LOC 0.76 0.81 0.78 3830 PER 0.84 0.84 0.84 3706 micro avg 0.76 0.78 0.77 11558 macro avg 0.76 0.78 0.77 11558 ### Dutch Number of documents: 10000 precision recall f1-score support ORG 0.87 0.87 0.87 3930 PER 0.95 0.95 0.95 4377 LOC 0.91 0.92 0.91 4813 micro avg 0.91 0.92 0.91 13120 macro avg 0.91 0.92 0.91 13120 ### English Number of documents: 10000 precision recall f1-score support LOC 0.83 0.84 0.84 4781 PER 0.89 0.90 0.89 4559 ORG 0.75 0.75 0.75 4633 micro avg 0.82 0.83 0.83 13973 macro avg 0.82 0.83 0.83 13973 ### Estonian Number of documents: 10000 precision recall f1-score support LOC 0.89 0.92 0.91 5654 ORG 0.85 0.85 0.85 3878 PER 0.94 0.94 0.94 4026 micro avg 0.90 0.91 0.90 13558 macro avg 0.90 0.91 0.90 13558 ### Finnish Number of documents: 10000 precision recall f1-score support ORG 0.84 0.83 0.84 4104 LOC 0.88 0.90 0.89 5307 PER 0.95 0.94 0.94 4519 micro avg 0.89 0.89 0.89 13930 macro avg 0.89 0.89 0.89 13930 ### French Number of documents: 10000 precision recall f1-score support LOC 0.90 0.89 0.89 4808 ORG 0.84 0.87 0.85 3876 PER 0.94 0.93 0.94 4249 micro avg 0.89 0.90 0.90 12933 macro avg 0.89 0.90 0.90 12933 ### Georgian Number of documents: 10000 precision recall f1-score support PER 0.90 0.91 0.90 3964 ORG 0.83 0.77 0.80 3757 LOC 0.82 0.88 0.85 4894 micro avg 0.84 0.86 0.85 12615 macro avg 0.84 0.86 0.85 12615 ### German Number of documents: 10000 precision recall f1-score support LOC 0.85 0.90 0.87 4939 PER 0.94 0.91 0.92 4452 ORG 0.79 0.78 0.79 4247 micro avg 0.86 0.86 0.86 13638 macro avg 0.86 0.86 0.86 13638 ### Greek Number of documents: 10000 precision recall f1-score support ORG 0.86 0.85 0.85 3771 LOC 0.88 0.91 0.90 4436 PER 0.91 0.93 0.92 3894 micro avg 0.88 0.90 0.89 12101 macro avg 0.88 0.90 0.89 12101 ### Hebrew Number of documents: 10000 precision recall f1-score support PER 0.87 0.88 0.87 4206 ORG 0.76 0.75 0.76 4190 LOC 0.85 0.85 0.85 4538 micro avg 0.83 0.83 0.83 12934 macro avg 0.82 0.83 0.83 12934 ### Hindi Number of documents: 1000 precision recall f1-score support ORG 0.78 0.81 0.79 362 LOC 0.83 0.85 0.84 422 PER 0.90 0.95 0.92 427 micro avg 0.84 0.87 0.85 1211 macro avg 0.84 0.87 0.85 1211 ### Hungarian Number of documents: 10000 precision recall f1-score support PER 0.95 0.95 0.95 4347 ORG 0.87 0.88 0.87 3988 LOC 0.90 0.92 0.91 5544 micro avg 0.91 0.92 0.91 13879 macro avg 0.91 0.92 0.91 13879 ### Indonesian Number of documents: 10000 precision recall f1-score support ORG 0.88 0.89 0.88 3735 LOC 0.93 0.95 0.94 3694 PER 0.93 0.93 0.93 3947 micro avg 0.91 0.92 0.92 11376 macro avg 0.91 0.92 0.92 11376 ### Italian Number of documents: 10000 precision recall f1-score support LOC 0.88 0.88 0.88 4592 ORG 0.86 0.86 0.86 4088 PER 0.96 0.96 0.96 4732 micro avg 0.90 0.90 0.90 13412 macro avg 0.90 0.90 0.90 13412 ### Japanese Number of documents: 10000 precision recall f1-score support ORG 0.62 0.61 0.62 4184 PER 0.76 0.81 0.78 3812 LOC 0.68 0.74 0.71 4281 micro avg 0.69 0.72 0.70 12277 macro avg 0.69 0.72 0.70 12277 ### Javanese Number of documents: 100 precision recall f1-score support ORG 0.79 0.80 0.80 46 PER 0.81 0.96 0.88 26 LOC 0.75 0.75 0.75 40 micro avg 0.78 0.82 0.80 112 macro avg 0.78 0.82 0.80 112 ### Kazakh Number of documents: 1000 precision recall f1-score support ORG 0.76 0.61 0.68 307 LOC 0.78 0.90 0.84 461 PER 0.87 0.91 0.89 367 micro avg 0.81 0.83 0.82 1135 macro avg 0.81 0.83 0.81 1135 ### Korean Number of documents: 10000 precision recall f1-score support LOC 0.86 0.89 0.88 5097 ORG 0.79 0.74 0.77 4218 PER 0.83 0.86 0.84 4014 micro avg 0.83 0.83 0.83 13329 macro avg 0.83 0.83 0.83 13329 ### Malay Number of documents: 1000 precision recall f1-score support ORG 0.87 0.89 0.88 368 PER 0.92 0.91 0.91 366 LOC 0.94 0.95 0.95 354 micro avg 0.91 0.92 0.91 1088 macro avg 0.91 0.92 0.91 1088 ### Malayalam Number of documents: 1000 precision recall f1-score support ORG 0.75 0.74 0.75 347 PER 0.84 0.89 0.86 417 LOC 0.74 0.75 0.75 391 micro avg 0.78 0.80 0.79 1155 macro avg 0.78 0.80 0.79 1155 ### Marathi Number of documents: 1000 precision recall f1-score support PER 0.89 0.94 0.92 394 LOC 0.82 0.84 0.83 457 ORG 0.84 0.78 0.81 339 micro avg 0.85 0.86 0.85 1190 macro avg 0.85 0.86 0.85 1190 ### Persian Number of documents: 10000 precision recall f1-score support PER 0.93 0.92 0.93 3540 LOC 0.93 0.93 0.93 3584 ORG 0.89 0.92 0.90 3370 micro avg 0.92 0.92 0.92 10494 macro avg 0.92 0.92 0.92 10494 ### Portuguese Number of documents: 10000 precision recall f1-score support LOC 0.90 0.91 0.91 4819 PER 0.94 0.92 0.93 4184 ORG 0.84 0.88 0.86 3670 micro avg 0.89 0.91 0.90 12673 macro avg 0.90 0.91 0.90 12673 ### Russian Number of documents: 10000 precision recall f1-score support PER 0.93 0.96 0.95 3574 LOC 0.87 0.89 0.88 4619 ORG 0.82 0.80 0.81 3858 micro avg 0.87 0.88 0.88 12051 macro avg 0.87 0.88 0.88 12051 ### Spanish Number of documents: 10000 precision recall f1-score support PER 0.95 0.93 0.94 3891 ORG 0.86 0.88 0.87 3709 LOC 0.89 0.91 0.90 4553 micro avg 0.90 0.91 0.90 12153 macro avg 0.90 0.91 0.90 12153 ### Swahili Number of documents: 1000 precision recall f1-score support ORG 0.82 0.85 0.83 349 PER 0.95 0.92 0.94 403 LOC 0.86 0.89 0.88 450 micro avg 0.88 0.89 0.88 1202 macro avg 0.88 0.89 0.88 1202 ### Tagalog Number of documents: 1000 precision recall f1-score support LOC 0.90 0.91 0.90 338 ORG 0.83 0.91 0.87 339 PER 0.96 0.93 0.95 350 micro avg 0.90 0.92 0.91 1027 macro avg 0.90 0.92 0.91 1027 ### Tamil Number of documents: 1000 precision recall f1-score support PER 0.90 0.92 0.91 392 ORG 0.77 0.76 0.76 370 LOC 0.78 0.81 0.79 421 micro avg 0.82 0.83 0.82 1183 macro avg 0.82 0.83 0.82 1183 ### Telugu Number of documents: 1000 precision recall f1-score support ORG 0.67 0.55 0.61 347 LOC 0.78 0.87 0.82 453 PER 0.73 0.86 0.79 393 micro avg 0.74 0.77 0.76 1193 macro avg 0.73 0.77 0.75 1193 ### Thai Number of documents: 10000 precision recall f1-score support LOC 0.63 0.76 0.69 3928 PER 0.78 0.83 0.80 6537 ORG 0.59 0.59 0.59 4257 micro avg 0.68 0.74 0.71 14722 macro avg 0.68 0.74 0.71 14722 ### Turkish Number of documents: 10000 precision recall f1-score support PER 0.94 0.94 0.94 4337 ORG 0.88 0.89 0.88 4094 LOC 0.90 0.92 0.91 4929 micro avg 0.90 0.92 0.91 13360 macro avg 0.91 0.92 0.91 13360 ### Urdu Number of documents: 1000 precision recall f1-score support LOC 0.90 0.95 0.93 352 PER 0.96 0.96 0.96 333 ORG 0.91 0.90 0.90 326 micro avg 0.92 0.94 0.93 1011 macro avg 0.92 0.94 0.93 1011 ### Vietnamese Number of documents: 10000 precision recall f1-score support ORG 0.86 0.87 0.86 3579 LOC 0.88 0.91 0.90 3811 PER 0.92 0.93 0.93 3717 micro avg 0.89 0.90 0.90 11107 macro avg 0.89 0.90 0.90 11107 ### Yoruba Number of documents: 100 precision recall f1-score support LOC 0.54 0.72 0.62 36 ORG 0.58 0.31 0.41 35 PER 0.77 1.00 0.87 36 micro avg 0.64 0.68 0.66 107 macro avg 0.63 0.68 0.63 107 ```</summary></entry><entry><title type="html">XLNet Token Classification Base - NER CoNLL (xlnet_base_token_classifier_conll03)</title><link href="/2021/09/28/xlnet_base_token_classifier_conll03_en.html" rel="alternate" type="text/html" title="XLNet Token Classification Base - NER CoNLL (xlnet_base_token_classifier_conll03)" /><published>2021-09-28T00:00:00+00:00</published><updated>2021-09-28T00:00:00+00:00</updated><id>/2021/09/28/xlnet_base_token_classifier_conll03_en</id><content type="html" xml:base="/2021/09/28/xlnet_base_token_classifier_conll03_en.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;XLNet Model&lt;/code&gt; with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;xlnet_base_token_classifier_conll03&lt;/strong&gt; is a fine-tuned XLNet model that is ready to use for &lt;strong&gt;Named Entity Recognition&lt;/strong&gt; and achieves &lt;strong&gt;state-of-the-art performance&lt;/strong&gt; for the NER task. This model has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER), and Miscellaneous (MISC).&lt;/p&gt;

&lt;p&gt;We used &lt;a href=&quot;https://huggingface.co/transformers/model_doc/xlnet.html#tfxlnetfortokenclassification&quot;&gt;TFXLNetForTokenClassification&lt;/a&gt; to train this model and used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;XlnetForTokenClassification&lt;/code&gt; annotator in Spark NLP 🚀 for prediction at scale!&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/xlnet_base_token_classifier_conll03_en_3.3.0_3.0_1632831424304.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XlnetForTokenClassification&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xlnet_base_token_classifier_conll03'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# since output column is IOB/IOB2 style, NerConverter can extract entities
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entities'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'My name is John!'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;XlnetForTokenClassification&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;xlnet_base_token_classifier_conll03&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// since output column is IOB/IOB2 style, NerConverter can extract entities&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;My&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;John!&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+------------------------------------------------------------------------------------+
 |result                                                                              |
 +------------------------------------------------------------------------------------+
 |[B-PER, I-PER, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O, B-LOC]|
 +------------------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;xlnet_base_token_classifier_conll03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.3.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, document]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;en&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max sentense length:&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.clips.uantwerpen.be/conll2003/ner/&quot;&gt;https://www.clips.uantwerpen.be/conll2003/ner/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;             precision    recall  f1-score   support

       B-LOC       0.94      0.97      0.96      1837
      B-MISC       0.84      0.92      0.88       922
       B-ORG       0.89      0.94      0.91      1341
       B-PER       0.96      0.97      0.97      1842
       I-LOC       0.92      0.93      0.92       257
      I-MISC       0.84      0.81      0.82       346
       I-ORG       0.93      0.89      0.91       751
       I-PER       0.96      0.98      0.97      1307
           O       1.00      0.99      1.00     42759

    accuracy                           0.98     51362
   macro avg       0.92      0.93      0.93     51362
weighted avg       0.99      0.98      0.99     51362



processed 51362 tokens with 5942 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 6219 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 5550.
accuracy:  94.57%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  98.49%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  89.24%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  93.40%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  91.28
              LOC: precision:  93.30%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  96.30%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  94.78  1896
             MISC: precision:  79.47%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  88.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  83.60  1023
              ORG: precision:  84.71%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  90.08%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  87.31  1426
              PER: precision:  93.92%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  95.55%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  94.73  1874
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="ner" /><category term="en" /><category term="english" /><category term="open_source" /><category term="xlnet" /><category term="base" /><category term="token_classification" /><summary type="html">Description XLNet Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. xlnet_base_token_classifier_conll03 is a fine-tuned XLNet model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. This model has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER), and Miscellaneous (MISC). We used TFXLNetForTokenClassification to train this model and used XlnetForTokenClassification annotator in Spark NLP 🚀 for prediction at scale! Predicted Entities Live Demo Open in Colab Download How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') tokenClassifier = XlnetForTokenClassification \ .pretrained('xlnet_base_token_classifier_conll03', 'en') \ .setInputCols(['token', 'document']) \ .setOutputCol('ner') \ .setCaseSensitive(True) \ .setMaxSentenceLength(512) # since output column is IOB/IOB2 style, NerConverter can extract entities ner_converter = NerConverter() \ .setInputCols(['document', 'token', 'ner']) \ .setOutputCol('entities') pipeline = Pipeline(stages=[ document_assembler, tokenizer, tokenClassifier, ner_converter ]) example = spark.createDataFrame([['My name is John!']]).toDF(&quot;text&quot;) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val tokenClassifier = XlnetForTokenClassification.pretrained(&quot;xlnet_base_token_classifier_conll03&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(true) .setMaxSentenceLength(512) // since output column is IOB/IOB2 style, NerConverter can extract entities val ner_converter = NerConverter() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, tokenClassifier, ner_converter)) val example = Seq.empty[&quot;My name is John!&quot;].toDS.toDF(&quot;text&quot;) val result = pipeline.fit(example).transform(example) Results +------------------------------------------------------------------------------------+ |result | +------------------------------------------------------------------------------------+ |[B-PER, I-PER, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O, B-LOC]| +------------------------------------------------------------------------------------+ Model Information Model Name: xlnet_base_token_classifier_conll03 Compatibility: Spark NLP 3.3.0+ License: Open Source Edition: Official Input Labels: [token, document] Output Labels: [ner] Language: en Case sensitive: true Max sentense length: 512 Data Source https://www.clips.uantwerpen.be/conll2003/ner/ Benchmarking precision recall f1-score support B-LOC 0.94 0.97 0.96 1837 B-MISC 0.84 0.92 0.88 922 B-ORG 0.89 0.94 0.91 1341 B-PER 0.96 0.97 0.97 1842 I-LOC 0.92 0.93 0.92 257 I-MISC 0.84 0.81 0.82 346 I-ORG 0.93 0.89 0.91 751 I-PER 0.96 0.98 0.97 1307 O 1.00 0.99 1.00 42759 accuracy 0.98 51362 macro avg 0.92 0.93 0.93 51362 weighted avg 0.99 0.98 0.99 51362 processed 51362 tokens with 5942 phrases; found: 6219 phrases; correct: 5550. accuracy: 94.57%; (non-O) accuracy: 98.49%; precision: 89.24%; recall: 93.40%; FB1: 91.28 LOC: precision: 93.30%; recall: 96.30%; FB1: 94.78 1896 MISC: precision: 79.47%; recall: 88.18%; FB1: 83.60 1023 ORG: precision: 84.71%; recall: 90.08%; FB1: 87.31 1426 PER: precision: 93.92%; recall: 95.55%; FB1: 94.73 1874</summary></entry></feed>