<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-02-13T12:55:00+00:00</updated><id>/feed.xml</id><title type="html">Spark NLP</title><subtitle>High Performance NLP with Apache Spark
</subtitle><author><name>{&quot;type&quot;=&gt;nil, &quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;googleplus&quot;=&gt;nil, &quot;telegram&quot;=&gt;nil, &quot;medium&quot;=&gt;nil, &quot;zhihu&quot;=&gt;nil, &quot;douban&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;npm&quot;=&gt;nil}</name></author><entry><title type="html">Extract Demographic Entities from Social Determinants of Health Texts</title><link href="/2023/02/10/ner_sdoh_demographics_wip_en.html" rel="alternate" type="text/html" title="Extract Demographic Entities from Social Determinants of Health Texts" /><published>2023-02-10T00:00:00+00:00</published><updated>2023-02-10T00:00:00+00:00</updated><id>/2023/02/10/ner_sdoh_demographics_wip_en</id><content type="html" xml:base="/2023/02/10/ner_sdoh_demographics_wip_en.html">## Description

This model extracts demographic information related to Social Determinants of Health from various kinds of biomedical documents.

## Predicted Entities

`Family_Member`, `Age`, `Gender`, `Geographic_Entity`, `Race_Ethnicity`, `Language`, `Spiritual_Beliefs`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_sdoh_demographics_wip_en_4.2.8_3.0_1675998706136.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/clinical/models/ner_sdoh_demographics_wip_en_4.2.8_3.0_1675998706136.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
document_assembler = DocumentAssembler()\
    .setInputCol(&quot;text&quot;)\
    .setOutputCol(&quot;document&quot;)

sentence_detector = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;, &quot;en&quot;)\
    .setInputCols([&quot;document&quot;])\
    .setOutputCol(&quot;sentence&quot;)

tokenizer = Tokenizer()\
    .setInputCols([&quot;sentence&quot;])\
    .setOutputCol(&quot;token&quot;)

clinical_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\
    .setInputCols([&quot;sentence&quot;, &quot;token&quot;])\
    .setOutputCol(&quot;embeddings&quot;)

ner_model = MedicalNerModel.pretrained(&quot;ner_sdoh_demographics_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\
    .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;])\
    .setOutputCol(&quot;ner&quot;)

ner_converter = NerConverterInternal()\
    .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;])\
    .setOutputCol(&quot;ner_chunk&quot;)

pipeline = Pipeline(stages=[
    document_assembler, 
    sentence_detector,
    tokenizer,
    clinical_embeddings,
    ner_model,
    ner_converter   
    ])

sample_texts = [&quot;SOCIAL HISTORY: He is a former tailor from Korea.&quot;,
             &quot;He lives alone,single and no children.&quot;,
             &quot;Pt is a 61 years old married, Caucasian, Catholic woman. Pt speaks English reasonably well.&quot;]


data = spark.createDataFrame(sample_texts, StringType()).toDF(&quot;text&quot;)

result = pipeline.fit(data).transform(data)
```
```scala
val document_assembler = new DocumentAssembler()
    .setInputCol(&quot;text&quot;)
    .setOutputCol(&quot;document&quot;)

val sentence_detector = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;, &quot;en&quot;)
    .setInputCols(&quot;document&quot;)
    .setOutputCol(&quot;sentence&quot;)

val tokenizer = new Tokenizer()
    .setInputCols(&quot;sentence&quot;)
    .setOutputCol(&quot;token&quot;)

val clinical_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;)
    .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;))
    .setOutputCol(&quot;embeddings&quot;)

val ner_model = MedicalNerModel.pretrained(&quot;ner_sdoh_demographics_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;)
    .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;))
    .setOutputCol(&quot;ner&quot;)

val ner_converter = new NerConverterInternal()
    .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;))
    .setOutputCol(&quot;ner_chunk&quot;)

val pipeline = new Pipeline().setStages(Array(
    document_assembler, 
    sentence_detector,
    tokenizer,
    clinical_embeddings,
    ner_model,
    ner_converter   
))

val data = Seq(&quot;Pt is a 61 years old married, Caucasian, Catholic woman. Pt speaks English reasonably well.&quot;).toDS.toDF(&quot;text&quot;)

val result = pipeline.fit(data).transform(data)
```
&lt;/div&gt;

## Results

```bash
+-----------------+-----+---+------------+
|ner_label        |begin|end|chunk       |
+-----------------+-----+---+------------+
|Gender           |16   |17 |He          |
|Geographic_Entity|43   |47 |Korea       |
|Gender           |0    |1  |He          |
|Family_Member    |29   |36 |children    |
|Age              |8    |19 |61 years old|
|Race_Ethnicity   |30   |38 |Caucasian   |
|Spiritual_Beliefs|41   |48 |Catholic    |
|Gender           |50   |54 |woman       |
|Language         |67   |73 |English     |
+-----------------+-----+---+------------+
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|ner_sdoh_demographics_wip|
|Compatibility:|Healthcare NLP 4.2.8+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[sentence, token, embeddings]|
|Output Labels:|[ner]|
|Language:|en|
|Size:|858.4 KB|

## Benchmarking

```bash
	    label	     tp	    fp	   fn	   total	precision	   recall	       f1
              Age	 1346.0	  73.0	 74.0	  1420.0	 0.948555	 0.947887	 0.948221
Spiritual_Beliefs	  100.0	  13.0	 16.0	   116.0	 0.884956	 0.862069	 0.873362
    Family_Member	 4468.0	 134.0	 43.0	  4511.0	 0.970882	 0.990468	 0.980577
   Race_Ethnicity	   56.0	   0.0	 13.0	    69.0	 1.000000	 0.811594	 0.896000
           Gender	 9825.0	  67.0	247.0	 10072.0	 0.993227	 0.975477	 0.984272
Geographic_Entity	  225.0	   9.0	 29.0	   254.0	 0.961538	 0.885827	 0.922131
         Language	   51.0	   9.0	  5.0	    56.0	 0.850000	 0.910714	 0.879310
```</content><author><name>John Snow Labs</name></author><category term="licensed" /><category term="clinical" /><category term="social_determinants" /><category term="en" /><category term="ner" /><category term="demographics" /><category term="sdoh" /><category term="public_health" /><summary type="html">Description This model extracts demographic information related to Social Determinants of Health from various kinds of biomedical documents. Predicted Entities Family_Member, Age, Gender, Geographic_Entity, Race_Ethnicity, Language, Spiritual_Beliefs Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) sentence_detector = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;, &quot;en&quot;)\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer()\ .setInputCols([&quot;sentence&quot;])\ .setOutputCol(&quot;token&quot;) clinical_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\ .setInputCols([&quot;sentence&quot;, &quot;token&quot;])\ .setOutputCol(&quot;embeddings&quot;) ner_model = MedicalNerModel.pretrained(&quot;ner_sdoh_demographics_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\ .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;])\ .setOutputCol(&quot;ner&quot;) ner_converter = NerConverterInternal()\ .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;])\ .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[ document_assembler, sentence_detector, tokenizer, clinical_embeddings, ner_model, ner_converter ]) sample_texts = [&quot;SOCIAL HISTORY: He is a former tailor from Korea.&quot;, &quot;He lives alone,single and no children.&quot;, &quot;Pt is a 61 years old married, Caucasian, Catholic woman. Pt speaks English reasonably well.&quot;] data = spark.createDataFrame(sample_texts, StringType()).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) val document_assembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence_detector = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val clinical_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val ner_model = MedicalNerModel.pretrained(&quot;ner_sdoh_demographics_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = new NerConverterInternal() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) val pipeline = new Pipeline().setStages(Array( document_assembler, sentence_detector, tokenizer, clinical_embeddings, ner_model, ner_converter )) val data = Seq(&quot;Pt is a 61 years old married, Caucasian, Catholic woman. Pt speaks English reasonably well.&quot;).toDS.toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) Results +-----------------+-----+---+------------+ |ner_label |begin|end|chunk | +-----------------+-----+---+------------+ |Gender |16 |17 |He | |Geographic_Entity|43 |47 |Korea | |Gender |0 |1 |He | |Family_Member |29 |36 |children | |Age |8 |19 |61 years old| |Race_Ethnicity |30 |38 |Caucasian | |Spiritual_Beliefs|41 |48 |Catholic | |Gender |50 |54 |woman | |Language |67 |73 |English | +-----------------+-----+---+------------+ Model Information Model Name: ner_sdoh_demographics_wip Compatibility: Healthcare NLP 4.2.8+ License: Licensed Edition: Official Input Labels: [sentence, token, embeddings] Output Labels: [ner] Language: en Size: 858.4 KB Benchmarking label tp fp fn total precision recall f1 Age 1346.0 73.0 74.0 1420.0 0.948555 0.947887 0.948221 Spiritual_Beliefs 100.0 13.0 16.0 116.0 0.884956 0.862069 0.873362 Family_Member 4468.0 134.0 43.0 4511.0 0.970882 0.990468 0.980577 Race_Ethnicity 56.0 0.0 13.0 69.0 1.000000 0.811594 0.896000 Gender 9825.0 67.0 247.0 10072.0 0.993227 0.975477 0.984272 Geographic_Entity 225.0 9.0 29.0 254.0 0.961538 0.885827 0.922131 Language 51.0 9.0 5.0 56.0 0.850000 0.910714 0.879310</summary></entry><entry><title type="html">Extract Income and Social Status Entities from Social Determinants of Health Texts</title><link href="/2023/02/10/ner_sdoh_income_social_status_wip_en.html" rel="alternate" type="text/html" title="Extract Income and Social Status Entities from Social Determinants of Health Texts" /><published>2023-02-10T00:00:00+00:00</published><updated>2023-02-10T00:00:00+00:00</updated><id>/2023/02/10/ner_sdoh_income_social_status_wip_en</id><content type="html" xml:base="/2023/02/10/ner_sdoh_income_social_status_wip_en.html">## Description

This model extracts income and social status information related to Social Determinants of Health from various kinds of biomedical documents.

## Predicted Entities

`Education`, `Marital_Status`, `Financial_Status`, `Population_Group`, `Employment`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_sdoh_income_social_status_wip_en_4.2.8_3.0_1675999206708.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/clinical/models/ner_sdoh_income_social_status_wip_en_4.2.8_3.0_1675999206708.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
document_assembler = DocumentAssembler()\
    .setInputCol(&quot;text&quot;)\
    .setOutputCol(&quot;document&quot;)

sentence_detector = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;, &quot;en&quot;)\
    .setInputCols([&quot;document&quot;])\
    .setOutputCol(&quot;sentence&quot;)

tokenizer = Tokenizer()\
    .setInputCols([&quot;sentence&quot;])\
    .setOutputCol(&quot;token&quot;)

clinical_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\
    .setInputCols([&quot;sentence&quot;, &quot;token&quot;])\
    .setOutputCol(&quot;embeddings&quot;)

ner_model = MedicalNerModel.pretrained(&quot;ner_sdoh_income_social_status_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\
    .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;])\
    .setOutputCol(&quot;ner&quot;)

ner_converter = NerConverterInternal()\
    .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;])\
    .setOutputCol(&quot;ner_chunk&quot;)

pipeline = Pipeline(stages=[
    document_assembler, 
    sentence_detector,
    tokenizer,
    clinical_embeddings,
    ner_model,
    ner_converter   
    ])

sample_texts = [&quot;Pt is described as divorced and pleasant when approached but keeps to himself. Pt is working as a plumber, but he gets financial diffuculties. He has a son student at college. His family is imigrant for 2 years.&quot;]

data = spark.createDataFrame(sample_texts, StringType()).toDF(&quot;text&quot;)

result = pipeline.fit(data).transform(data)
```
```scala
val document_assembler = new DocumentAssembler()
    .setInputCol(&quot;text&quot;)
    .setOutputCol(&quot;document&quot;)

val sentence_detector = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;, &quot;en&quot;)
    .setInputCols(&quot;document&quot;)
    .setOutputCol(&quot;sentence&quot;)

val tokenizer = new Tokenizer()
    .setInputCols(&quot;sentence&quot;)
    .setOutputCol(&quot;token&quot;)

val clinical_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;)
    .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;))
    .setOutputCol(&quot;embeddings&quot;)

val ner_model = MedicalNerModel.pretrained(&quot;ner_sdoh_income_social_status_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;)
    .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;))
    .setOutputCol(&quot;ner&quot;)

val ner_converter = new NerConverterInternal()
    .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;))
    .setOutputCol(&quot;ner_chunk&quot;)

val pipeline = new Pipeline().setStages(Array(
    document_assembler, 
    sentence_detector,
    tokenizer,
    clinical_embeddings,
    ner_model,
    ner_converter   
))

val data = Seq(&quot;Pt is described as divorced and pleasant when approached but keeps to himself. Pt is working as a plumber, but he gets financial diffuculties. He has a son student at college. His family is imigrant for 2 years.&quot;).toDS.toDF(&quot;text&quot;)

val result = pipeline.fit(data).transform(data)
```
&lt;/div&gt;

## Results

```bash
+-----------+----------------+-----+---+----------------------+
|sentence_id|ner_label       |begin|end|chunk                 |
+-----------+----------------+-----+---+----------------------+
|0          |Marital_Status  |19   |26 |divorced              |
|1          |Employment      |98   |104|plumber               |
|1          |Financial_Status|119  |140|financial diffuculties|
|2          |Education       |156  |162|student               |
|2          |Education       |167  |173|college               |
|3          |Population_Group|190  |197|imigrant              |
+-----------+----------------+-----+---+----------------------+
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|ner_sdoh_income_social_status_wip|
|Compatibility:|Healthcare NLP 4.2.8+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[sentence, token, embeddings]|
|Output Labels:|[ner]|
|Language:|en|
|Size:|856.8 KB|

## Benchmarking

```bash
           label	    tp	   fp	    fn	 total	precision	   recall	       f1
       Education	  95.0	 20.0	  18.0	 113.0	 0.826087	 0.840708	 0.833333
Population_Group	  41.0	  0.0	   5.0	  46.0	 1.000000	 0.891304	 0.942529
Financial_Status	 286.0	 52.0	  82.0	 368.0	 0.846154	 0.777174	 0.810198
      Employment	3968.0	142.0	 215.0	4183.0	 0.965450	 0.948601	 0.956952
  Marital_Status	 167.0	  1.0	   7.0	 174.0	 0.994048	 0.959770	 0.976608
```</content><author><name>John Snow Labs</name></author><category term="licensed" /><category term="clinical" /><category term="social_determinants" /><category term="en" /><category term="ner" /><category term="income" /><category term="social_status" /><category term="sdoh" /><category term="public_health" /><summary type="html">Description This model extracts income and social status information related to Social Determinants of Health from various kinds of biomedical documents. Predicted Entities Education, Marital_Status, Financial_Status, Population_Group, Employment Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) sentence_detector = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;, &quot;en&quot;)\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer()\ .setInputCols([&quot;sentence&quot;])\ .setOutputCol(&quot;token&quot;) clinical_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\ .setInputCols([&quot;sentence&quot;, &quot;token&quot;])\ .setOutputCol(&quot;embeddings&quot;) ner_model = MedicalNerModel.pretrained(&quot;ner_sdoh_income_social_status_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\ .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;])\ .setOutputCol(&quot;ner&quot;) ner_converter = NerConverterInternal()\ .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;])\ .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[ document_assembler, sentence_detector, tokenizer, clinical_embeddings, ner_model, ner_converter ]) sample_texts = [&quot;Pt is described as divorced and pleasant when approached but keeps to himself. Pt is working as a plumber, but he gets financial diffuculties. He has a son student at college. His family is imigrant for 2 years.&quot;] data = spark.createDataFrame(sample_texts, StringType()).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) val document_assembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence_detector = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val clinical_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val ner_model = MedicalNerModel.pretrained(&quot;ner_sdoh_income_social_status_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = new NerConverterInternal() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) val pipeline = new Pipeline().setStages(Array( document_assembler, sentence_detector, tokenizer, clinical_embeddings, ner_model, ner_converter )) val data = Seq(&quot;Pt is described as divorced and pleasant when approached but keeps to himself. Pt is working as a plumber, but he gets financial diffuculties. He has a son student at college. His family is imigrant for 2 years.&quot;).toDS.toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) Results +-----------+----------------+-----+---+----------------------+ |sentence_id|ner_label |begin|end|chunk | +-----------+----------------+-----+---+----------------------+ |0 |Marital_Status |19 |26 |divorced | |1 |Employment |98 |104|plumber | |1 |Financial_Status|119 |140|financial diffuculties| |2 |Education |156 |162|student | |2 |Education |167 |173|college | |3 |Population_Group|190 |197|imigrant | +-----------+----------------+-----+---+----------------------+ Model Information Model Name: ner_sdoh_income_social_status_wip Compatibility: Healthcare NLP 4.2.8+ License: Licensed Edition: Official Input Labels: [sentence, token, embeddings] Output Labels: [ner] Language: en Size: 856.8 KB Benchmarking label tp fp fn total precision recall f1 Education 95.0 20.0 18.0 113.0 0.826087 0.840708 0.833333 Population_Group 41.0 0.0 5.0 46.0 1.000000 0.891304 0.942529 Financial_Status 286.0 52.0 82.0 368.0 0.846154 0.777174 0.810198 Employment 3968.0 142.0 215.0 4183.0 0.965450 0.948601 0.956952 Marital_Status 167.0 1.0 7.0 174.0 0.994048 0.959770 0.976608</summary></entry><entry><title type="html">Detect SDOH of Social Environment</title><link href="/2023/02/10/ner_sdoh_social_environment_wip_en.html" rel="alternate" type="text/html" title="Detect SDOH of Social Environment" /><published>2023-02-10T00:00:00+00:00</published><updated>2023-02-10T00:00:00+00:00</updated><id>/2023/02/10/ner_sdoh_social_environment_wip_en</id><content type="html" xml:base="/2023/02/10/ner_sdoh_social_environment_wip_en.html">## Description

This model extracts social environment terminologies related to Social Determinants of Health from various kinds of documents.

## Predicted Entities

`Social_Support`, `Chidhood_Event`, `Social_Exclusion`, `Violence_Abuse_Legal`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/ner_sdoh_social_environment_wip_en_4.2.8_3.0_1675998295035.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/clinical/models/ner_sdoh_social_environment_wip_en_4.2.8_3.0_1675998295035.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
document_assembler = DocumentAssembler()\
    .setInputCol(&quot;text&quot;)\
    .setOutputCol(&quot;document&quot;)

sentence_detector = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;, &quot;en&quot;)\
    .setInputCols([&quot;document&quot;])\
    .setOutputCol(&quot;sentence&quot;)

tokenizer = Tokenizer()\
    .setInputCols([&quot;sentence&quot;])\
    .setOutputCol(&quot;token&quot;)

clinical_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\
    .setInputCols([&quot;sentence&quot;, &quot;token&quot;])\
    .setOutputCol(&quot;embeddings&quot;)

ner_model = MedicalNerModel.pretrained(&quot;ner_sdoh_social_environment_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\
    .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;])\
    .setOutputCol(&quot;ner&quot;)

ner_converter = NerConverterInternal()\
    .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;])\
    .setOutputCol(&quot;ner_chunk&quot;)

pipeline = Pipeline(stages=[
    document_assembler, 
    sentence_detector,
    tokenizer,
    clinical_embeddings,
    ner_model,
    ner_converter   
    ])

sample_texts = [&quot;He is the primary caregiver.&quot;,
             &quot;There is some evidence of abuse.&quot;,
             &quot;She stated that she was in a safe environment in prison, but that her siblings lived in an unsafe neighborhood, she was very afraid for them and witnessed their ostracism by other people.&quot;,
             &quot;Medical history: Jane was born in a low - income household and experienced significant trauma during her childhood, including physical and emotional abuse.&quot;]

data = spark.createDataFrame(sample_texts, StringType()).toDF(&quot;text&quot;)

result = pipeline.fit(data).transform(data)
```
```scala
val document_assembler = new DocumentAssembler()
    .setInputCol(&quot;text&quot;)
    .setOutputCol(&quot;document&quot;)

val sentence_detector = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;, &quot;en&quot;)
    .setInputCols(&quot;document&quot;)
    .setOutputCol(&quot;sentence&quot;)

val tokenizer = new Tokenizer()
    .setInputCols(&quot;sentence&quot;)
    .setOutputCol(&quot;token&quot;)

val clinical_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;)
    .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;))
    .setOutputCol(&quot;embeddings&quot;)

val ner_model = MedicalNerModel.pretrained(&quot;ner_sdoh_social_environment_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;)
    .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;))
    .setOutputCol(&quot;ner&quot;)

val ner_converter = new NerConverterInternal()
    .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;))
    .setOutputCol(&quot;ner_chunk&quot;)

val pipeline = new Pipeline().setStages(Array(
    document_assembler, 
    sentence_detector,
    tokenizer,
    clinical_embeddings,
    ner_model,
    ner_converter   
))

val data = Seq(&quot;Medical history: Jane was born in a low - income household and experienced significant trauma during her childhood, including physical and emotional abuse.&quot;).toDS.toDF(&quot;text&quot;)

val result = pipeline.fit(data).transform(data)

```
&lt;/div&gt;

## Results

```bash
+--------------------+-----+---+---------------------------+
|ner_label           |begin|end|chunk                      |
+--------------------+-----+---+---------------------------+
|Social_Support      |10   |26 |primary caregiver          |
|Violence_Abuse_Legal|26   |30 |abuse                      |
|Violence_Abuse_Legal|49   |54 |prison                     |
|Social_Exclusion    |161  |169|ostracism                  |
|Chidhood_Event      |87   |113|trauma during her childhood|
|Violence_Abuse_Legal|139  |153|emotional abuse            |
+--------------------+-----+---+---------------------------+
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|ner_sdoh_social_environment_wip|
|Compatibility:|Healthcare NLP 4.2.8+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[sentence, token, embeddings]|
|Output Labels:|[ner]|
|Language:|en|
|Size:|858.7 KB|

## Benchmarking

```bash
	       label	    tp	  fp	   fn	  total	 precision	  recall	      f1
      Chidhood_Event	  34.0	 6.0	  5.0	   39.0	  0.850000	0.871795	0.860759
    Social_Exclusion	  45.0	 6.0	 12.0  	 57.0	  0.882353	0.789474	0.833333
      Social_Support	1139.0	57.0	103.0	 1242.0	  0.952341	0.917069	0.934372
Violence_Abuse_Legal	 235.0	38.0	 44.0	  279.0	  0.860806	0.842294	0.851449
```</content><author><name>John Snow Labs</name></author><category term="licensed" /><category term="clinical" /><category term="social_determinants" /><category term="en" /><category term="ner" /><category term="social" /><category term="environment" /><category term="sdoh" /><category term="public_health" /><summary type="html">Description This model extracts social environment terminologies related to Social Determinants of Health from various kinds of documents. Predicted Entities Social_Support, Chidhood_Event, Social_Exclusion, Violence_Abuse_Legal Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) sentence_detector = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;, &quot;en&quot;)\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer()\ .setInputCols([&quot;sentence&quot;])\ .setOutputCol(&quot;token&quot;) clinical_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\ .setInputCols([&quot;sentence&quot;, &quot;token&quot;])\ .setOutputCol(&quot;embeddings&quot;) ner_model = MedicalNerModel.pretrained(&quot;ner_sdoh_social_environment_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\ .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;])\ .setOutputCol(&quot;ner&quot;) ner_converter = NerConverterInternal()\ .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;])\ .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[ document_assembler, sentence_detector, tokenizer, clinical_embeddings, ner_model, ner_converter ]) sample_texts = [&quot;He is the primary caregiver.&quot;, &quot;There is some evidence of abuse.&quot;, &quot;She stated that she was in a safe environment in prison, but that her siblings lived in an unsafe neighborhood, she was very afraid for them and witnessed their ostracism by other people.&quot;, &quot;Medical history: Jane was born in a low - income household and experienced significant trauma during her childhood, including physical and emotional abuse.&quot;] data = spark.createDataFrame(sample_texts, StringType()).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) val document_assembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence_detector = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val clinical_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val ner_model = MedicalNerModel.pretrained(&quot;ner_sdoh_social_environment_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = new NerConverterInternal() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) val pipeline = new Pipeline().setStages(Array( document_assembler, sentence_detector, tokenizer, clinical_embeddings, ner_model, ner_converter )) val data = Seq(&quot;Medical history: Jane was born in a low - income household and experienced significant trauma during her childhood, including physical and emotional abuse.&quot;).toDS.toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) Results +--------------------+-----+---+---------------------------+ |ner_label |begin|end|chunk | +--------------------+-----+---+---------------------------+ |Social_Support |10 |26 |primary caregiver | |Violence_Abuse_Legal|26 |30 |abuse | |Violence_Abuse_Legal|49 |54 |prison | |Social_Exclusion |161 |169|ostracism | |Chidhood_Event |87 |113|trauma during her childhood| |Violence_Abuse_Legal|139 |153|emotional abuse | +--------------------+-----+---+---------------------------+ Model Information Model Name: ner_sdoh_social_environment_wip Compatibility: Healthcare NLP 4.2.8+ License: Licensed Edition: Official Input Labels: [sentence, token, embeddings] Output Labels: [ner] Language: en Size: 858.7 KB Benchmarking label tp fp fn total precision recall f1 Chidhood_Event 34.0 6.0 5.0 39.0 0.850000 0.871795 0.860759 Social_Exclusion 45.0 6.0 12.0 57.0 0.882353 0.789474 0.833333 Social_Support 1139.0 57.0 103.0 1242.0 0.952341 0.917069 0.934372 Violence_Abuse_Legal 235.0 38.0 44.0 279.0 0.860806 0.842294 0.851449</summary></entry><entry><title type="html">Mapping RxNorm and RxNorm Extension Codes with Corresponding Drug Brand Names</title><link href="/2023/02/09/rxnorm_drug_brandname_mapper_en.html" rel="alternate" type="text/html" title="Mapping RxNorm and RxNorm Extension Codes with Corresponding Drug Brand Names" /><published>2023-02-09T00:00:00+00:00</published><updated>2023-02-09T00:00:00+00:00</updated><id>/2023/02/09/rxnorm_drug_brandname_mapper_en</id><content type="html" xml:base="/2023/02/09/rxnorm_drug_brandname_mapper_en.html">## Description

This pretrained model maps RxNorm and RxNorm Extension codes with their corresponding drug brand names. It returns 2 types of brand names for the corresponding RxNorm or RxNorm Extension code.

## Predicted Entities

`rxnorm_brandname`, `rxnorm_extension_brandname`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/26.Chunk_Mapping.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/clinical/models/rxnorm_drug_brandname_mapper_en_4.3.0_3.0_1675966478332.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/clinical/models/rxnorm_drug_brandname_mapper_en_4.3.0_3.0_1675966478332.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
documentAssembler = DocumentAssembler()\
      .setInputCol(&quot;text&quot;)\
      .setOutputCol(&quot;chunk&quot;)

sbert_embedder = BertSentenceEmbeddings\
      .pretrained(&quot;sbiobert_base_cased_mli&quot;, &quot;en&quot;,&quot;clinical/models&quot;)\
      .setInputCols([&quot;chunk&quot;])\
      .setOutputCol(&quot;sbert_embeddings&quot;)
    
rxnorm_resolver = SentenceEntityResolverModel\
      .pretrained(&quot;sbiobertresolve_rxnorm_augmented&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\
      .setInputCols([&quot;chunk&quot;, &quot;sbert_embeddings&quot;])\
      .setOutputCol(&quot;rxnorm_code&quot;)\
      .setDistanceFunction(&quot;EUCLIDEAN&quot;)

resolver2chunk = Resolution2Chunk()\
    .setInputCols([&quot;rxnorm_code&quot;]) \
    .setOutputCol(&quot;rxnorm_chunk&quot;)\

chunkerMapper = ChunkMapperModel.pretrained(&quot;rxnorm_drug_brandname_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\
      .setInputCols([&quot;rxnorm_chunk&quot;])\
      .setOutputCol(&quot;mappings&quot;)\
      .setRels([&quot;rxnorm_brandname&quot;, &quot;rxnorm_extension_brandname&quot;])


pipeline = Pipeline(
    stages = [
        documentAssembler,
        sbert_embedder,
        rxnorm_resolver,
        resolver2chunk,
        chunkerMapper
        ])

model = pipeline.fit(spark.createDataFrame([['']]).toDF('text')) 

pipeline = LightPipeline(model)

result = pipeline.fullAnnotate(['metformin', 'advil'])

```
```scala
val documentAssembler = new DocumentAssembler()\
      .setInputCol(&quot;text&quot;)\
      .setOutputCol(&quot;chunk&quot;)

val sbert_embedder = BertSentenceEmbeddings\
      .pretrained(&quot;sbiobert_base_cased_mli&quot;, &quot;en&quot;,&quot;clinical/models&quot;)\
      .setInputCols([&quot;chunk&quot;])\
      .setOutputCol(&quot;sbert_embeddings&quot;)
    
val rxnorm_resolver = SentenceEntityResolverModel\
      .pretrained(&quot;sbiobertresolve_rxnorm_augmented&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\
      .setInputCols([&quot;chunk&quot;, &quot;sbert_embeddings&quot;])\
      .setOutputCol(&quot;rxnorm_code&quot;)\
      .setDistanceFunction(&quot;EUCLIDEAN&quot;)

val resolver2chunk = new Resolution2Chunk()\
    .setInputCols([&quot;rxnorm_code&quot;]) \
    .setOutputCol(&quot;rxnorm_chunk&quot;)\

val chunkerMapper = ChunkMapperModel.pretrained(&quot;rxnorm_drug_brandname_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\
      .setInputCols([&quot;rxnorm_chunk&quot;])\
      .setOutputCol(&quot;mappings&quot;)\
      .setRels([&quot;rxnorm_brandname&quot;, &quot;rxnorm_extension_brandname&quot;])



val pipeline = new Pipeline(stages = Array(
documentAssembler,
sbert_embedder,
rxnorm_resolver,
resolver2chunk
chunkerMapper
))

val data = Seq(Array(&quot;metformin&quot;, &quot;advil&quot;)).toDS.toDF(&quot;text&quot;)

val result= pipeline.fit(data).transform(data)

```
&lt;/div&gt;

## Results

```bash
+--------------+-------------+--------------------------------------------------+--------------------------+
|     drug_name|rxnorm_result|                                    mapping_result|                 relation |
+--------------+-------------+--------------------------------------------------+--------------------------+
|     metformin|         6809|Actoplus Met (metformin):::Avandamet (metformin...|          rxnorm_brandname|
|     metformin|         6809|A FORMIN (metformin):::ABERIN MAX (metformin)::...|rxnorm_extension_brandname|
|         advil|       153010|                                     Advil (Advil)|          rxnorm_brandname|
|         advil|       153010|                                              NONE|rxnorm_extension_brandname|
+--------------+-------------+--------------------------------------------------+--------------------------+
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|rxnorm_drug_brandname_mapper|
|Compatibility:|Healthcare NLP 4.3.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[rxnorm_chunk]|
|Output Labels:|[mappings]|
|Language:|en|
|Size:|4.0 MB|</content><author><name>John Snow Labs</name></author><category term="chunk_mappig" /><category term="rxnorm" /><category term="drug_brand_name" /><category term="rxnorm_extension" /><category term="en" /><category term="clinical" /><category term="licensed" /><summary type="html">Description This pretrained model maps RxNorm and RxNorm Extension codes with their corresponding drug brand names. It returns 2 types of brand names for the corresponding RxNorm or RxNorm Extension code. Predicted Entities rxnorm_brandname, rxnorm_extension_brandname Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU documentAssembler = DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;chunk&quot;) sbert_embedder = BertSentenceEmbeddings\ .pretrained(&quot;sbiobert_base_cased_mli&quot;, &quot;en&quot;,&quot;clinical/models&quot;)\ .setInputCols([&quot;chunk&quot;])\ .setOutputCol(&quot;sbert_embeddings&quot;) rxnorm_resolver = SentenceEntityResolverModel\ .pretrained(&quot;sbiobertresolve_rxnorm_augmented&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\ .setInputCols([&quot;chunk&quot;, &quot;sbert_embeddings&quot;])\ .setOutputCol(&quot;rxnorm_code&quot;)\ .setDistanceFunction(&quot;EUCLIDEAN&quot;) resolver2chunk = Resolution2Chunk()\ .setInputCols([&quot;rxnorm_code&quot;]) \ .setOutputCol(&quot;rxnorm_chunk&quot;)\ chunkerMapper = ChunkMapperModel.pretrained(&quot;rxnorm_drug_brandname_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\ .setInputCols([&quot;rxnorm_chunk&quot;])\ .setOutputCol(&quot;mappings&quot;)\ .setRels([&quot;rxnorm_brandname&quot;, &quot;rxnorm_extension_brandname&quot;]) pipeline = Pipeline( stages = [ documentAssembler, sbert_embedder, rxnorm_resolver, resolver2chunk, chunkerMapper ]) model = pipeline.fit(spark.createDataFrame([['']]).toDF('text')) pipeline = LightPipeline(model) result = pipeline.fullAnnotate(['metformin', 'advil']) val documentAssembler = new DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;chunk&quot;) val sbert_embedder = BertSentenceEmbeddings\ .pretrained(&quot;sbiobert_base_cased_mli&quot;, &quot;en&quot;,&quot;clinical/models&quot;)\ .setInputCols([&quot;chunk&quot;])\ .setOutputCol(&quot;sbert_embeddings&quot;) val rxnorm_resolver = SentenceEntityResolverModel\ .pretrained(&quot;sbiobertresolve_rxnorm_augmented&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\ .setInputCols([&quot;chunk&quot;, &quot;sbert_embeddings&quot;])\ .setOutputCol(&quot;rxnorm_code&quot;)\ .setDistanceFunction(&quot;EUCLIDEAN&quot;) val resolver2chunk = new Resolution2Chunk()\ .setInputCols([&quot;rxnorm_code&quot;]) \ .setOutputCol(&quot;rxnorm_chunk&quot;)\ val chunkerMapper = ChunkMapperModel.pretrained(&quot;rxnorm_drug_brandname_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;)\ .setInputCols([&quot;rxnorm_chunk&quot;])\ .setOutputCol(&quot;mappings&quot;)\ .setRels([&quot;rxnorm_brandname&quot;, &quot;rxnorm_extension_brandname&quot;]) val pipeline = new Pipeline(stages = Array( documentAssembler, sbert_embedder, rxnorm_resolver, resolver2chunk chunkerMapper )) val data = Seq(Array(&quot;metformin&quot;, &quot;advil&quot;)).toDS.toDF(&quot;text&quot;) val result= pipeline.fit(data).transform(data) Results +--------------+-------------+--------------------------------------------------+--------------------------+ | drug_name|rxnorm_result| mapping_result| relation | +--------------+-------------+--------------------------------------------------+--------------------------+ | metformin| 6809|Actoplus Met (metformin):::Avandamet (metformin...| rxnorm_brandname| | metformin| 6809|A FORMIN (metformin):::ABERIN MAX (metformin)::...|rxnorm_extension_brandname| | advil| 153010| Advil (Advil)| rxnorm_brandname| | advil| 153010| NONE|rxnorm_extension_brandname| +--------------+-------------+--------------------------------------------------+--------------------------+ Model Information Model Name: rxnorm_drug_brandname_mapper Compatibility: Healthcare NLP 4.3.0+ License: Licensed Edition: Official Input Labels: [rxnorm_chunk] Output Labels: [mappings] Language: en Size: 4.0 MB</summary></entry><entry><title type="html">French CamemBertForQuestionAnswering Base squadFR (camembert_base_qa_fquad)</title><link href="/2023/02/08/camembert_base_qa_fquad_fr.html" rel="alternate" type="text/html" title="French CamemBertForQuestionAnswering Base squadFR (camembert_base_qa_fquad)" /><published>2023-02-08T00:00:00+00:00</published><updated>2023-02-08T00:00:00+00:00</updated><id>/2023/02/08/camembert_base_qa_fquad_fr</id><content type="html" xml:base="/2023/02/08/camembert_base_qa_fquad_fr.html">## Description

Pretrained CamemBertForQuestionAnswering model, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP. `camembert_base_qa_fquad ` is a French model originally fine-tuned on a combo of three French Q&amp;A datasets:

- PIAFv1.1
- FQuADv1.0
- SQuAD-FR (SQuAD automatically translated to French)

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/camembert_base_qa_fquad_fr_4.3.0_3.2_1675865521345.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/camembert_base_qa_fquad_fr_4.3.0_3.2_1675865521345.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
Document_Assembler = MultiDocumentAssembler()\
     .setInputCols([&quot;question&quot;, &quot;context&quot;])\
     .setOutputCols([&quot;document_question&quot;, &quot;document_context&quot;])

Question_Answering = CamemBertForQuestionAnswering(&quot;camembert_base_qa_fquad&quot;,&quot;fr&quot;)\
     .setInputCols([&quot;document_question&quot;, &quot;document_context&quot;])\
     .setOutputCol(&quot;answer&quot;)\
     .setCaseSensitive(True)
    
pipeline = Pipeline(stages=[Document_Assembler, Question_Answering])

data = spark.createDataFrame([[&quot;Où est-ce que je vis?&quot;,&quot;Mon nom est Wolfgang et je vis à Berlin.&quot;]]).toDF(&quot;question&quot;, &quot;context&quot;)

result = pipeline.fit(data).transform(data)
```
```scala
val Document_Assembler = new MultiDocumentAssembler()
     .setInputCols(Array(&quot;question&quot;, &quot;context&quot;))
     .setOutputCols(Array(&quot;document_question&quot;, &quot;document_context&quot;))

val Question_Answering = CamemBertForQuestionAnswering(&quot;camembert_base_qa_fquad&quot;,&quot;fr&quot;)
     .setInputCols(Array(&quot;document_question&quot;, &quot;document_context&quot;))
     .setOutputCol(&quot;answer&quot;)
     .setCaseSensitive(True)
    
val pipeline = new Pipeline().setStages(Array(Document_Assembler, Question_Answering))

val data = Seq(&quot;Où est-ce que je vis?&quot;,&quot;Mon nom est Wolfgang et je vis à Berlin.&quot;).toDS.toDF(&quot;question&quot;, &quot;context&quot;)

val result = pipeline.fit(data).transform(data)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|camembert_base_qa_fquad|
|Compatibility:|Spark NLP 4.3.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[document_question, document_context]|
|Output Labels:|[answer]|
|Language:|fr|
|Size:|411.9 MB|

## References

https://huggingface.co/etalab-ia/camembert-base-squadFR-fquad-piaf

## Benchmarking

```bash
{&quot;f1&quot;: 80.61, &quot;exact_match&quot;: 59.54}
```</content><author><name>John Snow Labs</name></author><category term="fr" /><category term="french" /><category term="question_answering" /><category term="camembert" /><category term="open_source" /><category term="tensorflow" /><summary type="html">Description Pretrained CamemBertForQuestionAnswering model, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP. camembert_base_qa_fquad is a French model originally fine-tuned on a combo of three French Q&amp;amp;A datasets: PIAFv1.1 FQuADv1.0 SQuAD-FR (SQuAD automatically translated to French) Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU Document_Assembler = MultiDocumentAssembler()\ .setInputCols([&quot;question&quot;, &quot;context&quot;])\ .setOutputCols([&quot;document_question&quot;, &quot;document_context&quot;]) Question_Answering = CamemBertForQuestionAnswering(&quot;camembert_base_qa_fquad&quot;,&quot;fr&quot;)\ .setInputCols([&quot;document_question&quot;, &quot;document_context&quot;])\ .setOutputCol(&quot;answer&quot;)\ .setCaseSensitive(True) pipeline = Pipeline(stages=[Document_Assembler, Question_Answering]) data = spark.createDataFrame([[&quot;Où est-ce que je vis?&quot;,&quot;Mon nom est Wolfgang et je vis à Berlin.&quot;]]).toDF(&quot;question&quot;, &quot;context&quot;) result = pipeline.fit(data).transform(data) val Document_Assembler = new MultiDocumentAssembler() .setInputCols(Array(&quot;question&quot;, &quot;context&quot;)) .setOutputCols(Array(&quot;document_question&quot;, &quot;document_context&quot;)) val Question_Answering = CamemBertForQuestionAnswering(&quot;camembert_base_qa_fquad&quot;,&quot;fr&quot;) .setInputCols(Array(&quot;document_question&quot;, &quot;document_context&quot;)) .setOutputCol(&quot;answer&quot;) .setCaseSensitive(True) val pipeline = new Pipeline().setStages(Array(Document_Assembler, Question_Answering)) val data = Seq(&quot;Où est-ce que je vis?&quot;,&quot;Mon nom est Wolfgang et je vis à Berlin.&quot;).toDS.toDF(&quot;question&quot;, &quot;context&quot;) val result = pipeline.fit(data).transform(data) Model Information Model Name: camembert_base_qa_fquad Compatibility: Spark NLP 4.3.0+ License: Open Source Edition: Official Input Labels: [document_question, document_context] Output Labels: [answer] Language: fr Size: 411.9 MB References https://huggingface.co/etalab-ia/camembert-base-squadFR-fquad-piaf Benchmarking {&quot;f1&quot;: 80.61, &quot;exact_match&quot;: 59.54}</summary></entry><entry><title type="html">Zero-Shot Named Entity Recognition (Generic sample)</title><link href="/2023/02/08/zero_shot_ner_roberta_en.html" rel="alternate" type="text/html" title="Zero-Shot Named Entity Recognition (Generic sample)" /><published>2023-02-08T00:00:00+00:00</published><updated>2023-02-08T00:00:00+00:00</updated><id>/2023/02/08/zero_shot_ner_roberta_en</id><content type="html" xml:base="/2023/02/08/zero_shot_ner_roberta_en.html">## Description

This model is trained with Zero-Shot Named Entity Recognition (NER) approach and it can detect any kind of defined entities with no training dataset, just pretrained RoBERTa embeddings (included in the model).

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/zero_shot_ner_roberta_en_4.3.0_3.2_1675890474068.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/zero_shot_ner_roberta_en_4.3.0_3.2_1675890474068.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
documentAssembler = DocumentAssembler() \
    .setInputCol(&quot;text&quot;) \
    .setOutputCol(&quot;document&quot;)

sentenceDetector = SentenceDetector() \
    .setInputCols([&quot;document&quot;]) \
    .setOutputCol(&quot;sentence&quot;)

tokenizer = Tokenizer() \
    .setInputCols([&quot;sentence&quot;]) \
    .setOutputCol(&quot;token&quot;)
    
zero_shot_ner = ZeroShotNerModel.pretrained(&quot;zero_shot_ner_roberta&quot;, &quot;en&quot;, &quot;clincial/models&quot;)\
    .setInputCols([&quot;sentence&quot;, &quot;token&quot;])\
    .setOutputCol(&quot;zero_shot_ner&quot;)\
    .setEntityDefinitions(
        {
            &quot;NAME&quot;: [&quot;What is his name?&quot;, &quot;What is my name?&quot;, &quot;What is her name?&quot;],
            &quot;CITY&quot;: [&quot;Which city?&quot;, &quot;Which is the city?&quot;]
        })

ner_converter = NerConverter()\
    .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;zero_shot_ner&quot;])\
    .setOutputCol(&quot;ner_chunk&quot;)\

pipeline = Pipeline(stages = [
    documentAssembler, 
    sentenceDetector, 
    tokenizer, 
    zero_shot_ner, 
    ner_converter])

data = spark.createDataFrame([&quot;Hellen works in London, Paris and Berlin. My name is Clara, I live in New York and Hellen lives in Paris.&quot;,
                              &quot;John is a man who works in London, London and London.&quot;], StringType()).toDF(&quot;text&quot;)

result = pipeline.fit(data).transform(data)
```
```scala
val documentAssembler = new DocumentAssembler()
    .setInputCol(&quot;text&quot;)
    .setOutputCol(&quot;document&quot;)

val sentenceDetector = new SentenceDetector() 
    .setInputCols(Array(&quot;document&quot;)) 
    .setOutputCol(&quot;sentence&quot;)

val tokenizer = new Tokenizer() 
    .setInputCols(Array(&quot;sentence&quot;)) 
    .setOutputCol(&quot;token&quot;)
    
val zero_shot_ner = ZeroShotNerModel.pretrained(&quot;zero_shot_ner_roberta&quot;, &quot;en&quot;, &quot;clincial/models&quot;)
    .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;))
    .setOutputCol(&quot;zero_shot_ner&quot;)
    .setEntityDefinitions(Map(
            &quot;NAME&quot;-&gt; Array(&quot;What is his name?&quot;, &quot;What is my name?&quot;, &quot;What is her name?&quot;),
            &quot;CITY&quot;-&gt; Array(&quot;Which city?&quot;, &quot;Which is the city?&quot;)
    ))

val ner_converter = new NerConverter()
    .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;zero_shot_ner&quot;))
    .setOutputCol(&quot;ner_chunk&quot;)

val pipeline = new .setStages(Array(
    documentAssembler, 
    sentenceDetector, 
    tokenizer, 
    zero_shot_ner, 
    ner_converter))

val data = Seq(Array(&quot;Hellen works in London, Paris and Berlin. My name is Clara, I live in New York and Hellen lives in Paris.&quot;,
                                     &quot;John is a man who works in London, London and London.&quot;)toDS().toDF(&quot;text&quot;)

val result = pipeline.fit(data).transform(data)
```
&lt;/div&gt;

## Results

```bash
+------+---------+--------+-----+---+----------+
| token|ner_label|sentence|begin|end|confidence|
+------+---------+--------+-----+---+----------+
|Hellen|   B-NAME|       0|    0|  5|0.13306311|
| works|        O|       0|    7| 11|      null|
|    in|        O|       0|   13| 14|      null|
|London|   B-CITY|       0|   16| 21| 0.4064213|
|     ,|        O|       0|   22| 22|      null|
| Paris|   B-CITY|       0|   24| 28|0.04597357|
|   and|        O|       0|   30| 32|      null|
|Berlin|   B-CITY|       0|   34| 39|0.16265489|
|     .|        O|       0|   40| 40|      null|
|    My|        O|       1|   42| 43|      null|
|  name|        O|       1|   45| 48|      null|
|    is|        O|       1|   50| 51|      null|
| Clara|   B-NAME|       1|   53| 57| 0.9274031|
|     ,|        O|       1|   58| 58|      null|
|     I|        O|       1|   60| 60|      null|
|  live|        O|       1|   62| 65|      null|
|    in|        O|       1|   67| 68|      null|
|   New|   B-CITY|       1|   70| 72|0.82799006|
|  York|   I-CITY|       1|   74| 77|0.82799006|
|   and|        O|       1|   79| 81|      null|
|Hellen|   B-NAME|       1|   83| 88|0.40429682|
| lives|        O|       1|   90| 94|      null|
|    in|        O|       1|   96| 97|      null|
| Paris|   B-CITY|       1|   99|103|0.49216735|
|     .|        O|       1|  104|104|      null|
|  John|   B-NAME|       0|    0|  3|0.14063153|
|    is|        O|       0|    5|  6|      null|
|     a|        O|       0|    8|  8|      null|
|   man|        O|       0|   10| 12|      null|
|   who|        O|       0|   14| 16|      null|
| works|        O|       0|   18| 22|      null|
|    in|        O|       0|   24| 25|      null|
|London|   B-CITY|       0|   27| 32|0.15521188|
|     ,|        O|       0|   33| 33|      null|
|London|   B-CITY|       0|   35| 40|0.12151082|
|   and|        O|       0|   42| 44|      null|
|London|   B-CITY|       0|   46| 51| 0.2650951|
|     .|        O|       0|   52| 52|      null|
+------+---------+--------+-----+---+----------+
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|zero_shot_ner_roberta|
|Compatibility:|Spark NLP 4.3.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[document_question, document_context]|
|Output Labels:|[answer]|
|Language:|en|
|Size:|463.8 MB|
|Case sensitive:|true|
|Max sentence length:|512|</content><author><name>John Snow Labs</name></author><category term="ner" /><category term="zero_shot" /><category term="roberta" /><category term="qa" /><category term="en" /><category term="open_source" /><category term="tensorflow" /><summary type="html">Description This model is trained with Zero-Shot Named Entity Recognition (NER) approach and it can detect any kind of defined entities with no training dataset, just pretrained RoBERTa embeddings (included in the model). Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU documentAssembler = DocumentAssembler() \ .setInputCol(&quot;text&quot;) \ .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() \ .setInputCols([&quot;document&quot;]) \ .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() \ .setInputCols([&quot;sentence&quot;]) \ .setOutputCol(&quot;token&quot;) zero_shot_ner = ZeroShotNerModel.pretrained(&quot;zero_shot_ner_roberta&quot;, &quot;en&quot;, &quot;clincial/models&quot;)\ .setInputCols([&quot;sentence&quot;, &quot;token&quot;])\ .setOutputCol(&quot;zero_shot_ner&quot;)\ .setEntityDefinitions( { &quot;NAME&quot;: [&quot;What is his name?&quot;, &quot;What is my name?&quot;, &quot;What is her name?&quot;], &quot;CITY&quot;: [&quot;Which city?&quot;, &quot;Which is the city?&quot;] }) ner_converter = NerConverter()\ .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;zero_shot_ner&quot;])\ .setOutputCol(&quot;ner_chunk&quot;)\ pipeline = Pipeline(stages = [ documentAssembler, sentenceDetector, tokenizer, zero_shot_ner, ner_converter]) data = spark.createDataFrame([&quot;Hellen works in London, Paris and Berlin. My name is Clara, I live in New York and Hellen lives in Paris.&quot;, &quot;John is a man who works in London, London and London.&quot;], StringType()).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val zero_shot_ner = ZeroShotNerModel.pretrained(&quot;zero_shot_ner_roberta&quot;, &quot;en&quot;, &quot;clincial/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;zero_shot_ner&quot;) .setEntityDefinitions(Map( &quot;NAME&quot;-&amp;gt; Array(&quot;What is his name?&quot;, &quot;What is my name?&quot;, &quot;What is her name?&quot;), &quot;CITY&quot;-&amp;gt; Array(&quot;Which city?&quot;, &quot;Which is the city?&quot;) )) val ner_converter = new NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;zero_shot_ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) val pipeline = new .setStages(Array( documentAssembler, sentenceDetector, tokenizer, zero_shot_ner, ner_converter)) val data = Seq(Array(&quot;Hellen works in London, Paris and Berlin. My name is Clara, I live in New York and Hellen lives in Paris.&quot;, &quot;John is a man who works in London, London and London.&quot;)toDS().toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) Results +------+---------+--------+-----+---+----------+ | token|ner_label|sentence|begin|end|confidence| +------+---------+--------+-----+---+----------+ |Hellen| B-NAME| 0| 0| 5|0.13306311| | works| O| 0| 7| 11| null| | in| O| 0| 13| 14| null| |London| B-CITY| 0| 16| 21| 0.4064213| | ,| O| 0| 22| 22| null| | Paris| B-CITY| 0| 24| 28|0.04597357| | and| O| 0| 30| 32| null| |Berlin| B-CITY| 0| 34| 39|0.16265489| | .| O| 0| 40| 40| null| | My| O| 1| 42| 43| null| | name| O| 1| 45| 48| null| | is| O| 1| 50| 51| null| | Clara| B-NAME| 1| 53| 57| 0.9274031| | ,| O| 1| 58| 58| null| | I| O| 1| 60| 60| null| | live| O| 1| 62| 65| null| | in| O| 1| 67| 68| null| | New| B-CITY| 1| 70| 72|0.82799006| | York| I-CITY| 1| 74| 77|0.82799006| | and| O| 1| 79| 81| null| |Hellen| B-NAME| 1| 83| 88|0.40429682| | lives| O| 1| 90| 94| null| | in| O| 1| 96| 97| null| | Paris| B-CITY| 1| 99|103|0.49216735| | .| O| 1| 104|104| null| | John| B-NAME| 0| 0| 3|0.14063153| | is| O| 0| 5| 6| null| | a| O| 0| 8| 8| null| | man| O| 0| 10| 12| null| | who| O| 0| 14| 16| null| | works| O| 0| 18| 22| null| | in| O| 0| 24| 25| null| |London| B-CITY| 0| 27| 32|0.15521188| | ,| O| 0| 33| 33| null| |London| B-CITY| 0| 35| 40|0.12151082| | and| O| 0| 42| 44| null| |London| B-CITY| 0| 46| 51| 0.2650951| | .| O| 0| 52| 52| null| +------+---------+--------+-----+---+----------+ Model Information Model Name: zero_shot_ner_roberta Compatibility: Spark NLP 4.3.0+ License: Open Source Edition: Official Input Labels: [document_question, document_context] Output Labels: [answer] Language: en Size: 463.8 MB Case sensitive: true Max sentence length: 512</summary></entry><entry><title type="html">ASR HubertForCTC - asr_hubert_large_ls960</title><link href="/2023/02/07/asr_hubert_large_ls960_en.html" rel="alternate" type="text/html" title="ASR HubertForCTC - asr_hubert_large_ls960" /><published>2023-02-07T00:00:00+00:00</published><updated>2023-02-07T00:00:00+00:00</updated><id>/2023/02/07/asr_hubert_large_ls960_en</id><content type="html" xml:base="/2023/02/07/asr_hubert_large_ls960_en.html">## Description

Hubert Model with a language modeling head on top for Connectionist Temporal Classification (CTC). 
Hubert was proposed in HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.

The large model fine-tuned on 960h of Librispeech on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/asr_hubert_large_ls960_en_4.3.0_3.0_1675767067233.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/asr_hubert_large_ls960_en_4.3.0_3.0_1675767067233.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
                
audio_assembler = AudioAssembler()\
  .setInputCol(&quot;audio_content&quot;)\
  .setOutputCol(&quot;audio_assembler&quot;)

speech_to_text = HubertForCTC.pretrained(&quot;asr_hubert_large_ls960&quot;, &quot;en&quot;)\
  .setInputCols(&quot;audio_assembler&quot;)\
  .setOutputCol(&quot;text&quot;)

pipeline = Pipeline(stages=[
  audio_assembler,
  speech_to_text,
])

pipelineModel = pipeline.fit(audioDf)

pipelineDF = pipelineModel.transform(audioDf)
```
```scala

val audioAssembler = new AudioAssembler()
    .setInputCol(&quot;audio_content&quot;) 
    .setOutputCol(&quot;audio_assembler&quot;)

val speechToText = HubertForCTC
    .pretrained(&quot;asr_hubert_large_ls960&quot;, &quot;en&quot;)
    .setInputCols(&quot;audio_assembler&quot;) 
    .setOutputCol(&quot;text&quot;) 

val pipeline = new Pipeline().setStages(Array(audioAssembler, speechToText))

val pipelineModel = pipeline.fit(audioDf)

val pipelineDF = pipelineModel.transform(audioDf)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|asr_hubert_large_ls960|
|Compatibility:|Spark NLP 4.3.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[audio_assembler]|
|Output Labels:|[text]|
|Language:|en|
|Size:|1.5 GB|

## References

[https://huggingface.co/facebook/hubert-large-ls960-ft](https://huggingface.co/facebook/hubert-large-ls960-ft)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="hubert" /><category term="audio" /><category term="en" /><category term="english" /><category term="asr" /><category term="speech" /><category term="librispeech_asr" /><category term="tensorflow" /><summary type="html">Description Hubert Model with a language modeling head on top for Connectionist Temporal Classification (CTC). Hubert was proposed in HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed. The large model fine-tuned on 960h of Librispeech on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU audio_assembler = AudioAssembler()\ .setInputCol(&quot;audio_content&quot;)\ .setOutputCol(&quot;audio_assembler&quot;) speech_to_text = HubertForCTC.pretrained(&quot;asr_hubert_large_ls960&quot;, &quot;en&quot;)\ .setInputCols(&quot;audio_assembler&quot;)\ .setOutputCol(&quot;text&quot;) pipeline = Pipeline(stages=[ audio_assembler, speech_to_text, ]) pipelineModel = pipeline.fit(audioDf) pipelineDF = pipelineModel.transform(audioDf) val audioAssembler = new AudioAssembler() .setInputCol(&quot;audio_content&quot;) .setOutputCol(&quot;audio_assembler&quot;) val speechToText = HubertForCTC .pretrained(&quot;asr_hubert_large_ls960&quot;, &quot;en&quot;) .setInputCols(&quot;audio_assembler&quot;) .setOutputCol(&quot;text&quot;) val pipeline = new Pipeline().setStages(Array(audioAssembler, speechToText)) val pipelineModel = pipeline.fit(audioDf) val pipelineDF = pipelineModel.transform(audioDf) Model Information Model Name: asr_hubert_large_ls960 Compatibility: Spark NLP 4.3.0+ License: Open Source Edition: Official Input Labels: [audio_assembler] Output Labels: [text] Language: en Size: 1.5 GB References https://huggingface.co/facebook/hubert-large-ls960-ft</summary></entry><entry><title type="html">SwinForImageClassification - image_classifier_swin_base_patch4_window12_384_in22k</title><link href="/2023/02/07/image_classifier_swin_base_patch4_window12_384_in22k_en.html" rel="alternate" type="text/html" title="SwinForImageClassification - image_classifier_swin_base_patch4_window12_384_in22k" /><published>2023-02-07T00:00:00+00:00</published><updated>2023-02-07T00:00:00+00:00</updated><id>/2023/02/07/image_classifier_swin_base_patch4_window12_384_in22k_en</id><content type="html" xml:base="/2023/02/07/image_classifier_swin_base_patch4_window12_384_in22k_en.html">## Description

Pretrained Swin model for Image Classification, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP.

Swin Transformer was introduced in the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Liu et al.

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/image_classifier_swin_base_patch4_window12_384_in22k_en_4.3.0_3.0_1675783085913.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/image_classifier_swin_base_patch4_window12_384_in22k_en_4.3.0_3.0_1675783085913.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
                
image_assembler = ImageAssembler()\
  .setInputCol(&quot;image&quot;)
  .setOutputCol(&quot;image_assembler&quot;)

imageClassifier = SwinForImageClassification.pretrained(&quot;image_classifier_swin_base_patch4_window12_384_in22k&quot;, &quot;en&quot;)\
  .setInputCols(&quot;image_assembler&quot;)\
  .setOutputCol(&quot;class&quot;)

pipeline = Pipeline(stages=[
  image_assembler,
  imageClassifier,
])

pipelineModel = pipeline.fit(imageDF)

pipelineDF = pipelineModel.transform(imageDF)
```
```scala

val imageAssembler = new ImageAssembler()
    .setInputCol(&quot;image&quot;)
    .setOutputCol(&quot;image_assembler&quot;)

val imageClassifier = SwinForImageClassification
    .pretrained(&quot;image_classifier_swin_base_patch4_window12_384_in22k&quot;, &quot;en&quot;)
    .setInputCols(&quot;image_assembler&quot;) 
    .setOutputCol(&quot;class&quot;) 

val pipeline = new Pipeline().setStages(Array(imageAssembler, imageClassifier))

val pipelineModel = pipeline.fit(imageDF)

val pipelineDF = pipelineModel.transform(imageDF)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|image_classifier_swin_base_patch4_window12_384_in22k|
|Compatibility:|Spark NLP 4.3.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[image_assembler]|
|Output Labels:|[class]|
|Language:|en|
|Size:|826.8 MB|

## References

[https://huggingface.co/microsoft/swin_base_patch4_window12_384_in22k](https://huggingface.co/microsoft/swin_base_patch4_window12_384_in22k)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="swin" /><category term="image" /><category term="en" /><category term="english" /><category term="image_classification" /><category term="imagenet" /><category term="tensorflow" /><summary type="html">Description Pretrained Swin model for Image Classification, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP. Swin Transformer was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU image_assembler = ImageAssembler()\ .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) imageClassifier = SwinForImageClassification.pretrained(&quot;image_classifier_swin_base_patch4_window12_384_in22k&quot;, &quot;en&quot;)\ .setInputCols(&quot;image_assembler&quot;)\ .setOutputCol(&quot;class&quot;) pipeline = Pipeline(stages=[ image_assembler, imageClassifier, ]) pipelineModel = pipeline.fit(imageDF) pipelineDF = pipelineModel.transform(imageDF) val imageAssembler = new ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) val imageClassifier = SwinForImageClassification .pretrained(&quot;image_classifier_swin_base_patch4_window12_384_in22k&quot;, &quot;en&quot;) .setInputCols(&quot;image_assembler&quot;) .setOutputCol(&quot;class&quot;) val pipeline = new Pipeline().setStages(Array(imageAssembler, imageClassifier)) val pipelineModel = pipeline.fit(imageDF) val pipelineDF = pipelineModel.transform(imageDF) Model Information Model Name: image_classifier_swin_base_patch4_window12_384_in22k Compatibility: Spark NLP 4.3.0+ License: Open Source Edition: Official Input Labels: [image_assembler] Output Labels: [class] Language: en Size: 826.8 MB References https://huggingface.co/microsoft/swin_base_patch4_window12_384_in22k</summary></entry><entry><title type="html">SwinForImageClassification - image_classifier_swin_base_patch4_window7_224</title><link href="/2023/02/07/image_classifier_swin_base_patch4_window7_224_en.html" rel="alternate" type="text/html" title="SwinForImageClassification - image_classifier_swin_base_patch4_window7_224" /><published>2023-02-07T00:00:00+00:00</published><updated>2023-02-07T00:00:00+00:00</updated><id>/2023/02/07/image_classifier_swin_base_patch4_window7_224_en</id><content type="html" xml:base="/2023/02/07/image_classifier_swin_base_patch4_window7_224_en.html">## Description

Pretrained Swin model for Image Classification, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP.

Swin Transformer was introduced in the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Liu et al.

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/image_classifier_swin_base_patch4_window7_224_en_4.3.0_3.0_1675783112124.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/image_classifier_swin_base_patch4_window7_224_en_4.3.0_3.0_1675783112124.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
                
image_assembler = ImageAssembler()\
  .setInputCol(&quot;image&quot;)\
  .setOutputCol(&quot;image_assembler&quot;)

imageClassifier = SwinForImageClassification.pretrained(&quot;image_classifier_swin_base_patch4_window7_224&quot;, &quot;en&quot;)\
  .setInputCols(&quot;image_assembler&quot;)\
  .setOutputCol(&quot;class&quot;)

pipeline = Pipeline(stages=[
  image_assembler,
  imageClassifier,
])

pipelineModel = pipeline.fit(imageDF)

pipelineDF = pipelineModel.transform(imageDF)
```
```scala

val imageAssembler = new ImageAssembler()
    .setInputCol(&quot;image&quot;)
    .setOutputCol(&quot;image_assembler&quot;)

val imageClassifier = SwinForImageClassification
    .pretrained(&quot;image_classifier_swin_base_patch4_window7_224&quot;, &quot;en&quot;)
    .setInputCols(&quot;image_assembler&quot;) 
    .setOutputCol(&quot;class&quot;) 

val pipeline = new Pipeline().setStages(Array(imageAssembler, imageClassifier))

val pipelineModel = pipeline.fit(imageDF)

val pipelineDF = pipelineModel.transform(imageDF)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|image_classifier_swin_base_patch4_window7_224|
|Compatibility:|Spark NLP 4.3.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[image_assembler]|
|Output Labels:|[class]|
|Language:|en|
|Size:|665.2 MB|

## References

[https://huggingface.co/microsoft/swin_base_patch4_window7_224](https://huggingface.co/microsoft/swin_base_patch4_window7_224)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="swin" /><category term="image" /><category term="en" /><category term="english" /><category term="image_classification" /><category term="imagenet" /><category term="tensorflow" /><summary type="html">Description Pretrained Swin model for Image Classification, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP. Swin Transformer was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU image_assembler = ImageAssembler()\ .setInputCol(&quot;image&quot;)\ .setOutputCol(&quot;image_assembler&quot;) imageClassifier = SwinForImageClassification.pretrained(&quot;image_classifier_swin_base_patch4_window7_224&quot;, &quot;en&quot;)\ .setInputCols(&quot;image_assembler&quot;)\ .setOutputCol(&quot;class&quot;) pipeline = Pipeline(stages=[ image_assembler, imageClassifier, ]) pipelineModel = pipeline.fit(imageDF) pipelineDF = pipelineModel.transform(imageDF) val imageAssembler = new ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) val imageClassifier = SwinForImageClassification .pretrained(&quot;image_classifier_swin_base_patch4_window7_224&quot;, &quot;en&quot;) .setInputCols(&quot;image_assembler&quot;) .setOutputCol(&quot;class&quot;) val pipeline = new Pipeline().setStages(Array(imageAssembler, imageClassifier)) val pipelineModel = pipeline.fit(imageDF) val pipelineDF = pipelineModel.transform(imageDF) Model Information Model Name: image_classifier_swin_base_patch4_window7_224 Compatibility: Spark NLP 4.3.0+ License: Open Source Edition: Official Input Labels: [image_assembler] Output Labels: [class] Language: en Size: 665.2 MB References https://huggingface.co/microsoft/swin_base_patch4_window7_224</summary></entry><entry><title type="html">SwinForImageClassification - image_classifier_swin_base_patch4_window7_224_in22k</title><link href="/2023/02/07/image_classifier_swin_base_patch4_window7_224_in22k_en.html" rel="alternate" type="text/html" title="SwinForImageClassification - image_classifier_swin_base_patch4_window7_224_in22k" /><published>2023-02-07T00:00:00+00:00</published><updated>2023-02-07T00:00:00+00:00</updated><id>/2023/02/07/image_classifier_swin_base_patch4_window7_224_in22k_en</id><content type="html" xml:base="/2023/02/07/image_classifier_swin_base_patch4_window7_224_in22k_en.html">## Description

Pretrained Swin model for Image Classification, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP.

Swin Transformer was introduced in the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Liu et al.

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/image_classifier_swin_base_patch4_window7_224_in22k_en_4.3.0_3.0_1675783139673.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/image_classifier_swin_base_patch4_window7_224_in22k_en_4.3.0_3.0_1675783139673.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
                
image_assembler = ImageAssembler()\
  .setInputCol(&quot;image&quot;)\
  .setOutputCol(&quot;image_assembler&quot;)

imageClassifier = SwinForImageClassification.pretrained(&quot;image_classifier_swin_base_patch4_window7_224_in22k&quot;, &quot;en&quot;)\
  .setInputCols(&quot;image_assembler&quot;)\
  .setOutputCol(&quot;class&quot;)

pipeline = Pipeline(stages=[
  image_assembler,
  imageClassifier,
])

pipelineModel = pipeline.fit(imageDF)

pipelineDF = pipelineModel.transform(imageDF)
```
```scala

val imageAssembler = new ImageAssembler()
    .setInputCol(&quot;image&quot;)
    .setOutputCol(&quot;image_assembler&quot;)

val imageClassifier = SwinForImageClassification
    .pretrained(&quot;image_classifier_swin_base_patch4_window7_224_in22k&quot;, &quot;en&quot;)
    .setInputCols(&quot;image_assembler&quot;) 
    .setOutputCol(&quot;class&quot;) 

val pipeline = new Pipeline().setStages(Array(imageAssembler, imageClassifier))

val pipelineModel = pipeline.fit(imageDF)

val pipelineDF = pipelineModel.transform(imageDF)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|image_classifier_swin_base_patch4_window7_224_in22k|
|Compatibility:|Spark NLP 4.3.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[image_assembler]|
|Output Labels:|[class]|
|Language:|en|
|Size:|825.5 MB|

## References

[https://huggingface.co/microsoft/swin_base_patch4_window7_224_in22k](https://huggingface.co/microsoft/swin_base_patch4_window7_224_in22k)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="swin" /><category term="image" /><category term="en" /><category term="english" /><category term="image_classification" /><category term="imagenet" /><category term="tensorflow" /><summary type="html">Description Pretrained Swin model for Image Classification, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP. Swin Transformer was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU image_assembler = ImageAssembler()\ .setInputCol(&quot;image&quot;)\ .setOutputCol(&quot;image_assembler&quot;) imageClassifier = SwinForImageClassification.pretrained(&quot;image_classifier_swin_base_patch4_window7_224_in22k&quot;, &quot;en&quot;)\ .setInputCols(&quot;image_assembler&quot;)\ .setOutputCol(&quot;class&quot;) pipeline = Pipeline(stages=[ image_assembler, imageClassifier, ]) pipelineModel = pipeline.fit(imageDF) pipelineDF = pipelineModel.transform(imageDF) val imageAssembler = new ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) val imageClassifier = SwinForImageClassification .pretrained(&quot;image_classifier_swin_base_patch4_window7_224_in22k&quot;, &quot;en&quot;) .setInputCols(&quot;image_assembler&quot;) .setOutputCol(&quot;class&quot;) val pipeline = new Pipeline().setStages(Array(imageAssembler, imageClassifier)) val pipelineModel = pipeline.fit(imageDF) val pipelineDF = pipelineModel.transform(imageDF) Model Information Model Name: image_classifier_swin_base_patch4_window7_224_in22k Compatibility: Spark NLP 4.3.0+ License: Open Source Edition: Official Input Labels: [image_assembler] Output Labels: [class] Language: en Size: 825.5 MB References https://huggingface.co/microsoft/swin_base_patch4_window7_224_in22k</summary></entry></feed>