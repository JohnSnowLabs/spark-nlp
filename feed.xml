<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-02-22T14:51:57+00:00</updated><id>/feed.xml</id><title type="html">Spark NLP</title><subtitle>High Performance NLP with Apache Spark
</subtitle><author><name>{&quot;type&quot;=&gt;nil, &quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;googleplus&quot;=&gt;nil, &quot;telegram&quot;=&gt;nil, &quot;medium&quot;=&gt;nil, &quot;zhihu&quot;=&gt;nil, &quot;douban&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;npm&quot;=&gt;nil}</name></author><entry><title type="html">Classifying Proposal Comments</title><link href="/2023/02/17/legclf_bert_support_proposal_en.html" rel="alternate" type="text/html" title="Classifying Proposal Comments" /><published>2023-02-17T00:00:00+00:00</published><updated>2023-02-17T00:00:00+00:00</updated><id>/2023/02/17/legclf_bert_support_proposal_en</id><content type="html" xml:base="/2023/02/17/legclf_bert_support_proposal_en.html">## Description

Given a proposal on a socially important issue, the model classifies whether a comment is `In_Favor`, `Against`, or `Neutral` towards the proposal.

## Predicted Entities

`In_Favor`, `Neutral`, `Against`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/legal/models/legclf_bert_support_proposal_en_1.0.0_3.0_1676599695968.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/legal/models/legclf_bert_support_proposal_en_1.0.0_3.0_1676599695968.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
document_assembler = nlp.DocumentAssembler() \
    .setInputCol(&quot;text&quot;)\
    .setOutputCol(&quot;document&quot;)

tokenizer = nlp.Tokenizer()\
    .setInputCols([&quot;document&quot;]) \
    .setOutputCol(&quot;token&quot;)

classifier = legal.BertForSequenceClassification.pretrained(&quot;legclf_bert_support_proposal&quot;, &quot;en&quot;, &quot;legal/models&quot;)\
    .setInputCols([&quot;document&quot;, &quot;token&quot;])\
    .setOutputCol(&quot;class&quot;)

clf_pipeline = nlp.Pipeline(stages=[
    document_assembler, 
    tokenizer,
    classifier    
])

empty_df = spark.createDataFrame([['']]).toDF(&quot;text&quot;)

model = clf_pipeline.fit(empty_df)

sample_text = [&quot;&quot;&quot;This is one of the most boring movies I have ever seen, its horrible. Christopher Lee is good but he is hardly in it, the only good part is the opening scene. Don't be fooled by the title. &quot;End of the World&quot; is truly a bad movie, I stopped watching it close to the end it was so bad, only for die-hard b-movie fans that have the brain to stand this vomit.&quot;&quot;&quot;,

&quot;&quot;&quot;Of course, there is still a lot of possible improvement in the pipeline, but we definitely don't have to wait for some genius new technology to start. Why am I so definitely against this proposal though it sounds so reasonable and helpful? I'm definitely against the notion that we'll have to wait for a new genius industrial technology to show up to even think of starting a proper transformation. In my opinion, the opposite is true: We have to start right now with what we have &amp; by the way develop better concepts of how to use all the technology &amp; methods already available optimally. And for me, nuclear energy which is - by the way - relaunched with this proposal, is definitely not part of the game, not even in the modular mini-nuke version of Mr. Gates. There are people who know much more about renewable energy than Mr. Gates &amp; completely energy independent who hate that book because of this crap.&quot;&quot;&quot;,

&quot;&quot;&quot;One common defense policy would strengthen the voice and influence in our own backyard. A strong EU army can be a stabilizing factor in the unstable regions around our continent. We Europeans should take our safety and defense into our own hands and not rely on the US to do it for us.&quot;&quot;&quot;
]

test = spark.createDataFrame(pd.DataFrame({&quot;text&quot;: sample_text}))

result = model.transform(test)
```

&lt;/div&gt;

## Results

```bash
+--------+--------------------+
|   class|            document|
+--------+--------------------+
| Neutral|This is one of the...|
| Against|Of course, there ...|
|In_Favor|One common defense...|
+--------+--------------------+
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|legclf_bert_support_proposal|
|Compatibility:|Legal NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[document, token]|
|Output Labels:|[class]|
|Language:|en|
|Size:|403.0 MB|
|Case sensitive:|true|
|Max sentence length:|512|

## References

Train dataset available [here](https://touche.webis.de/clef23/touche23-web/multilingual-stance-classification.html#data)

## Benchmarking

```bash
label         precision  recall  f1-score  support 
Against       0.84       0.87    0.86      85      
In_Favor      0.87       0.84    0.86      90      
Neutral       0.98       0.98    0.98      57      
accuracy      -          -       0.89      232     
macro-avg     0.90       0.90    0.90      232     
weighted-avg  0.89       0.89    0.89      232   
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="legal" /><category term="licensed" /><category term="classification" /><category term="proposal" /><category term="support" /><category term="tensorflow" /><summary type="html">Description Given a proposal on a socially important issue, the model classifies whether a comment is In_Favor, Against, or Neutral towards the proposal. Predicted Entities In_Favor, Neutral, Against Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler() \ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) tokenizer = nlp.Tokenizer()\ .setInputCols([&quot;document&quot;]) \ .setOutputCol(&quot;token&quot;) classifier = legal.BertForSequenceClassification.pretrained(&quot;legclf_bert_support_proposal&quot;, &quot;en&quot;, &quot;legal/models&quot;)\ .setInputCols([&quot;document&quot;, &quot;token&quot;])\ .setOutputCol(&quot;class&quot;) clf_pipeline = nlp.Pipeline(stages=[ document_assembler, tokenizer, classifier ]) empty_df = spark.createDataFrame([['']]).toDF(&quot;text&quot;) model = clf_pipeline.fit(empty_df) sample_text = [&quot;&quot;&quot;This is one of the most boring movies I have ever seen, its horrible. Christopher Lee is good but he is hardly in it, the only good part is the opening scene. Don't be fooled by the title. &quot;End of the World&quot; is truly a bad movie, I stopped watching it close to the end it was so bad, only for die-hard b-movie fans that have the brain to stand this vomit.&quot;&quot;&quot;, &quot;&quot;&quot;Of course, there is still a lot of possible improvement in the pipeline, but we definitely don't have to wait for some genius new technology to start. Why am I so definitely against this proposal though it sounds so reasonable and helpful? I'm definitely against the notion that we'll have to wait for a new genius industrial technology to show up to even think of starting a proper transformation. In my opinion, the opposite is true: We have to start right now with what we have &amp;amp; by the way develop better concepts of how to use all the technology &amp;amp; methods already available optimally. And for me, nuclear energy which is - by the way - relaunched with this proposal, is definitely not part of the game, not even in the modular mini-nuke version of Mr. Gates. There are people who know much more about renewable energy than Mr. Gates &amp;amp; completely energy independent who hate that book because of this crap.&quot;&quot;&quot;, &quot;&quot;&quot;One common defense policy would strengthen the voice and influence in our own backyard. A strong EU army can be a stabilizing factor in the unstable regions around our continent. We Europeans should take our safety and defense into our own hands and not rely on the US to do it for us.&quot;&quot;&quot; ] test = spark.createDataFrame(pd.DataFrame({&quot;text&quot;: sample_text})) result = model.transform(test) Results +--------+--------------------+ | class| document| +--------+--------------------+ | Neutral|This is one of the...| | Against|Of course, there ...| |In_Favor|One common defense...| +--------+--------------------+ Model Information Model Name: legclf_bert_support_proposal Compatibility: Legal NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [document, token] Output Labels: [class] Language: en Size: 403.0 MB Case sensitive: true Max sentence length: 512 References Train dataset available here Benchmarking label precision recall f1-score support Against 0.84 0.87 0.86 85 In_Favor 0.87 0.84 0.86 90 Neutral 0.98 0.98 0.98 57 accuracy - - 0.89 232 macro-avg 0.90 0.90 0.90 232 weighted-avg 0.89 0.89 0.89 232</summary></entry><entry><title type="html">Legal Proposal Classification</title><link href="/2023/02/17/legclf_proposal_topic_en.html" rel="alternate" type="text/html" title="Legal Proposal Classification" /><published>2023-02-17T00:00:00+00:00</published><updated>2023-02-17T00:00:00+00:00</updated><id>/2023/02/17/legclf_proposal_topic_en</id><content type="html" xml:base="/2023/02/17/legclf_proposal_topic_en.html">## Description

Given a proposal on a socially important issue, this model classifies it according to its topic.

## Predicted Entities

`Democracy`, `Digital`, `EU_In_The_World`, `Economy`, `Education`, `Green_Deal`, `Health`, `Migration`, `Other`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/legal/models/legclf_proposal_topic_en_1.0.0_3.0_1676594573703.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/legal/models/legclf_proposal_topic_en_1.0.0_3.0_1676594573703.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
document_assembler = nlp.DocumentAssembler()\
    .setInputCol(&quot;text&quot;)\
    .setOutputCol(&quot;document&quot;)
    
sentence_embeddings = nlp.UniversalSentenceEncoder.pretrained()\
    .setInputCols(&quot;document&quot;)\
    .setOutputCol(&quot;sentence_embeddings&quot;)

classifier = legal.ClassifierDLModel.pretrained(&quot;legclf_proposal_topic&quot;, &quot;en&quot;, &quot;legal/models&quot;)\
    .setInputCols([&quot;sentence_embeddings&quot;])\
    .setOutputCol(&quot;class&quot;)

clf_pipeline = nlp.Pipeline(stages=[
            document_assembler, 
            sentence_embeddings,
            classifier
            ])

empty_df = spark.createDataFrame([['']]).toDF(&quot;text&quot;)

model = clf_pipeline.fit(empty_df)

text = [&quot;&quot;&quot;In order to involve young people in the European Union, they need to understand the role, importance, and impact of the European Union on their lives and how they can contribute to the EU. I believe that many Europeans do not know the values of Europe, how they can contribute to the EU, etc. To do this, it was necessary to create an education program on the European Union that could cut across all countries, including a discipline on the EU, visits by young people to the European institutions, and a 'channel of communication' between young people and the EU. The same could be done for older people in senior universities.&quot;&quot;&quot;]

data = spark.createDataFrame([text]).toDF(&quot;text&quot;)

result = model.transform(data)
```

&lt;/div&gt;

## Results

```bash
+-----------+
|     result|
+-----------+
|[Education]|
+-----------+
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|legclf_proposal_topic|
|Compatibility:|Legal NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[sentence_embeddings]|
|Output Labels:|[class]|
|Language:|en|
|Size:|22.6 MB|

## References

Training dataset available [here](https://touche.webis.de/clef23/touche23-web/multilingual-stance-classification.html#data)

## Benchmarking

```bash
label            precision  recall  f1-score  support 
Democracy        0.86       0.90    0.88      62      
Digital          0.85       0.80    0.82      35      
EU_In_The_World  0.78       0.72    0.75      39      
Economy          0.82       0.77    0.80      43      
Education        0.89       0.87    0.88      46      
Green_Deal       0.85       0.92    0.88      49      
Health           0.87       0.95    0.91      21      
Migration        0.86       0.89    0.87      27      
Other            1.00       0.97    0.98      32      
accuracy         -          -       0.86      354     
macro-avg        0.86       0.87    0.86      354     
weighted-avg     0.86       0.86    0.86      354 
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="legal" /><category term="classification" /><category term="proposal" /><category term="licensed" /><category term="tensorflow" /><summary type="html">Description Given a proposal on a socially important issue, this model classifies it according to its topic. Predicted Entities Democracy, Digital, EU_In_The_World, Economy, Education, Green_Deal, Health, Migration, Other Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) sentence_embeddings = nlp.UniversalSentenceEncoder.pretrained()\ .setInputCols(&quot;document&quot;)\ .setOutputCol(&quot;sentence_embeddings&quot;) classifier = legal.ClassifierDLModel.pretrained(&quot;legclf_proposal_topic&quot;, &quot;en&quot;, &quot;legal/models&quot;)\ .setInputCols([&quot;sentence_embeddings&quot;])\ .setOutputCol(&quot;class&quot;) clf_pipeline = nlp.Pipeline(stages=[ document_assembler, sentence_embeddings, classifier ]) empty_df = spark.createDataFrame([['']]).toDF(&quot;text&quot;) model = clf_pipeline.fit(empty_df) text = [&quot;&quot;&quot;In order to involve young people in the European Union, they need to understand the role, importance, and impact of the European Union on their lives and how they can contribute to the EU. I believe that many Europeans do not know the values of Europe, how they can contribute to the EU, etc. To do this, it was necessary to create an education program on the European Union that could cut across all countries, including a discipline on the EU, visits by young people to the European institutions, and a 'channel of communication' between young people and the EU. The same could be done for older people in senior universities.&quot;&quot;&quot;] data = spark.createDataFrame([text]).toDF(&quot;text&quot;) result = model.transform(data) Results +-----------+ | result| +-----------+ |[Education]| +-----------+ Model Information Model Name: legclf_proposal_topic Compatibility: Legal NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [sentence_embeddings] Output Labels: [class] Language: en Size: 22.6 MB References Training dataset available here Benchmarking label precision recall f1-score support Democracy 0.86 0.90 0.88 62 Digital 0.85 0.80 0.82 35 EU_In_The_World 0.78 0.72 0.75 39 Economy 0.82 0.77 0.80 43 Education 0.89 0.87 0.88 46 Green_Deal 0.85 0.92 0.88 49 Health 0.87 0.95 0.91 21 Migration 0.86 0.89 0.87 27 Other 1.00 0.97 0.98 32 accuracy - - 0.86 354 macro-avg 0.86 0.87 0.86 354 weighted-avg 0.86 0.86 0.86 354</summary></entry><entry><title type="html">Legal Relation Extraction Pretrained Pipeline(Parties, Alias, Dates, Document Type) (Lg, Unidirectional)</title><link href="/2023/02/17/legpipe_re_contract_doc_parties_alias_en.html" rel="alternate" type="text/html" title="Legal Relation Extraction Pretrained Pipeline(Parties, Alias, Dates, Document Type) (Lg, Unidirectional)" /><published>2023-02-17T00:00:00+00:00</published><updated>2023-02-17T00:00:00+00:00</updated><id>/2023/02/17/legpipe_re_contract_doc_parties_alias_en</id><content type="html" xml:base="/2023/02/17/legpipe_re_contract_doc_parties_alias_en.html">## Description

This is a Legal Relation Extraction Pretrained Pipeline to get the relations linking the different concepts together, if such relation exists. The list of relations is:

- dated_as: A Document has an Effective Date
- has_alias: The Alias of a Party all along the document
- has_collective_alias: An Alias hold by several parties at the same time
- signed_by: Between a Party and the document they signed

## Predicted Entities

`dated_as`, `has_alias`, `has_collective_alias`, `signed_by`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/legal/models/legpipe_re_contract_doc_parties_alias_en_1.0.0_3.0_1676647465198.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/legal/models/legpipe_re_contract_doc_parties_alias_en_1.0.0_3.0_1676647465198.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python

legal_pipeline = nlp.PretrainedPipeline(&quot;legpipe_re_contract_doc_parties_alias&quot;, &quot;en&quot;, &quot;finance/models&quot;)
text = '''THIS Lease Agreement , is made and entered into this _____day of May, 2006 by and between Apple, Inc., (hereinafter called &quot;Landlord&quot;), and IMI Global, Inc., with a mailing address of ___, (hereinafter referred as &quot;Tenant&quot;).'''
result = legal_pipeline.annotate(text)

```

&lt;/div&gt;

## Results

```bash

+---------+-----------------+--------------------+-----------------+----------------+----------+------------------+
|relations|relations_entity1|    relations_chunk1|relations_entity2|relations_chunk2|confidence|syntactic_distance|
+---------+-----------------+--------------------+-----------------+----------------+----------+------------------+
| dated_as|              DOC|THIS Lease Agreement|          EFFDATE|   of May,  2006| 0.9999546|                 6|
|signed_by|              DOC|THIS Lease Agreement|            PARTY|      Apple, Inc|  0.988555|                 5|
|signed_by|              DOC|THIS Lease Agreement|            PARTY|IMI Global,  Inc| 0.9568861|                 7|
|has_alias|            PARTY|          Apple, Inc|            ALIAS|        Landlord|0.99999475|                 4|
|has_alias|            PARTY|    IMI Global,  Inc|            ALIAS|          Tenant| 0.9999893|                 4|
+---------+-----------------+--------------------+-----------------+----------------+----------+------------------+

```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|legpipe_re_contract_doc_parties_alias|
|Type:|pipeline|
|Compatibility:|Legal NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Language:|en|
|Size:|910.2 MB|

## Included Models

- DocumentAssembler
- SentenceDetector
- TokenizerModel
- RoBertaEmbeddings
- PerceptronModel
- DependencyParserModel
- LegalNerModel
- NerConverter
- RENerChunksFilter
- RelationExtractionDLModel</content><author><name>John Snow Labs</name></author><category term="legal" /><category term="licensed" /><category term="agreements" /><category term="en" /><category term="pipeline" /><summary type="html">Description This is a Legal Relation Extraction Pretrained Pipeline to get the relations linking the different concepts together, if such relation exists. The list of relations is: dated_as: A Document has an Effective Date has_alias: The Alias of a Party all along the document has_collective_alias: An Alias hold by several parties at the same time signed_by: Between a Party and the document they signed Predicted Entities dated_as, has_alias, has_collective_alias, signed_by Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU legal_pipeline = nlp.PretrainedPipeline(&quot;legpipe_re_contract_doc_parties_alias&quot;, &quot;en&quot;, &quot;finance/models&quot;) text = '''THIS Lease Agreement , is made and entered into this _____day of May, 2006 by and between Apple, Inc., (hereinafter called &quot;Landlord&quot;), and IMI Global, Inc., with a mailing address of ___, (hereinafter referred as &quot;Tenant&quot;).''' result = legal_pipeline.annotate(text) Results +---------+-----------------+--------------------+-----------------+----------------+----------+------------------+ |relations|relations_entity1| relations_chunk1|relations_entity2|relations_chunk2|confidence|syntactic_distance| +---------+-----------------+--------------------+-----------------+----------------+----------+------------------+ | dated_as| DOC|THIS Lease Agreement| EFFDATE| of May, 2006| 0.9999546| 6| |signed_by| DOC|THIS Lease Agreement| PARTY| Apple, Inc| 0.988555| 5| |signed_by| DOC|THIS Lease Agreement| PARTY|IMI Global, Inc| 0.9568861| 7| |has_alias| PARTY| Apple, Inc| ALIAS| Landlord|0.99999475| 4| |has_alias| PARTY| IMI Global, Inc| ALIAS| Tenant| 0.9999893| 4| +---------+-----------------+--------------------+-----------------+----------------+----------+------------------+ Model Information Model Name: legpipe_re_contract_doc_parties_alias Type: pipeline Compatibility: Legal NLP 1.0.0+ License: Licensed Edition: Official Language: en Size: 910.2 MB Included Models DocumentAssembler SentenceDetector TokenizerModel RoBertaEmbeddings PerceptronModel DependencyParserModel LegalNerModel NerConverter RENerChunksFilter RelationExtractionDLModel</summary></entry><entry><title type="html">Legal Relation Extraction (Parties, Alias, Dates, Document Type) (Lg, Unidirectional)</title><link href="/2023/02/17/legre_contract_doc_parties_lg_en.html" rel="alternate" type="text/html" title="Legal Relation Extraction (Parties, Alias, Dates, Document Type) (Lg, Unidirectional)" /><published>2023-02-17T00:00:00+00:00</published><updated>2023-02-17T00:00:00+00:00</updated><id>/2023/02/17/legre_contract_doc_parties_lg_en</id><content type="html" xml:base="/2023/02/17/legre_contract_doc_parties_lg_en.html">## Description

IMPORTANT: Don't run this model on the whole legal agreement. Instead:
- Split by paragraphs. You can use [notebook 1](https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/tutorials/Certification_Trainings_JSL) in Finance or Legal as inspiration;
- Use the `legclf_introduction_clause` Text Classifier to select only these paragraphs; 
- 
This is a Legal Relation Extraction model, which can be used after the NER Model for extracting Parties, Document Types, Effective Dates and Aliases, called legner_contract_doc_parties.

As an output, you will get the relations linking the different concepts together, if such relation exists. The list of relations is:

- dated_as: A Document has an Effective Date
- has_alias: The Alias of a Party all along the document
- has_collective_alias: An Alias hold by several parties at the same time
- signed_by: Between a Party and the document they signed

This is a `lg` model with Unidirectional Relations, meaning that the model retrieves in chunk1 the left side of the relation (source), and in chunk2 the right side (target).

## Predicted Entities

`dated_as`, `has_alias`, `has_collective_alias`, `signed_by`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/legal/models/legre_contract_doc_parties_lg_en_1.0.0_3.0_1676633934665.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/legal/models/legre_contract_doc_parties_lg_en_1.0.0_3.0_1676633934665.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
document_assembler = nlp.DocumentAssembler()\
  .setInputCol(&quot;text&quot;)\
  .setOutputCol(&quot;document&quot;)

sen = nlp.SentenceDetector()\
    .setInputCols([&quot;document&quot;])\
    .setOutputCol(&quot;sentence&quot;)

tokenizer = nlp.Tokenizer()\
    .setInputCols(&quot;sentence&quot;)\
    .setOutputCol(&quot;token&quot;)

embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;, &quot;en&quot;) \
    .setInputCols(&quot;sentence&quot;, &quot;token&quot;)\
    .setOutputCol(&quot;embeddings&quot;)\
    .setMaxSentenceLength(512)

pos_tagger = nlp.PerceptronModel()\
    .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) \
    .setInputCols([&quot;sentence&quot;, &quot;token&quot;])\
    .setOutputCol(&quot;pos_tags&quot;)
    
dependency_parser = nlp.DependencyParserModel()\
    .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;)\
    .setInputCols([&quot;sentence&quot;, &quot;pos_tags&quot;, &quot;token&quot;])\
    .setOutputCol(&quot;dependencies&quot;)

ner_model = legal.NerModel.pretrained('legner_contract_doc_parties_lg', 'en', 'legal/models')\
    .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;])\
    .setOutputCol(&quot;ner&quot;)

ner_converter = nlp.NerConverter()\
    .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;])\
    .setOutputCol(&quot;ner_chunk&quot;)

re_ner_chunk_filter = legal.RENerChunksFilter() \
    .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;])\
    .setOutputCol(&quot;re_ner_chunks&quot;)\
    .setMaxSyntacticDistance(7)\
    .setRelationPairs([&quot;DOC-EFFDATE&quot;, &quot;DOC-PARTY&quot;, &quot;PARTY-FORMER_NAME&quot;, &quot;PARTY-ALIAS&quot;])

re_model = legal.RelationExtractionDLModel().pretrained('legre_contract_doc_parties_lg', 'en', 'legal/models')\
    .setPredictionThreshold(0.5)\
    .setInputCols([&quot;re_ner_chunks&quot;, &quot;sentence&quot;])\
    .setOutputCol(&quot;relations&quot;)

nlpPipeline = nlp.Pipeline(stages=[
    document_assembler,
    sen,
    tokenizer,
    embeddings,
    pos_tagger,
    dependency_parser,
    ner_model,
    ner_converter,
    re_ner_chunk_filter,
    re_model
    ])

empty_df = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)

model = nlpPipeline.fit(empty_df)

text=&quot;&quot;&quot;THIS Lease Agreement ,  is made and  entered  into this  _____day of May,  2006 by and between Apple, Inc.,  (hereinafter called &quot;Landlord&quot;),  and IMI Global,  Inc., with a mailing address of ___,  (hereinafter referred as &quot;Tenant&quot;).&quot;&quot;&quot;

data = spark.createDataFrame([[text]]).toDF(&quot;text&quot;)

result = model.transform(data)
```

&lt;/div&gt;

## Results

```bash

+---------+-----------------+--------------------+-----------------+----------------+----------+------------------+
|relations|relations_entity1|    relations_chunk1|relations_entity2|relations_chunk2|confidence|syntactic_distance|
+---------+-----------------+--------------------+-----------------+----------------+----------+------------------+
| dated_as|              DOC|THIS Lease Agreement|          EFFDATE|   of May,  2006| 0.9999546|                 6|
|signed_by|              DOC|THIS Lease Agreement|            PARTY|      Apple, Inc|  0.988555|                 5|
|signed_by|              DOC|THIS Lease Agreement|            PARTY|IMI Global,  Inc| 0.9568861|                 7|
|has_alias|            PARTY|          Apple, Inc|            ALIAS|        Landlord|0.99999475|                 4|
|has_alias|            PARTY|    IMI Global,  Inc|            ALIAS|          Tenant| 0.9999893|                 4|
+---------+-----------------+--------------------+-----------------+----------------+----------+------------------+

```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|legre_contract_doc_parties_lg|
|Compatibility:|Legal NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Language:|en|
|Size:|406.0 MB|

## Benchmarking

```bash
              
Label                         Recall  Precision     F1   Support
dated_as                      1.000     1.000     1.000     19
has_alias                     1.000     1.000     1.000     29
has_collective_alias          1.000     1.000     1.000     25
signed_by                     1.000     1.000     1.000     47
Avg.                          1.000     1.000     1.000     -
Weighted-Avg.                 1.000     1.000     1.000     -
```</content><author><name>John Snow Labs</name></author><category term="legal" /><category term="licensed" /><category term="agreements" /><category term="en" /><category term="tensorflow" /><summary type="html">Description IMPORTANT: Don’t run this model on the whole legal agreement. Instead: Split by paragraphs. You can use notebook 1 in Finance or Legal as inspiration; Use the legclf_introduction_clause Text Classifier to select only these paragraphs; This is a Legal Relation Extraction model, which can be used after the NER Model for extracting Parties, Document Types, Effective Dates and Aliases, called legner_contract_doc_parties. As an output, you will get the relations linking the different concepts together, if such relation exists. The list of relations is: dated_as: A Document has an Effective Date has_alias: The Alias of a Party all along the document has_collective_alias: An Alias hold by several parties at the same time signed_by: Between a Party and the document they signed This is a lg model with Unidirectional Relations, meaning that the model retrieves in chunk1 the left side of the relation (source), and in chunk2 the right side (target). Predicted Entities dated_as, has_alias, has_collective_alias, signed_by Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) sen = nlp.SentenceDetector()\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer()\ .setInputCols(&quot;sentence&quot;)\ .setOutputCol(&quot;token&quot;) embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;)\ .setOutputCol(&quot;embeddings&quot;)\ .setMaxSentenceLength(512) pos_tagger = nlp.PerceptronModel()\ .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) \ .setInputCols([&quot;sentence&quot;, &quot;token&quot;])\ .setOutputCol(&quot;pos_tags&quot;) dependency_parser = nlp.DependencyParserModel()\ .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;)\ .setInputCols([&quot;sentence&quot;, &quot;pos_tags&quot;, &quot;token&quot;])\ .setOutputCol(&quot;dependencies&quot;) ner_model = legal.NerModel.pretrained('legner_contract_doc_parties_lg', 'en', 'legal/models')\ .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;])\ .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter()\ .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;])\ .setOutputCol(&quot;ner_chunk&quot;) re_ner_chunk_filter = legal.RENerChunksFilter() \ .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;])\ .setOutputCol(&quot;re_ner_chunks&quot;)\ .setMaxSyntacticDistance(7)\ .setRelationPairs([&quot;DOC-EFFDATE&quot;, &quot;DOC-PARTY&quot;, &quot;PARTY-FORMER_NAME&quot;, &quot;PARTY-ALIAS&quot;]) re_model = legal.RelationExtractionDLModel().pretrained('legre_contract_doc_parties_lg', 'en', 'legal/models')\ .setPredictionThreshold(0.5)\ .setInputCols([&quot;re_ner_chunks&quot;, &quot;sentence&quot;])\ .setOutputCol(&quot;relations&quot;) nlpPipeline = nlp.Pipeline(stages=[ document_assembler, sen, tokenizer, embeddings, pos_tagger, dependency_parser, ner_model, ner_converter, re_ner_chunk_filter, re_model ]) empty_df = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) model = nlpPipeline.fit(empty_df) text=&quot;&quot;&quot;THIS Lease Agreement , is made and entered into this _____day of May, 2006 by and between Apple, Inc., (hereinafter called &quot;Landlord&quot;), and IMI Global, Inc., with a mailing address of ___, (hereinafter referred as &quot;Tenant&quot;).&quot;&quot;&quot; data = spark.createDataFrame([[text]]).toDF(&quot;text&quot;) result = model.transform(data) Results +---------+-----------------+--------------------+-----------------+----------------+----------+------------------+ |relations|relations_entity1| relations_chunk1|relations_entity2|relations_chunk2|confidence|syntactic_distance| +---------+-----------------+--------------------+-----------------+----------------+----------+------------------+ | dated_as| DOC|THIS Lease Agreement| EFFDATE| of May, 2006| 0.9999546| 6| |signed_by| DOC|THIS Lease Agreement| PARTY| Apple, Inc| 0.988555| 5| |signed_by| DOC|THIS Lease Agreement| PARTY|IMI Global, Inc| 0.9568861| 7| |has_alias| PARTY| Apple, Inc| ALIAS| Landlord|0.99999475| 4| |has_alias| PARTY| IMI Global, Inc| ALIAS| Tenant| 0.9999893| 4| +---------+-----------------+--------------------+-----------------+----------------+----------+------------------+ Model Information Model Name: legre_contract_doc_parties_lg Compatibility: Legal NLP 1.0.0+ License: Licensed Edition: Official Language: en Size: 406.0 MB Benchmarking Label Recall Precision F1 Support dated_as 1.000 1.000 1.000 19 has_alias 1.000 1.000 1.000 29 has_collective_alias 1.000 1.000 1.000 25 signed_by 1.000 1.000 1.000 47 Avg. 1.000 1.000 1.000 - Weighted-Avg. 1.000 1.000 1.000 -</summary></entry><entry><title type="html">Finance Capital Call Notices Document Classifier (Bert Sentence Embeddings)</title><link href="/2023/02/16/finclf_capital_call_notices_en.html" rel="alternate" type="text/html" title="Finance Capital Call Notices Document Classifier (Bert Sentence Embeddings)" /><published>2023-02-16T00:00:00+00:00</published><updated>2023-02-16T00:00:00+00:00</updated><id>/2023/02/16/finclf_capital_call_notices_en</id><content type="html" xml:base="/2023/02/16/finclf_capital_call_notices_en.html">## Description

The `finclf_capital_call_notices` model is a Bert Sentence Embeddings Document Classifier used to classify if the document belongs to the class `capital_call_notices` or not (Binary Classification).

## Predicted Entities

`capital_call_notices`, `other`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/finance/models/finclf_capital_call_notices_en_1.0.0_3.0_1676590287518.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/finance/models/finclf_capital_call_notices_en_1.0.0_3.0_1676590287518.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
document_assembler = nlp.DocumentAssembler()\
    .setInputCol(&quot;text&quot;)\
    .setOutputCol(&quot;document&quot;)
  
embeddings = nlp.BertSentenceEmbeddings.pretrained(&quot;sent_bert_base_cased&quot;, &quot;en&quot;)\
    .setInputCols(&quot;document&quot;)\
    .setOutputCol(&quot;sentence_embeddings&quot;)
    
doc_classifier = finance.ClassifierDLModel.pretrained(&quot;finclf_capital_call_notices&quot;, &quot;en&quot;, &quot;finance/models&quot;)\
    .setInputCols([&quot;sentence_embeddings&quot;])\
    .setOutputCol(&quot;category&quot;)
    
nlpPipeline = nlp.Pipeline(stages=[
    document_assembler, 
    embeddings,
    doc_classifier])
 
df = spark.createDataFrame([[&quot;YOUR TEXT HERE&quot;]]).toDF(&quot;text&quot;)

model = nlpPipeline.fit(df)

result = model.transform(df)
```

&lt;/div&gt;

## Results

```bash
+-------+
|result|
+-------+
|[capital_call_notices]|
|[other]|
|[other]|
|[capital_call_notices]|
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|finclf_capital_call_notices|
|Compatibility:|Finance NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[sentence_embeddings]|
|Output Labels:|[class]|
|Language:|en|
|Size:|22.4 MB|

## References

Financial documents and classified in-house + SEC documents

## Benchmarking

```bash
label                 precision  recall  f1-score  support 
capital_call_notices  1.00       1.00    1.00      12      
other                 1.00       1.00    1.00      23      
accuracy              -          -       1.00      35      
macro-avg             1.00       1.00    1.00      35      
weighted-avg          1.00       1.00    1.00      35   
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="licensed" /><category term="finance" /><category term="capital_calls" /><category term="classification" /><category term="tensorflow" /><summary type="html">Description The finclf_capital_call_notices model is a Bert Sentence Embeddings Document Classifier used to classify if the document belongs to the class capital_call_notices or not (Binary Classification). Predicted Entities capital_call_notices, other Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) embeddings = nlp.BertSentenceEmbeddings.pretrained(&quot;sent_bert_base_cased&quot;, &quot;en&quot;)\ .setInputCols(&quot;document&quot;)\ .setOutputCol(&quot;sentence_embeddings&quot;) doc_classifier = finance.ClassifierDLModel.pretrained(&quot;finclf_capital_call_notices&quot;, &quot;en&quot;, &quot;finance/models&quot;)\ .setInputCols([&quot;sentence_embeddings&quot;])\ .setOutputCol(&quot;category&quot;) nlpPipeline = nlp.Pipeline(stages=[ document_assembler, embeddings, doc_classifier]) df = spark.createDataFrame([[&quot;YOUR TEXT HERE&quot;]]).toDF(&quot;text&quot;) model = nlpPipeline.fit(df) result = model.transform(df) Results +-------+ |result| +-------+ |[capital_call_notices]| |[other]| |[other]| |[capital_call_notices]| Model Information Model Name: finclf_capital_call_notices Compatibility: Finance NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [sentence_embeddings] Output Labels: [class] Language: en Size: 22.4 MB References Financial documents and classified in-house + SEC documents Benchmarking label precision recall f1-score support capital_call_notices 1.00 1.00 1.00 12 other 1.00 1.00 1.00 23 accuracy - - 1.00 35 macro-avg 1.00 1.00 1.00 35 weighted-avg 1.00 1.00 1.00 35</summary></entry><entry><title type="html">Legal Proposal Summarization</title><link href="/2023/02/16/legsum_proposal_en.html" rel="alternate" type="text/html" title="Legal Proposal Summarization" /><published>2023-02-16T00:00:00+00:00</published><updated>2023-02-16T00:00:00+00:00</updated><id>/2023/02/16/legsum_proposal_en</id><content type="html" xml:base="/2023/02/16/legsum_proposal_en.html">## Description

This model is fine-tuned with a legal dataset (about EU proposals). Summarizes a proposal given on a socially important issue.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/legal/models/legsum_proposal_en_1.0.0_3.0_1676587991098.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/legal/models/legsum_proposal_en_1.0.0_3.0_1676587991098.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
document_assembler = nlp.DocumentAssembler()\
    .setInputCol(&quot;text&quot;)\
    .setOutputCol(&quot;document&quot;)

t5 = nlp.T5Transformer().pretrained(&quot;legsum_proposal&quot;, &quot;en&quot;, &quot;legal/models&quot;)\
    .setTask(&quot;summarize&quot;)\
    .setMaxOutputLength(512)\
    .setInputCols([&quot;document&quot;])\
    .setOutputCol(&quot;summaries&quot;)

text = &quot;&quot;&quot;The main reason for migration is poverty, and often times it is down to corruption in the leadership of poor countries. What people in such countries demand time and again is that the EU does not engage with their government, and does not supply financial support (which tends to end up in the wrong hands). The EU needs a strict line of engagement. One could envision a rating list by the EU that defines clear requirements support receiving nations must fulfill. Support should be granted in the form of improved economic conditions, such as increased import quota, discounted machinery, and technical know-how injection, not in terms of financial support. Countries failing to fulfill the requirements, especially those with indications of corruption must be put under strict embargoes.&quot;&quot;&quot;

data_df = spark.createDataFrame([[text]]).toDF(&quot;text&quot;)

pipeline = nlp.Pipeline().setStages([document_assembler, t5])

results = pipeline.fit(data_df).transform(data_df)

results.select(&quot;summaries.result&quot;).show(truncate=False)
```

&lt;/div&gt;

## Results

```bash
People in poor countries demand that the EU does not engage with their government and do not provide financial support.
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|legsum_proposal|
|Compatibility:|Legal NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[documents]|
|Output Labels:|[summaries]|
|Language:|en|
|Size:|925.9 MB|

## References

Training dataset available [here](https://touche.webis.de/clef23/touche23-web/multilingual-stance-classification.html#data)</content><author><name>John Snow Labs</name></author><category term="en" /><category term="licensed" /><category term="summarization" /><category term="legal" /><category term="tensorflow" /><summary type="html">Description This model is fine-tuned with a legal dataset (about EU proposals). Summarizes a proposal given on a socially important issue. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) t5 = nlp.T5Transformer().pretrained(&quot;legsum_proposal&quot;, &quot;en&quot;, &quot;legal/models&quot;)\ .setTask(&quot;summarize&quot;)\ .setMaxOutputLength(512)\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;summaries&quot;) text = &quot;&quot;&quot;The main reason for migration is poverty, and often times it is down to corruption in the leadership of poor countries. What people in such countries demand time and again is that the EU does not engage with their government, and does not supply financial support (which tends to end up in the wrong hands). The EU needs a strict line of engagement. One could envision a rating list by the EU that defines clear requirements support receiving nations must fulfill. Support should be granted in the form of improved economic conditions, such as increased import quota, discounted machinery, and technical know-how injection, not in terms of financial support. Countries failing to fulfill the requirements, especially those with indications of corruption must be put under strict embargoes.&quot;&quot;&quot; data_df = spark.createDataFrame([[text]]).toDF(&quot;text&quot;) pipeline = nlp.Pipeline().setStages([document_assembler, t5]) results = pipeline.fit(data_df).transform(data_df) results.select(&quot;summaries.result&quot;).show(truncate=False) Results People in poor countries demand that the EU does not engage with their government and do not provide financial support. Model Information Model Name: legsum_proposal Compatibility: Legal NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [documents] Output Labels: [summaries] Language: en Size: 925.9 MB References Training dataset available here</summary></entry><entry><title type="html">Extract Broker Suggestions from Broker Reports</title><link href="/2023/02/16/finclf_bert_broker_reports_suggested_actions_en.html" rel="alternate" type="text/html" title="Extract Broker Suggestions from Broker Reports" /><published>2023-02-16T00:00:00+00:00</published><updated>2023-02-16T00:00:00+00:00</updated><id>/2023/02/16/finclf_bert_broker_reports_suggested_actions_en</id><content type="html" xml:base="/2023/02/16/finclf_bert_broker_reports_suggested_actions_en.html">## Description

This Text Classifier will identify whether a broker's report suggests to Buy,Sell, Hold, Accumulate, Reduce, Neutral, Other.

## Predicted Entities

`Buy`, `Sell`, `Hold`, `Accumulate`, `Reduce`, `Neutral`, `Other`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/finance/models/finclf_bert_broker_reports_suggested_actions_en_1.0.0_3.0_1676568964802.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/finance/models/finclf_bert_broker_reports_suggested_actions_en_1.0.0_3.0_1676568964802.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
document_assembler = nlp.DocumentAssembler() \
    .setInputCol('text') \
    .setOutputCol('document')

tokenizer = nlp.Tokenizer() \
    .setInputCols(['document']) \
    .setOutputCol('token')

# Load newly trained classifier
sequenceClassifier_loaded = finance.BertForSequenceClassification.pretrained(&quot;finclf_bert_broker_reports_suggested_actions&quot;, &quot;en&quot;, &quot;finance/models&quot;)\
  .setInputCols([&quot;document&quot;,'token'])\
  .setOutputCol(&quot;class&quot;)

pipeline = nlp.Pipeline(stages=[
    document_assembler, 
    tokenizer,
    sequenceClassifier_loaded    
])

# Generating example
example = spark.createDataFrame([['''UPL 
   
 
Estimate change   
TP change   
Rating change   
 
Bloomberg  UPLL IN  
Equity Shares (m)  765 
M.Cap.(INRb)/(USDb)  538.2 / 6.5  
52-Week Range (INR)  848 / 608  
1, 6, 12 Rel. Per (%)  0/-20/-3  
12M Avg Val (INR M)  2009  
 
Financials &amp; Valuation s (INR b)  
Y/E Mar  2022 2023E 2024E 
Sales  462.4  537.0  593.4  
EBITDA  101.7  121.5  135.3  
PAT 48.5  54.9  61.0  
EBITDA (%)           22.0           22.6            22.8  
EPS (INR)           63.5           71.7            79.7  
EPS Gr. (%)           39.9           13.0            11.1  
BV/Sh. (INR)  429 512 652 
Ratios        
Net D/E             1.0             0.8              0.5  
RoE (%)           24.5           23.1            20.7  
RoCE (%)           15.1           16.2            16.5  
Payout (%)           21.1           18.0            17.6  
Valuations        
P/E (x)           11.3           10.0              9.0  
EV/EBITDA (x)             7.6             6.3              5.2  
Div Yield (%)             1.4             1.7              2.0  
FCF Yield (%)             4.4             7.2            14.0  
 
Shareholding pattern (%)  
 Sep-22 Jun-22 Sep-21 
Promoter  29.0  29.0  28.0  
DII 17.2  16.5  18.0  
FII 42.8  36.4  35.1  
Others  11.1  18.1  19.0  
Note: FII includes depository receipts  
 
  CMP: INR 717                   TP: INR 780 (+9%)                       Neutral  
 
Higher working capital adversely impacts CFO  
Earnings better than expected    
 UPLL reported strong revenue growth of 18% YoY , driven primarily  by an 
increase in price realization ( up 21% YoY). However,  volume s declined (down 
7% YoY) in 2QFY23, led by rationalization of product mix toward high margin 
products. Except Europe (+1% YoY), all other key geographies registered a 
strong sales growth of over 20% YoY.  
 Gross debt increased to INR 326b in 2QFY23 from INR 301b in 1Q FY23 with 
net debt rising INR20b QoQ to INR 285b, due to an  increas e in working 
capital requirement . This increase in working capital also resulted in cash 
outflow from operation of INR45.94b in 1HFY23  v/s cash outflow INR24.15b 
in 1HFY22 .  
 We largely maintain our FY23E/FY24 E earnings . We reiterate our Neutral 
rating on the stock with a TP of INR 780 (premised on 1 0x FY24E P/E) .''']]).toDF(&quot;text&quot;)

result = pipeline.fit(example).transform(example)

# Checking results
result.select(&quot;text&quot;, &quot;class.result&quot;).show(truncate=False)
```

&lt;/div&gt;

## Results

```bash

+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+
|text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |result   |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+
|UPL 
   
 
Estimate change   
TP change   
Rating change   
 
Bloomberg  UPLL IN  
Equity Shares (m)  765 
M.Cap.(INRb)/(USDb)  538.2 / 6.5  
52-Week Range (INR)  848 / 608  
1, 6, 12 Rel. Per (%)  0/-20/-3  
12M Avg Val (INR M)  2009  
 
Financials &amp; Valuation s (INR b)  
Y/E Mar  2022 2023E 2024E 
Sales  462.4  537.0  593.4  
EBITDA  101.7  121.5  135.3  
PAT 48.5  54.9  61.0  
EBITDA (%)           22.0           22.6            22.8  
EPS (INR)           63.5           71.7            79.7  
EPS Gr. (%)           39.9           13.0            11.1  
BV/Sh. (INR)  429 512 652 
Ratios        
Net D/E             1.0             0.8              0.5  
RoE (%)           24.5           23.1            20.7  
RoCE (%)           15.1           16.2            16.5  
Payout (%)           21.1           18.0            17.6  
Valuations        
P/E (x)           11.3           10.0              9.0  
EV/EBITDA (x)             7.6             6.3              5.2  
Div Yield (%)             1.4             1.7              2.0  
FCF Yield (%)             4.4             7.2            14.0  
 
Shareholding pattern (%)  
 Sep-22 Jun-22 Sep-21 
Promoter  29.0  29.0  28.0  
DII 17.2  16.5  18.0  
FII 42.8  36.4  35.1  
Others  11.1  18.1  19.0  
Note: FII includes depository receipts  
 
  CMP: INR 717                   TP: INR 780 (+9%)                       Neutral  
 
Higher working capital adversely impacts CFO  
Earnings better than expected    
 UPLL reported strong revenue growth of 18% YoY , driven primarily  by an 
increase in price realization ( up 21% YoY). However,  volume s declined (down 
7% YoY) in 2QFY23, led by rationalization of product mix toward high margin 
products. Except Europe (+1% YoY), all other key geographies registered a 
strong sales growth of over 20% YoY.  
 Gross debt increased to INR 326b in 2QFY23 from INR 301b in 1Q FY23 with 
net debt rising INR20b QoQ to INR 285b, due to an  increas e in working 
capital requirement . This increase in working capital also resulted in cash 
outflow from operation of INR45.94b in 1HFY23  v/s cash outflow INR24.15b 
in 1HFY22 .  
 We largely maintain our FY23E/FY24 E earnings . We reiterate our Neutral 
rating on the stock with a TP of INR 780 (premised on 1 0x FY24E P/E) .|[Neutral]|
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+


```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|finclf_bert_broker_reports_suggested_actions|
|Compatibility:|Finance NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[document, token]|
|Output Labels:|[class]|
|Language:|en|
|Size:|402.5 MB|
|Case sensitive:|true|
|Max sentence length:|128|

## References

An in-house annotated dataset of broker reports.

## Benchmarking

```bash
              
label          precision    recall  f1-score   support
  Accumulate       1.00      1.00      1.00        26
         Buy       0.88      0.79      0.83        19
        Hold       0.96      0.92      0.94        24
     Neutral       0.77      1.00      0.87        17
      Reduce       1.00      1.00      1.00        17
        Sell       0.90      0.95      0.93        20
       other       1.00      0.80      0.89        15
    accuracy         -         -       0.93       138
   macro-avg       0.93      0.92      0.92       138
weighted-avg       0.93      0.93      0.93       138
```</content><author><name>John Snow Labs</name></author><category term="bert" /><category term="broker_reports" /><category term="licensed" /><category term="agreements" /><category term="finance" /><category term="en" /><category term="tensorflow" /><summary type="html">Description This Text Classifier will identify whether a broker’s report suggests to Buy,Sell, Hold, Accumulate, Reduce, Neutral, Other. Predicted Entities Buy, Sell, Hold, Accumulate, Reduce, Neutral, Other Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = nlp.Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') # Load newly trained classifier sequenceClassifier_loaded = finance.BertForSequenceClassification.pretrained(&quot;finclf_bert_broker_reports_suggested_actions&quot;, &quot;en&quot;, &quot;finance/models&quot;)\ .setInputCols([&quot;document&quot;,'token'])\ .setOutputCol(&quot;class&quot;) pipeline = nlp.Pipeline(stages=[ document_assembler, tokenizer, sequenceClassifier_loaded ]) # Generating example example = spark.createDataFrame([['''UPL Estimate change TP change Rating change Bloomberg UPLL IN Equity Shares (m) 765 M.Cap.(INRb)/(USDb) 538.2 / 6.5 52-Week Range (INR) 848 / 608 1, 6, 12 Rel. Per (%) 0/-20/-3 12M Avg Val (INR M) 2009 Financials &amp;amp; Valuation s (INR b) Y/E Mar 2022 2023E 2024E Sales 462.4 537.0 593.4 EBITDA 101.7 121.5 135.3 PAT 48.5 54.9 61.0 EBITDA (%) 22.0 22.6 22.8 EPS (INR) 63.5 71.7 79.7 EPS Gr. (%) 39.9 13.0 11.1 BV/Sh. (INR) 429 512 652 Ratios Net D/E 1.0 0.8 0.5 RoE (%) 24.5 23.1 20.7 RoCE (%) 15.1 16.2 16.5 Payout (%) 21.1 18.0 17.6 Valuations P/E (x) 11.3 10.0 9.0 EV/EBITDA (x) 7.6 6.3 5.2 Div Yield (%) 1.4 1.7 2.0 FCF Yield (%) 4.4 7.2 14.0 Shareholding pattern (%) Sep-22 Jun-22 Sep-21 Promoter 29.0 29.0 28.0 DII 17.2 16.5 18.0 FII 42.8 36.4 35.1 Others 11.1 18.1 19.0 Note: FII includes depository receipts CMP: INR 717 TP: INR 780 (+9%) Neutral Higher working capital adversely impacts CFO Earnings better than expected  UPLL reported strong revenue growth of 18% YoY , driven primarily by an increase in price realization ( up 21% YoY). However, volume s declined (down 7% YoY) in 2QFY23, led by rationalization of product mix toward high margin products. Except Europe (+1% YoY), all other key geographies registered a strong sales growth of over 20% YoY.  Gross debt increased to INR 326b in 2QFY23 from INR 301b in 1Q FY23 with net debt rising INR20b QoQ to INR 285b, due to an increas e in working capital requirement . This increase in working capital also resulted in cash outflow from operation of INR45.94b in 1HFY23 v/s cash outflow INR24.15b in 1HFY22 .  We largely maintain our FY23E/FY24 E earnings . We reiterate our Neutral rating on the stock with a TP of INR 780 (premised on 1 0x FY24E P/E) .''']]).toDF(&quot;text&quot;) result = pipeline.fit(example).transform(example) # Checking results result.select(&quot;text&quot;, &quot;class.result&quot;).show(truncate=False) Results +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+ |text |result | +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+ |UPL Estimate change TP change Rating change Bloomberg UPLL IN Equity Shares (m) 765 M.Cap.(INRb)/(USDb) 538.2 / 6.5 52-Week Range (INR) 848 / 608 1, 6, 12 Rel. Per (%) 0/-20/-3 12M Avg Val (INR M) 2009 Financials &amp;amp; Valuation s (INR b) Y/E Mar 2022 2023E 2024E Sales 462.4 537.0 593.4 EBITDA 101.7 121.5 135.3 PAT 48.5 54.9 61.0 EBITDA (%) 22.0 22.6 22.8 EPS (INR) 63.5 71.7 79.7 EPS Gr. (%) 39.9 13.0 11.1 BV/Sh. (INR) 429 512 652 Ratios Net D/E 1.0 0.8 0.5 RoE (%) 24.5 23.1 20.7 RoCE (%) 15.1 16.2 16.5 Payout (%) 21.1 18.0 17.6 Valuations P/E (x) 11.3 10.0 9.0 EV/EBITDA (x) 7.6 6.3 5.2 Div Yield (%) 1.4 1.7 2.0 FCF Yield (%) 4.4 7.2 14.0 Shareholding pattern (%) Sep-22 Jun-22 Sep-21 Promoter 29.0 29.0 28.0 DII 17.2 16.5 18.0 FII 42.8 36.4 35.1 Others 11.1 18.1 19.0 Note: FII includes depository receipts CMP: INR 717 TP: INR 780 (+9%) Neutral Higher working capital adversely impacts CFO Earnings better than expected  UPLL reported strong revenue growth of 18% YoY , driven primarily by an increase in price realization ( up 21% YoY). However, volume s declined (down 7% YoY) in 2QFY23, led by rationalization of product mix toward high margin products. Except Europe (+1% YoY), all other key geographies registered a strong sales growth of over 20% YoY.  Gross debt increased to INR 326b in 2QFY23 from INR 301b in 1Q FY23 with net debt rising INR20b QoQ to INR 285b, due to an increas e in working capital requirement . This increase in working capital also resulted in cash outflow from operation of INR45.94b in 1HFY23 v/s cash outflow INR24.15b in 1HFY22 .  We largely maintain our FY23E/FY24 E earnings . We reiterate our Neutral rating on the stock with a TP of INR 780 (premised on 1 0x FY24E P/E) .|[Neutral]| +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+ Model Information Model Name: finclf_bert_broker_reports_suggested_actions Compatibility: Finance NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [document, token] Output Labels: [class] Language: en Size: 402.5 MB Case sensitive: true Max sentence length: 128 References An in-house annotated dataset of broker reports. Benchmarking label precision recall f1-score support Accumulate 1.00 1.00 1.00 26 Buy 0.88 0.79 0.83 19 Hold 0.96 0.92 0.94 24 Neutral 0.77 1.00 0.87 17 Reduce 1.00 1.00 1.00 17 Sell 0.90 0.95 0.93 20 other 1.00 0.80 0.89 15 accuracy - - 0.93 138 macro-avg 0.93 0.92 0.92 138 weighted-avg 0.93 0.93 0.93 138</summary></entry><entry><title type="html">Legal Applicable Law Clause Binary Classifier</title><link href="/2023/02/13/legclf_applic_law_clause_en.html" rel="alternate" type="text/html" title="Legal Applicable Law Clause Binary Classifier" /><published>2023-02-13T00:00:00+00:00</published><updated>2023-02-13T00:00:00+00:00</updated><id>/2023/02/13/legclf_applic_law_clause_en</id><content type="html" xml:base="/2023/02/13/legclf_applic_law_clause_en.html">## Description

This model is a Binary Classifier (True, False) for the `applic_law` clause type. To use this model, make sure you provide enough context as an input. Adding Sentence Splitters to the pipeline will make the model see only sentences, not the whole text, so it's better to skip it, unless you want to do Binary Classification as sentence level.

If you have big legal documents, and you want to look for clauses, we recommend you to split the documents using any of the techniques available in our Legal NLP Workshop Tokenization &amp; Splitting Tutorial (link [here](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings_JSL/Legal/1.Tokenization_Splitting.ipynb)), namely:
- Paragraph splitting (by multiline);
- Splitting by headers / subheaders;
- etc.

Take into consideration the embeddings of this model allows up to 512 tokens. If you have more than that, consider splitting in smaller pieces (you can also check the same tutorial link provided above).

This model can be combined with any of the other 200+ Legal Clauses Classifiers you will find in Models Hub, getting as an output a series of True/False values for each of the legal clause model you have added.

## Predicted Entities

`applic_law`, `other`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/legal/models/legclf_applic_law_clause_en_1.0.0_3.0_1676302179504.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/legal/models/legclf_applic_law_clause_en_1.0.0_3.0_1676302179504.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python

document_assembler = nlp.DocumentAssembler()\
    .setInputCol(&quot;text&quot;)\
    .setOutputCol(&quot;document&quot;)
  
embeddings = nlp.BertSentenceEmbeddings.pretrained(&quot;sent_bert_base_cased&quot;, &quot;en&quot;)\
    .setInputCols(&quot;document&quot;)\
    .setOutputCol(&quot;sentence_embeddings&quot;)
    
doc_classifier = legal.ClassifierDLModel.pretrained(&quot;legclf_applic_law_clause&quot;, &quot;en&quot;, &quot;legal/models&quot;)\
    .setInputCols([&quot;sentence_embeddings&quot;])\
    .setOutputCol(&quot;category&quot;)
    
nlpPipeline = nlp.Pipeline(stages=[
    document_assembler, 
    embeddings,
    doc_classifier])
 
df = spark.createDataFrame([[&quot;YOUR TEXT HERE&quot;]]).toDF(&quot;text&quot;)

model = nlpPipeline.fit(df)

result = model.transform(df)

```

&lt;/div&gt;

## Results

```bash

+-------+
|result|
+-------+
|[applic_law]|
|[other]|
|[other]|
|[applic_law]|

```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|legclf_applic_law_clause|
|Compatibility:|Legal NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[sentence_embeddings]|
|Output Labels:|[class]|
|Language:|en|
|Size:|22.6 MB|

## References

Legal documents, scrapped from the Internet, and classified in-house

## Benchmarking

```bash
       label  precision    recall  f1-score   support
  applic_law       1.00      0.95      0.97        20
       other       0.93      1.00      0.96        13
    accuracy          -         -      0.97        33
   macro-avg       0.96      0.97      0.97        33
weighted-avg       0.97      0.97      0.97        33
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="legal" /><category term="classification" /><category term="applicable" /><category term="law" /><category term="licensed" /><category term="tensorflow" /><summary type="html">Description This model is a Binary Classifier (True, False) for the applic_law clause type. To use this model, make sure you provide enough context as an input. Adding Sentence Splitters to the pipeline will make the model see only sentences, not the whole text, so it’s better to skip it, unless you want to do Binary Classification as sentence level. If you have big legal documents, and you want to look for clauses, we recommend you to split the documents using any of the techniques available in our Legal NLP Workshop Tokenization &amp;amp; Splitting Tutorial (link here), namely: Paragraph splitting (by multiline); Splitting by headers / subheaders; etc. Take into consideration the embeddings of this model allows up to 512 tokens. If you have more than that, consider splitting in smaller pieces (you can also check the same tutorial link provided above). This model can be combined with any of the other 200+ Legal Clauses Classifiers you will find in Models Hub, getting as an output a series of True/False values for each of the legal clause model you have added. Predicted Entities applic_law, other Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) embeddings = nlp.BertSentenceEmbeddings.pretrained(&quot;sent_bert_base_cased&quot;, &quot;en&quot;)\ .setInputCols(&quot;document&quot;)\ .setOutputCol(&quot;sentence_embeddings&quot;) doc_classifier = legal.ClassifierDLModel.pretrained(&quot;legclf_applic_law_clause&quot;, &quot;en&quot;, &quot;legal/models&quot;)\ .setInputCols([&quot;sentence_embeddings&quot;])\ .setOutputCol(&quot;category&quot;) nlpPipeline = nlp.Pipeline(stages=[ document_assembler, embeddings, doc_classifier]) df = spark.createDataFrame([[&quot;YOUR TEXT HERE&quot;]]).toDF(&quot;text&quot;) model = nlpPipeline.fit(df) result = model.transform(df) Results +-------+ |result| +-------+ |[applic_law]| |[other]| |[other]| |[applic_law]| Model Information Model Name: legclf_applic_law_clause Compatibility: Legal NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [sentence_embeddings] Output Labels: [class] Language: en Size: 22.6 MB References Legal documents, scrapped from the Internet, and classified in-house Benchmarking label precision recall f1-score support applic_law 1.00 0.95 0.97 20 other 0.93 1.00 0.96 13 accuracy - - 0.97 33 macro-avg 0.96 0.97 0.97 33 weighted-avg 0.97 0.97 0.97 33</summary></entry><entry><title type="html">Legal Assignment Clause Binary Classifier</title><link href="/2023/02/13/legclf_assignment_clause_en.html" rel="alternate" type="text/html" title="Legal Assignment Clause Binary Classifier" /><published>2023-02-13T00:00:00+00:00</published><updated>2023-02-13T00:00:00+00:00</updated><id>/2023/02/13/legclf_assignment_clause_en</id><content type="html" xml:base="/2023/02/13/legclf_assignment_clause_en.html">## Description

This model is a Binary Classifier (True, False) for the `assignment` clause type. To use this model, make sure you provide enough context as an input. Adding Sentence Splitters to the pipeline will make the model see only sentences, not the whole text, so it's better to skip it, unless you want to do Binary Classification as sentence level.

If you have big legal documents, and you want to look for clauses, we recommend you to split the documents using any of the techniques available in our Legal NLP Workshop Tokenization &amp; Splitting Tutorial (link [here](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings_JSL/Legal/1.Tokenization_Splitting.ipynb)), namely:
- Paragraph splitting (by multiline);
- Splitting by headers / subheaders;
- etc.

Take into consideration the embeddings of this model allows up to 512 tokens. If you have more than that, consider splitting in smaller pieces (you can also check the same tutorial link provided above).

This model can be combined with any of the other 200+ Legal Clauses Classifiers you will find in Models Hub, getting as an output a series of True/False values for each of the legal clause model you have added.

## Predicted Entities

`assignment`, `other`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/legal/models/legclf_assignment_clause_en_1.0.0_3.0_1676302845555.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/legal/models/legclf_assignment_clause_en_1.0.0_3.0_1676302845555.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python

document_assembler = nlp.DocumentAssembler()\
    .setInputCol(&quot;text&quot;)\
    .setOutputCol(&quot;document&quot;)
  
embeddings = nlp.BertSentenceEmbeddings.pretrained(&quot;sent_bert_base_cased&quot;, &quot;en&quot;)\
    .setInputCols(&quot;document&quot;)\
    .setOutputCol(&quot;sentence_embeddings&quot;)
    
doc_classifier = legal.ClassifierDLModel.pretrained(&quot;legclf_assignment_clause&quot;, &quot;en&quot;, &quot;legal/models&quot;)\
    .setInputCols([&quot;sentence_embeddings&quot;])\
    .setOutputCol(&quot;category&quot;)
    
nlpPipeline = nlp.Pipeline(stages=[
    document_assembler, 
    embeddings,
    doc_classifier])
 
df = spark.createDataFrame([[&quot;YOUR TEXT HERE&quot;]]).toDF(&quot;text&quot;)

model = nlpPipeline.fit(df)

result = model.transform(df)

```

&lt;/div&gt;

## Results

```bash

+-------+
|result|
+-------+
|[assignment]|
|[other]|
|[other]|
|[assignment]|

```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|legclf_assignment_clause|
|Compatibility:|Legal NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[sentence_embeddings]|
|Output Labels:|[class]|
|Language:|en|
|Size:|22.5 MB|

## References

Legal documents, scrapped from the Internet, and classified in-house

## Benchmarking

```bash
       label  precision    recall  f1-score   support
  assignment       0.95      1.00      0.98        20
       other       1.00      0.91      0.95        11
    accuracy          -         -      0.97        31
   macro-avg       0.98      0.95      0.96        31
weighted-avg       0.97      0.97      0.97        31
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="legal" /><category term="classification" /><category term="assignment" /><category term="licensed" /><category term="tensorflow" /><summary type="html">Description This model is a Binary Classifier (True, False) for the assignment clause type. To use this model, make sure you provide enough context as an input. Adding Sentence Splitters to the pipeline will make the model see only sentences, not the whole text, so it’s better to skip it, unless you want to do Binary Classification as sentence level. If you have big legal documents, and you want to look for clauses, we recommend you to split the documents using any of the techniques available in our Legal NLP Workshop Tokenization &amp;amp; Splitting Tutorial (link here), namely: Paragraph splitting (by multiline); Splitting by headers / subheaders; etc. Take into consideration the embeddings of this model allows up to 512 tokens. If you have more than that, consider splitting in smaller pieces (you can also check the same tutorial link provided above). This model can be combined with any of the other 200+ Legal Clauses Classifiers you will find in Models Hub, getting as an output a series of True/False values for each of the legal clause model you have added. Predicted Entities assignment, other Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) embeddings = nlp.BertSentenceEmbeddings.pretrained(&quot;sent_bert_base_cased&quot;, &quot;en&quot;)\ .setInputCols(&quot;document&quot;)\ .setOutputCol(&quot;sentence_embeddings&quot;) doc_classifier = legal.ClassifierDLModel.pretrained(&quot;legclf_assignment_clause&quot;, &quot;en&quot;, &quot;legal/models&quot;)\ .setInputCols([&quot;sentence_embeddings&quot;])\ .setOutputCol(&quot;category&quot;) nlpPipeline = nlp.Pipeline(stages=[ document_assembler, embeddings, doc_classifier]) df = spark.createDataFrame([[&quot;YOUR TEXT HERE&quot;]]).toDF(&quot;text&quot;) model = nlpPipeline.fit(df) result = model.transform(df) Results +-------+ |result| +-------+ |[assignment]| |[other]| |[other]| |[assignment]| Model Information Model Name: legclf_assignment_clause Compatibility: Legal NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [sentence_embeddings] Output Labels: [class] Language: en Size: 22.5 MB References Legal documents, scrapped from the Internet, and classified in-house Benchmarking label precision recall f1-score support assignment 0.95 1.00 0.98 20 other 1.00 0.91 0.95 11 accuracy - - 0.97 31 macro-avg 0.98 0.95 0.96 31 weighted-avg 0.97 0.97 0.97 31</summary></entry><entry><title type="html">Legal Definition Of Confidential Information Clause Binary Classifier</title><link href="/2023/02/13/legclf_def_of_conf_info_clause_en.html" rel="alternate" type="text/html" title="Legal Definition Of Confidential Information Clause Binary Classifier" /><published>2023-02-13T00:00:00+00:00</published><updated>2023-02-13T00:00:00+00:00</updated><id>/2023/02/13/legclf_def_of_conf_info_clause_en</id><content type="html" xml:base="/2023/02/13/legclf_def_of_conf_info_clause_en.html">## Description

This model is a Binary Classifier (True, False) for the `def_of_conf_info` clause type. To use this model, make sure you provide enough context as an input. Adding Sentence Splitters to the pipeline will make the model see only sentences, not the whole text, so it's better to skip it, unless you want to do Binary Classification as sentence level.

If you have big legal documents, and you want to look for clauses, we recommend you to split the documents using any of the techniques available in our Legal NLP Workshop Tokenization &amp; Splitting Tutorial (link [here](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings_JSL/Legal/1.Tokenization_Splitting.ipynb)), namely:
- Paragraph splitting (by multiline);
- Splitting by headers / subheaders;
- etc.

Take into consideration the embeddings of this model allows up to 512 tokens. If you have more than that, consider splitting in smaller pieces (you can also check the same tutorial link provided above).

This model can be combined with any of the other 200+ Legal Clauses Classifiers you will find in Models Hub, getting as an output a series of True/False values for each of the legal clause model you have added.

## Predicted Entities

`def_of_conf_info`, `other`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/legal/models/legclf_def_of_conf_info_clause_en_1.0.0_3.0_1676302657181.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/legal/models/legclf_def_of_conf_info_clause_en_1.0.0_3.0_1676302657181.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python

document_assembler = nlp.DocumentAssembler()\
    .setInputCol(&quot;text&quot;)\
    .setOutputCol(&quot;document&quot;)
  
embeddings = nlp.BertSentenceEmbeddings.pretrained(&quot;sent_bert_base_cased&quot;, &quot;en&quot;)\
    .setInputCols(&quot;document&quot;)\
    .setOutputCol(&quot;sentence_embeddings&quot;)
    
doc_classifier = legal.ClassifierDLModel.pretrained(&quot;legclf_def_of_conf_info_clause&quot;, &quot;en&quot;, &quot;legal/models&quot;)\
    .setInputCols([&quot;sentence_embeddings&quot;])\
    .setOutputCol(&quot;category&quot;)
    
nlpPipeline = nlp.Pipeline(stages=[
    document_assembler, 
    embeddings,
    doc_classifier])
 
df = spark.createDataFrame([[&quot;YOUR TEXT HERE&quot;]]).toDF(&quot;text&quot;)

model = nlpPipeline.fit(df)

result = model.transform(df)

```

&lt;/div&gt;

## Results

```bash

+-------+
|result|
+-------+
|[def_of_conf_info]|
|[other]|
|[other]|
|[def_of_conf_info]|

```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|legclf_def_of_conf_info_clause|
|Compatibility:|Legal NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[sentence_embeddings]|
|Output Labels:|[class]|
|Language:|en|
|Size:|22.5 MB|

## References

Legal documents, scrapped from the Internet, and classified in-house

## Benchmarking

```bash
           label  precision    recall  f1-score   support
def_of_conf_info       0.91      1.00      0.95        20
           other       1.00      0.85      0.92        13
        accuracy          -         -      0.94        33
       macro-avg       0.95      0.92      0.93        33
    weighted-avg       0.94      0.94      0.94        33
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="legal" /><category term="classification" /><category term="definition" /><category term="confidential" /><category term="information" /><category term="licensed" /><category term="tensorflow" /><summary type="html">Description This model is a Binary Classifier (True, False) for the def_of_conf_info clause type. To use this model, make sure you provide enough context as an input. Adding Sentence Splitters to the pipeline will make the model see only sentences, not the whole text, so it’s better to skip it, unless you want to do Binary Classification as sentence level. If you have big legal documents, and you want to look for clauses, we recommend you to split the documents using any of the techniques available in our Legal NLP Workshop Tokenization &amp;amp; Splitting Tutorial (link here), namely: Paragraph splitting (by multiline); Splitting by headers / subheaders; etc. Take into consideration the embeddings of this model allows up to 512 tokens. If you have more than that, consider splitting in smaller pieces (you can also check the same tutorial link provided above). This model can be combined with any of the other 200+ Legal Clauses Classifiers you will find in Models Hub, getting as an output a series of True/False values for each of the legal clause model you have added. Predicted Entities def_of_conf_info, other Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) embeddings = nlp.BertSentenceEmbeddings.pretrained(&quot;sent_bert_base_cased&quot;, &quot;en&quot;)\ .setInputCols(&quot;document&quot;)\ .setOutputCol(&quot;sentence_embeddings&quot;) doc_classifier = legal.ClassifierDLModel.pretrained(&quot;legclf_def_of_conf_info_clause&quot;, &quot;en&quot;, &quot;legal/models&quot;)\ .setInputCols([&quot;sentence_embeddings&quot;])\ .setOutputCol(&quot;category&quot;) nlpPipeline = nlp.Pipeline(stages=[ document_assembler, embeddings, doc_classifier]) df = spark.createDataFrame([[&quot;YOUR TEXT HERE&quot;]]).toDF(&quot;text&quot;) model = nlpPipeline.fit(df) result = model.transform(df) Results +-------+ |result| +-------+ |[def_of_conf_info]| |[other]| |[other]| |[def_of_conf_info]| Model Information Model Name: legclf_def_of_conf_info_clause Compatibility: Legal NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [sentence_embeddings] Output Labels: [class] Language: en Size: 22.5 MB References Legal documents, scrapped from the Internet, and classified in-house Benchmarking label precision recall f1-score support def_of_conf_info 0.91 1.00 0.95 20 other 1.00 0.85 0.92 13 accuracy - - 0.94 33 macro-avg 0.95 0.92 0.93 33 weighted-avg 0.94 0.94 0.94 33</summary></entry></feed>