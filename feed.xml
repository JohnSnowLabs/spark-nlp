<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-08-11T11:56:43+00:00</updated><id>/feed.xml</id><title type="html">Spark NLP</title><subtitle>High Performance NLP with Apache Spark
</subtitle><author><name>{&quot;type&quot;=&gt;nil, &quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;googleplus&quot;=&gt;nil, &quot;telegram&quot;=&gt;nil, &quot;medium&quot;=&gt;nil, &quot;zhihu&quot;=&gt;nil, &quot;douban&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;npm&quot;=&gt;nil}</name></author><entry><title type="html">BERT Token Classification - Few-NERD (bert_base_token_classifier_few_nerd)</title><link href="/2021/08/08/bert_base_token_classifier_few_nerd_en.html" rel="alternate" type="text/html" title="BERT Token Classification - Few-NERD (bert_base_token_classifier_few_nerd)" /><published>2021-08-08T00:00:00+00:00</published><updated>2021-08-08T00:00:00+00:00</updated><id>/2021/08/08/bert_base_token_classifier_few_nerd_en</id><content type="html" xml:base="/2021/08/08/bert_base_token_classifier_few_nerd_en.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BERT Model&lt;/code&gt; with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.&lt;/p&gt;

&lt;p&gt;This model is fine-tuned on the Few-NERD dataset. Few-NERD is a large-scale, fine-grained manually annotated named entity recognition dataset, which contains 8 coarse-grained types, 66 fine-grained types, 188,200 sentences, 491,711 entities, and 4,601,223 tokens. Three benchmark tasks are built, one is supervised (Few-NERD (SUP)) and the other two are few-shot (Few-NERD (INTRA) and Few-NERD (INTER)). Few-NERD is collected by researchers from Tsinghua University and DAMO Academy, Alibaba Group.&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;art-broadcastprogram&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;art-film&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;art-music&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;art-other&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;art-painting&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;art-writtenart&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;building-airport&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;building-hospital&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;building-hotel&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;building-library&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;building-other&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;building-restaurant&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;building-sportsfacility&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;building-theater&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event-attack/battle/war/militaryconflict&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event-disaster&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event-election&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event-other&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event-protest&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event-sportsevent&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location-GPE&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location-bodiesofwater&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location-island&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location-mountain&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location-other&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location-park&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location-road/railway/highway/transit&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-company&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-education&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-government/governmentagency&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-media/newspaper&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-other&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-politicalparty&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-religion&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-showorganization&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-sportsleague&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-sportsteam&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-astronomything&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-award&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-biologything&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-chemicalthing&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-currency&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-disease&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-educationaldegree&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-god&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-language&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-law&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-livingthing&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-medical&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person-actor&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person-artist/author&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person-athlete&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person-director&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person-other&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person-politician&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person-scholar&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person-soldier&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-airplane&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-car&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-food&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-game&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-other&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-ship&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-software&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-train&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-weapon&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_base_token_classifier_few_nerd_en_3.2.0_2.4_1628433381711.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForTokenClassification&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bert_base_token_classifier_few_nerd'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'My name is John!'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;BertForTokenClassification&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bert_base_token_classifier_few_nerd&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;My&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;John!&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;bert_base_token_classifier_few_nerd&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.2.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, document]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;en&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max sentense length:&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/thunlp/Few-NERD&quot;&gt;https://github.com/thunlp/Few-NERD&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Test:

                        precision    recall  f1-score   support

                                       O       0.98      0.98      0.98    365750
                    art-broadcastprogram       0.66      0.66      0.66       890
                                art-film       0.78      0.78      0.78      1039
                               art-music       0.85      0.81      0.83      1773
                               art-other       0.40      0.40      0.40       729
                            art-painting       0.51      0.43      0.47        91
                          art-writtenart       0.69      0.70      0.70      1570
                        building-airport       0.83      0.88      0.85       391
                       building-hospital       0.80      0.89      0.84       577
                          building-hotel       0.87      0.80      0.83       526
                        building-library       0.81      0.86      0.83       715
                          building-other       0.64      0.67      0.65      3448
                     building-restaurant       0.72      0.57      0.64       283
                 building-sportsfacility       0.65      0.82      0.72       495
                        building-theater       0.78      0.90      0.83       529
event-attack/battle/war/militaryconflict       0.82      0.87      0.85      1583
                          event-disaster       0.67      0.73      0.70       317
                          event-election       0.56      0.46      0.51       282
                             event-other       0.65      0.57      0.60      1634
                           event-protest       0.41      0.48      0.44       227
                       event-sportsevent       0.74      0.80      0.77      1975
                            location-GPE       0.82      0.86      0.84     13112
                  location-bodiesofwater       0.83      0.82      0.83      1210
                         location-island       0.81      0.81      0.81       666
                       location-mountain       0.82      0.78      0.80       734
                          location-other       0.45      0.36      0.40      2207
                           location-park       0.71      0.81      0.76       634
   location-road/railway/highway/transit       0.76      0.79      0.77      1861
                    organization-company       0.75      0.77      0.76      3982
                  organization-education       0.87      0.88      0.88      3432
organization-government/governmentagency       0.65      0.60      0.62      2178
            organization-media/newspaper       0.63      0.67      0.65      1291
                      organization-other       0.63      0.64      0.64      5989
             organization-politicalparty       0.75      0.81      0.78      1199
                   organization-religion       0.65      0.74      0.69       830
           organization-showorganization       0.74      0.78      0.76       933
               organization-sportsleague       0.75      0.60      0.67      1088
                 organization-sportsteam       0.79      0.84      0.81      2374
                    other-astronomything       0.80      0.82      0.81       625
                             other-award       0.80      0.73      0.77      1873
                      other-biologything       0.69      0.70      0.69      1282
                     other-chemicalthing       0.70      0.56      0.62       881
                          other-currency       0.75      0.85      0.80       608
                           other-disease       0.71      0.73      0.72       825
                 other-educationaldegree       0.73      0.80      0.76       599
                               other-god       0.70      0.67      0.69       316
                          other-language       0.75      0.83      0.78       539
                               other-law       0.82      0.82      0.82       966
                       other-livingthing       0.64      0.71      0.67       696
                           other-medical       0.53      0.45      0.49       293
                            person-actor       0.85      0.82      0.83      1510
                    person-artist/author       0.74      0.77      0.76      3083
                          person-athlete       0.84      0.86      0.85      2519
                         person-director       0.73      0.73      0.73       535
                            person-other       0.71      0.68      0.70      7601
                       person-politician       0.72      0.72      0.72      2588
                          person-scholar       0.54      0.59      0.56       657
                          person-soldier       0.63      0.67      0.65       573
                        product-airplane       0.79      0.69      0.73       781
                             product-car       0.84      0.79      0.81       779
                            product-food       0.53      0.56      0.54       345
                            product-game       0.81      0.81      0.81       534
                           product-other       0.60      0.45      0.51      1751
                            product-ship       0.65      0.71      0.68       333
                        product-software       0.62      0.66      0.64       693
                           product-train       0.50      0.72      0.59       274
                          product-weapon       0.74      0.70      0.72       611

                                accuracy                           0.93    463214
                               macro avg       0.71      0.72      0.71    463214
                            weighted avg       0.93      0.93      0.93    463214



processed 463214 tokens with 48764 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 51017 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 34149.
accuracy:  73.78%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  92.88%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  66.94%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  70.03%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  68.45
              GPE: precision:  79.57%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  84.68%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  82.05  11001
            actor: precision:  81.64%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  78.81%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  80.20  779
         airplane: precision:  65.69%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  52.48%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  58.35  306
          airport: precision:  74.17%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  78.87%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  76.45  151
    artist/author: precision:  69.20%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  74.45%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  71.73  1857
   astronomything: precision:  70.49%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.30%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  71.87  366
          athlete: precision:  80.10%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  83.94%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  81.98  1553
attack/battle/war/militaryconflict: precision:  72.32%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  81.75%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  76.75  607
            award: precision:  58.38%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  59.30%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  58.83  519
     biologything: precision:  61.19%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  63.36%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  62.25  907
    bodiesofwater: precision:  76.54%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  77.16%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  76.85  618
 broadcastprogram: precision:  57.97%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  60.98%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  59.44  345
              car: precision:  68.66%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  67.74%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  68.20  367
    chemicalthing: precision:  57.74%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  50.92%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  54.12  478
          company: precision:  66.25%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  68.84%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  67.52  1991
         currency: precision:  66.60%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  76.23%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  71.09  467
         director: precision:  68.20%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  68.93%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  68.56  283
         disaster: precision:  46.54%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  57.36%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  51.39  159
          disease: precision:  58.45%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  65.62%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  61.83  503
        education: precision:  77.32%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  80.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  78.73  1151
educationaldegree: precision:  55.30%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  62.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  58.68  217
         election: precision:  26.83%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  26.51%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  26.67  82
             film: precision:  73.77%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  74.32%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.05  408
             food: precision:  43.35%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  44.67%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  44.00  203
             game: precision:  67.61%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.47%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  70.42  213
              god: precision:  67.04%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  70.98%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  68.95  270
government/governmentagency: precision:  47.22%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  45.37%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  46.28  737
         hospital: precision:  67.01%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  77.38%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  71.82  194
            hotel: precision:  68.93%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  66.67%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  67.78  177
           island: precision:  72.58%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  72.58%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  72.58  361
         language: precision:  68.77%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  80.56%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.20  506
              law: precision:  56.51%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  62.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  59.35  292
          library: precision:  66.37%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.89%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  69.93  226
      livingthing: precision:  59.27%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  63.12%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  61.13  491
  media/newspaper: precision:  52.84%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  62.66%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  57.34  721
          medical: precision:  52.25%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  52.97%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  52.61  222
         mountain: precision:  73.95%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  71.93%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  72.93  357
            music: precision:  76.52%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  74.13%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.31  558
            other: precision:  59.20%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  59.14%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  59.17  10514
         painting: precision:  37.04%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  40.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  38.46  27
             park: precision:  61.15%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.61%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  66.81  260
   politicalparty: precision:  61.72%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  74.45%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  67.49  661
       politician: precision:  66.98%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  68.20%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  67.58  1508
          protest: precision:  28.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  39.77%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  32.86  125
         religion: precision:  51.49%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  59.02%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  55.00  470
       restaurant: precision:  60.19%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  51.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  55.32  108
road/railway/highway/transit: precision:  64.51%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  69.78%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  67.04  834
          scholar: precision:  50.13%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  54.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  52.22  399
             ship: precision:  49.20%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  50.83%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  50.00  187
 showorganization: precision:  63.75%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  71.70%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  67.49  469
         software: precision:  56.28%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  62.53%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  59.24  430
          soldier: precision:  55.59%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  61.63%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  58.45  367
      sportsevent: precision:  55.30%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  63.48%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  59.11  792
   sportsfacility: precision:  59.68%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  75.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  66.67  253
     sportsleague: precision:  65.14%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  58.91%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  61.87  416
       sportsteam: precision:  70.38%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  79.51%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.66  1384
          theater: precision:  67.23%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  80.20%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  73.15  235
            train: precision:  39.51%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  54.70%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  45.88  162
           weapon: precision:  55.96%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  50.99%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  53.36  277
       writtenart: precision:  56.65%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  60.95%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  58.73  496

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="few_nerd" /><category term="ner" /><category term="open_source" /><category term="en" /><category term="english" /><category term="bert" /><category term="token_classification" /><summary type="html">Description BERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. This model is fine-tuned on the Few-NERD dataset. Few-NERD is a large-scale, fine-grained manually annotated named entity recognition dataset, which contains 8 coarse-grained types, 66 fine-grained types, 188,200 sentences, 491,711 entities, and 4,601,223 tokens. Three benchmark tasks are built, one is supervised (Few-NERD (SUP)) and the other two are few-shot (Few-NERD (INTRA) and Few-NERD (INTER)). Few-NERD is collected by researchers from Tsinghua University and DAMO Academy, Alibaba Group. Predicted Entities art-broadcastprogram art-film art-music art-other art-painting art-writtenart building-airport building-hospital building-hotel building-library building-other building-restaurant building-sportsfacility building-theater event-attack/battle/war/militaryconflict event-disaster event-election event-other event-protest event-sportsevent location-GPE location-bodiesofwater location-island location-mountain location-other location-park location-road/railway/highway/transit organization-company organization-education organization-government/governmentagency organization-media/newspaper organization-other organization-politicalparty organization-religion organization-showorganization organization-sportsleague organization-sportsteam other-astronomything other-award other-biologything other-chemicalthing other-currency other-disease other-educationaldegree other-god other-language other-law other-livingthing other-medical person-actor person-artist/author person-athlete person-director person-other person-politician person-scholar person-soldier product-airplane product-car product-food product-game product-other product-ship product-software product-train product-weapon Live Demo Open in Colab Download How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') tokenClassifier = BertForTokenClassification \ .pretrained('bert_base_token_classifier_few_nerd', 'en') \ .setInputCols(['token', 'document']) \ .setOutputCol('ner') \ .setCaseSensitive(True) \ .setMaxSentenceLength(512) pipeline = Pipeline(stages=[ document_assembler, tokenizer, tokenClassifier ]) example = spark.createDataFrame([['My name is John!']]).toDF(&quot;text&quot;) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_base_token_classifier_few_nerd&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(true) .setMaxSentenceLength(512) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, tokenClassifier)) val example = Seq.empty[&quot;My name is John!&quot;].toDS.toDF(&quot;text&quot;) val result = pipeline.fit(example).transform(example) Model Information Model Name: bert_base_token_classifier_few_nerd Compatibility: Spark NLP 3.2.0+ License: Open Source Edition: Official Input Labels: [token, document] Output Labels: [ner] Language: en Case sensitive: true Max sentense length: 512 Data Source https://github.com/thunlp/Few-NERD Benchmarking Test: precision recall f1-score support O 0.98 0.98 0.98 365750 art-broadcastprogram 0.66 0.66 0.66 890 art-film 0.78 0.78 0.78 1039 art-music 0.85 0.81 0.83 1773 art-other 0.40 0.40 0.40 729 art-painting 0.51 0.43 0.47 91 art-writtenart 0.69 0.70 0.70 1570 building-airport 0.83 0.88 0.85 391 building-hospital 0.80 0.89 0.84 577 building-hotel 0.87 0.80 0.83 526 building-library 0.81 0.86 0.83 715 building-other 0.64 0.67 0.65 3448 building-restaurant 0.72 0.57 0.64 283 building-sportsfacility 0.65 0.82 0.72 495 building-theater 0.78 0.90 0.83 529 event-attack/battle/war/militaryconflict 0.82 0.87 0.85 1583 event-disaster 0.67 0.73 0.70 317 event-election 0.56 0.46 0.51 282 event-other 0.65 0.57 0.60 1634 event-protest 0.41 0.48 0.44 227 event-sportsevent 0.74 0.80 0.77 1975 location-GPE 0.82 0.86 0.84 13112 location-bodiesofwater 0.83 0.82 0.83 1210 location-island 0.81 0.81 0.81 666 location-mountain 0.82 0.78 0.80 734 location-other 0.45 0.36 0.40 2207 location-park 0.71 0.81 0.76 634 location-road/railway/highway/transit 0.76 0.79 0.77 1861 organization-company 0.75 0.77 0.76 3982 organization-education 0.87 0.88 0.88 3432 organization-government/governmentagency 0.65 0.60 0.62 2178 organization-media/newspaper 0.63 0.67 0.65 1291 organization-other 0.63 0.64 0.64 5989 organization-politicalparty 0.75 0.81 0.78 1199 organization-religion 0.65 0.74 0.69 830 organization-showorganization 0.74 0.78 0.76 933 organization-sportsleague 0.75 0.60 0.67 1088 organization-sportsteam 0.79 0.84 0.81 2374 other-astronomything 0.80 0.82 0.81 625 other-award 0.80 0.73 0.77 1873 other-biologything 0.69 0.70 0.69 1282 other-chemicalthing 0.70 0.56 0.62 881 other-currency 0.75 0.85 0.80 608 other-disease 0.71 0.73 0.72 825 other-educationaldegree 0.73 0.80 0.76 599 other-god 0.70 0.67 0.69 316 other-language 0.75 0.83 0.78 539 other-law 0.82 0.82 0.82 966 other-livingthing 0.64 0.71 0.67 696 other-medical 0.53 0.45 0.49 293 person-actor 0.85 0.82 0.83 1510 person-artist/author 0.74 0.77 0.76 3083 person-athlete 0.84 0.86 0.85 2519 person-director 0.73 0.73 0.73 535 person-other 0.71 0.68 0.70 7601 person-politician 0.72 0.72 0.72 2588 person-scholar 0.54 0.59 0.56 657 person-soldier 0.63 0.67 0.65 573 product-airplane 0.79 0.69 0.73 781 product-car 0.84 0.79 0.81 779 product-food 0.53 0.56 0.54 345 product-game 0.81 0.81 0.81 534 product-other 0.60 0.45 0.51 1751 product-ship 0.65 0.71 0.68 333 product-software 0.62 0.66 0.64 693 product-train 0.50 0.72 0.59 274 product-weapon 0.74 0.70 0.72 611 accuracy 0.93 463214 macro avg 0.71 0.72 0.71 463214 weighted avg 0.93 0.93 0.93 463214 processed 463214 tokens with 48764 phrases; found: 51017 phrases; correct: 34149. accuracy: 73.78%; (non-O) accuracy: 92.88%; precision: 66.94%; recall: 70.03%; FB1: 68.45 GPE: precision: 79.57%; recall: 84.68%; FB1: 82.05 11001 actor: precision: 81.64%; recall: 78.81%; FB1: 80.20 779 airplane: precision: 65.69%; recall: 52.48%; FB1: 58.35 306 airport: precision: 74.17%; recall: 78.87%; FB1: 76.45 151 artist/author: precision: 69.20%; recall: 74.45%; FB1: 71.73 1857 astronomything: precision: 70.49%; recall: 73.30%; FB1: 71.87 366 athlete: precision: 80.10%; recall: 83.94%; FB1: 81.98 1553 attack/battle/war/militaryconflict: precision: 72.32%; recall: 81.75%; FB1: 76.75 607 award: precision: 58.38%; recall: 59.30%; FB1: 58.83 519 biologything: precision: 61.19%; recall: 63.36%; FB1: 62.25 907 bodiesofwater: precision: 76.54%; recall: 77.16%; FB1: 76.85 618 broadcastprogram: precision: 57.97%; recall: 60.98%; FB1: 59.44 345 car: precision: 68.66%; recall: 67.74%; FB1: 68.20 367 chemicalthing: precision: 57.74%; recall: 50.92%; FB1: 54.12 478 company: precision: 66.25%; recall: 68.84%; FB1: 67.52 1991 currency: precision: 66.60%; recall: 76.23%; FB1: 71.09 467 director: precision: 68.20%; recall: 68.93%; FB1: 68.56 283 disaster: precision: 46.54%; recall: 57.36%; FB1: 51.39 159 disease: precision: 58.45%; recall: 65.62%; FB1: 61.83 503 education: precision: 77.32%; recall: 80.18%; FB1: 78.73 1151 educationaldegree: precision: 55.30%; recall: 62.50%; FB1: 58.68 217 election: precision: 26.83%; recall: 26.51%; FB1: 26.67 82 film: precision: 73.77%; recall: 74.32%; FB1: 74.05 408 food: precision: 43.35%; recall: 44.67%; FB1: 44.00 203 game: precision: 67.61%; recall: 73.47%; FB1: 70.42 213 god: precision: 67.04%; recall: 70.98%; FB1: 68.95 270 government/governmentagency: precision: 47.22%; recall: 45.37%; FB1: 46.28 737 hospital: precision: 67.01%; recall: 77.38%; FB1: 71.82 194 hotel: precision: 68.93%; recall: 66.67%; FB1: 67.78 177 island: precision: 72.58%; recall: 72.58%; FB1: 72.58 361 language: precision: 68.77%; recall: 80.56%; FB1: 74.20 506 law: precision: 56.51%; recall: 62.50%; FB1: 59.35 292 library: precision: 66.37%; recall: 73.89%; FB1: 69.93 226 livingthing: precision: 59.27%; recall: 63.12%; FB1: 61.13 491 media/newspaper: precision: 52.84%; recall: 62.66%; FB1: 57.34 721 medical: precision: 52.25%; recall: 52.97%; FB1: 52.61 222 mountain: precision: 73.95%; recall: 71.93%; FB1: 72.93 357 music: precision: 76.52%; recall: 74.13%; FB1: 75.31 558 other: precision: 59.20%; recall: 59.14%; FB1: 59.17 10514 painting: precision: 37.04%; recall: 40.00%; FB1: 38.46 27 park: precision: 61.15%; recall: 73.61%; FB1: 66.81 260 politicalparty: precision: 61.72%; recall: 74.45%; FB1: 67.49 661 politician: precision: 66.98%; recall: 68.20%; FB1: 67.58 1508 protest: precision: 28.00%; recall: 39.77%; FB1: 32.86 125 religion: precision: 51.49%; recall: 59.02%; FB1: 55.00 470 restaurant: precision: 60.19%; recall: 51.18%; FB1: 55.32 108 road/railway/highway/transit: precision: 64.51%; recall: 69.78%; FB1: 67.04 834 scholar: precision: 50.13%; recall: 54.50%; FB1: 52.22 399 ship: precision: 49.20%; recall: 50.83%; FB1: 50.00 187 showorganization: precision: 63.75%; recall: 71.70%; FB1: 67.49 469 software: precision: 56.28%; recall: 62.53%; FB1: 59.24 430 soldier: precision: 55.59%; recall: 61.63%; FB1: 58.45 367 sportsevent: precision: 55.30%; recall: 63.48%; FB1: 59.11 792 sportsfacility: precision: 59.68%; recall: 75.50%; FB1: 66.67 253 sportsleague: precision: 65.14%; recall: 58.91%; FB1: 61.87 416 sportsteam: precision: 70.38%; recall: 79.51%; FB1: 74.66 1384 theater: precision: 67.23%; recall: 80.20%; FB1: 73.15 235 train: precision: 39.51%; recall: 54.70%; FB1: 45.88 162 weapon: precision: 55.96%; recall: 50.99%; FB1: 53.36 277 writtenart: precision: 56.65%; recall: 60.95%; FB1: 58.73 496</summary></entry><entry><title type="html">DistilBERT Token Classification - Few-NERD (distilbert_base_token_classifier_few_nerd)</title><link href="/2021/08/08/distilbert_base_token_classifier_few_nerd_en.html" rel="alternate" type="text/html" title="DistilBERT Token Classification - Few-NERD (distilbert_base_token_classifier_few_nerd)" /><published>2021-08-08T00:00:00+00:00</published><updated>2021-08-08T00:00:00+00:00</updated><id>/2021/08/08/distilbert_base_token_classifier_few_nerd_en</id><content type="html" xml:base="/2021/08/08/distilbert_base_token_classifier_few_nerd_en.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DistilBERT Model&lt;/code&gt; with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.&lt;/p&gt;

&lt;p&gt;This model is fine-tuned on the Few-NERD dataset. Few-NERD is a large-scale, fine-grained manually annotated named entity recognition dataset, which contains 8 coarse-grained types, 66 fine-grained types, 188,200 sentences, 491,711 entities, and 4,601,223 tokens. Three benchmark tasks are built, one is supervised (Few-NERD (SUP)) and the other two are few-shot (Few-NERD (INTRA) and Few-NERD (INTER)). Few-NERD is collected by researchers from Tsinghua University and DAMO Academy, Alibaba Group.&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;art-broadcastprogram&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;art-film&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;art-music&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;art-other&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;art-painting&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;art-writtenart&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;building-airport&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;building-hospital&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;building-hotel&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;building-library&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;building-other&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;building-restaurant&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;building-sportsfacility&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;building-theater&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event-attack/battle/war/militaryconflict&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event-disaster&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event-election&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event-other&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event-protest&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;event-sportsevent&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location-GPE&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location-bodiesofwater&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location-island&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location-mountain&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location-other&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location-park&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location-road/railway/highway/transit&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-company&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-education&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-government/governmentagency&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-media/newspaper&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-other&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-politicalparty&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-religion&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-showorganization&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-sportsleague&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;organization-sportsteam&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-astronomything&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-award&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-biologything&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-chemicalthing&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-currency&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-disease&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-educationaldegree&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-god&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-language&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-law&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-livingthing&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;other-medical&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person-actor&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person-artist/author&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person-athlete&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person-director&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person-other&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person-politician&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person-scholar&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person-soldier&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-airplane&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-car&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-food&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-game&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-other&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-ship&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-software&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-train&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product-weapon&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/distilbert_base_token_classifier_few_nerd_en_3.2.0_2.4_1628435975886.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DistilBertForTokenClassification&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'distilbert_base_token_classifier_few_nerd'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'My name is John!'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;DistilBertForTokenClassification&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;distilbert_base_token_classifier_few_nerd&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;My&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;John!&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;distilbert_base_token_classifier_few_nerd&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.2.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, document]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;en&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max sentense length:&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/thunlp/Few-NERD&quot;&gt;https://github.com/thunlp/Few-NERD&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Test:


                                          precision    recall  f1-score   support

                                       O       0.98      0.98      0.98    365750
                    art-broadcastprogram       0.66      0.66      0.66       890
                                art-film       0.76      0.74      0.75      1039
                               art-music       0.89      0.79      0.84      1773
                               art-other       0.39      0.41      0.40       729
                            art-painting       0.48      0.46      0.47        91
                          art-writtenart       0.68      0.72      0.70      1570
                        building-airport       0.84      0.88      0.86       391
                       building-hospital       0.79      0.89      0.84       577
                          building-hotel       0.85      0.80      0.83       526
                        building-library       0.83      0.87      0.85       715
                          building-other       0.64      0.67      0.66      3448
                     building-restaurant       0.63      0.52      0.57       283
                 building-sportsfacility       0.63      0.80      0.71       495
                        building-theater       0.77      0.85      0.81       529
event-attack/battle/war/militaryconflict       0.82      0.87      0.84      1583
                          event-disaster       0.73      0.71      0.72       317
                          event-election       0.64      0.46      0.53       282
                             event-other       0.64      0.61      0.62      1634
                           event-protest       0.42      0.33      0.37       227
                       event-sportsevent       0.73      0.78      0.75      1975
                            location-GPE       0.82      0.86      0.84     13112
                  location-bodiesofwater       0.84      0.82      0.83      1210
                         location-island       0.81      0.80      0.81       666
                       location-mountain       0.83      0.78      0.80       734
                          location-other       0.43      0.37      0.40      2207
                           location-park       0.72      0.80      0.76       634
   location-road/railway/highway/transit       0.77      0.79      0.78      1861
                    organization-company       0.71      0.77      0.74      3982
                  organization-education       0.87      0.88      0.87      3432
organization-government/governmentagency       0.63      0.56      0.59      2178
            organization-media/newspaper       0.63      0.64      0.63      1291
                      organization-other       0.62      0.64      0.63      5989
             organization-politicalparty       0.75      0.79      0.77      1199
                   organization-religion       0.65      0.72      0.68       830
           organization-showorganization       0.71      0.75      0.73       933
               organization-sportsleague       0.74      0.59      0.66      1088
                 organization-sportsteam       0.79      0.81      0.80      2374
                    other-astronomything       0.80      0.80      0.80       625
                             other-award       0.81      0.72      0.77      1873
                      other-biologything       0.70      0.68      0.69      1282
                     other-chemicalthing       0.70      0.56      0.62       881
                          other-currency       0.74      0.81      0.78       608
                           other-disease       0.71      0.71      0.71       825
                 other-educationaldegree       0.72      0.79      0.75       599
                               other-god       0.68      0.61      0.64       316
                          other-language       0.75      0.82      0.78       539
                               other-law       0.83      0.81      0.82       966
                       other-livingthing       0.62      0.70      0.66       696
                           other-medical       0.59      0.47      0.52       293
                            person-actor       0.84      0.80      0.82      1510
                    person-artist/author       0.73      0.77      0.75      3083
                          person-athlete       0.83      0.84      0.84      2519
                         person-director       0.75      0.69      0.72       535
                            person-other       0.69      0.68      0.68      7601
                       person-politician       0.70      0.72      0.71      2588
                          person-scholar       0.57      0.56      0.56       657
                          person-soldier       0.64      0.65      0.65       573
                        product-airplane       0.79      0.68      0.73       781
                             product-car       0.81      0.77      0.79       779
                            product-food       0.55      0.52      0.53       345
                            product-game       0.74      0.80      0.77       534
                           product-other       0.59      0.44      0.51      1751
                            product-ship       0.69      0.76      0.72       333
                        product-software       0.64      0.61      0.62       693
                           product-train       0.54      0.69      0.61       274
                          product-weapon       0.74      0.68      0.71       611

                                accuracy                           0.93    463214
                               macro avg       0.71      0.71      0.71    463214
                            weighted avg       0.93      0.93      0.93    463214



processed 463214 tokens with 48764 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 50982 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 33677.
accuracy:  72.96%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  92.73%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  66.06%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  69.06%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  67.53
              GPE: precision:  78.80%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  84.16%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  81.39  11040
            actor: precision:  79.59%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  76.83%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  78.18  779
         airplane: precision:  68.03%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  52.22%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  59.08  294
          airport: precision:  77.70%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  80.99%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  79.31  148
    artist/author: precision:  68.07%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.99%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  70.91  1876
   astronomything: precision:  71.43%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.86%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  72.63  364
          athlete: precision:  79.02%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  82.86%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  80.90  1554
attack/battle/war/militaryconflict: precision:  69.39%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  80.63%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.59  624
            award: precision:  60.16%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  58.51%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  59.33  497
     biologything: precision:  61.11%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  62.79%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  61.94  900
    bodiesofwater: precision:  77.96%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  77.32%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  77.64  608
 broadcastprogram: precision:  57.76%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  61.28%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  59.47  348
              car: precision:  68.98%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  69.35%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  69.17  374
    chemicalthing: precision:  56.49%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  49.82%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  52.94  478
          company: precision:  62.69%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  68.48%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  65.45  2093
         currency: precision:  66.37%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  72.06%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  69.10  443
         director: precision:  68.08%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  63.21%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  65.56  260
         disaster: precision:  50.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  56.59%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  53.09  146
          disease: precision:  61.30%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  65.40%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  63.28  478
        education: precision:  77.39%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  79.55%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  78.45  1141
educationaldegree: precision:  50.93%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  56.77%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  53.69  214
         election: precision:  27.40%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  24.10%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  25.64  73
             film: precision:  71.21%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  68.40%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  69.77  389
             food: precision:  45.05%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  41.62%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  43.27  182
             game: precision:  61.90%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  72.96%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  66.98  231
              god: precision:  63.36%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  65.10%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  64.22  262
government/governmentagency: precision:  46.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  41.59%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  43.91  686
         hospital: precision:  69.63%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  79.17%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.09  191
            hotel: precision:  65.93%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  65.57%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  65.75  182
           island: precision:  71.27%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  71.47%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  71.37  362
         language: precision:  69.28%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  79.86%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.19  498
              law: precision:  55.17%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  60.61%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  57.76  290
          library: precision:  65.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.89%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  69.44  229
      livingthing: precision:  56.56%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  62.69%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  59.47  511
  media/newspaper: precision:  53.78%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  62.01%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  57.60  701
          medical: precision:  58.10%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  55.71%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  56.88  210
         mountain: precision:  73.03%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  70.84%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  71.92  356
            music: precision:  76.67%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.61%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.11  553
            other: precision:  57.66%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  58.75%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  58.20  10723
         painting: precision:  33.33%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  40.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  36.36  30
             park: precision:  60.87%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  71.30%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  65.67  253
   politicalparty: precision:  60.38%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  69.53%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  64.63  631
       politician: precision:  64.91%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  66.71%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  65.80  1522
          protest: precision:  26.14%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  26.14%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  26.14  88
         religion: precision:  49.78%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  56.34%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  52.86  464
       restaurant: precision:  49.54%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  42.52%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  45.76  109
road/railway/highway/transit: precision:  66.63%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  71.47%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  68.96  827
          scholar: precision:  51.22%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  51.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  51.36  369
             ship: precision:  58.85%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  67.96%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  63.08  209
 showorganization: precision:  58.58%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  65.47%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  61.83  466
         software: precision:  56.30%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  58.91%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  57.58  405
          soldier: precision:  55.24%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  58.91%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  57.02  353
      sportsevent: precision:  53.74%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  62.46%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  57.77  802
   sportsfacility: precision:  59.92%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  75.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  66.81  252
     sportsleague: precision:  61.78%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  55.87%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  58.68  416
       sportsteam: precision:  70.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  76.65%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  73.45  1332
          theater: precision:  64.32%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  74.11%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  68.87  227
            train: precision:  40.25%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  54.70%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  46.38  159
           weapon: precision:  59.41%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  52.96%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  56.00  271
       writtenart: precision:  53.83%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  59.44%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  56.49  509
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="token_classification" /><category term="ner" /><category term="distilbert" /><category term="few_nerd" /><category term="open_source" /><category term="en" /><category term="english" /><summary type="html">Description DistilBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. This model is fine-tuned on the Few-NERD dataset. Few-NERD is a large-scale, fine-grained manually annotated named entity recognition dataset, which contains 8 coarse-grained types, 66 fine-grained types, 188,200 sentences, 491,711 entities, and 4,601,223 tokens. Three benchmark tasks are built, one is supervised (Few-NERD (SUP)) and the other two are few-shot (Few-NERD (INTRA) and Few-NERD (INTER)). Few-NERD is collected by researchers from Tsinghua University and DAMO Academy, Alibaba Group. Predicted Entities art-broadcastprogram art-film art-music art-other art-painting art-writtenart building-airport building-hospital building-hotel building-library building-other building-restaurant building-sportsfacility building-theater event-attack/battle/war/militaryconflict event-disaster event-election event-other event-protest event-sportsevent location-GPE location-bodiesofwater location-island location-mountain location-other location-park location-road/railway/highway/transit organization-company organization-education organization-government/governmentagency organization-media/newspaper organization-other organization-politicalparty organization-religion organization-showorganization organization-sportsleague organization-sportsteam other-astronomything other-award other-biologything other-chemicalthing other-currency other-disease other-educationaldegree other-god other-language other-law other-livingthing other-medical person-actor person-artist/author person-athlete person-director person-other person-politician person-scholar person-soldier product-airplane product-car product-food product-game product-other product-ship product-software product-train product-weapon Live Demo Open in Colab Download How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') tokenClassifier = DistilBertForTokenClassification \ .pretrained('distilbert_base_token_classifier_few_nerd', 'en') \ .setInputCols(['token', 'document']) \ .setOutputCol('ner') \ .setCaseSensitive(True) \ .setMaxSentenceLength(512) pipeline = Pipeline(stages=[ document_assembler, tokenizer, tokenClassifier ]) example = spark.createDataFrame([['My name is John!']]).toDF(&quot;text&quot;) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val tokenClassifier = DistilBertForTokenClassification.pretrained(&quot;distilbert_base_token_classifier_few_nerd&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(true) .setMaxSentenceLength(512) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, tokenClassifier)) val example = Seq.empty[&quot;My name is John!&quot;].toDS.toDF(&quot;text&quot;) val result = pipeline.fit(example).transform(example) Model Information Model Name: distilbert_base_token_classifier_few_nerd Compatibility: Spark NLP 3.2.0+ License: Open Source Edition: Official Input Labels: [token, document] Output Labels: [ner] Language: en Case sensitive: true Max sentense length: 512 Data Source https://github.com/thunlp/Few-NERD Benchmarking Test: precision recall f1-score support O 0.98 0.98 0.98 365750 art-broadcastprogram 0.66 0.66 0.66 890 art-film 0.76 0.74 0.75 1039 art-music 0.89 0.79 0.84 1773 art-other 0.39 0.41 0.40 729 art-painting 0.48 0.46 0.47 91 art-writtenart 0.68 0.72 0.70 1570 building-airport 0.84 0.88 0.86 391 building-hospital 0.79 0.89 0.84 577 building-hotel 0.85 0.80 0.83 526 building-library 0.83 0.87 0.85 715 building-other 0.64 0.67 0.66 3448 building-restaurant 0.63 0.52 0.57 283 building-sportsfacility 0.63 0.80 0.71 495 building-theater 0.77 0.85 0.81 529 event-attack/battle/war/militaryconflict 0.82 0.87 0.84 1583 event-disaster 0.73 0.71 0.72 317 event-election 0.64 0.46 0.53 282 event-other 0.64 0.61 0.62 1634 event-protest 0.42 0.33 0.37 227 event-sportsevent 0.73 0.78 0.75 1975 location-GPE 0.82 0.86 0.84 13112 location-bodiesofwater 0.84 0.82 0.83 1210 location-island 0.81 0.80 0.81 666 location-mountain 0.83 0.78 0.80 734 location-other 0.43 0.37 0.40 2207 location-park 0.72 0.80 0.76 634 location-road/railway/highway/transit 0.77 0.79 0.78 1861 organization-company 0.71 0.77 0.74 3982 organization-education 0.87 0.88 0.87 3432 organization-government/governmentagency 0.63 0.56 0.59 2178 organization-media/newspaper 0.63 0.64 0.63 1291 organization-other 0.62 0.64 0.63 5989 organization-politicalparty 0.75 0.79 0.77 1199 organization-religion 0.65 0.72 0.68 830 organization-showorganization 0.71 0.75 0.73 933 organization-sportsleague 0.74 0.59 0.66 1088 organization-sportsteam 0.79 0.81 0.80 2374 other-astronomything 0.80 0.80 0.80 625 other-award 0.81 0.72 0.77 1873 other-biologything 0.70 0.68 0.69 1282 other-chemicalthing 0.70 0.56 0.62 881 other-currency 0.74 0.81 0.78 608 other-disease 0.71 0.71 0.71 825 other-educationaldegree 0.72 0.79 0.75 599 other-god 0.68 0.61 0.64 316 other-language 0.75 0.82 0.78 539 other-law 0.83 0.81 0.82 966 other-livingthing 0.62 0.70 0.66 696 other-medical 0.59 0.47 0.52 293 person-actor 0.84 0.80 0.82 1510 person-artist/author 0.73 0.77 0.75 3083 person-athlete 0.83 0.84 0.84 2519 person-director 0.75 0.69 0.72 535 person-other 0.69 0.68 0.68 7601 person-politician 0.70 0.72 0.71 2588 person-scholar 0.57 0.56 0.56 657 person-soldier 0.64 0.65 0.65 573 product-airplane 0.79 0.68 0.73 781 product-car 0.81 0.77 0.79 779 product-food 0.55 0.52 0.53 345 product-game 0.74 0.80 0.77 534 product-other 0.59 0.44 0.51 1751 product-ship 0.69 0.76 0.72 333 product-software 0.64 0.61 0.62 693 product-train 0.54 0.69 0.61 274 product-weapon 0.74 0.68 0.71 611 accuracy 0.93 463214 macro avg 0.71 0.71 0.71 463214 weighted avg 0.93 0.93 0.93 463214 processed 463214 tokens with 48764 phrases; found: 50982 phrases; correct: 33677. accuracy: 72.96%; (non-O) accuracy: 92.73%; precision: 66.06%; recall: 69.06%; FB1: 67.53 GPE: precision: 78.80%; recall: 84.16%; FB1: 81.39 11040 actor: precision: 79.59%; recall: 76.83%; FB1: 78.18 779 airplane: precision: 68.03%; recall: 52.22%; FB1: 59.08 294 airport: precision: 77.70%; recall: 80.99%; FB1: 79.31 148 artist/author: precision: 68.07%; recall: 73.99%; FB1: 70.91 1876 astronomything: precision: 71.43%; recall: 73.86%; FB1: 72.63 364 athlete: precision: 79.02%; recall: 82.86%; FB1: 80.90 1554 attack/battle/war/militaryconflict: precision: 69.39%; recall: 80.63%; FB1: 74.59 624 award: precision: 60.16%; recall: 58.51%; FB1: 59.33 497 biologything: precision: 61.11%; recall: 62.79%; FB1: 61.94 900 bodiesofwater: precision: 77.96%; recall: 77.32%; FB1: 77.64 608 broadcastprogram: precision: 57.76%; recall: 61.28%; FB1: 59.47 348 car: precision: 68.98%; recall: 69.35%; FB1: 69.17 374 chemicalthing: precision: 56.49%; recall: 49.82%; FB1: 52.94 478 company: precision: 62.69%; recall: 68.48%; FB1: 65.45 2093 currency: precision: 66.37%; recall: 72.06%; FB1: 69.10 443 director: precision: 68.08%; recall: 63.21%; FB1: 65.56 260 disaster: precision: 50.00%; recall: 56.59%; FB1: 53.09 146 disease: precision: 61.30%; recall: 65.40%; FB1: 63.28 478 education: precision: 77.39%; recall: 79.55%; FB1: 78.45 1141 educationaldegree: precision: 50.93%; recall: 56.77%; FB1: 53.69 214 election: precision: 27.40%; recall: 24.10%; FB1: 25.64 73 film: precision: 71.21%; recall: 68.40%; FB1: 69.77 389 food: precision: 45.05%; recall: 41.62%; FB1: 43.27 182 game: precision: 61.90%; recall: 72.96%; FB1: 66.98 231 god: precision: 63.36%; recall: 65.10%; FB1: 64.22 262 government/governmentagency: precision: 46.50%; recall: 41.59%; FB1: 43.91 686 hospital: precision: 69.63%; recall: 79.17%; FB1: 74.09 191 hotel: precision: 65.93%; recall: 65.57%; FB1: 65.75 182 island: precision: 71.27%; recall: 71.47%; FB1: 71.37 362 language: precision: 69.28%; recall: 79.86%; FB1: 74.19 498 law: precision: 55.17%; recall: 60.61%; FB1: 57.76 290 library: precision: 65.50%; recall: 73.89%; FB1: 69.44 229 livingthing: precision: 56.56%; recall: 62.69%; FB1: 59.47 511 media/newspaper: precision: 53.78%; recall: 62.01%; FB1: 57.60 701 medical: precision: 58.10%; recall: 55.71%; FB1: 56.88 210 mountain: precision: 73.03%; recall: 70.84%; FB1: 71.92 356 music: precision: 76.67%; recall: 73.61%; FB1: 75.11 553 other: precision: 57.66%; recall: 58.75%; FB1: 58.20 10723 painting: precision: 33.33%; recall: 40.00%; FB1: 36.36 30 park: precision: 60.87%; recall: 71.30%; FB1: 65.67 253 politicalparty: precision: 60.38%; recall: 69.53%; FB1: 64.63 631 politician: precision: 64.91%; recall: 66.71%; FB1: 65.80 1522 protest: precision: 26.14%; recall: 26.14%; FB1: 26.14 88 religion: precision: 49.78%; recall: 56.34%; FB1: 52.86 464 restaurant: precision: 49.54%; recall: 42.52%; FB1: 45.76 109 road/railway/highway/transit: precision: 66.63%; recall: 71.47%; FB1: 68.96 827 scholar: precision: 51.22%; recall: 51.50%; FB1: 51.36 369 ship: precision: 58.85%; recall: 67.96%; FB1: 63.08 209 showorganization: precision: 58.58%; recall: 65.47%; FB1: 61.83 466 software: precision: 56.30%; recall: 58.91%; FB1: 57.58 405 soldier: precision: 55.24%; recall: 58.91%; FB1: 57.02 353 sportsevent: precision: 53.74%; recall: 62.46%; FB1: 57.77 802 sportsfacility: precision: 59.92%; recall: 75.50%; FB1: 66.81 252 sportsleague: precision: 61.78%; recall: 55.87%; FB1: 58.68 416 sportsteam: precision: 70.50%; recall: 76.65%; FB1: 73.45 1332 theater: precision: 64.32%; recall: 74.11%; FB1: 68.87 227 train: precision: 40.25%; recall: 54.70%; FB1: 46.38 159 weapon: precision: 59.41%; recall: 52.96%; FB1: 56.00 271 writtenart: precision: 53.83%; recall: 59.44%; FB1: 56.49 509</summary></entry><entry><title type="html">BERT Token Classification - NER CoNLL (bert_base_token_classifier_conll03)</title><link href="/2021/08/05/bert_base_token_classifier_conll03_en.html" rel="alternate" type="text/html" title="BERT Token Classification - NER CoNLL (bert_base_token_classifier_conll03)" /><published>2021-08-05T00:00:00+00:00</published><updated>2021-08-05T00:00:00+00:00</updated><id>/2021/08/05/bert_base_token_classifier_conll03_en</id><content type="html" xml:base="/2021/08/05/bert_base_token_classifier_conll03_en.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BERT Model&lt;/code&gt; with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;bert_base_token_classifier_conll03&lt;/strong&gt; is a fine-tuned BERT model that is ready to use for &lt;strong&gt;Named Entity Recognition&lt;/strong&gt; and achieves &lt;strong&gt;state-of-the-art performance&lt;/strong&gt; for the NER task. This model has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER), and Miscellaneous (MISC).&lt;/p&gt;

&lt;p&gt;We used &lt;a href=&quot;https://huggingface.co/transformers/model_doc/bert.html#tfbertfortokenclassification&quot;&gt;TFBertForTokenClassification&lt;/a&gt; to train this model and used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BertForTokenClassification&lt;/code&gt; annotator in Spark NLP 🚀 for prediction at scale!&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PER&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LOC&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ORG&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MISC&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_base_token_classifier_conll03_en_3.2.0_2.4_1628165842529.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForTokenClassification&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bert_base_token_classifier_conll03'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# since output column is IOB/IOB2 style, NerConverter can extract entities
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entities'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'My name is John!'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;BertForTokenClassification&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bert_base_token_classifier_conll03&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// since output column is IOB/IOB2 style, NerConverter can extract entities&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;My&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;John!&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+------------------------------------------------------------------------------------+
 |result                                                                              |
 +------------------------------------------------------------------------------------+
 |[B-PER, I-PER, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O, B-LOC]|
 +------------------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;bert_base_token_classifier_conll03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.2.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, document]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;en&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max sentense length:&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.clips.uantwerpen.be/conll2003/ner/&quot;&gt;https://www.clips.uantwerpen.be/conll2003/ner/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Test:

              precision    recall  f1-score   support

       B-LOC       0.94      0.90      0.92      1668
       I-ORG       0.85      0.93      0.88       835
      I-MISC       0.63      0.80      0.71       216
       I-LOC       0.87      0.84      0.86       257
       I-PER       0.98      0.98      0.98      1156
      B-MISC       0.78      0.82      0.80       702
       B-ORG       0.88      0.91      0.89      1661
       B-PER       0.96      0.94      0.95      1617

   micro avg       0.90      0.91      0.91      8112
   macro avg       0.86      0.89      0.87      8112
weighted avg       0.90      0.91      0.91      8112



processed 46435 tokens with 5648 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 5730 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 5050.
accuracy:  91.33%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  97.83%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  88.13%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  89.41%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  88.77
              LOC: precision:  92.57%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  89.69%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  91.11  1616
             MISC: precision:  71.92%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  79.91%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.71  780
              ORG: precision:  84.89%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  88.92%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.86  1740
              PER: precision:  95.11%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  93.75%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  94.43  1594


Dev:

            precision    recall  f1-score   support

       B-LOC       0.96      0.91      0.93      1837
       I-ORG       0.90      0.94      0.92       751
      I-MISC       0.83      0.84      0.84       346
       I-LOC       0.92      0.93      0.93       257
       I-PER       0.99      0.98      0.98      1307
      B-MISC       0.88      0.90      0.89       922
       B-ORG       0.90      0.92      0.91      1341
       B-PER       0.97      0.97      0.97      1842

   micro avg       0.94      0.93      0.93      8603
   macro avg       0.92      0.92      0.92      8603
weighted avg       0.94      0.93      0.93      8603



processed 51362 tokens with 5942 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 5961 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 5457.
accuracy:  93.33%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  98.64%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  91.55%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  91.84%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  91.69
              LOC: precision:  95.09%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  90.64%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  92.81  1751
             MISC: precision:  83.45%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  87.53%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  85.44  967
              ORG: precision:  86.43%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  90.75%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  88.54  1408
              PER: precision:  96.35%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  95.98%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  96.17  1835

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="ner" /><category term="conll" /><category term="en" /><category term="english" /><category term="token_classification" /><category term="bert" /><category term="open_source" /><summary type="html">Description BERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. bert_base_token_classifier_conll03 is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. This model has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER), and Miscellaneous (MISC). We used TFBertForTokenClassification to train this model and used BertForTokenClassification annotator in Spark NLP 🚀 for prediction at scale! Predicted Entities PER, LOC, ORG, MISC Live Demo Open in Colab Download How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') tokenClassifier = BertForTokenClassification \ .pretrained('bert_base_token_classifier_conll03', 'en') \ .setInputCols(['token', 'document']) \ .setOutputCol('ner') \ .setCaseSensitive(True) \ .setMaxSentenceLength(512) # since output column is IOB/IOB2 style, NerConverter can extract entities ner_converter = NerConverter() \ .setInputCols(['document', 'token', 'ner']) \ .setOutputCol('entities') pipeline = Pipeline(stages=[ document_assembler, tokenizer, tokenClassifier, ner_converter ]) example = spark.createDataFrame([['My name is John!']]).toDF(&quot;text&quot;) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_base_token_classifier_conll03&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(true) .setMaxSentenceLength(512) // since output column is IOB/IOB2 style, NerConverter can extract entities val ner_converter = NerConverter() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, tokenClassifier, ner_converter)) val example = Seq.empty[&quot;My name is John!&quot;].toDS.toDF(&quot;text&quot;) val result = pipeline.fit(example).transform(example) Results +------------------------------------------------------------------------------------+ |result | +------------------------------------------------------------------------------------+ |[B-PER, I-PER, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O, B-LOC]| +------------------------------------------------------------------------------------+ Model Information Model Name: bert_base_token_classifier_conll03 Compatibility: Spark NLP 3.2.0+ License: Open Source Edition: Official Input Labels: [token, document] Output Labels: [ner] Language: en Case sensitive: true Max sentense length: 512 Data Source https://www.clips.uantwerpen.be/conll2003/ner/ Benchmarking Test: precision recall f1-score support B-LOC 0.94 0.90 0.92 1668 I-ORG 0.85 0.93 0.88 835 I-MISC 0.63 0.80 0.71 216 I-LOC 0.87 0.84 0.86 257 I-PER 0.98 0.98 0.98 1156 B-MISC 0.78 0.82 0.80 702 B-ORG 0.88 0.91 0.89 1661 B-PER 0.96 0.94 0.95 1617 micro avg 0.90 0.91 0.91 8112 macro avg 0.86 0.89 0.87 8112 weighted avg 0.90 0.91 0.91 8112 processed 46435 tokens with 5648 phrases; found: 5730 phrases; correct: 5050. accuracy: 91.33%; (non-O) accuracy: 97.83%; precision: 88.13%; recall: 89.41%; FB1: 88.77 LOC: precision: 92.57%; recall: 89.69%; FB1: 91.11 1616 MISC: precision: 71.92%; recall: 79.91%; FB1: 75.71 780 ORG: precision: 84.89%; recall: 88.92%; FB1: 86.86 1740 PER: precision: 95.11%; recall: 93.75%; FB1: 94.43 1594 Dev: precision recall f1-score support B-LOC 0.96 0.91 0.93 1837 I-ORG 0.90 0.94 0.92 751 I-MISC 0.83 0.84 0.84 346 I-LOC 0.92 0.93 0.93 257 I-PER 0.99 0.98 0.98 1307 B-MISC 0.88 0.90 0.89 922 B-ORG 0.90 0.92 0.91 1341 B-PER 0.97 0.97 0.97 1842 micro avg 0.94 0.93 0.93 8603 macro avg 0.92 0.92 0.92 8603 weighted avg 0.94 0.93 0.93 8603 processed 51362 tokens with 5942 phrases; found: 5961 phrases; correct: 5457. accuracy: 93.33%; (non-O) accuracy: 98.64%; precision: 91.55%; recall: 91.84%; FB1: 91.69 LOC: precision: 95.09%; recall: 90.64%; FB1: 92.81 1751 MISC: precision: 83.45%; recall: 87.53%; FB1: 85.44 967 ORG: precision: 86.43%; recall: 90.75%; FB1: 88.54 1408 PER: precision: 96.35%; recall: 95.98%; FB1: 96.17 1835</summary></entry><entry><title type="html">BERT Token Classification - NER OntoNotes (bert_base_token_classifier_ontonote)</title><link href="/2021/08/05/bert_base_token_classifier_ontonote_en.html" rel="alternate" type="text/html" title="BERT Token Classification - NER OntoNotes (bert_base_token_classifier_ontonote)" /><published>2021-08-05T00:00:00+00:00</published><updated>2021-08-05T00:00:00+00:00</updated><id>/2021/08/05/bert_base_token_classifier_ontonote_en</id><content type="html" xml:base="/2021/08/05/bert_base_token_classifier_ontonote_en.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BERT Model&lt;/code&gt; with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;bert_base_token_classifier_ontonote&lt;/strong&gt; is a fine-tuned BERT model that is ready to use for &lt;strong&gt;Named Entity Recognition&lt;/strong&gt; and achieves &lt;strong&gt;state-of-the-art performance&lt;/strong&gt; for the NER task. This model has been trained to recognize four types of entities: CARDINAL, DATE, EVENT, FAC, GPE, LANGUAGE, LAW, LOC, MONEY, NORP, ORDINAL, ORG, PERCENT, PERSON, PRODUCT, QUANTITY, TIME, and WORK_OF_ART.&lt;/p&gt;

&lt;p&gt;We used &lt;a href=&quot;https://huggingface.co/transformers/model_doc/bert.html#tfbertfortokenclassification&quot;&gt;TFBertForTokenClassification&lt;/a&gt; to train this model and used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BertForTokenClassification&lt;/code&gt; annotator in Spark NLP 🚀 for prediction at scale!&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CARDINAL&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DATE&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EVENT&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FAC&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPE&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LANGUAGE&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LAW&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LOC&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MONEY&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NORP&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ORDINAL&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ORG&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PERCENT&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PERSON&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PRODUCT&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QUANTITY&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TIME&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WORK_OF_ART&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_base_token_classifier_ontonote_en_3.2.0_2.4_1628174984240.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForTokenClassification&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bert_base_token_classifier_ontonote'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# since output column is IOB/IOB2 style, NerConverter can extract entities
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entities'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'My name is John!'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;BertForTokenClassification&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bert_base_token_classifier_ontonote&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// since output column is IOB/IOB2 style, NerConverter can extract entities&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;My&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;John!&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+------------------------------------------------------------------------------------+
 |result                                                                              |
 +------------------------------------------------------------------------------------+
 |[B-PERSON, I-PERSON, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PERSON, O, O, O, O, B-LOC]|
 +------------------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;bert_base_token_classifier_ontonote&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.2.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, document]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;en&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max sentense length:&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://catalog.ldc.upenn.edu/LDC2013T19&quot;&gt;https://catalog.ldc.upenn.edu/LDC2013T19&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Test:

         precision    recall  f1-score   support

   B-CARDINAL       0.86      0.88      0.87       935
       B-DATE       0.88      0.90      0.89      1602
      B-EVENT       0.71      0.62      0.66        63
        B-FAC       0.75      0.76      0.76       135
        B-GPE       0.97      0.91      0.94      2240
   B-LANGUAGE       0.79      0.68      0.73        22
        B-LAW       0.76      0.65      0.70        40
        B-LOC       0.78      0.83      0.80       179
      B-MONEY       0.88      0.90      0.89       314
       B-NORP       0.92      0.96      0.94       841
    B-ORDINAL       0.81      0.93      0.87       195
        B-ORG       0.87      0.89      0.88      1795
    B-PERCENT       0.92      0.95      0.93       349
     B-PERSON       0.96      0.95      0.95      1988
    B-PRODUCT       0.75      0.78      0.76        76
   B-QUANTITY       0.81      0.82      0.82       105
       B-TIME       0.69      0.70      0.69       212
B-WORK_OF_ART       0.66      0.74      0.70       166
   I-CARDINAL       0.83      0.88      0.86       331
       I-DATE       0.89      0.92      0.90      2011
      I-EVENT       0.70      0.71      0.70       130
        I-FAC       0.82      0.85      0.84       213
        I-GPE       0.96      0.89      0.93       628
        I-LAW       0.85      0.65      0.74       106
        I-LOC       0.85      0.84      0.84       180
      I-MONEY       0.94      0.96      0.95       685
       I-NORP       0.99      0.79      0.88       160
    I-ORDINAL       0.00      0.00      0.00         4
        I-ORG       0.91      0.93      0.92      2406
    I-PERCENT       0.95      0.95      0.95       523
     I-PERSON       0.96      0.96      0.96      1412
    I-PRODUCT       0.81      0.80      0.80        69
   I-QUANTITY       0.83      0.92      0.87       206
       I-TIME       0.71      0.77      0.74       255
I-WORK_OF_ART       0.71      0.66      0.68       337
            O       0.99      0.99      0.99    131815

     accuracy                           0.98    152728
    macro avg       0.82      0.81      0.82    152728
 weighted avg       0.98      0.98      0.98    152728



processed 152728 tokens with 11257 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 11537 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 9906.
accuracy:  90.22%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  98.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  85.86%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  88.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.92
         CARDINAL: precision:  83.35%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  86.20%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.75  967
             DATE: precision:  82.23%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  86.95%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.53  1694
            EVENT: precision:  58.06%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  57.14%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  57.60  62
              FAC: precision:  68.67%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  76.30%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  72.28  150
              GPE: precision:  95.59%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  90.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  92.71  2109
         LANGUAGE: precision:  78.95%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  68.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  73.17  19
              LAW: precision:  63.16%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  60.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  61.54  38
              LOC: precision:  71.29%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  80.45%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.59  202
            MONEY: precision:  85.40%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  87.58%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.48  322
             NORP: precision:  89.82%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  93.34%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  91.55  874
          ORDINAL: precision:  81.17%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  92.82%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.60  223
              ORG: precision:  82.80%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  86.35%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.54  1872
          PERCENT: precision:  86.23%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  89.68%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  87.92  363
           PERSON: precision:  93.93%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  94.22%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  94.07  1994
          PRODUCT: precision:  70.37%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  75.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  72.61  81
         QUANTITY: precision:  73.28%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  80.95%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  76.92  116
             TIME: precision:  58.02%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  66.51%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  61.98  243
      WORK_OF_ART: precision:  52.40%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  65.66%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  58.29  208
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="ner" /><category term="en" /><category term="english" /><category term="token_classification" /><category term="bert" /><category term="open_source" /><category term="ontonotes" /><summary type="html">Description BERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. bert_base_token_classifier_ontonote is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. This model has been trained to recognize four types of entities: CARDINAL, DATE, EVENT, FAC, GPE, LANGUAGE, LAW, LOC, MONEY, NORP, ORDINAL, ORG, PERCENT, PERSON, PRODUCT, QUANTITY, TIME, and WORK_OF_ART. We used TFBertForTokenClassification to train this model and used BertForTokenClassification annotator in Spark NLP 🚀 for prediction at scale! Predicted Entities CARDINAL, DATE, EVENT, FAC, GPE, LANGUAGE, LAW, LOC, MONEY, NORP, ORDINAL, ORG, PERCENT, PERSON, PRODUCT, QUANTITY, TIME, WORK_OF_ART Live Demo Open in Colab Download How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') tokenClassifier = BertForTokenClassification \ .pretrained('bert_base_token_classifier_ontonote', 'en') \ .setInputCols(['token', 'document']) \ .setOutputCol('ner') \ .setCaseSensitive(True) \ .setMaxSentenceLength(512) # since output column is IOB/IOB2 style, NerConverter can extract entities ner_converter = NerConverter() \ .setInputCols(['document', 'token', 'ner']) \ .setOutputCol('entities') pipeline = Pipeline(stages=[ document_assembler, tokenizer, tokenClassifier, ner_converter ]) example = spark.createDataFrame([['My name is John!']]).toDF(&quot;text&quot;) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_base_token_classifier_ontonote&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(true) .setMaxSentenceLength(512) // since output column is IOB/IOB2 style, NerConverter can extract entities val ner_converter = NerConverter() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, tokenClassifier, ner_converter)) val example = Seq.empty[&quot;My name is John!&quot;].toDS.toDF(&quot;text&quot;) val result = pipeline.fit(example).transform(example) Results +------------------------------------------------------------------------------------+ |result | +------------------------------------------------------------------------------------+ |[B-PERSON, I-PERSON, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PERSON, O, O, O, O, B-LOC]| +------------------------------------------------------------------------------------+ Model Information Model Name: bert_base_token_classifier_ontonote Compatibility: Spark NLP 3.2.0+ License: Open Source Edition: Official Input Labels: [token, document] Output Labels: [ner] Language: en Case sensitive: true Max sentense length: 512 Data Source https://catalog.ldc.upenn.edu/LDC2013T19 Benchmarking Test: precision recall f1-score support B-CARDINAL 0.86 0.88 0.87 935 B-DATE 0.88 0.90 0.89 1602 B-EVENT 0.71 0.62 0.66 63 B-FAC 0.75 0.76 0.76 135 B-GPE 0.97 0.91 0.94 2240 B-LANGUAGE 0.79 0.68 0.73 22 B-LAW 0.76 0.65 0.70 40 B-LOC 0.78 0.83 0.80 179 B-MONEY 0.88 0.90 0.89 314 B-NORP 0.92 0.96 0.94 841 B-ORDINAL 0.81 0.93 0.87 195 B-ORG 0.87 0.89 0.88 1795 B-PERCENT 0.92 0.95 0.93 349 B-PERSON 0.96 0.95 0.95 1988 B-PRODUCT 0.75 0.78 0.76 76 B-QUANTITY 0.81 0.82 0.82 105 B-TIME 0.69 0.70 0.69 212 B-WORK_OF_ART 0.66 0.74 0.70 166 I-CARDINAL 0.83 0.88 0.86 331 I-DATE 0.89 0.92 0.90 2011 I-EVENT 0.70 0.71 0.70 130 I-FAC 0.82 0.85 0.84 213 I-GPE 0.96 0.89 0.93 628 I-LAW 0.85 0.65 0.74 106 I-LOC 0.85 0.84 0.84 180 I-MONEY 0.94 0.96 0.95 685 I-NORP 0.99 0.79 0.88 160 I-ORDINAL 0.00 0.00 0.00 4 I-ORG 0.91 0.93 0.92 2406 I-PERCENT 0.95 0.95 0.95 523 I-PERSON 0.96 0.96 0.96 1412 I-PRODUCT 0.81 0.80 0.80 69 I-QUANTITY 0.83 0.92 0.87 206 I-TIME 0.71 0.77 0.74 255 I-WORK_OF_ART 0.71 0.66 0.68 337 O 0.99 0.99 0.99 131815 accuracy 0.98 152728 macro avg 0.82 0.81 0.82 152728 weighted avg 0.98 0.98 0.98 152728 processed 152728 tokens with 11257 phrases; found: 11537 phrases; correct: 9906. accuracy: 90.22%; (non-O) accuracy: 98.00%; precision: 85.86%; recall: 88.00%; FB1: 86.92 CARDINAL: precision: 83.35%; recall: 86.20%; FB1: 84.75 967 DATE: precision: 82.23%; recall: 86.95%; FB1: 84.53 1694 EVENT: precision: 58.06%; recall: 57.14%; FB1: 57.60 62 FAC: precision: 68.67%; recall: 76.30%; FB1: 72.28 150 GPE: precision: 95.59%; recall: 90.00%; FB1: 92.71 2109 LANGUAGE: precision: 78.95%; recall: 68.18%; FB1: 73.17 19 LAW: precision: 63.16%; recall: 60.00%; FB1: 61.54 38 LOC: precision: 71.29%; recall: 80.45%; FB1: 75.59 202 MONEY: precision: 85.40%; recall: 87.58%; FB1: 86.48 322 NORP: precision: 89.82%; recall: 93.34%; FB1: 91.55 874 ORDINAL: precision: 81.17%; recall: 92.82%; FB1: 86.60 223 ORG: precision: 82.80%; recall: 86.35%; FB1: 84.54 1872 PERCENT: precision: 86.23%; recall: 89.68%; FB1: 87.92 363 PERSON: precision: 93.93%; recall: 94.22%; FB1: 94.07 1994 PRODUCT: precision: 70.37%; recall: 75.00%; FB1: 72.61 81 QUANTITY: precision: 73.28%; recall: 80.95%; FB1: 76.92 116 TIME: precision: 58.02%; recall: 66.51%; FB1: 61.98 243 WORK_OF_ART: precision: 52.40%; recall: 65.66%; FB1: 58.29 208</summary></entry><entry><title type="html">BERT Token Classification Large - NER CoNLL (bert_large_token_classifier_conll03)</title><link href="/2021/08/05/bert_large_token_classifier_conll03_en.html" rel="alternate" type="text/html" title="BERT Token Classification Large - NER CoNLL (bert_large_token_classifier_conll03)" /><published>2021-08-05T00:00:00+00:00</published><updated>2021-08-05T00:00:00+00:00</updated><id>/2021/08/05/bert_large_token_classifier_conll03_en</id><content type="html" xml:base="/2021/08/05/bert_large_token_classifier_conll03_en.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BERT Model&lt;/code&gt; with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;bert_large_token_classifier_conll03&lt;/strong&gt; is a fine-tuned BERT model that is ready to use for &lt;strong&gt;Named Entity Recognition&lt;/strong&gt; and achieves &lt;strong&gt;state-of-the-art performance&lt;/strong&gt; for the NER task. This model has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER), and Miscellaneous (MISC).&lt;/p&gt;

&lt;p&gt;We used &lt;a href=&quot;https://huggingface.co/transformers/model_doc/bert.html#tfbertfortokenclassification&quot;&gt;TFBertForTokenClassification&lt;/a&gt; to train this model and used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BertForTokenClassification&lt;/code&gt; annotator in Spark NLP 🚀 for prediction at scale!&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PER&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LOC&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ORG&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MISC&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_large_token_classifier_conll03_en_3.2.0_2.4_1628171471927.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForTokenClassification&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bert_large_token_classifier_conll03'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# since output column is IOB/IOB2 style, NerConverter can extract entities
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entities'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'My name is John!'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;BertForTokenClassification&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bert_large_token_classifier_conll03&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// since output column is IOB/IOB2 style, NerConverter can extract entities&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;My&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;John!&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+------------------------------------------------------------------------------------+
 |result                                                                              |
 +------------------------------------------------------------------------------------+
 |[B-PER, I-PER, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O, B-LOC]|
 +------------------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;bert_large_token_classifier_conll03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.2.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, document]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;en&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max sentense length:&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.clips.uantwerpen.be/conll2003/ner/&quot;&gt;https://www.clips.uantwerpen.be/conll2003/ner/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Dev:

         precision    recall  f1-score   support

       B-LOC       0.96      0.90      0.93      1837
       I-ORG       0.93      0.95      0.94       751
      I-MISC       0.91      0.87      0.89       346
       I-LOC       0.91      0.94      0.93       257
       I-PER       0.99      0.98      0.99      1307
      B-MISC       0.94      0.90      0.92       922
       B-ORG       0.88      0.95      0.91      1341
       B-PER       0.98      0.98      0.98      1842

   micro avg       0.95      0.94      0.94      8603
   macro avg       0.94      0.93      0.93      8603
weighted avg       0.95      0.94      0.94      8603



processed 51362 tokens with 5942 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 5915 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 5497.
accuracy:  93.99%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  98.80%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  92.93%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  92.51%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  92.72
              LOC: precision:  95.25%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  89.49%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  92.28  1726
             MISC: precision:  90.35%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  88.39%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  89.36  902
              ORG: precision:  86.75%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  93.21%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  89.86  1441
              PER: precision:  96.86%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  97.07%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  96.96  1846


Test:

         precision    recall  f1-score   support

       B-LOC       0.93      0.89      0.91      1668
       I-ORG       0.86      0.94      0.90       835
      I-MISC       0.68      0.75      0.71       216
       I-LOC       0.87      0.86      0.87       257
       I-PER       0.98      0.98      0.98      1156
      B-MISC       0.84      0.82      0.83       702
       B-ORG       0.87      0.92      0.90      1661
       B-PER       0.97      0.96      0.97      1617

   micro avg       0.91      0.92      0.91      8112
   macro avg       0.88      0.89      0.88      8112
weighted avg       0.91      0.92      0.92      8112



processed 46435 tokens with 5648 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 5682 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 5104.
accuracy:  92.01%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  98.09%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  89.83%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  90.37%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  90.10
              LOC: precision:  92.39%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  88.85%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  90.59  1604
             MISC: precision:  79.75%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  80.20%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  79.97  706
              ORG: precision:  85.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  91.27%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  88.29  1773
              PER: precision:  96.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  95.42%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  95.96  1599

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="ner" /><category term="conll" /><category term="en" /><category term="english" /><category term="token_classification" /><category term="bert" /><category term="open_source" /><category term="large" /><summary type="html">Description BERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. bert_large_token_classifier_conll03 is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. This model has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER), and Miscellaneous (MISC). We used TFBertForTokenClassification to train this model and used BertForTokenClassification annotator in Spark NLP 🚀 for prediction at scale! Predicted Entities PER, LOC, ORG, MISC Live Demo Open in Colab Download How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') tokenClassifier = BertForTokenClassification \ .pretrained('bert_large_token_classifier_conll03', 'en') \ .setInputCols(['token', 'document']) \ .setOutputCol('ner') \ .setCaseSensitive(True) \ .setMaxSentenceLength(512) # since output column is IOB/IOB2 style, NerConverter can extract entities ner_converter = NerConverter() \ .setInputCols(['document', 'token', 'ner']) \ .setOutputCol('entities') pipeline = Pipeline(stages=[ document_assembler, tokenizer, tokenClassifier, ner_converter ]) example = spark.createDataFrame([['My name is John!']]).toDF(&quot;text&quot;) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_large_token_classifier_conll03&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(true) .setMaxSentenceLength(512) // since output column is IOB/IOB2 style, NerConverter can extract entities val ner_converter = NerConverter() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, tokenClassifier, ner_converter)) val example = Seq.empty[&quot;My name is John!&quot;].toDS.toDF(&quot;text&quot;) val result = pipeline.fit(example).transform(example) Results +------------------------------------------------------------------------------------+ |result | +------------------------------------------------------------------------------------+ |[B-PER, I-PER, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O, B-LOC]| +------------------------------------------------------------------------------------+ Model Information Model Name: bert_large_token_classifier_conll03 Compatibility: Spark NLP 3.2.0+ License: Open Source Edition: Official Input Labels: [token, document] Output Labels: [ner] Language: en Case sensitive: true Max sentense length: 512 Data Source https://www.clips.uantwerpen.be/conll2003/ner/ Benchmarking Dev: precision recall f1-score support B-LOC 0.96 0.90 0.93 1837 I-ORG 0.93 0.95 0.94 751 I-MISC 0.91 0.87 0.89 346 I-LOC 0.91 0.94 0.93 257 I-PER 0.99 0.98 0.99 1307 B-MISC 0.94 0.90 0.92 922 B-ORG 0.88 0.95 0.91 1341 B-PER 0.98 0.98 0.98 1842 micro avg 0.95 0.94 0.94 8603 macro avg 0.94 0.93 0.93 8603 weighted avg 0.95 0.94 0.94 8603 processed 51362 tokens with 5942 phrases; found: 5915 phrases; correct: 5497. accuracy: 93.99%; (non-O) accuracy: 98.80%; precision: 92.93%; recall: 92.51%; FB1: 92.72 LOC: precision: 95.25%; recall: 89.49%; FB1: 92.28 1726 MISC: precision: 90.35%; recall: 88.39%; FB1: 89.36 902 ORG: precision: 86.75%; recall: 93.21%; FB1: 89.86 1441 PER: precision: 96.86%; recall: 97.07%; FB1: 96.96 1846 Test: precision recall f1-score support B-LOC 0.93 0.89 0.91 1668 I-ORG 0.86 0.94 0.90 835 I-MISC 0.68 0.75 0.71 216 I-LOC 0.87 0.86 0.87 257 I-PER 0.98 0.98 0.98 1156 B-MISC 0.84 0.82 0.83 702 B-ORG 0.87 0.92 0.90 1661 B-PER 0.97 0.96 0.97 1617 micro avg 0.91 0.92 0.91 8112 macro avg 0.88 0.89 0.88 8112 weighted avg 0.91 0.92 0.92 8112 processed 46435 tokens with 5648 phrases; found: 5682 phrases; correct: 5104. accuracy: 92.01%; (non-O) accuracy: 98.09%; precision: 89.83%; recall: 90.37%; FB1: 90.10 LOC: precision: 92.39%; recall: 88.85%; FB1: 90.59 1604 MISC: precision: 79.75%; recall: 80.20%; FB1: 79.97 706 ORG: precision: 85.50%; recall: 91.27%; FB1: 88.29 1773 PER: precision: 96.50%; recall: 95.42%; FB1: 95.96 1599</summary></entry><entry><title type="html">BERT Token Classification Large - NER OntoNotes (bert_large_token_classifier_ontonote)</title><link href="/2021/08/05/bert_large_token_classifier_ontonote_en.html" rel="alternate" type="text/html" title="BERT Token Classification Large - NER OntoNotes (bert_large_token_classifier_ontonote)" /><published>2021-08-05T00:00:00+00:00</published><updated>2021-08-05T00:00:00+00:00</updated><id>/2021/08/05/bert_large_token_classifier_ontonote_en</id><content type="html" xml:base="/2021/08/05/bert_large_token_classifier_ontonote_en.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BERT Model&lt;/code&gt; with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;bert_large_token_classifier_ontonote&lt;/strong&gt; is a fine-tuned BERT model that is ready to use for &lt;strong&gt;Named Entity Recognition&lt;/strong&gt; and achieves &lt;strong&gt;state-of-the-art performance&lt;/strong&gt; for the NER task. This model has been trained to recognize four types of entities: CARDINAL, DATE, EVENT, FAC, GPE, LANGUAGE, LAW, LOC, MONEY, NORP, ORDINAL, ORG, PERCENT, PERSON, PRODUCT, QUANTITY, TIME, and WORK_OF_ART.&lt;/p&gt;

&lt;p&gt;We used &lt;a href=&quot;https://huggingface.co/transformers/model_doc/bert.html#tfbertfortokenclassification&quot;&gt;TFBertForTokenClassification&lt;/a&gt; to train this model and used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BertForTokenClassification&lt;/code&gt; annotator in Spark NLP 🚀 for prediction at scale!&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CARDINAL&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DATE&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EVENT&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FAC&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPE&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LANGUAGE&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LAW&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LOC&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MONEY&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NORP&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ORDINAL&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ORG&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PERCENT&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PERSON&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PRODUCT&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QUANTITY&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TIME&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WORK_OF_ART&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_large_token_classifier_ontonote_en_3.2.0_2.4_1628176479421.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForTokenClassification&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bert_large_token_classifier_ontonote'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# since output column is IOB/IOB2 style, NerConverter can extract entities
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entities'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'My name is John!'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;BertForTokenClassification&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bert_large_token_classifier_ontonote&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// since output column is IOB/IOB2 style, NerConverter can extract entities&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;My&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;John!&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+------------------------------------------------------------------------------------+
 |result                                                                              |
 +------------------------------------------------------------------------------------+
 |[B-PERSON, I-PERSON, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PERSON, O, O, O, O, B-LOC]|
 +------------------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;bert_large_token_classifier_ontonote&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.2.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, document]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;en&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max sentense length:&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://catalog.ldc.upenn.edu/LDC2013T19&quot;&gt;https://catalog.ldc.upenn.edu/LDC2013T19&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Test:

           precision    recall  f1-score   support

   B-CARDINAL       0.86      0.86      0.86       935
       B-DATE       0.88      0.89      0.88      1602
      B-EVENT       0.76      0.67      0.71        63
        B-FAC       0.77      0.84      0.81       135
        B-GPE       0.98      0.92      0.95      2240
   B-LANGUAGE       0.79      0.68      0.73        22
        B-LAW       0.77      0.68      0.72        40
        B-LOC       0.73      0.82      0.78       179
      B-MONEY       0.90      0.89      0.89       314
       B-NORP       0.94      0.96      0.95       841
    B-ORDINAL       0.82      0.91      0.87       195
        B-ORG       0.90      0.91      0.91      1795
    B-PERCENT       0.94      0.93      0.94       349
     B-PERSON       0.95      0.96      0.95      1988
    B-PRODUCT       0.79      0.80      0.80        76
   B-QUANTITY       0.82      0.83      0.82       105
       B-TIME       0.69      0.69      0.69       212
B-WORK_OF_ART       0.71      0.72      0.71       166
   I-CARDINAL       0.83      0.89      0.86       331
       I-DATE       0.90      0.90      0.90      2011
      I-EVENT       0.76      0.74      0.75       130
        I-FAC       0.79      0.91      0.85       213
        I-GPE       0.94      0.89      0.92       628
        I-LAW       0.82      0.66      0.73       106
        I-LOC       0.89      0.83      0.86       180
      I-MONEY       0.94      0.96      0.95       685
       I-NORP       0.98      0.91      0.94       160
    I-ORDINAL       0.00      0.00      0.00         4
        I-ORG       0.92      0.93      0.93      2406
    I-PERCENT       0.96      0.95      0.96       523
     I-PERSON       0.97      0.94      0.96      1412
    I-PRODUCT       0.81      0.81      0.81        69
   I-QUANTITY       0.87      0.92      0.89       206
       I-TIME       0.68      0.73      0.70       255
I-WORK_OF_ART       0.72      0.66      0.69       337
            O       0.99      0.99      0.99    131815

     accuracy                           0.98    152728
    macro avg       0.83      0.82      0.82    152728
 weighted avg       0.98      0.98      0.98    152728



processed 152728 tokens with 11257 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 11394 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 10001.
accuracy:  90.30%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  98.10%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  87.77%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  88.84%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  88.31
         CARDINAL: precision:  83.37%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  84.17%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  83.77  944
             DATE: precision:  83.84%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  86.14%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.98  1646
            EVENT: precision:  64.06%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  65.08%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  64.57  64
              FAC: precision:  69.38%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  82.22%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.25  160
              GPE: precision:  96.64%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  91.25%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  93.87  2115
         LANGUAGE: precision:  78.95%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  68.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  73.17  19
              LAW: precision:  54.76%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  57.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  56.10  42
              LOC: precision:  70.10%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  79.89%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.67  204
            MONEY: precision:  87.70%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  88.54%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  88.11  317
             NORP: precision:  93.60%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  95.72%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  94.65  860
          ORDINAL: precision:  82.41%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  91.28%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.62  216
              ORG: precision:  87.26%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  89.30%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  88.27  1837
          PERCENT: precision:  89.43%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  89.68%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  89.56  350
           PERSON: precision:  93.70%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  95.02%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  94.36  2016
          PRODUCT: precision:  68.29%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.68%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  70.89  82
         QUANTITY: precision:  78.57%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  83.81%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  81.11  112
             TIME: precision:  58.85%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  62.74%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  60.73  226
      WORK_OF_ART: precision:  61.96%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  68.67%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  65.14  184
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="ner" /><category term="en" /><category term="english" /><category term="bert" /><category term="token_classification" /><category term="ontonotes" /><category term="large" /><summary type="html">Description BERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. bert_large_token_classifier_ontonote is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. This model has been trained to recognize four types of entities: CARDINAL, DATE, EVENT, FAC, GPE, LANGUAGE, LAW, LOC, MONEY, NORP, ORDINAL, ORG, PERCENT, PERSON, PRODUCT, QUANTITY, TIME, and WORK_OF_ART. We used TFBertForTokenClassification to train this model and used BertForTokenClassification annotator in Spark NLP 🚀 for prediction at scale! Predicted Entities CARDINAL, DATE, EVENT, FAC, GPE, LANGUAGE, LAW, LOC, MONEY, NORP, ORDINAL, ORG, PERCENT, PERSON, PRODUCT, QUANTITY, TIME, WORK_OF_ART Live Demo Open in Colab Download How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') tokenClassifier = BertForTokenClassification \ .pretrained('bert_large_token_classifier_ontonote', 'en') \ .setInputCols(['token', 'document']) \ .setOutputCol('ner') \ .setCaseSensitive(True) \ .setMaxSentenceLength(512) # since output column is IOB/IOB2 style, NerConverter can extract entities ner_converter = NerConverter() \ .setInputCols(['document', 'token', 'ner']) \ .setOutputCol('entities') pipeline = Pipeline(stages=[ document_assembler, tokenizer, tokenClassifier, ner_converter ]) example = spark.createDataFrame([['My name is John!']]).toDF(&quot;text&quot;) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_large_token_classifier_ontonote&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(true) .setMaxSentenceLength(512) // since output column is IOB/IOB2 style, NerConverter can extract entities val ner_converter = NerConverter() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, tokenClassifier, ner_converter)) val example = Seq.empty[&quot;My name is John!&quot;].toDS.toDF(&quot;text&quot;) val result = pipeline.fit(example).transform(example) Results +------------------------------------------------------------------------------------+ |result | +------------------------------------------------------------------------------------+ |[B-PERSON, I-PERSON, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PERSON, O, O, O, O, B-LOC]| +------------------------------------------------------------------------------------+ Model Information Model Name: bert_large_token_classifier_ontonote Compatibility: Spark NLP 3.2.0+ License: Open Source Edition: Official Input Labels: [token, document] Output Labels: [ner] Language: en Case sensitive: true Max sentense length: 512 Data Source https://catalog.ldc.upenn.edu/LDC2013T19 Benchmarking Test: precision recall f1-score support B-CARDINAL 0.86 0.86 0.86 935 B-DATE 0.88 0.89 0.88 1602 B-EVENT 0.76 0.67 0.71 63 B-FAC 0.77 0.84 0.81 135 B-GPE 0.98 0.92 0.95 2240 B-LANGUAGE 0.79 0.68 0.73 22 B-LAW 0.77 0.68 0.72 40 B-LOC 0.73 0.82 0.78 179 B-MONEY 0.90 0.89 0.89 314 B-NORP 0.94 0.96 0.95 841 B-ORDINAL 0.82 0.91 0.87 195 B-ORG 0.90 0.91 0.91 1795 B-PERCENT 0.94 0.93 0.94 349 B-PERSON 0.95 0.96 0.95 1988 B-PRODUCT 0.79 0.80 0.80 76 B-QUANTITY 0.82 0.83 0.82 105 B-TIME 0.69 0.69 0.69 212 B-WORK_OF_ART 0.71 0.72 0.71 166 I-CARDINAL 0.83 0.89 0.86 331 I-DATE 0.90 0.90 0.90 2011 I-EVENT 0.76 0.74 0.75 130 I-FAC 0.79 0.91 0.85 213 I-GPE 0.94 0.89 0.92 628 I-LAW 0.82 0.66 0.73 106 I-LOC 0.89 0.83 0.86 180 I-MONEY 0.94 0.96 0.95 685 I-NORP 0.98 0.91 0.94 160 I-ORDINAL 0.00 0.00 0.00 4 I-ORG 0.92 0.93 0.93 2406 I-PERCENT 0.96 0.95 0.96 523 I-PERSON 0.97 0.94 0.96 1412 I-PRODUCT 0.81 0.81 0.81 69 I-QUANTITY 0.87 0.92 0.89 206 I-TIME 0.68 0.73 0.70 255 I-WORK_OF_ART 0.72 0.66 0.69 337 O 0.99 0.99 0.99 131815 accuracy 0.98 152728 macro avg 0.83 0.82 0.82 152728 weighted avg 0.98 0.98 0.98 152728 processed 152728 tokens with 11257 phrases; found: 11394 phrases; correct: 10001. accuracy: 90.30%; (non-O) accuracy: 98.10%; precision: 87.77%; recall: 88.84%; FB1: 88.31 CARDINAL: precision: 83.37%; recall: 84.17%; FB1: 83.77 944 DATE: precision: 83.84%; recall: 86.14%; FB1: 84.98 1646 EVENT: precision: 64.06%; recall: 65.08%; FB1: 64.57 64 FAC: precision: 69.38%; recall: 82.22%; FB1: 75.25 160 GPE: precision: 96.64%; recall: 91.25%; FB1: 93.87 2115 LANGUAGE: precision: 78.95%; recall: 68.18%; FB1: 73.17 19 LAW: precision: 54.76%; recall: 57.50%; FB1: 56.10 42 LOC: precision: 70.10%; recall: 79.89%; FB1: 74.67 204 MONEY: precision: 87.70%; recall: 88.54%; FB1: 88.11 317 NORP: precision: 93.60%; recall: 95.72%; FB1: 94.65 860 ORDINAL: precision: 82.41%; recall: 91.28%; FB1: 86.62 216 ORG: precision: 87.26%; recall: 89.30%; FB1: 88.27 1837 PERCENT: precision: 89.43%; recall: 89.68%; FB1: 89.56 350 PERSON: precision: 93.70%; recall: 95.02%; FB1: 94.36 2016 PRODUCT: precision: 68.29%; recall: 73.68%; FB1: 70.89 82 QUANTITY: precision: 78.57%; recall: 83.81%; FB1: 81.11 112 TIME: precision: 58.85%; recall: 62.74%; FB1: 60.73 226 WORK_OF_ART: precision: 61.96%; recall: 68.67%; FB1: 65.14 184</summary></entry><entry><title type="html">BERT Token Classification - ParsBERT for Persian Language Understanding (bert_token_classifier_parsbert_armanner)</title><link href="/2021/08/05/bert_token_classifier_parsbert_armanner_fa.html" rel="alternate" type="text/html" title="BERT Token Classification - ParsBERT for Persian Language Understanding (bert_token_classifier_parsbert_armanner)" /><published>2021-08-05T00:00:00+00:00</published><updated>2021-08-05T00:00:00+00:00</updated><id>/2021/08/05/bert_token_classifier_parsbert_armanner_fa</id><content type="html" xml:base="/2021/08/05/bert_token_classifier_parsbert_armanner_fa.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;h2 id=&quot;parsbert-transformer-based-model-for-persian-language-understanding&quot;&gt;ParsBERT: Transformer-based Model for Persian Language Understanding&lt;/h2&gt;

&lt;p&gt;ParsBERT is a monolingual language model based on Google’s BERT architecture with the same configurations as BERT-Base.&lt;/p&gt;

&lt;p&gt;Paper presenting ParsBERT: &lt;a href=&quot;https://arxiv.org/abs/2005.12515&quot;&gt;arXiv:2005.12515&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;All the models (downstream tasks) are uncased and trained with whole word masking. (coming soon stay tuned)&lt;/p&gt;

&lt;h2 id=&quot;persian-ner-arman-peyma-armanpeyma&quot;&gt;Persian NER [ARMAN, PEYMA, ARMAN+PEYMA]&lt;/h2&gt;

&lt;p&gt;This task aims to extract named entities in the text, such as names, and label them with appropriate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NER&lt;/code&gt; classes such as locations, organizations, etc. The datasets used for this task contain sentences that are marked with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IOB&lt;/code&gt; format. In this format, tokens that are not part of an entity are tagged as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;”O”&lt;/code&gt; the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;”B”&lt;/code&gt;tag corresponds to the first word of an object, and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;”I”&lt;/code&gt; tag corresponds to the rest of the terms of the same entity. Both &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;”B”&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;”I”&lt;/code&gt; tags are followed by a hyphen (or underscore), followed by the entity category. Therefore, the NER task is a multi-class token classification problem that labels the tokens upon being fed a raw text. There are two primary datasets used in Persian NER, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ARMAN&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PEYMA&lt;/code&gt;. In ParsBERT, we prepared ner for both datasets as well as a combination of both datasets.&lt;/p&gt;

&lt;h3 id=&quot;arman&quot;&gt;ARMAN&lt;/h3&gt;

&lt;p&gt;ARMAN dataset holds 7,682 sentences with 250,015 sentences tagged over six different classes.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Organization&lt;/li&gt;
  &lt;li&gt;Location&lt;/li&gt;
  &lt;li&gt;Facility&lt;/li&gt;
  &lt;li&gt;Event&lt;/li&gt;
  &lt;li&gt;Product&lt;/li&gt;
  &lt;li&gt;Person&lt;/li&gt;
&lt;/ol&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Label&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;#&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Organization&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;30108&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Location&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12924&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Facility&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4458&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Event&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7557&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Product&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4389&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Person&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;15645&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;cite&quot;&gt;Cite&lt;/h2&gt;
&lt;p&gt;Please cite the following paper in your publication if you are using &lt;a href=&quot;https://arxiv.org/abs/2005.12515&quot;&gt;ParsBERT&lt;/a&gt; in your research:&lt;/p&gt;
&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@article{ParsBERT,
    title={ParsBERT: Transformer-based Model for Persian Language Understanding},
    author={Mehrdad Farahani, Mohammad Gharachorloo, Marzieh Farahani, Mohammad Manthouri},
    journal={ArXiv},
    year={2020},
    volume={abs/2005.12515}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_token_classifier_parsbert_armanner_fa_3.2.0_2.4_1628181836965.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForTokenClassification&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bert_token_classifier_parsbert_armanner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'fa'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# since output column is IOB/IOB2 style, NerConverter can extract entities
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entities'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;دفتر مرکزی شرکت کامیکو در شهر ساسکاتون ساسکاچوان قرار دارد.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;BertForTokenClassification&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bert_token_classifier_parsbert_armanner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;fa&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// since output column is IOB/IOB2 style, NerConverter can extract entities&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;دفتر&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;مرکزی&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;شرکت&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;کامیکو&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;در&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;شهر&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;ساسکاتون&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;ساسکاچوان&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;قرار&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;دارد.&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;bert_token_classifier_parsbert_armanner&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.2.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, document]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;fa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max sentense length:&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://huggingface.co/HooshvareLab/bert-base-parsbert-armanner-uncased&quot;&gt;https://huggingface.co/HooshvareLab/bert-base-parsbert-armanner-uncased&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The following table summarizes the F1 score obtained by ParsBERT as compared to other models and architectures.

| Dataset | ParsBERT | MorphoBERT | Beheshti-NER | LSTM-CRF | Rule-Based CRF | BiLSTM-CRF |
|---------|----------|------------|--------------|----------|----------------|------------|
| ARMAN   | 93.10&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;   | 89.9       | 84.03        | 86.55    | -              | 77.45      |

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="fa" /><category term="persian" /><category term="farsi" /><category term="ner" /><category term="token_classification" /><category term="bert" /><category term="parsbert" /><category term="open_source" /><summary type="html">Description ParsBERT: Transformer-based Model for Persian Language Understanding ParsBERT is a monolingual language model based on Google’s BERT architecture with the same configurations as BERT-Base. Paper presenting ParsBERT: arXiv:2005.12515 All the models (downstream tasks) are uncased and trained with whole word masking. (coming soon stay tuned) Persian NER [ARMAN, PEYMA, ARMAN+PEYMA] This task aims to extract named entities in the text, such as names, and label them with appropriate NER classes such as locations, organizations, etc. The datasets used for this task contain sentences that are marked with IOB format. In this format, tokens that are not part of an entity are tagged as ”O” the ”B”tag corresponds to the first word of an object, and the ”I” tag corresponds to the rest of the terms of the same entity. Both ”B” and ”I” tags are followed by a hyphen (or underscore), followed by the entity category. Therefore, the NER task is a multi-class token classification problem that labels the tokens upon being fed a raw text. There are two primary datasets used in Persian NER, ARMAN, and PEYMA. In ParsBERT, we prepared ner for both datasets as well as a combination of both datasets. ARMAN ARMAN dataset holds 7,682 sentences with 250,015 sentences tagged over six different classes. Organization Location Facility Event Product Person Label # Organization 30108 Location 12924 Facility 4458 Event 7557 Product 4389 Person 15645 Cite Please cite the following paper in your publication if you are using ParsBERT in your research: @article{ParsBERT, title={ParsBERT: Transformer-based Model for Persian Language Understanding}, author={Mehrdad Farahani, Mohammad Gharachorloo, Marzieh Farahani, Mohammad Manthouri}, journal={ArXiv}, year={2020}, volume={abs/2005.12515} } Live Demo Open in Colab Download How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') tokenClassifier = BertForTokenClassification \ .pretrained('bert_token_classifier_parsbert_armanner', 'fa') \ .setInputCols(['token', 'document']) \ .setOutputCol('ner') \ .setCaseSensitive(False) \ .setMaxSentenceLength(512) # since output column is IOB/IOB2 style, NerConverter can extract entities ner_converter = NerConverter() \ .setInputCols(['document', 'token', 'ner']) \ .setOutputCol('entities') pipeline = Pipeline(stages=[ document_assembler, tokenizer, tokenClassifier, ner_converter ]) example = spark.createDataFrame([[&quot;دفتر مرکزی شرکت کامیکو در شهر ساسکاتون ساسکاچوان قرار دارد.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_parsbert_armanner&quot;, &quot;fa&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(false) .setMaxSentenceLength(512) // since output column is IOB/IOB2 style, NerConverter can extract entities val ner_converter = NerConverter() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, tokenClassifier, ner_converter)) val example = Seq.empty[&quot;دفتر مرکزی شرکت کامیکو در شهر ساسکاتون ساسکاچوان قرار دارد.&quot;].toDS.toDF(&quot;text&quot;) val result = pipeline.fit(example).transform(example) Model Information Model Name: bert_token_classifier_parsbert_armanner Compatibility: Spark NLP 3.2.0+ License: Open Source Edition: Official Input Labels: [token, document] Output Labels: [ner] Language: fa Case sensitive: true Max sentense length: 512 Data Source https://huggingface.co/HooshvareLab/bert-base-parsbert-armanner-uncased Benchmarking The following table summarizes the F1 score obtained by ParsBERT as compared to other models and architectures. | Dataset | ParsBERT | MorphoBERT | Beheshti-NER | LSTM-CRF | Rule-Based CRF | BiLSTM-CRF | |---------|----------|------------|--------------|----------|----------------|------------| | ARMAN | 93.10* | 89.9 | 84.03 | 86.55 | - | 77.45 |</summary></entry><entry><title type="html">BERT Token Classification - ParsBERT for Persian Language Understanding (bert_token_classifier_parsbert_ner)</title><link href="/2021/08/05/bert_token_classifier_parsbert_ner_fa.html" rel="alternate" type="text/html" title="BERT Token Classification - ParsBERT for Persian Language Understanding (bert_token_classifier_parsbert_ner)" /><published>2021-08-05T00:00:00+00:00</published><updated>2021-08-05T00:00:00+00:00</updated><id>/2021/08/05/bert_token_classifier_parsbert_ner_fa</id><content type="html" xml:base="/2021/08/05/bert_token_classifier_parsbert_ner_fa.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;h2 id=&quot;parsbert-transformer-based-model-for-persian-language-understanding&quot;&gt;ParsBERT: Transformer-based Model for Persian Language Understanding&lt;/h2&gt;

&lt;p&gt;ParsBERT is a monolingual language model based on Google’s BERT architecture with the same configurations as BERT-Base.&lt;/p&gt;

&lt;p&gt;Paper presenting ParsBERT: &lt;a href=&quot;https://arxiv.org/abs/2005.12515&quot;&gt;arXiv:2005.12515&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;All the models (downstream tasks) are uncased and trained with whole word masking. (coming soon stay tuned)&lt;/p&gt;

&lt;h2 id=&quot;persian-ner-arman-peyma-armanpeyma&quot;&gt;Persian NER [ARMAN, PEYMA, ARMAN+PEYMA]&lt;/h2&gt;

&lt;p&gt;This task aims to extract named entities in the text, such as names, and label with appropriate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NER&lt;/code&gt; classes such as locations, organizations, etc. The datasets used for this task contain sentences that are marked with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IOB&lt;/code&gt; format. In this format, tokens that are not part of an entity are tagged as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;”O”&lt;/code&gt; the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;”B”&lt;/code&gt;tag corresponds to the first word of an object, and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;”I”&lt;/code&gt; tag corresponds to the rest of the terms of the same entity. Both &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;”B”&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;”I”&lt;/code&gt; tags are followed by a hyphen (or underscore), followed by the entity category. Therefore, the NER task is a multi-class token classification problem that labels the tokens upon being fed a raw text. There are two primary datasets used in Persian NER, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ARMAN&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PEYMA&lt;/code&gt;. In ParsBERT, we prepared ner for both datasets as well as a combination of both datasets.&lt;/p&gt;

&lt;h3 id=&quot;peyma&quot;&gt;PEYMA&lt;/h3&gt;

&lt;p&gt;PEYMA dataset includes 7,145 sentences with a total of 302,530 tokens from which 41,148 tokens are tagged with seven different classes.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Organization&lt;/li&gt;
  &lt;li&gt;Money&lt;/li&gt;
  &lt;li&gt;Location&lt;/li&gt;
  &lt;li&gt;Date&lt;/li&gt;
  &lt;li&gt;Time&lt;/li&gt;
  &lt;li&gt;Person&lt;/li&gt;
  &lt;li&gt;Percent&lt;/li&gt;
&lt;/ol&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Label&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;#&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Organization&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;16964&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Money&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2037&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Location&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8782&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Date&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4259&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Time&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;732&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Person&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7675&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Percent&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;699&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;arman&quot;&gt;ARMAN&lt;/h3&gt;

&lt;p&gt;ARMAN dataset holds 7,682 sentences with 250,015 sentences tagged over six different classes.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Organization&lt;/li&gt;
  &lt;li&gt;Location&lt;/li&gt;
  &lt;li&gt;Facility&lt;/li&gt;
  &lt;li&gt;Event&lt;/li&gt;
  &lt;li&gt;Product&lt;/li&gt;
  &lt;li&gt;Person&lt;/li&gt;
&lt;/ol&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Label&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;#&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Organization&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;30108&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Location&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12924&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Facility&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4458&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Event&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7557&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Product&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4389&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Person&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;15645&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;cite&quot;&gt;Cite&lt;/h2&gt;

&lt;p&gt;Please cite the following paper in your publication if you are using &lt;a href=&quot;https://arxiv.org/abs/2005.12515&quot;&gt;ParsBERT&lt;/a&gt; in your research:&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@article{ParsBERT,
    title={ParsBERT: Transformer-based Model for Persian Language Understanding},
    author={Mehrdad Farahani, Mohammad Gharachorloo, Marzieh Farahani, Mohammad Manthouri},
    journal={ArXiv},
    year={2020},
    volume={abs/2005.12515}
}

&lt;span class=&quot;gu&quot;&gt;## Predicted Entities&lt;/span&gt;

&lt;span class=&quot;sb&quot;&gt;`B-date`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`B-event`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`B-facility`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`B-location`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`B-money`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`B-organization`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`B-percent`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`B-person`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`B-product`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`B-time`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`I-date`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`I-event`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`I-facility`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`I-location`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`I-money`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`I-organization`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`I-percent`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`I-person`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`I-product`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`I-time`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`O`&lt;/span&gt;

{:.btn-box}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;button&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;button button-orange&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;disabled&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Live Demo&lt;span class=&quot;nt&quot;&gt;&amp;lt;/button&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;button&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;button button-orange&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;disabled&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Open in Colab&lt;span class=&quot;nt&quot;&gt;&amp;lt;/button&amp;gt;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;Download&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;sx&quot;&gt;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_token_classifier_parsbert_ner_fa_3.2.0_2.4_1628185221737.zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;{:.button.button-orange.button-orange-trans.arr.button-icon}

&lt;span class=&quot;gu&quot;&gt;## How to use&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;



&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tabs-box&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;markdown=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;1&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;top_tab_li&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;   
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;button&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tab-li code-selector-active python-button&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Python&lt;span class=&quot;nt&quot;&gt;&amp;lt;/button&amp;gt;&amp;lt;button&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tab-li code-selector-un-active scala-button&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Scala&lt;span class=&quot;nt&quot;&gt;&amp;lt;/button&amp;gt;&amp;lt;button&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tab-li code-selector-un-active nlu-button&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;NLU&lt;span class=&quot;nt&quot;&gt;&amp;lt;/button&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;```&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;python
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForTokenClassification&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bert_token_classifier_parsbert_ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'fa'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# since output column is IOB/IOB2 style, NerConverter can extract entities
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entities'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;دفتر مرکزی شرکت کامیکو در شهر ساسکاتون ساسکاچوان قرار دارد.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;BertForTokenClassification&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bert_token_classifier_parsbert_ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;fa&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// since output column is IOB/IOB2 style, NerConverter can extract entities&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;دفتر&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;مرکزی&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;شرکت&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;کامیکو&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;در&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;شهر&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;ساسکاتون&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;ساسکاچوان&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;قرار&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;دارد.&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&amp;lt;/div&amp;gt;&lt;/p&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;bert_token_classifier_parsbert_ner&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.2.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, document]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;fa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max sentense length:&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://huggingface.co/HooshvareLab/bert-base-parsbert-ner-uncased&quot;&gt;https://huggingface.co/HooshvareLab/bert-base-parsbert-ner-uncased&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The following table summarizes the F1 score obtained by ParsBERT as compared to other models and architectures.

| Dataset         | ParsBERT | MorphoBERT |  Beheshti-NER  |  LSTM-CRF  |  Rule-Based CRF  |  BiLSTM-CRF  |
|:---------------:|:--------:|:----------:|:--------------:|:----------:|:----------------:|:------------:|
|  ARMAN + PEYMA  |   95.13&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; |      -     |        -       |      -     |         -        |       -      |
|  PEYMA          |   98.79&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; |      -     |      90.59     |      -     |       84.00      |       -      |
|  ARMAN          |   93.10&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; |    89.9    |      84.03     |    86.55   |         -        |     77.45    |

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="fa" /><category term="persian" /><category term="farsi" /><category term="ner" /><category term="token_classification" /><category term="bert" /><category term="parsbert" /><category term="open_source" /><summary type="html">Description ParsBERT: Transformer-based Model for Persian Language Understanding ParsBERT is a monolingual language model based on Google’s BERT architecture with the same configurations as BERT-Base. Paper presenting ParsBERT: arXiv:2005.12515 All the models (downstream tasks) are uncased and trained with whole word masking. (coming soon stay tuned) Persian NER [ARMAN, PEYMA, ARMAN+PEYMA] This task aims to extract named entities in the text, such as names, and label with appropriate NER classes such as locations, organizations, etc. The datasets used for this task contain sentences that are marked with IOB format. In this format, tokens that are not part of an entity are tagged as ”O” the ”B”tag corresponds to the first word of an object, and the ”I” tag corresponds to the rest of the terms of the same entity. Both ”B” and ”I” tags are followed by a hyphen (or underscore), followed by the entity category. Therefore, the NER task is a multi-class token classification problem that labels the tokens upon being fed a raw text. There are two primary datasets used in Persian NER, ARMAN, and PEYMA. In ParsBERT, we prepared ner for both datasets as well as a combination of both datasets. PEYMA PEYMA dataset includes 7,145 sentences with a total of 302,530 tokens from which 41,148 tokens are tagged with seven different classes. Organization Money Location Date Time Person Percent Label # Organization 16964 Money 2037 Location 8782 Date 4259 Time 732 Person 7675 Percent 699 ARMAN ARMAN dataset holds 7,682 sentences with 250,015 sentences tagged over six different classes. Organization Location Facility Event Product Person Label # Organization 30108 Location 12924 Facility 4458 Event 7557 Product 4389 Person 15645 Cite Please cite the following paper in your publication if you are using ParsBERT in your research: @article{ParsBERT, title={ParsBERT: Transformer-based Model for Persian Language Understanding}, author={Mehrdad Farahani, Mohammad Gharachorloo, Marzieh Farahani, Mohammad Manthouri}, journal={ArXiv}, year={2020}, volume={abs/2005.12515} } ## Predicted Entities `B-date` `B-event` `B-facility` `B-location` `B-money` `B-organization` `B-percent` `B-person` `B-product` `B-time` `I-date` `I-event` `I-facility` `I-location` `I-money` `I-organization` `I-percent` `I-person` `I-product` `I-time` `O` {:.btn-box} &amp;lt;button class=&quot;button button-orange&quot; disabled&amp;gt;Live Demo&amp;lt;/button&amp;gt; &amp;lt;button class=&quot;button button-orange&quot; disabled&amp;gt;Open in Colab&amp;lt;/button&amp;gt; [Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_token_classifier_parsbert_ner_fa_3.2.0_2.4_1628185221737.zip){:.button.button-orange.button-orange-trans.arr.button-icon} ## How to use &amp;lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&amp;gt; &amp;lt;div class=&quot;top_tab_li&quot;&amp;gt; &amp;lt;button class=&quot;tab-li code-selector-active python-button&quot;&amp;gt;Python&amp;lt;/button&amp;gt;&amp;lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&amp;gt;Scala&amp;lt;/button&amp;gt;&amp;lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&amp;gt;NLU&amp;lt;/button&amp;gt; &amp;lt;/div&amp;gt; ```python document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') tokenClassifier = BertForTokenClassification \ .pretrained('bert_token_classifier_parsbert_ner', 'fa') \ .setInputCols(['token', 'document']) \ .setOutputCol('ner') \ .setCaseSensitive(False) \ .setMaxSentenceLength(512) # since output column is IOB/IOB2 style, NerConverter can extract entities ner_converter = NerConverter() \ .setInputCols(['document', 'token', 'ner']) \ .setOutputCol('entities') pipeline = Pipeline(stages=[ document_assembler, tokenizer, tokenClassifier, ner_converter ]) example = spark.createDataFrame([[&quot;دفتر مرکزی شرکت کامیکو در شهر ساسکاتون ساسکاچوان قرار دارد.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_parsbert_ner&quot;, &quot;fa&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(false) .setMaxSentenceLength(512) // since output column is IOB/IOB2 style, NerConverter can extract entities val ner_converter = NerConverter() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, tokenClassifier, ner_converter)) val example = Seq.empty[&quot;دفتر مرکزی شرکت کامیکو در شهر ساسکاتون ساسکاچوان قرار دارد.&quot;].toDS.toDF(&quot;text&quot;) val result = pipeline.fit(example).transform(example) &amp;lt;/div&amp;gt; Model Information Model Name: bert_token_classifier_parsbert_ner Compatibility: Spark NLP 3.2.0+ License: Open Source Edition: Official Input Labels: [token, document] Output Labels: [ner] Language: fa Case sensitive: true Max sentense length: 512 Data Source https://huggingface.co/HooshvareLab/bert-base-parsbert-ner-uncased Benchmarking The following table summarizes the F1 score obtained by ParsBERT as compared to other models and architectures. | Dataset | ParsBERT | MorphoBERT | Beheshti-NER | LSTM-CRF | Rule-Based CRF | BiLSTM-CRF | |:---------------:|:--------:|:----------:|:--------------:|:----------:|:----------------:|:------------:| | ARMAN + PEYMA | 95.13* | - | - | - | - | - | | PEYMA | 98.79* | - | 90.59 | - | 84.00 | - | | ARMAN | 93.10* | 89.9 | 84.03 | 86.55 | - | 77.45 |</summary></entry><entry><title type="html">BERT Token Classification - ParsBERT for Persian Language Understanding (bert_token_classifier_parsbert_peymaner)</title><link href="/2021/08/05/bert_token_classifier_parsbert_peymaner_fa.html" rel="alternate" type="text/html" title="BERT Token Classification - ParsBERT for Persian Language Understanding (bert_token_classifier_parsbert_peymaner)" /><published>2021-08-05T00:00:00+00:00</published><updated>2021-08-05T00:00:00+00:00</updated><id>/2021/08/05/bert_token_classifier_parsbert_peymaner_fa</id><content type="html" xml:base="/2021/08/05/bert_token_classifier_parsbert_peymaner_fa.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;h2 id=&quot;parsbert-transformer-based-model-for-persian-language-understanding&quot;&gt;ParsBERT: Transformer-based Model for Persian Language Understanding&lt;/h2&gt;

&lt;p&gt;ParsBERT is a monolingual language model based on Google’s BERT architecture with the same configurations as BERT-Base.&lt;/p&gt;

&lt;p&gt;Paper presenting ParsBERT: &lt;a href=&quot;https://arxiv.org/abs/2005.12515&quot;&gt;arXiv:2005.12515&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;All the models (downstream tasks) are uncased and trained with whole word masking. (coming soon stay tuned)&lt;/p&gt;

&lt;h2 id=&quot;persian-ner-arman-peyma-armanpeyma&quot;&gt;Persian NER [ARMAN, PEYMA, ARMAN+PEYMA]&lt;/h2&gt;

&lt;p&gt;This task aims to extract named entities in the text, such as names, and label them with appropriate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NER&lt;/code&gt; classes such as locations, organizations, etc. The datasets used for this task contain sentences that are marked with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IOB&lt;/code&gt; format. In this format, tokens that are not part of an entity are tagged as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;”O”&lt;/code&gt; the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;”B”&lt;/code&gt;tag corresponds to the first word of an object, and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;”I”&lt;/code&gt; tag corresponds to the rest of the terms of the same entity. Both &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;”B”&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;”I”&lt;/code&gt; tags are followed by a hyphen (or underscore), followed by the entity category. Therefore, the NER task is a multi-class token classification problem that labels the tokens upon being fed a raw text. There are two primary datasets used in Persian NER, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ARMAN&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PEYMA&lt;/code&gt;. In ParsBERT, we prepared ner for both datasets as well as a combination of both datasets.&lt;/p&gt;

&lt;h3 id=&quot;peyma&quot;&gt;PEYMA&lt;/h3&gt;

&lt;p&gt;PEYMA dataset includes 7,145 sentences with a total of 302,530 tokens from which 41,148 tokens are tagged with seven different classes.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Organization&lt;/li&gt;
  &lt;li&gt;Money&lt;/li&gt;
  &lt;li&gt;Location&lt;/li&gt;
  &lt;li&gt;Date&lt;/li&gt;
  &lt;li&gt;Time&lt;/li&gt;
  &lt;li&gt;Person&lt;/li&gt;
  &lt;li&gt;Percent&lt;/li&gt;
&lt;/ol&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Label&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;#&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Organization&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;16964&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Money&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2037&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Location&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8782&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Date&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4259&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Time&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;732&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Person&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7675&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Percent&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;699&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;cite&quot;&gt;Cite&lt;/h2&gt;

&lt;p&gt;Please cite the following paper in your publication if you are using &lt;a href=&quot;https://arxiv.org/abs/2005.12515&quot;&gt;ParsBERT&lt;/a&gt; in your research:&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@article{ParsBERT,
    title={ParsBERT: Transformer-based Model for Persian Language Understanding},
    author={Mehrdad Farahani, Mohammad Gharachorloo, Marzieh Farahani, Mohammad Manthouri},
    journal={ArXiv},
    year={2020},
    volume={abs/2005.12515}
}

&lt;span class=&quot;gu&quot;&gt;## Predicted Entities&lt;/span&gt;

&lt;span class=&quot;sb&quot;&gt;`B_DAT`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`B_LOC`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`B_MON`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`B_ORG`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`B_PCT`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`B_PER`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`B_TIM`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`I_DAT`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`I_LOC`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`I_MON`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`I_ORG`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`I_PCT`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`I_PER`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`I_TIM`&lt;/span&gt;
&lt;span class=&quot;sb&quot;&gt;`O`&lt;/span&gt;

{:.btn-box}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;button&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;button button-orange&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;disabled&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Live Demo&lt;span class=&quot;nt&quot;&gt;&amp;lt;/button&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;button&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;button button-orange&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;disabled&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Open in Colab&lt;span class=&quot;nt&quot;&gt;&amp;lt;/button&amp;gt;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;Download&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;sx&quot;&gt;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_token_classifier_parsbert_peymaner_fa_3.2.0_2.4_1628185661823.zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;{:.button.button-orange.button-orange-trans.arr.button-icon}

&lt;span class=&quot;gu&quot;&gt;## How to use&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;



&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tabs-box&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;markdown=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;1&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;top_tab_li&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;   
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;button&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tab-li code-selector-active python-button&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Python&lt;span class=&quot;nt&quot;&gt;&amp;lt;/button&amp;gt;&amp;lt;button&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tab-li code-selector-un-active scala-button&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Scala&lt;span class=&quot;nt&quot;&gt;&amp;lt;/button&amp;gt;&amp;lt;button&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tab-li code-selector-un-active nlu-button&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;NLU&lt;span class=&quot;nt&quot;&gt;&amp;lt;/button&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;```&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;python
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForTokenClassification&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bert_token_classifier_parsbert_peymaner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'fa'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# since output column is IOB/IOB2 style, NerConverter can extract entities
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entities'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;دفتر مرکزی شرکت کامیکو در شهر ساسکاتون ساسکاچوان قرار دارد.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;BertForTokenClassification&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bert_token_classifier_parsbert_peymaner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;fa&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// since output column is IOB/IOB2 style, NerConverter can extract entities&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;دفتر&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;مرکزی&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;شرکت&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;کامیکو&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;در&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;شهر&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;ساسکاتون&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;ساسکاچوان&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;قرار&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;دارد.&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&amp;lt;/div&amp;gt;&lt;/p&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;bert_token_classifier_parsbert_peymaner&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.2.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, document]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;fa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max sentense length:&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://huggingface.co/HooshvareLab/bert-base-parsbert-peymaner-uncased&quot;&gt;https://huggingface.co/HooshvareLab/bert-base-parsbert-peymaner-uncased&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The following table summarizes the F1 score obtained by ParsBERT as compared to other models and architectures.

| Dataset | ParsBERT | MorphoBERT | Beheshti-NER | LSTM-CRF | Rule-Based CRF | BiLSTM-CRF |
|---------|----------|------------|--------------|----------|----------------|------------|
| PEYMA   | 98.79&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;   | -          | 90.59        | -        | 84.00          | -          |

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="fa" /><category term="persian" /><category term="open_source" /><category term="ner" /><category term="token_classification" /><category term="bert" /><category term="parsbert" /><category term="farsi" /><summary type="html">Description ParsBERT: Transformer-based Model for Persian Language Understanding ParsBERT is a monolingual language model based on Google’s BERT architecture with the same configurations as BERT-Base. Paper presenting ParsBERT: arXiv:2005.12515 All the models (downstream tasks) are uncased and trained with whole word masking. (coming soon stay tuned) Persian NER [ARMAN, PEYMA, ARMAN+PEYMA] This task aims to extract named entities in the text, such as names, and label them with appropriate NER classes such as locations, organizations, etc. The datasets used for this task contain sentences that are marked with IOB format. In this format, tokens that are not part of an entity are tagged as ”O” the ”B”tag corresponds to the first word of an object, and the ”I” tag corresponds to the rest of the terms of the same entity. Both ”B” and ”I” tags are followed by a hyphen (or underscore), followed by the entity category. Therefore, the NER task is a multi-class token classification problem that labels the tokens upon being fed a raw text. There are two primary datasets used in Persian NER, ARMAN, and PEYMA. In ParsBERT, we prepared ner for both datasets as well as a combination of both datasets. PEYMA PEYMA dataset includes 7,145 sentences with a total of 302,530 tokens from which 41,148 tokens are tagged with seven different classes. Organization Money Location Date Time Person Percent Label # Organization 16964 Money 2037 Location 8782 Date 4259 Time 732 Person 7675 Percent 699 Cite Please cite the following paper in your publication if you are using ParsBERT in your research: @article{ParsBERT, title={ParsBERT: Transformer-based Model for Persian Language Understanding}, author={Mehrdad Farahani, Mohammad Gharachorloo, Marzieh Farahani, Mohammad Manthouri}, journal={ArXiv}, year={2020}, volume={abs/2005.12515} } ## Predicted Entities `B_DAT` `B_LOC` `B_MON` `B_ORG` `B_PCT` `B_PER` `B_TIM` `I_DAT` `I_LOC` `I_MON` `I_ORG` `I_PCT` `I_PER` `I_TIM` `O` {:.btn-box} &amp;lt;button class=&quot;button button-orange&quot; disabled&amp;gt;Live Demo&amp;lt;/button&amp;gt; &amp;lt;button class=&quot;button button-orange&quot; disabled&amp;gt;Open in Colab&amp;lt;/button&amp;gt; [Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_token_classifier_parsbert_peymaner_fa_3.2.0_2.4_1628185661823.zip){:.button.button-orange.button-orange-trans.arr.button-icon} ## How to use &amp;lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&amp;gt; &amp;lt;div class=&quot;top_tab_li&quot;&amp;gt; &amp;lt;button class=&quot;tab-li code-selector-active python-button&quot;&amp;gt;Python&amp;lt;/button&amp;gt;&amp;lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&amp;gt;Scala&amp;lt;/button&amp;gt;&amp;lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&amp;gt;NLU&amp;lt;/button&amp;gt; &amp;lt;/div&amp;gt; ```python document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') tokenClassifier = BertForTokenClassification \ .pretrained('bert_token_classifier_parsbert_peymaner', 'fa') \ .setInputCols(['token', 'document']) \ .setOutputCol('ner') \ .setCaseSensitive(False) \ .setMaxSentenceLength(512) # since output column is IOB/IOB2 style, NerConverter can extract entities ner_converter = NerConverter() \ .setInputCols(['document', 'token', 'ner']) \ .setOutputCol('entities') pipeline = Pipeline(stages=[ document_assembler, tokenizer, tokenClassifier, ner_converter ]) example = spark.createDataFrame([[&quot;دفتر مرکزی شرکت کامیکو در شهر ساسکاتون ساسکاچوان قرار دارد.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_parsbert_peymaner&quot;, &quot;fa&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(false) .setMaxSentenceLength(512) // since output column is IOB/IOB2 style, NerConverter can extract entities val ner_converter = NerConverter() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, tokenClassifier, ner_converter)) val example = Seq.empty[&quot;دفتر مرکزی شرکت کامیکو در شهر ساسکاتون ساسکاچوان قرار دارد.&quot;].toDS.toDF(&quot;text&quot;) val result = pipeline.fit(example).transform(example) &amp;lt;/div&amp;gt; Model Information Model Name: bert_token_classifier_parsbert_peymaner Compatibility: Spark NLP 3.2.0+ License: Open Source Edition: Official Input Labels: [token, document] Output Labels: [ner] Language: fa Case sensitive: true Max sentense length: 512 Data Source https://huggingface.co/HooshvareLab/bert-base-parsbert-peymaner-uncased Benchmarking The following table summarizes the F1 score obtained by ParsBERT as compared to other models and architectures. | Dataset | ParsBERT | MorphoBERT | Beheshti-NER | LSTM-CRF | Rule-Based CRF | BiLSTM-CRF | |---------|----------|------------|--------------|----------|----------------|------------| | PEYMA | 98.79* | - | 90.59 | - | 84.00 | - |</summary></entry><entry><title type="html">BERT Token Classification - BETO Spanish Language Understanding (bert_token_classifier_spanish_ner)</title><link href="/2021/08/05/bert_token_classifier_spanish_ner_es.html" rel="alternate" type="text/html" title="BERT Token Classification - BETO Spanish Language Understanding (bert_token_classifier_spanish_ner)" /><published>2021-08-05T00:00:00+00:00</published><updated>2021-08-05T00:00:00+00:00</updated><id>/2021/08/05/bert_token_classifier_spanish_ner_es</id><content type="html" xml:base="/2021/08/05/bert_token_classifier_spanish_ner_es.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BERT Model&lt;/code&gt; with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.&lt;/p&gt;

&lt;p&gt;This model is a fine-tuned on &lt;a href=&quot;https://www.kaggle.com/nltkdata/conll-corpora&quot;&gt;NER-C&lt;/a&gt; version of the Spanish BERT cased &lt;a href=&quot;https://github.com/dccuchile/beto&quot;&gt;(BETO)&lt;/a&gt; for &lt;strong&gt;NER&lt;/strong&gt; downstream task.&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B-LOC&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B-MISC&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B-ORG&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B-PER&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I-LOC&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I-MISC&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I-ORG&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I-PER&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_token_classifier_spanish_ner_es_3.2.0_2.4_1628186970366.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForTokenClassification&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bert_token_classifier_spanish_ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'es'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# since output column is IOB/IOB2 style, NerConverter can extract entities
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entities'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Me llamo Wolfgang y vivo en Berlin&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenClassifier&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;BertForTokenClassification&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bert_token_classifier_spanish_ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;es&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setCaseSensitive&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setMaxSentenceLength&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// since output column is IOB/IOB2 style, NerConverter can extract entities&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenClassifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Me&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;llamo&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Wolfgang&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;vivo&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;en&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Berlin&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;bert_token_classifier_spanish_ner&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.2.0+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[token, document]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;es&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Max sentense length:&lt;/td&gt;
      &lt;td&gt;512&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://huggingface.co/mrm8488/bert-spanish-cased-finetuned-ner&quot;&gt;https://huggingface.co/mrm8488/bert-spanish-cased-finetuned-ner&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;|                                                      Metric                                                       |  &lt;span class=&quot;c&quot;&gt;# score  |&lt;/span&gt;
| :------------------------------------------------------------------------------------: | :-------: |
| F1                                       | &lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;90.17&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;  
| Precision                                | &lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;89.86&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt; | 
| Recall                                   | &lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;90.47&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt; |    

&lt;span class=&quot;c&quot;&gt;## Comparison:&lt;/span&gt;

|                                                      Model                                                       |  &lt;span class=&quot;c&quot;&gt;# F1 score  |Size(MB)|&lt;/span&gt;
| :--------------------------------------------------------------------------------------------------------------: | :-------: |:------|
|                                        bert-base-spanish-wwm-cased &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;BETO&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;                                        |   88.43   | 421
| &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;bert-spanish-cased-finetuned-ner &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;this one&lt;span class=&quot;o&quot;&gt;)](&lt;/span&gt;https://huggingface.co/mrm8488/bert-spanish-cased-finetuned-ner&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; | &lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;90.17&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt; | 420 |
|                                              Best Multilingual BERT                                              |   87.38   | 681 |
|[TinyBERT-spanish-uncased-finetuned-ner]&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;https://huggingface.co/mrm8488/TinyBERT-spanish-uncased-finetuned-ner&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; | 70.00 | &lt;span class=&quot;k&quot;&gt;**&lt;/span&gt;55&lt;span class=&quot;k&quot;&gt;**&lt;/span&gt; |

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="spanish" /><category term="es" /><category term="bert" /><category term="ner" /><category term="token_classification" /><category term="open_source" /><summary type="html">Description BERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. This model is a fine-tuned on NER-C version of the Spanish BERT cased (BETO) for NER downstream task. Predicted Entities B-LOC B-MISC B-ORG B-PER I-LOC I-MISC I-ORG I-PER O Live Demo Open in Colab Download How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') tokenClassifier = BertForTokenClassification \ .pretrained('bert_token_classifier_spanish_ner', 'es') \ .setInputCols(['token', 'document']) \ .setOutputCol('ner') \ .setCaseSensitive(True) \ .setMaxSentenceLength(512) # since output column is IOB/IOB2 style, NerConverter can extract entities ner_converter = NerConverter() \ .setInputCols(['document', 'token', 'ner']) \ .setOutputCol('entities') pipeline = Pipeline(stages=[ document_assembler, tokenizer, tokenClassifier, ner_converter ]) example = spark.createDataFrame([[&quot;Me llamo Wolfgang y vivo en Berlin&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_spanish_ner&quot;, &quot;es&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(true) .setMaxSentenceLength(512) // since output column is IOB/IOB2 style, NerConverter can extract entities val ner_converter = NerConverter() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, tokenClassifier, ner_converter)) val example = Seq.empty[&quot;Me llamo Wolfgang y vivo en Berlin&quot;].toDS.toDF(&quot;text&quot;) val result = pipeline.fit(example).transform(example) Model Information Model Name: bert_token_classifier_spanish_ner Compatibility: Spark NLP 3.2.0+ License: Open Source Edition: Official Input Labels: [token, document] Output Labels: [ner] Language: es Case sensitive: false Max sentense length: 512 Data Source https://huggingface.co/mrm8488/bert-spanish-cased-finetuned-ner Benchmarking | Metric | # score | | :------------------------------------------------------------------------------------: | :-------: | | F1 | **90.17** | Precision | **89.86** | | Recall | **90.47** | ## Comparison: | Model | # F1 score |Size(MB)| | :--------------------------------------------------------------------------------------------------------------: | :-------: |:------| | bert-base-spanish-wwm-cased (BETO) | 88.43 | 421 | [bert-spanish-cased-finetuned-ner (this one)](https://huggingface.co/mrm8488/bert-spanish-cased-finetuned-ner) | **90.17** | 420 | | Best Multilingual BERT | 87.38 | 681 | |[TinyBERT-spanish-uncased-finetuned-ner](https://huggingface.co/mrm8488/TinyBERT-spanish-uncased-finetuned-ner) | 70.00 | **55** |</summary></entry></feed>