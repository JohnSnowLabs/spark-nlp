<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-07-07T07:38:37+00:00</updated><id>/feed.xml</id><title type="html">Spark NLP</title><subtitle>High Performance NLP with Apache Spark
</subtitle><author><name>{&quot;type&quot;=&gt;nil, &quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;googleplus&quot;=&gt;nil, &quot;telegram&quot;=&gt;nil, &quot;medium&quot;=&gt;nil, &quot;zhihu&quot;=&gt;nil, &quot;douban&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;npm&quot;=&gt;nil}</name></author><entry><title type="html">Multilingual XLMRoBerta Embeddings Cased Model</title><link href="/2023/06/29/xlmroberta_embeddings_paraphrase_mpnet_base_v2_xx.html" rel="alternate" type="text/html" title="Multilingual XLMRoBerta Embeddings Cased Model" /><published>2023-06-29T00:00:00+00:00</published><updated>2023-06-29T00:00:00+00:00</updated><id>/2023/06/29/xlmroberta_embeddings_paraphrase_mpnet_base_v2_xx</id><content type="html" xml:base="/2023/06/29/xlmroberta_embeddings_paraphrase_mpnet_base_v2_xx.html">## Description

Pretrained XLMRoberta Embeddings model is a multilingual embedding model adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/xlmroberta_embeddings_paraphrase_mpnet_base_v2_xx_4.4.4_3.0_1688073546075.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/xlmroberta_embeddings_paraphrase_mpnet_base_v2_xx_4.4.4_3.0_1688073546075.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
documentAssembler = DocumentAssembler() \
    .setInputCol(&quot;text&quot;) \
    .setOutputCol(&quot;document&quot;)

tokenizer = Tokenizer() \
    .setInputCols(&quot;document&quot;) \
    .setOutputCol(&quot;token&quot;)

embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlmroberta_embeddings_paraphrase_mpnet_base_v2&quot;,&quot;xx&quot;) \
    .setInputCols([&quot;document&quot;, &quot;token&quot;]) \
    .setOutputCol(&quot;embeddings&quot;) \
    .setCaseSensitive(True)

pipeline = Pipeline(stages=[documentAssembler, 
                            tokenizer, 
                            embeddings])

data = spark.createDataFrame([[&quot;I love Spark NLP&quot;]]).toDF(&quot;text&quot;)
result = pipeline.fit(data).transform(data)
```
```scala
val documentAssembler = new DocumentAssembler() 
      .setInputCol(&quot;text&quot;) 
      .setOutputCol(&quot;document&quot;)
 
val tokenizer = new Tokenizer() 
    .setInputCols(&quot;document&quot;)
    .setOutputCol(&quot;token&quot;)

val embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlmroberta_embeddings_paraphrase_mpnet_base_v2&quot;, &quot;xx&quot;) 
    .setInputCols(Array(&quot;document&quot;, &quot;token&quot;)) 
    .setOutputCol(&quot;embeddings&quot;)

val pipeline = new Pipeline().setStages(Array(documentAssembler, 
                                              tokenizer, 
                                              embeddings))

val data = Seq(&quot;I love Spark NLP&quot;).toDS.toDF(&quot;text&quot;)
val result = pipeline.fit(data).transform(data)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|xlmroberta_embeddings_paraphrase_mpnet_base_v2|
|Compatibility:|Spark NLP 4.4.4+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[sentence, token]|
|Output Labels:|[embeddings]|
|Language:|xx|
|Size:|1.0 GB|
|Case sensitive:|true|

## References

https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2</content><author><name>John Snow Labs</name></author><category term="xx" /><category term="embeddings" /><category term="xlmroberta" /><category term="open_source" /><category term="transformer" /><category term="tensorflow" /><summary type="html">Description Pretrained XLMRoberta Embeddings model is a multilingual embedding model adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU documentAssembler = DocumentAssembler() \ .setInputCol(&quot;text&quot;) \ .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() \ .setInputCols(&quot;document&quot;) \ .setOutputCol(&quot;token&quot;) embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlmroberta_embeddings_paraphrase_mpnet_base_v2&quot;,&quot;xx&quot;) \ .setInputCols([&quot;document&quot;, &quot;token&quot;]) \ .setOutputCol(&quot;embeddings&quot;) \ .setCaseSensitive(True) pipeline = Pipeline(stages=[documentAssembler, tokenizer, embeddings]) data = spark.createDataFrame([[&quot;I love Spark NLP&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlmroberta_embeddings_paraphrase_mpnet_base_v2&quot;, &quot;xx&quot;) .setInputCols(Array(&quot;document&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline().setStages(Array(documentAssembler, tokenizer, embeddings)) val data = Seq(&quot;I love Spark NLP&quot;).toDS.toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) Model Information Model Name: xlmroberta_embeddings_paraphrase_mpnet_base_v2 Compatibility: Spark NLP 4.4.4+ License: Open Source Edition: Official Input Labels: [sentence, token] Output Labels: [embeddings] Language: xx Size: 1.0 GB Case sensitive: true References https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2</summary></entry><entry><title type="html">BERT Embeddings (Base Cased)</title><link href="/2023/06/29/bert_base_cased_en.html" rel="alternate" type="text/html" title="BERT Embeddings (Base Cased)" /><published>2023-06-29T00:00:00+00:00</published><updated>2023-06-29T00:00:00+00:00</updated><id>/2023/06/29/bert_base_cased_en</id><content type="html" xml:base="/2023/06/29/bert_base_cased_en.html">## Description

This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper &quot;[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)&quot;.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_base_cased_en_5.0.0_3.0_1688044252396.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/bert_base_cased_en_5.0.0_3.0_1688044252396.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;

{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
...
embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings])
pipeline_model = nlp_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;))
result = pipeline_model.transform(spark.createDataFrame([['I love NLP']], [&quot;text&quot;]))
```

```scala
...
val embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings))
val data = Seq(&quot;I love NLP&quot;).toDF(&quot;text&quot;)
val result = pipeline.fit(data).transform(data)
```

{:.nlu-block}
```python
import nlu

text = [&quot;I love NLP&quot;]
embeddings_df = nlu.load('en.embed.bert.base_cased').predict(text, output_level='token')
embeddings_df
```

&lt;/div&gt;

{:.h2_title}

## Results

```bash
Results

	token	en_embed_bert_base_cased_embeddings

	I	[0.43879568576812744, -0.40160006284713745, 0....
	love	[0.21737590432167053, -0.3865768313407898, -0....
	NLP	[-0.16226479411125183, -0.053727392107248306, ...



{:.model-param}
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|bert_base_cased|
|Compatibility:|Spark NLP 5.0.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[sentence, token]|
|Output Labels:|[bert]|
|Language:|en|
|Size:|403.6 MB|
|Case sensitive:|true|

## References

The model is imported from [https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1](https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="embeddings" /><category term="en" /><category term="onnx" /><summary type="html">Description This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU ... embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings]) pipeline_model = nlp_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) result = pipeline_model.transform(spark.createDataFrame([['I love NLP']], [&quot;text&quot;])) ... val embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings)) val data = Seq(&quot;I love NLP&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) import nlu text = [&quot;I love NLP&quot;] embeddings_df = nlu.load('en.embed.bert.base_cased').predict(text, output_level='token') embeddings_df Results Results token en_embed_bert_base_cased_embeddings I [0.43879568576812744, -0.40160006284713745, 0.... love [0.21737590432167053, -0.3865768313407898, -0.... NLP [-0.16226479411125183, -0.053727392107248306, ... {:.model-param} Model Information Model Name: bert_base_cased Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Official Input Labels: [sentence, token] Output Labels: [bert] Language: en Size: 403.6 MB Case sensitive: true References The model is imported from https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1</summary></entry><entry><title type="html">BERT Embeddings (Base Cased) Optimized</title><link href="/2023/06/29/bert_base_cased_opt_en.html" rel="alternate" type="text/html" title="BERT Embeddings (Base Cased) Optimized" /><published>2023-06-29T00:00:00+00:00</published><updated>2023-06-29T00:00:00+00:00</updated><id>/2023/06/29/bert_base_cased_opt_en</id><content type="html" xml:base="/2023/06/29/bert_base_cased_opt_en.html">## Description

This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper &quot;[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)&quot;.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_base_cased_opt_en_5.0.0_3.0_1688044364323.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/bert_base_cased_opt_en_5.0.0_3.0_1688044364323.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;

{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
...
embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased_opt&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings])
pipeline_model = nlp_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;))
result = pipeline_model.transform(spark.createDataFrame([['I love NLP']], [&quot;text&quot;]))
```

```scala
...
val embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased_opt&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings))
val data = Seq(&quot;I love NLP&quot;).toDF(&quot;text&quot;)
val result = pipeline.fit(data).transform(data)
```

{:.nlu-block}
```python
import nlu

text = [&quot;I love NLP&quot;]
embeddings_df = nlu.load('en.embed.bert.base_cased').predict(text, output_level='token')
embeddings_df
```

&lt;/div&gt;

{:.h2_title}

## Results

```bash
Results

	token	en_embed_bert_base_cased_embeddings

	I	[0.43879568576812744, -0.40160006284713745, 0....
	love	[0.21737590432167053, -0.3865768313407898, -0....
	NLP	[-0.16226479411125183, -0.053727392107248306, ...



{:.model-param}
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|bert_base_cased_opt|
|Compatibility:|Spark NLP 5.0.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[sentence, token]|
|Output Labels:|[bert]|
|Language:|en|
|Size:|403.7 MB|
|Case sensitive:|true|

## References

The model is imported from [https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1](https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="embeddings" /><category term="en" /><category term="onnx" /><summary type="html">Description This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU ... embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased_opt&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings]) pipeline_model = nlp_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) result = pipeline_model.transform(spark.createDataFrame([['I love NLP']], [&quot;text&quot;])) ... val embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased_opt&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings)) val data = Seq(&quot;I love NLP&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) import nlu text = [&quot;I love NLP&quot;] embeddings_df = nlu.load('en.embed.bert.base_cased').predict(text, output_level='token') embeddings_df Results Results token en_embed_bert_base_cased_embeddings I [0.43879568576812744, -0.40160006284713745, 0.... love [0.21737590432167053, -0.3865768313407898, -0.... NLP [-0.16226479411125183, -0.053727392107248306, ... {:.model-param} Model Information Model Name: bert_base_cased_opt Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Official Input Labels: [sentence, token] Output Labels: [bert] Language: en Size: 403.7 MB Case sensitive: true References The model is imported from https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1</summary></entry><entry><title type="html">BERT Embeddings (Base Cased) Quantized</title><link href="/2023/06/29/bert_base_cased_quantized_en.html" rel="alternate" type="text/html" title="BERT Embeddings (Base Cased) Quantized" /><published>2023-06-29T00:00:00+00:00</published><updated>2023-06-29T00:00:00+00:00</updated><id>/2023/06/29/bert_base_cased_quantized_en</id><content type="html" xml:base="/2023/06/29/bert_base_cased_quantized_en.html">## Description

This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper &quot;[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)&quot;.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bert_base_cased_quantized_en_5.0.0_3.0_1688044431004.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/bert_base_cased_quantized_en_5.0.0_3.0_1688044431004.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;

{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
...
embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased_quantized&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings])
pipeline_model = nlp_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;))
result = pipeline_model.transform(spark.createDataFrame([['I love NLP']], [&quot;text&quot;]))
```

```scala
...
val embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased_quantized&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings))
val data = Seq(&quot;I love NLP&quot;).toDF(&quot;text&quot;)
val result = pipeline.fit(data).transform(data)
```

{:.nlu-block}
```python
import nlu

text = [&quot;I love NLP&quot;]
embeddings_df = nlu.load('en.embed.bert.base_cased').predict(text, output_level='token')
embeddings_df
```

&lt;/div&gt;

{:.h2_title}

## Results

```bash
Results

	token	en_embed_bert_base_cased_embeddings

	I	[0.43879568576812744, -0.40160006284713745, 0....
	love	[0.21737590432167053, -0.3865768313407898, -0....
	NLP	[-0.16226479411125183, -0.053727392107248306, ...



{:.model-param}
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|bert_base_cased_quantized|
|Compatibility:|Spark NLP 5.0.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[sentence, token]|
|Output Labels:|[bert]|
|Language:|en|
|Size:|139.5 MB|
|Case sensitive:|true|

## References

The model is imported from [https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1](https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="embeddings" /><category term="en" /><category term="onnx" /><summary type="html">Description This model contains a deep bidirectional transformer trained on Wikipedia and the BookCorpus. The details are described in the paper “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU ... embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased_quantized&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings]) pipeline_model = nlp_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) result = pipeline_model.transform(spark.createDataFrame([['I love NLP']], [&quot;text&quot;])) ... val embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased_quantized&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings)) val data = Seq(&quot;I love NLP&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) import nlu text = [&quot;I love NLP&quot;] embeddings_df = nlu.load('en.embed.bert.base_cased').predict(text, output_level='token') embeddings_df Results Results token en_embed_bert_base_cased_embeddings I [0.43879568576812744, -0.40160006284713745, 0.... love [0.21737590432167053, -0.3865768313407898, -0.... NLP [-0.16226479411125183, -0.053727392107248306, ... {:.model-param} Model Information Model Name: bert_base_cased_quantized Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Official Input Labels: [sentence, token] Output Labels: [bert] Language: en Size: 139.5 MB Case sensitive: true References The model is imported from https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1</summary></entry><entry><title type="html">DeBERTa base model</title><link href="/2023/06/28/deberta_v3_base_en.html" rel="alternate" type="text/html" title="DeBERTa base model" /><published>2023-06-28T00:00:00+00:00</published><updated>2023-06-28T00:00:00+00:00</updated><id>/2023/06/28/deberta_v3_base_en</id><content type="html" xml:base="/2023/06/28/deberta_v3_base_en.html">## Description

The DeBERTa model was proposed in [[https://arxiv.org/abs/2006.03654 DeBERTa: Decoding-enhanced BERT with Disentangled Attention]] by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google’s BERT model released in 2018 and Facebook’s RoBERTa model released in 2019. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/deberta_v3_base_en_5.0.0_3.0_1687957496351.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/deberta_v3_base_en_5.0.0_3.0_1687957496351.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```


{:.nlu-block}
```python
import nlu
nlu.load(&quot;en.embed.deberta_v3_base&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;)
```

&lt;/div&gt;


{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|deberta_v3_base|
|Compatibility:|Spark NLP 5.0.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|435.2 MB|
|Case sensitive:|true|
|Max sentence length:|128|

## Benchmarking

```bash
Benchmarking
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="english" /><category term="open_source" /><category term="embeddings" /><category term="deberta" /><category term="v3" /><category term="base" /><category term="onnx" /><summary type="html">Description The DeBERTa model was proposed in [[https://arxiv.org/abs/2006.03654 DeBERTa: Decoding-enhanced BERT with Disentangled Attention]] by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google’s BERT model released in 2018 and Facebook’s RoBERTa model released in 2019. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu nlu.load(&quot;en.embed.deberta_v3_base&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;) Model Information Model Name: deberta_v3_base Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 435.2 MB Case sensitive: true Max sentence length: 128 Benchmarking Benchmarking</summary></entry><entry><title type="html">DeBERTa base model optimized for inference</title><link href="/2023/06/28/deberta_v3_base_opt_en.html" rel="alternate" type="text/html" title="DeBERTa base model optimized for inference" /><published>2023-06-28T00:00:00+00:00</published><updated>2023-06-28T00:00:00+00:00</updated><id>/2023/06/28/deberta_v3_base_opt_en</id><content type="html" xml:base="/2023/06/28/deberta_v3_base_opt_en.html">## Description

The DeBERTa model was proposed in [[https://arxiv.org/abs/2006.03654 DeBERTa: Decoding-enhanced BERT with Disentangled Attention]] by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google’s BERT model released in 2018 and Facebook’s RoBERTa model released in 2019. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/deberta_v3_base_opt_en_5.0.0_3.0_1687958380723.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/deberta_v3_base_opt_en_5.0.0_3.0_1687958380723.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base_opt&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base_opt&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```


{:.nlu-block}
```python
import nlu
nlu.load(&quot;en.embed.deberta_v3_base&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;)
```

&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|deberta_v3_base_opt|
|Compatibility:|Spark NLP 5.0.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|469.3 MB|
|Case sensitive:|true|
|Max sentence length:|128|

## Benchmarking

```bash
Benchmarking
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="english" /><category term="open_source" /><category term="embeddings" /><category term="deberta" /><category term="v3" /><category term="base" /><category term="onnx" /><summary type="html">Description The DeBERTa model was proposed in [[https://arxiv.org/abs/2006.03654 DeBERTa: Decoding-enhanced BERT with Disentangled Attention]] by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google’s BERT model released in 2018 and Facebook’s RoBERTa model released in 2019. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base_opt&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base_opt&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu nlu.load(&quot;en.embed.deberta_v3_base&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;) Model Information Model Name: deberta_v3_base_opt Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 469.3 MB Case sensitive: true Max sentence length: 128 Benchmarking Benchmarking</summary></entry><entry><title type="html">DeBERTa base model quantized</title><link href="/2023/06/28/deberta_v3_base_quantized_en.html" rel="alternate" type="text/html" title="DeBERTa base model quantized" /><published>2023-06-28T00:00:00+00:00</published><updated>2023-06-28T00:00:00+00:00</updated><id>/2023/06/28/deberta_v3_base_quantized_en</id><content type="html" xml:base="/2023/06/28/deberta_v3_base_quantized_en.html">## Description

The DeBERTa model was proposed in [[https://arxiv.org/abs/2006.03654 DeBERTa: Decoding-enhanced BERT with Disentangled Attention]] by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google’s BERT model released in 2018 and Facebook’s RoBERTa model released in 2019. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/deberta_v3_base_quantized_en_5.0.0_3.0_1687958846162.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/deberta_v3_base_quantized_en_5.0.0_3.0_1687958846162.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base_quantized&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base_quantized&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```


{:.nlu-block}
```python
import nlu
nlu.load(&quot;en.embed.deberta_v3_base&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;)
```

&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|deberta_v3_base_quantized|
|Compatibility:|Spark NLP 5.0.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|310.7 MB|
|Case sensitive:|true|
|Max sentence length:|128|

## Benchmarking

```bash
Benchmarking
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="english" /><category term="open_source" /><category term="embeddings" /><category term="deberta" /><category term="v3" /><category term="base" /><category term="onnx" /><summary type="html">Description The DeBERTa model was proposed in [[https://arxiv.org/abs/2006.03654 DeBERTa: Decoding-enhanced BERT with Disentangled Attention]] by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google’s BERT model released in 2018 and Facebook’s RoBERTa model released in 2019. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base_quantized&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = DeBertaEmbeddings.pretrained(&quot;deberta_v3_base_quantized&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu nlu.load(&quot;en.embed.deberta_v3_base&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;) Model Information Model Name: deberta_v3_base_quantized Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 310.7 MB Case sensitive: true Max sentence length: 128 Benchmarking Benchmarking</summary></entry><entry><title type="html">DistilBERT base model (cased)</title><link href="/2023/06/28/distilbert_base_cased_en.html" rel="alternate" type="text/html" title="DistilBERT base model (cased)" /><published>2023-06-28T00:00:00+00:00</published><updated>2023-06-28T00:00:00+00:00</updated><id>/2023/06/28/distilbert_base_cased_en</id><content type="html" xml:base="/2023/06/28/distilbert_base_cased_en.html">## Description

This model is a distilled version of the [BERT base model](https://huggingface.co/bert-base-cased). It was introduced in [this paper](https://arxiv.org/abs/1910.01108). The code for the distillation process can be found [here](https://github.com/huggingface/transformers/tree/master/examples/research_projects/distillation). This model is cased: it does make a difference between english and English.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/distilbert_base_cased_en_5.0.0_3.0_1687955596708.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/distilbert_base_cased_en_5.0.0_3.0_1687955596708.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings])
```
```scala
val embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings))
```


{:.nlu-block}
```python
import nlu
nlu.load(&quot;en.embed.distilbert&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;)
```

&lt;/div&gt;

{:.model-param}

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings])
```
```scala
val embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings))
```

{:.nlu-block}
```python
import nlu
nlu.load(&quot;en.embed.distilbert&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|distilbert_base_cased|
|Compatibility:|Spark NLP 5.0.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|243.8 MB|
|Case sensitive:|true|

## References

[https://huggingface.co/distilbert-base-cased](https://huggingface.co/distilbert-base-cased)

## Benchmarking

```bash
Benchmarking


When fine-tuned on downstream tasks, this model achieves the following results:

Glue test results:

| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |
|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|
|      | 81.5 | 87.8 | 88.2 | 90.4  | 47.2 | 85.5  | 85.6 | 60.6 |


```</content><author><name>John Snow Labs</name></author><category term="distilbert" /><category term="en" /><category term="english" /><category term="open_source" /><category term="embeddings" /><category term="onnx" /><summary type="html">Description This model is a distilled version of the BERT base model. It was introduced in this paper. The code for the distillation process can be found here. This model is cased: it does make a difference between english and English. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings]) val embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings)) import nlu nlu.load(&quot;en.embed.distilbert&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;) PythonScalaNLU embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings]) val embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings)) import nlu nlu.load(&quot;en.embed.distilbert&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;) Model Information Model Name: distilbert_base_cased Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 243.8 MB Case sensitive: true References https://huggingface.co/distilbert-base-cased Benchmarking Benchmarking When fine-tuned on downstream tasks, this model achieves the following results: Glue test results: | Task | MNLI | QQP | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE | |:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:| | | 81.5 | 87.8 | 88.2 | 90.4 | 47.2 | 85.5 | 85.6 | 60.6 |</summary></entry><entry><title type="html">DistilBERT base model (cased) optimized for inference</title><link href="/2023/06/28/distilbert_base_cased_opt_en.html" rel="alternate" type="text/html" title="DistilBERT base model (cased) optimized for inference" /><published>2023-06-28T00:00:00+00:00</published><updated>2023-06-28T00:00:00+00:00</updated><id>/2023/06/28/distilbert_base_cased_opt_en</id><content type="html" xml:base="/2023/06/28/distilbert_base_cased_opt_en.html">## Description

This model is a distilled version of the [BERT base model](https://huggingface.co/bert-base-cased). It was introduced in [this paper](https://arxiv.org/abs/1910.01108). The code for the distillation process can be found [here](https://github.com/huggingface/transformers/tree/master/examples/research_projects/distillation). This model is cased: it does make a difference between english and English.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/distilbert_base_cased_opt_en_5.0.0_3.0_1687955659414.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/distilbert_base_cased_opt_en_5.0.0_3.0_1687955659414.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased_opt&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings])
```
```scala
val embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased_opt&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings))
```


{:.nlu-block}
```python
import nlu
nlu.load(&quot;en.embed.distilbert&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;)
```

&lt;/div&gt;


{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|distilbert_base_cased_opt|
|Compatibility:|Spark NLP 5.0.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|243.8 MB|
|Case sensitive:|true|

## References

[https://huggingface.co/distilbert-base-cased](https://huggingface.co/distilbert-base-cased)

## Benchmarking

```bash
Benchmarking


When fine-tuned on downstream tasks, this model achieves the following results:

Glue test results:

| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |
|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|
|      | 81.5 | 87.8 | 88.2 | 90.4  | 47.2 | 85.5  | 85.6 | 60.6 |


```</content><author><name>John Snow Labs</name></author><category term="distilbert" /><category term="en" /><category term="english" /><category term="open_source" /><category term="embeddings" /><category term="onnx" /><summary type="html">Description This model is a distilled version of the BERT base model. It was introduced in this paper. The code for the distillation process can be found here. This model is cased: it does make a difference between english and English. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased_opt&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings]) val embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased_opt&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings)) import nlu nlu.load(&quot;en.embed.distilbert&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;) Model Information Model Name: distilbert_base_cased_opt Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 243.8 MB Case sensitive: true References https://huggingface.co/distilbert-base-cased Benchmarking Benchmarking When fine-tuned on downstream tasks, this model achieves the following results: Glue test results: | Task | MNLI | QQP | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE | |:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:| | | 81.5 | 87.8 | 88.2 | 90.4 | 47.2 | 85.5 | 85.6 | 60.6 |</summary></entry><entry><title type="html">DistilBERT base model (cased) quantized</title><link href="/2023/06/28/distilbert_base_cased_quantized_en.html" rel="alternate" type="text/html" title="DistilBERT base model (cased) quantized" /><published>2023-06-28T00:00:00+00:00</published><updated>2023-06-28T00:00:00+00:00</updated><id>/2023/06/28/distilbert_base_cased_quantized_en</id><content type="html" xml:base="/2023/06/28/distilbert_base_cased_quantized_en.html">## Description

This model is a distilled version of the [BERT base model](https://huggingface.co/bert-base-cased). It was introduced in [this paper](https://arxiv.org/abs/1910.01108). The code for the distillation process can be found [here](https://github.com/huggingface/transformers/tree/master/examples/research_projects/distillation). This model is cased: it does make a difference between english and English.

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/distilbert_base_cased_quantized_en_5.0.0_3.0_1687955697660.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/distilbert_base_cased_quantized_en_5.0.0_3.0_1687955697660.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use

&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased_quantized&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings])
```
```scala
val embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased_quantized&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings))
```


{:.nlu-block}
```python
import nlu
nlu.load(&quot;en.embed.distilbert&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;)
```

&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|distilbert_base_cased_quantized|
|Compatibility:|Spark NLP 5.0.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|111.1 MB|
|Case sensitive:|true|

## References

[https://huggingface.co/distilbert-base-cased](https://huggingface.co/distilbert-base-cased)

## Benchmarking

```bash
Benchmarking


When fine-tuned on downstream tasks, this model achieves the following results:

Glue test results:

| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |
|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|
|      | 81.5 | 87.8 | 88.2 | 90.4  | 47.2 | 85.5  | 85.6 | 60.6 |


```</content><author><name>John Snow Labs</name></author><category term="distilbert" /><category term="en" /><category term="english" /><category term="open_source" /><category term="embeddings" /><category term="onnx" /><summary type="html">Description This model is a distilled version of the BERT base model. It was introduced in this paper. The code for the distillation process can be found here. This model is cased: it does make a difference between english and English. Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased_quantized&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings]) val embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased_quantized&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, embeddings)) import nlu nlu.load(&quot;en.embed.distilbert&quot;).predict(&quot;&quot;&quot;Put your text here.&quot;&quot;&quot;) Model Information Model Name: distilbert_base_cased_quantized Compatibility: Spark NLP 5.0.0+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 111.1 MB Case sensitive: true References https://huggingface.co/distilbert-base-cased Benchmarking Benchmarking When fine-tuned on downstream tasks, this model achieves the following results: Glue test results: | Task | MNLI | QQP | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE | |:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:| | | 81.5 | 87.8 | 88.2 | 90.4 | 47.2 | 85.5 | 85.6 | 60.6 |</summary></entry></feed>