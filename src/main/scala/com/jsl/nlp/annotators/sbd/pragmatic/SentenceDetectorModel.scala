package com.jsl.nlp.annotators.sbd.pragmatic

import com.jsl.nlp.annotators.sbd.Sentence
import com.jsl.nlp.{Annotation, AnnotatorModel, DocumentAssembler}
import org.apache.spark.ml.util.{DefaultParamsReadable, Identifiable}

/**
  * Annotator that detects sentence boundaries using any provided approach
  * @param uid internal constructor requirement for serialization of params
  * @@ model: Model to use for boundaries detection
  */
class SentenceDetectorModel(override val uid: String) extends AnnotatorModel[SentenceDetectorModel] {

  import com.jsl.nlp.AnnotatorType._

  val model = new PragmaticMethod()

  def this() = this(Identifiable.randomUID("SENTENCE"))

  override val annotatorType: AnnotatorType = DOCUMENT

  override val requiredAnnotatorTypes: Array[AnnotatorType] = Array(DOCUMENT)

  setDefault(inputCols, Array(DOCUMENT))

  /**
    * Uses the model interface to prepare the context and extract the boundaries
    * @param annotations Annotations that correspond to inputAnnotationCols generated by previous annotators if any
    * @return One to many annotation relationship depending on how many sentences there are in the document
    */
  override def annotate(annotations: Seq[Annotation]): Seq[Annotation] = {
    annotations.flatMap(annotation => {
      val sentences: Seq[Sentence] = model.extractBounds(annotation.metadata(DOCUMENT))
      sentences.map(sentence => Annotation(
        this.annotatorType,
        sentence.begin,
        sentence.end,
        Map[String, String](annotatorType -> sentence.content)
      ))
    })
  }
}

object SentenceDetectorModel extends DefaultParamsReadable[SentenceDetectorModel]