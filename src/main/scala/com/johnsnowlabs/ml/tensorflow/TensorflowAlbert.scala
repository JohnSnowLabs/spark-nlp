package com.johnsnowlabs.ml.tensorflow

import com.johnsnowlabs.ml.tensorflow.sentencepiece._
import com.johnsnowlabs.nlp.annotators.common._
import org.tensorflow.ndarray.buffer.DataBuffers

import scala.collection.JavaConverters._

/**
  * This class is used to calculate ALBERT embeddings for For Sequence Batches of WordpieceTokenizedSentence.
  * Input for this model must be tokenzied with a SentencePieceModel,
  *
  * This Tensorflow model is using the weights provided by https://tfhub.dev/google/albert_base/3
  * * sequence_output: representations of every token in the input sequence with shape [batch_size, max_sequence_length, hidden_size].
  *
  * ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS - Google Research, Toyota Technological Institute at Chicago
  * This these embeddings represent the outputs generated by the Albert model.
  * All offical Albert releases by google in TF-HUB are supported with this Albert Wrapper:
  *
  * TF-HUB Models :
  * albert_base     = https://tfhub.dev/google/albert_base/3    |  768-embed-dim,   12-layer,  12-heads, 12M parameters
  * albert_large    = https://tfhub.dev/google/albert_large/3   |  1024-embed-dim,  24-layer,  16-heads, 18M parameters
  * albert_xlarge   = https://tfhub.dev/google/albert_xlarge/3  |  2048-embed-dim,  24-layer,  32-heads, 60M parameters
  * albert_xxlarge  = https://tfhub.dev/google/albert_xxlarge/3 |  4096-embed-dim,  12-layer,  64-heads, 235M parameters
  *
  * This model requires input tokenization with SentencePiece model, which is provided by Spark NLP
  *
  * For additional information see :
  * https://arxiv.org/pdf/1909.11942.pdf
  * https://github.com/google-research/ALBERT
  * https://tfhub.dev/s?q=albert
  *
  * Tips:
  *
  * ALBERT uses repeating layers which results in a small memory footprint,
  * however the computational cost remains similar to a BERT-like architecture with
  * the same number of hidden layers as it has to iterate through the same number of (repeating) layers.
  *
  * @param tensorflow       Albert Model wrapper with TensorFlowWrapper
  * @param spp              Albert SentencePiece model with SentencePieceWrapper
  * @param batchSize        size of batch
  * @param configProtoBytes Configuration for TensorFlow session
  */

class TensorflowAlbert(val tensorflow: TensorflowWrapper,
                       val spp: SentencePieceWrapper,
                       batchSize: Int,
                       configProtoBytes: Option[Array[Byte]] = None
                      ) extends Serializable {

  // keys representing the input and output tensors of the ALBERT model
  private val tokenIdsKey = "input_ids"
  private val maskIdsKey = "input_mask"
  private val segmentIdsKey = "segment_ids"
  private val outputSequenceKey = "module/seq_out"
  private val sentenceStartTokenId = Array(2)
  private val sentenceEndTokenId = Array(3)
  private val sentencePieceDelimiterId = 13

  def tag(batch: Seq[Array[Int]]): Seq[Array[Array[Float]]] = {

    val tensors = new TensorResources()
    val tensorsMasks = new TensorResources()
    val tensorsSegments = new TensorResources()

    /* Actual size of each sentence to skip padding in the TF model */
    val sequencesLength = batch.map(x => x.length).toArray
    val maxSentenceLength = sequencesLength.max

    val tokenBuffers = DataBuffers.ofInts(batch.length*maxSentenceLength)
    val maskBuffers = DataBuffers.ofInts(batch.length*maxSentenceLength)
    val segmentBuffers = DataBuffers.ofInts(batch.length*maxSentenceLength)

    val shape = Array(batch.length.toLong, maxSentenceLength)

    batch.map { tokenIds =>
      val diff = maxSentenceLength - tokenIds.length
      segmentBuffers.write(Array.fill(maxSentenceLength)(0))

      if (tokenIds.length >= maxSentenceLength) {
        tokenBuffers.write(tokenIds)
        maskBuffers.write(tokenIds.map(x=> if (x == 0) 0 else 1))
      }
      else {
        val newTokenIds = tokenIds ++ Array.fill(1, diff)(0).head
        tokenBuffers.write(newTokenIds)
        maskBuffers.write(newTokenIds.map(x=> if (x == 0) 0 else 1))
      }
    }

    //tokenBuffers.flip()
    //maskBuffers.flip()
    //segmentBuffers.flip()

    val tokenTensors = tensors.createIntBufferTensor(shape, tokenBuffers)
    val maskTensors = tensorsMasks.createIntBufferTensor(shape, maskBuffers)
    val segmentTensors = tensorsSegments.createIntBufferTensor(shape, segmentBuffers)

    val runner = tensorflow.getTFHubSession(configProtoBytes = configProtoBytes).runner

    runner
      .feed(tokenIdsKey, tokenTensors)
      .feed(maskIdsKey, maskTensors)
      .feed(segmentIdsKey, segmentTensors)
      .fetch(outputSequenceKey)

    val outs = runner.run().asScala
    val embeddings = TensorResources.extractFloats(outs.head)

    // TODO restore
    //tensors.clearSession(outs)
    tensors.clearTensors()
    //tokenBuffers.clear()
    //maskBuffers.clear()
    //segmentBuffers.clear()

    val dim = embeddings.length / (batch.length * maxSentenceLength)
    val shrinkedEmbeddings: Array[Array[Array[Float]]] = embeddings.grouped(dim).toArray.grouped(maxSentenceLength).toArray

    val emptyVector = Array.fill(dim)(0f)

    batch.zip(shrinkedEmbeddings).map { case (ids, embeddings) =>
      if (ids.length > embeddings.length) {
        embeddings.take(embeddings.length - 1) ++
          Array.fill(embeddings.length - ids.length)(emptyVector) ++
          Array(embeddings.last)
      } else {
        embeddings
      }
    }

  }

  def calculateEmbeddings(sentences: Seq[TokenizedSentence],
                          batchSize: Int,
                          maxSentenceLength: Int,
                          caseSensitive: Boolean
                         ): Seq[WordpieceEmbeddingsSentence] = {

    sentences.grouped(batchSize).toArray.flatMap { batch =>

      val tokensPiece = tokenize(batch, maxSentenceLength, caseSensitive)
      val tokenIds = tokensPiece.map { sentence =>
        // SentencePiece generates multiple tokenIDs
        // We need to be sure the maxSenetnceLength is respecetd
        val tokens = sentence.flatMap(x => x.tokens.map(x => x.pieceId)).take(maxSentenceLength - 3)
        sentenceStartTokenId ++ tokens ++ sentenceEndTokenId
      }
      val vectors = tag(tokenIds)
      val tokenIdsVectors = tokenIds.zip(vectors).map { x =>
        x._1.zip(x._2).toMap
      }

      tokensPiece.zipWithIndex.zip(tokenIdsVectors).map { case (tokens, vectors) =>

        val tokensWithEmbeddings =  tokens._1.map{ token =>
          /* Remove delimiter 'â–' token if appears alone */
          val subWord:TokenPiece = token.tokens.find(_.pieceId != sentencePieceDelimiterId).getOrElse(token.tokens.head)
          TokenPieceEmbeddings(
            subWord.wordpiece,
            subWord.token,
            subWord.pieceId,
            isWordStart = true,
            isOOV = false,
            vectors.apply(subWord.pieceId),
            subWord.begin,
            subWord.end
          )
        }
        WordpieceEmbeddingsSentence(tokensWithEmbeddings, tokens._2)
      }
    }

  }

  def tokenize(sentences: Seq[TokenizedSentence], maxSeqLength: Int, caseSensitive: Boolean):
  Seq[Array[WordpieceTokenizedSentence]] = {

    val sentecneTokenPieces = sentences.map { s =>
      val shrinkedSentence = s.indexedTokens.take(maxSeqLength - 3)
      shrinkedSentence.map{
        case(token) =>
          val tokenContent = if (caseSensitive) token.token else token.token.toLowerCase()
          val tokenPieces = spp.getSppModel.encodeAsPieces(tokenContent).toArray.map(x=>x.toString)
          val tokenIds = spp.getSppModel.encodeAsIds(tokenContent)
          WordpieceTokenizedSentence(
            tokenPieces.zip(tokenIds).map(x=> TokenPiece(x._1, token.token, x._2, false, token.begin, token.end))
          )
      }
    }
    sentecneTokenPieces
  }

}