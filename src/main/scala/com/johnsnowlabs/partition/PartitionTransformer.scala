/*
 * Copyright 2017-2025 John Snow Labs
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.johnsnowlabs.partition

import com.johnsnowlabs.nlp.AnnotatorType.{CHUNK, DOCUMENT}
import com.johnsnowlabs.nlp.{Annotation, AnnotatorModel, HasSimpleAnnotate}
import com.johnsnowlabs.partition.util.PartitionHelper.{
  datasetWithBinaryFile,
  datasetWithTextFile,
  isStringContent
}
import com.johnsnowlabs.reader.util.HasPdfProperties
import com.johnsnowlabs.reader.{HTMLElement, PdfToText}
import org.apache.spark.ml.PipelineModel
import org.apache.spark.ml.util.Identifiable
import org.apache.spark.sql.functions.{col, explode, udf}
import org.apache.spark.sql.types.{ArrayType, StringType, StructField, StructType}
import org.apache.spark.sql.{DataFrame, Dataset, Encoders, Row}
import org.slf4j.{Logger, LoggerFactory}

import scala.collection.JavaConverters._

/** The PartitionTransformer annotator allows you to use the Partition feature more smoothly
  * within existing Spark NLP workflows, enabling seamless reuse of your pipelines.
  * PartitionTransformer can be used for extracting structured content from various document types
  * using Spark NLP readers. It supports reading from files, URLs, in-memory strings, or byte
  * arrays, and returns parsed output as a structured Spark DataFrame.
  *
  * Supported formats include plain text, HTML, Word (.doc/.docx), Excel (.xls/.xlsx), PowerPoint
  * (.ppt/.pptx), email files (.eml, .msg), and PDFs.
  *
  * ==Example==
  * {{{
  * import com.johnsnowlabs.partition.PartitionTransformer
  * import com. johnsnowlabs. nlp. base. DocumentAssembler
  * import org.apache.spark.ml.Pipeline
  * import spark.implicits._
  * val urls = Seq("https://www.blizzard.com", "https://www.google.com/").toDS.toDF("text")
  *
  * val documentAssembler = new DocumentAssembler()
  *   .setInputCol("text")
  *   .setOutputCol("documents")
  *
  * val partition = new PartitionTransformer()
  *   .setInputCols("document")
  *   .setOutputCol("partition")
  *   .setContentType("url")
  *   .setHeaders(Map("Accept-Language" -> "es-ES"))
  *
  * val pipeline = new Pipeline()
  *   .setStages(Array(documentAssembler, partition))
  *
  * val pipelineModel = pipeline.fit(testDataSet)
  * val resultDf = pipelineModel.transform(testDataSet)
  *
  * resultDf.show()
  * +--------------------+--------------------+--------------------+
  * |                text|            document|           partition|
  * +--------------------+--------------------+--------------------+
  * |https://www.blizz...|[{Title, Juegos d...|[{document, 0, 16...|
  * |https://www.googl...|[{Title, Gmail Im...|[{document, 0, 28...|
  * +--------------------+--------------------+--------------------+
  * }}}
  */

class PartitionTransformer(override val uid: String)
    extends AnnotatorModel[PartitionTransformer]
    with HasSimpleAnnotate[PartitionTransformer]
    with HasReaderProperties
    with HasEmailReaderProperties
    with HasExcelReaderProperties
    with HasHTMLReaderProperties
    with HasPowerPointProperties
    with HasTextReaderProperties
    with HasPdfProperties
    with HasSemanticChunkerProperties {

  def this() = this(Identifiable.randomUID("PartitionTransformer"))
  protected val logger: Logger = LoggerFactory.getLogger(getClass.getName)

  /** Annotator reference id. Used to identify elements in metadata or to refer to this annotator
    * type
    */
  override val inputAnnotatorTypes: Array[AnnotatorType] = Array(DOCUMENT)
  override val outputAnnotatorType: AnnotatorType = DOCUMENT

  override def setInputCols(value: Array[String]): this.type = {
    val validAnnotatorTypes = Array(DOCUMENT, CHUNK)
    require(
      value.length == inputAnnotatorTypes.length,
      s"setInputCols in ${this.uid} expecting ${inputAnnotatorTypes.length} columns. " +
        s"Provided column amount: ${value.length}. " +
        s"Which should be columns from one of the following annotators: ${validAnnotatorTypes.mkString(", ")} ")
    set(inputCols, value)
  }

  /** takes a document and annotations and produces new annotations of this annotator's annotation
    * type
    *
    * @param annotations
    *   Annotations that correspond to inputAnnotationCols generated by previous annotators if any
    * @return
    *   any number of annotations processed for every input annotation. Not necessary one to one
    *   relationship
    */
  override def annotate(annotations: Seq[Annotation]): Seq[Annotation] = {
    annotations
  }

  override def _transform(
      dataset: Dataset[_],
      recursivePipeline: Option[PipelineModel]): DataFrame = {

    if ($(contentType) == "application/pdf") {
      dataset.sparkSession.conf.set("spark.sql.legacy.allowUntypedScalaUDF", "true")
      val pdfToText = new PdfToText()
        .setInputCol($(inputCols).head)
        .setOutputCol(getOutputCol)
        .setPageNumCol($(pageNumCol))
        .setOriginCol($(originCol))
        .setStoreSplittedPdf($(storeSplittedPdf))
      return pdfToText.transform(dataset)
    }

    val params = Map(
      "contentType" -> $(contentType),
      "storeContent" -> $(storeContent).toString,
      "titleFontSize" -> $(titleFontSize).toString,
      "inferTableStructure" -> $(inferTableStructure).toString,
      "includePageBreaks" -> $(includePageBreaks).toString,
      "addAttachmentContent" -> $(addAttachmentContent).toString,
      "cellSeparator" -> $(cellSeparator),
      "appendCells" -> $(appendCells).toString,
      "timeout" -> $(timeout).toString,
      "includeSlideNotes" -> $(includeSlideNotes).toString,
      "titleLengthSize" -> $(titleLengthSize).toString,
      "groupBrokenParagraphs" -> $(groupBrokenParagraphs).toString,
      "paragraphSplit" -> $(paragraphSplit),
      "shortLineWordThreshold" -> $(shortLineWordThreshold).toString,
      "maxLineCount" -> $(maxLineCount).toString,
      "threshold" -> $(threshold).toString,
      "chunkingStrategy" -> $(chunkingStrategy),
      "maxCharacters" -> $(maxCharacters).toString,
      "newAfterNChars" -> $(newAfterNChars).toString,
      "overlap" -> $(overlap).toString)
    val partitionInstance = new Partition(params.asJava)

    val inputColum = if (get(inputCols).isDefined) {
      $(inputCols).head
    } else {
      partitionInstance.getOutputColumn
    }
    partitionInstance.setOutputColumn(inputColum)

    val partitionDf = if (isStringContent($(contentType))) {
      val partitionUDF = udf((text: String) =>
        partitionInstance.partitionStringContent(text, $(this.headers).asJava))
      val schemaFieldOpt = dataset.schema.find(_.name == inputColum)

      schemaFieldOpt match {
        case Some(StructField(_, StringType, _, _)) =>
          val stringContentDF = datasetWithTextFile(dataset.sparkSession, $(contentPath))
          stringContentDF
            .withColumn(inputColum, partitionUDF(col("content")))

        case Some(StructField(_, ArrayType(struct: StructType, _), _, _))
            if struct == Annotation.dataType =>
          val flattenDf =
            dataset.withColumn("flatten_result", explode(col(s"$inputColum.result")))
          flattenDf
            .withColumn(inputColum, partitionUDF(col("flatten_result")))
            .drop("flatten_result")
            .toDF()

        case _ =>
          throw new IllegalArgumentException(
            s"Unsupported column type for '$inputColum'. Must be String or Annotation.")
      }

    } else {
      val binaryContentDF = datasetWithBinaryFile(dataset.sparkSession, $(contentPath))
      val partitionUDF =
        udf((input: Array[Byte]) => partitionInstance.partitionBytesContent(input))
      binaryContentDF.withColumn(inputColum, partitionUDF(col("content")))
    }

    val htmlElementColumns = findHTMLElementColumns(partitionDf)

    if (htmlElementColumns.isEmpty) {
      val schemaString = partitionDf.schema.treeString
      throw new IllegalArgumentException(
        s"""âŒ No column of type Array[HTMLElement] was found in the DataFrame.
           |
           |ðŸ’¡ Expected one or more columns with schema matching: Array[HTMLElement]
           |
           |ðŸ§ª DataFrame Schema:
           |$schemaString
           |
           |ðŸ‘‰ Make sure at least one column is an Array of structs with fields:
           |  - elementType: String
           |  - content: String
           |  - metadata: Map[String, String]
     """.stripMargin)
    }

    // Transform each matching column
    val transformedDf = htmlElementColumns.foldLeft(partitionDf) { (df, colName) =>
      df.withColumn(getOutputCol, wrapColumnMetadata(convertToAnnotations(col(colName))))
    }
    transformedDf
  }

  private def convertToAnnotations = udf { elements: Seq[Row] =>
    elements.map { row =>
      val content = row.getAs[String]("content")
      val metadata = row.getAs[Map[String, String]]("metadata")

      val begin = 0
      val end = if (content != null) content.length - 1 else 0

      Annotation(
        annotatorType = DOCUMENT,
        begin = begin,
        end = end,
        result = content,
        metadata = metadata,
        embeddings = Array.emptyFloatArray)
    }
  }

  private def findHTMLElementColumn(dataFrame: DataFrame): Option[String] = {
    val htmlElementSchema = Encoders.product[HTMLElement].schema
    dataFrame.schema.fields
      .find { field =>
        field.dataType match {
          case ArrayType(structType: StructType, _) =>
            structType == htmlElementSchema
          case _ => false
        }
      }
      .map(_.name)
  }

  private def findHTMLElementColumns(dataFrame: DataFrame): Seq[String] = {
    val htmlElementSchema = Encoders.product[HTMLElement].schema

    dataFrame.schema.fields.collect {
      case StructField(name, ArrayType(structType: StructType, _), _, _)
          if structType == htmlElementSchema =>
        name
    }
  }

}
