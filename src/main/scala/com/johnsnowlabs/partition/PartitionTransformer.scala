/*
 * Copyright 2017-2025 John Snow Labs
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.johnsnowlabs.partition

import com.johnsnowlabs.nlp.AnnotatorType.{CHUNK, DOCUMENT}
import com.johnsnowlabs.nlp.{Annotation, AnnotatorModel, HasSimpleAnnotate}
import com.johnsnowlabs.partition.util.PartitionHelper.{datasetWithBinaryFile, isStringContent}
import com.johnsnowlabs.reader.util.HasPdfProperties
import com.johnsnowlabs.reader.{HTMLElement, PdfToText}
import org.apache.spark.ml.PipelineModel
import org.apache.spark.ml.util.Identifiable
import org.apache.spark.sql.functions.{col, explode, udf}
import org.apache.spark.sql.types.{ArrayType, StructType}
import org.apache.spark.sql.{DataFrame, Dataset, Encoders, Row}
import org.slf4j.{Logger, LoggerFactory}

import scala.collection.JavaConverters._

class PartitionTransformer(override val uid: String)
    extends AnnotatorModel[PartitionTransformer]
    with HasSimpleAnnotate[PartitionTransformer]
    with HasReaderProperties
    with HasEmailReaderProperties
    with HasExcelReaderProperties
    with HasHTMLReaderProperties
    with HasPowerPointProperties
    with HasTextReaderProperties
    with HasPdfProperties {

  def this() = this(Identifiable.randomUID("PartitionTransformer"))
  protected val logger: Logger = LoggerFactory.getLogger(getClass.getName)

  /** Annotator reference id. Used to identify elements in metadata or to refer to this annotator
    * type
    */
  override val inputAnnotatorTypes: Array[AnnotatorType] = Array(DOCUMENT)
  override val outputAnnotatorType: AnnotatorType = DOCUMENT

  override def setInputCols(value: Array[String]): this.type = {
    val validAnnotatorTypes = Array(DOCUMENT, CHUNK)
    require(
      value.length == inputAnnotatorTypes.length,
      s"setInputCols in ${this.uid} expecting ${inputAnnotatorTypes.length} columns. " +
        s"Provided column amount: ${value.length}. " +
        s"Which should be columns from one of the following annotators: ${validAnnotatorTypes.mkString(", ")} ")
    set(inputCols, value)
  }

  /** takes a document and annotations and produces new annotations of this annotator's annotation
    * type
    *
    * @param annotations
    *   Annotations that correspond to inputAnnotationCols generated by previous annotators if any
    * @return
    *   any number of annotations processed for every input annotation. Not necessary one to one
    *   relationship
    */
  override def annotate(annotations: Seq[Annotation]): Seq[Annotation] = {
    annotations
  }

  override def _transform(
      dataset: Dataset[_],
      recursivePipeline: Option[PipelineModel]): DataFrame = {

    if ($(contentType) == "application/pdf") {
      dataset.sparkSession.conf.set("spark.sql.legacy.allowUntypedScalaUDF", "true")
      val pdfToText = new PdfToText()
        .setInputCol($(inputCols).head)
        .setOutputCol(getOutputCol)
        .setPageNumCol($(pageNumCol))
        .setOriginCol($(originCol))
        .setStoreSplittedPdf($(storeSplittedPdf))
      return pdfToText.transform(dataset)
    }

    val params = Map(
      "contentType" -> $(contentType),
      "storeContent" -> $(storeContent).toString,
      "titleFontSize" -> $(titleFontSize).toString,
      "inferTableStructure" -> $(inferTableStructure).toString,
      "includePageBreaks" -> $(includePageBreaks).toString,
      "addAttachmentContent" -> $(addAttachmentContent).toString,
      "cellSeparator" -> $(cellSeparator),
      "appendCells" -> $(appendCells).toString,
      "timeout" -> $(timeout).toString,
      "includeSlideNotes" -> $(includeSlideNotes).toString,
      "titleLengthSize" -> $(titleLengthSize).toString,
      "groupBrokenParagraphs" -> $(groupBrokenParagraphs).toString,
      "paragraphSplit" -> $(paragraphSplit),
      "shortLineWordThreshold" -> $(shortLineWordThreshold).toString,
      "maxLineCount" -> $(maxLineCount).toString,
      "threshold" -> $(threshold).toString)
    val partitionInstance = new Partition(params.asJava)
    partitionInstance.setOutputColumn($(inputCols).head)

    val inputColum = if (get(inputCols).isDefined) {
      $(inputCols).head
    } else {
      partitionInstance.getOutputColumn
    }
    partitionInstance.setOutputColumn($(inputCols).head)

    val partitionDf = if (isStringContent($(contentType))) {
      val flattenDf = dataset.withColumn("flatten_result", explode(col(s"$inputColum.result")))
      val partitionUDF = udf((text: String) =>
        partitionInstance.partitionStringContent(text, $(this.headers).asJava))
      flattenDf.withColumn(inputColum, partitionUDF(col("flatten_result"))).drop("flatten_result")
    } else {
      val binaryContentDF = datasetWithBinaryFile(dataset.sparkSession, $(contentPath))
      val partitionUDF =
        udf((input: Array[Byte]) => partitionInstance.partitionBytesContent(input))
      binaryContentDF.withColumn(inputColum, partitionUDF(col("content")))
    }

    val colName = findHTMLElementColumn(partitionDf).getOrElse {
      val schemaString = partitionDf.schema.treeString
      throw new IllegalArgumentException(
        s"""❌ No column of type Array[HTMLElement] was found in the DataFrame.
           |
           |💡 Expected a column with schema matching: Array[HTMLElement]
           |
           |🧪 DataFrame Schema:
           |$schemaString
           |
           |👉 Make sure at least one column is an Array of structs with fields:
           |  - elementType: String
           |  - content: String
           |  - metadata: Map[String, String]
     """.stripMargin)
    }
    partitionDf.withColumn(getOutputCol, wrapColumnMetadata(convertToAnnotations(col(colName))))
  }

  private def convertToAnnotations = udf { elements: Seq[Row] =>
    elements.map { row =>
      val content = row.getAs[String]("content")
      val metadata = row.getAs[Map[String, String]]("metadata")

      val begin = 0
      val end = if (content != null) content.length - 1 else 0

      Annotation(
        annotatorType = DOCUMENT,
        begin = begin,
        end = end,
        result = content,
        metadata = metadata,
        embeddings = Array.emptyFloatArray)
    }
  }

  private def findHTMLElementColumn(dataFrame: DataFrame): Option[String] = {
    val htmlElementSchema = Encoders.product[HTMLElement].schema
    dataFrame.schema.fields
      .find { field =>
        field.dataType match {
          case ArrayType(structType: StructType, _) =>
            structType == htmlElementSchema
          case _ => false
        }
      }
      .map(_.name)
  }

}
