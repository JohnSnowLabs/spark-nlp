package com.johnsnowlabs.nlp.embeddings

import com.johnsnowlabs.ml.pytorch.{PytorchBert, PytorchWrapper, ReadPytorchModel, WritePytorchModel}
import com.johnsnowlabs.nlp.annotators.common.{IndexedToken, Sentence, TokenizedSentence, TokenizedWithSentence, WordpieceEmbeddingsSentence, WordpieceTokenizedSentence}
import com.johnsnowlabs.nlp.annotators.tokenizer.wordpiece.{BasicTokenizer, WordpieceEncoder}
import com.johnsnowlabs.nlp.serialization.MapFeature
import com.johnsnowlabs.nlp.util.io.{ExternalResource, ReadAs, ResourceHelper}
import com.johnsnowlabs.nlp.{Annotation, AnnotatorModel, AnnotatorType, HasBatchedAnnotate, HasCaseSensitiveProperties, HasPretrained, ParamsAndFeaturesReadable}
import com.johnsnowlabs.storage.HasStorageRef
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.ml.param.IntParam
import org.apache.spark.ml.util.Identifiable
import org.apache.spark.sql.{DataFrame, SparkSession}

import java.io.File

class BertEmbeddingsPytorch(override val uid: String) extends AnnotatorModel[BertEmbeddingsPytorch]
  with HasBatchedAnnotate[BertEmbeddingsPytorch]
  with WritePytorchModel
  with HasEmbeddingsProperties
  with HasStorageRef
  with HasCaseSensitiveProperties
{

  def this() = this(Identifiable.randomUID("BERT_EMBEDDINGS_PT"))
  /** Annotator reference id. Used to identify elements in metadata or to refer to this annotator type */
  override val inputAnnotatorTypes: Array[String] = Array(AnnotatorType.DOCUMENT, AnnotatorType.TOKEN)
  override val outputAnnotatorType: AnnotatorType = AnnotatorType.WORD_EMBEDDINGS

  /** @group setParam */
  def sentenceStartTokenId: Int = {
    $$(vocabulary)("[CLS]")
  }

  /** @group setParam */
  def sentenceEndTokenId: Int = {
    $$(vocabulary)("[SEP]")
  }

  /**
   * Vocabulary used to encode the words to ids with WordPieceEncoder
   *
   * @group param
   * */
  val vocabulary: MapFeature[String, Int] = new MapFeature(this, "vocabulary")

  /** @group setParam */
  def setVocabulary(value: Map[String, Int]): this.type = set(vocabulary, value)

  /** Max sentence length to process (Default: `128`)
   *
   * @group param
   * */
  val maxSentenceLength = new IntParam(this, "maxSentenceLength", "Max sentence length to process")

  /** @group setParam */
  def setMaxSentenceLength(value: Int): this.type = {
    require(value <= 512, "BERT models do not support sequences longer than 512 because of trainable positional embeddings.")
    require(value >= 1, "The maxSentenceLength must be at least 1")
    set(maxSentenceLength, value)
    this
  }

  private var pytorchModel: Option[Broadcast[PytorchBert]] = None

  def setPytorchModelIfNotSet(spark: SparkSession, pytorchWrapper: PytorchWrapper): BertEmbeddingsPytorch = {
    if (pytorchModel.isEmpty) {
      pytorchModel = Some(spark.sparkContext.broadcast(
        new PytorchBert(pytorchWrapper, sentenceStartTokenId, sentenceEndTokenId))
      )
    }
    this
  }

  /** @group getParam */
  def getPytorchModelIfNotSet: PytorchBert = pytorchModel.get.value

  /** Set Embeddings dimensions for the BERT model
   * Only possible to set this when the first time is saved
   * dimension is not changeable, it comes from BERT config file
   *
   * @group setParam
   * */
  override def setDimension(value: Int): this.type = {
    if (get(dimension).isEmpty)
      set(this.dimension, value)
    this
  }

  /** Whether to lowercase tokens or not
   *
   * @group setParam
   * */
  override def setCaseSensitive(value: Boolean): this.type = {
    if (get(caseSensitive).isEmpty)
      set(this.caseSensitive, value)
    this
  }

  setDefault(
    dimension -> 768,
    batchSize -> 8,
    maxSentenceLength -> 128,
    caseSensitive -> false
  )

  /**
   * takes a document and annotations and produces new annotations of this annotator's annotation type
   *
   * @param batchedAnnotations Annotations in batches that correspond to inputAnnotationCols generated by previous annotators if any
   * @return any number of annotations processed for every batch of input annotations. Not necessary one to one relationship
   *
   *         IMPORTANT: !MUST! return sequences of equal lengths !!
   *         IMPORTANT: !MUST! return sentences that belong to the same original row !! (challenging)
   */
  override def batchAnnotate(batchedAnnotations: Seq[Array[Annotation]]): Seq[Seq[Annotation]] = {
    val batchedTokenizedSentences: Array[Array[TokenizedSentence]] = batchedAnnotations.map(annotations =>
      TokenizedWithSentence.unpack(annotations).toArray
    ).toArray

    /*Return empty if the real tokens are empty*/
    if (batchedTokenizedSentences.nonEmpty) batchedTokenizedSentences.map(tokenizedSentences => {
      val tokenized = tokenizeWithAlignment(tokenizedSentences)
      val withEmbeddings = getPytorchModelIfNotSet.calculateEmbeddings(tokenized, tokenizedSentences, $(batchSize),
        $(maxSentenceLength), $(caseSensitive))
      WordpieceEmbeddingsSentence.pack(withEmbeddings)
    }) else {
      Seq(Seq.empty[Annotation])
    }
  }

  def tokenizeWithAlignment(tokens: Seq[TokenizedSentence]): Seq[WordpieceTokenizedSentence] = {
    val basicTokenizer = new BasicTokenizer($(caseSensitive))
    val encoder = new WordpieceEncoder($$(vocabulary))

    tokens.map { tokenIndex =>
      // filter empty and only whitespace tokens
      val bertTokens = tokenIndex.indexedTokens.filter(x => x.token.nonEmpty && !x.token.equals(" ")).map { token =>
        val content = if ($(caseSensitive)) token.token else token.token.toLowerCase()
        val sentenceBegin = token.begin
        val sentenceEnd = token.end
        val sentenceInedx = tokenIndex.sentenceIndex
        val result = basicTokenizer.tokenize(Sentence(content, sentenceBegin, sentenceEnd, sentenceInedx))
        if (result.nonEmpty) result.head else IndexedToken("")
      }
      val wordpieceTokens = bertTokens.flatMap(token => encoder.encode(token)).take($(maxSentenceLength))
      WordpieceTokenizedSentence(wordpieceTokens)
    }
  }

  override protected def afterAnnotate(dataset: DataFrame): DataFrame = {
    dataset.withColumn(getOutputCol, wrapEmbeddingsMetadata(dataset.col(getOutputCol), $(dimension),
      Some($(storageRef))))
  }

  override def onWrite(path: String, spark: SparkSession): Unit = {
    super.onWrite(path, spark)
    writePytorchModel(path, spark, getPytorchModelIfNotSet.pytorchWrapper, BertEmbeddingsPytorch.torchscriptFile)
  }

}

trait ReadablePretrainedBertPyModel extends ParamsAndFeaturesReadable[BertEmbeddingsPytorch]
  with HasPretrained[BertEmbeddingsPytorch] {
  override val defaultModelName: Some[String] = Some("bert_base_cased_pt")

  /** Java compliant-overrides */
  override def pretrained(): BertEmbeddingsPytorch = super.pretrained()

  override def pretrained(name: String): BertEmbeddingsPytorch = super.pretrained(name)

  override def pretrained(name: String, lang: String): BertEmbeddingsPytorch = super.pretrained(name, lang)

  override def pretrained(name: String, lang: String, remoteLoc: String): BertEmbeddingsPytorch = super.pretrained(name, lang, remoteLoc)
}

trait ReadBertPytorchModel extends ReadPytorchModel with ParamsAndFeaturesReadable[BertEmbeddingsPytorch] {

  val torchscriptFile: String = "bert_pytorch"

  def readPytorch(instance: BertEmbeddingsPytorch, path: String, spark: SparkSession): Unit = {
    val pytorchWrapper = readPytorchModel(s"$path/$torchscriptFile", spark, "_bert")
    instance.setPytorchModelIfNotSet(spark, pytorchWrapper)
  }

  addReader(readPytorch)

  def loadTorchScriptModel(torchModelPath: String, spark: SparkSession): BertEmbeddingsPytorch = {
    val vocab = new File(torchModelPath, "vocab.txt")
    require(vocab.exists(), s"Vocabulary file vocab.txt not found in folder $torchModelPath")
    val vocabResource = new ExternalResource(vocab.getAbsolutePath, ReadAs.TEXT, Map("format" -> "text"))
    val words = ResourceHelper.parseLines(vocabResource).zipWithIndex.toMap

    val pytorchWrapper = PytorchWrapper(torchModelPath)

    new BertEmbeddingsPytorch()
      .setVocabulary(words)
      .setPytorchModelIfNotSet(spark, pytorchWrapper)
  }

}

object BertEmbeddingsPytorch extends ReadablePretrainedBertPyModel with ReadBertPytorchModel