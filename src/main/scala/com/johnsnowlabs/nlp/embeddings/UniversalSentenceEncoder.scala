package com.johnsnowlabs.nlp.embeddings

import java.io.File

import com.johnsnowlabs.ml.tensorflow.{ReadTensorflowModel, TensorflowUSE, TensorflowWrapper, WriteTensorflowModel}
import com.johnsnowlabs.nlp.AnnotatorType.{DOCUMENT, SENTENCE_EMBEDDINGS}
import com.johnsnowlabs.nlp.annotators.common.SentenceSplit
import com.johnsnowlabs.nlp.{Annotation, AnnotatorModel, HasPretrained, ParamsAndFeaturesReadable}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.ml.param.IntArrayParam
import org.apache.spark.ml.util.Identifiable
import org.apache.spark.sql.SparkSession

class UniversalSentenceEncoder(override val uid: String)
  extends AnnotatorModel[UniversalSentenceEncoder]
    with WriteTensorflowModel {

  /** Annotator reference id. Used to identify elements in metadata or to refer to this annotator type */
  def this() = this(Identifiable.randomUID("UNIVERSAL_SENTENCE_ENCODER"))

  override val outputAnnotatorType: AnnotatorType = SENTENCE_EMBEDDINGS

  override val inputAnnotatorTypes: Array[AnnotatorType] = Array(DOCUMENT)

  val configProtoBytes = new IntArrayParam(
    this,
    "configProtoBytes",
    "ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()"
  )

  def setConfigProtoBytes(
                           bytes: Array[Int]
                         ): UniversalSentenceEncoder.this.type = set(this.configProtoBytes, bytes)

  def getConfigProtoBytes: Option[Array[Byte]] =
    get(this.configProtoBytes).map(_.map(_.toByte))

  private var tfHubPath: String = ""
  def setTFhubPath(value: String): Unit = {
    tfHubPath = value
  }
  def getTFhubPath: String = tfHubPath

  private var _model: Option[Broadcast[TensorflowUSE]] = None

  def getModelIfNotSet: TensorflowUSE = _model.get.value

  def setModelIfNotSet(spark: SparkSession,
                       tensorflow: TensorflowWrapper): this.type = {
    if (_model.isEmpty) {

      _model = Some(
        spark.sparkContext.broadcast(
          new TensorflowUSE(tensorflow, configProtoBytes = getConfigProtoBytes)
        )
      )
    }
    this
  }

  /**
    * takes a document and annotations and produces new annotations of this annotator's annotation type
    *
    * @param annotations Annotations that correspond to inputAnnotationCols generated by previous annotators if any
    * @return any number of annotations processed for every input annotation. Not necessary one to one relationship
    */
  override def annotate(annotations: Seq[Annotation]): Seq[Annotation] = {
    val sentences = SentenceSplit.unpack(annotations)
    val nonEmptySentences = sentences.filter(_.content.nonEmpty)

    if(nonEmptySentences.nonEmpty)
      getModelIfNotSet.calculateEmbeddings(nonEmptySentences)
    else Seq.empty[Annotation]
  }

  override def onWrite(path: String, spark: SparkSession): Unit = {
    super.onWrite(path, spark)
    //    writeTensorflowHub(path, tfPath = getTFhubPath, spark)
    writeTensorflowModel(path, spark, getModelIfNotSet.tensorflow, "_use", UniversalSentenceEncoder.tfFile, configProtoBytes = getConfigProtoBytes)
  }

}

trait ReadablePretrainedUSEModel
  extends ParamsAndFeaturesReadable[UniversalSentenceEncoder]
    with HasPretrained[UniversalSentenceEncoder] {
  override val defaultModelName: Some[String] = Some("tfhub_use")

  /** Java compliant-overrides */
  override def pretrained(): UniversalSentenceEncoder = super.pretrained()
  override def pretrained(name: String): UniversalSentenceEncoder = super.pretrained(name)
  override def pretrained(name: String, lang: String): UniversalSentenceEncoder = super.pretrained(name, lang)
  override def pretrained(name: String, lang: String, remoteLoc: String): UniversalSentenceEncoder = super.pretrained(name, lang, remoteLoc)
}

trait ReadUSETensorflowModel extends ReadTensorflowModel {
  this: ParamsAndFeaturesReadable[UniversalSentenceEncoder] =>

  /*Needs to point to an actual folder rather than a .pb file*/
  override val tfFile: String = "use_tensorflow"

  def readTensorflow(instance: UniversalSentenceEncoder, path: String, spark: SparkSession): Unit = {

    val tf = readTensorflowModel(path, spark, "_use_tf", initAllTables = true)
    instance.setModelIfNotSet(spark, tf)
  }

  addReader(readTensorflow)

  def loadSavedModel(folder: String,
                     spark: SparkSession): UniversalSentenceEncoder = {
    val f = new File(folder)
    val savedModel = new File(folder, "saved_model.pb")

    require(f.exists, s"Folder $folder not found")
    require(f.isDirectory, s"File $folder is not folder")
    require(
      savedModel.exists(),
      s"savedModel file saved_model.pb not found in folder $folder"
    )

    val wrapper = TensorflowWrapper.read(folder, zipped = false, useBundle = true, tags = Array("serve"), initAllTables = true)

    val USE = new UniversalSentenceEncoder()
      .setModelIfNotSet(spark, wrapper)

    USE.setTFhubPath(folder)

    USE
  }
}

object UniversalSentenceEncoder extends ReadablePretrainedUSEModel with ReadUSETensorflowModel
