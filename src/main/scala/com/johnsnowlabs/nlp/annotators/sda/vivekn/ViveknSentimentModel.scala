package com.johnsnowlabs.nlp.annotators.sda.vivekn

import com.johnsnowlabs.nlp.annotators.common.{Tokenized, TokenizedSentence}
import com.johnsnowlabs.nlp.serialization.SerializationHelper
import com.johnsnowlabs.nlp.{Annotation, AnnotatorModel}
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.ml.param.IntParam
import org.apache.spark.ml.util.{DefaultParamsReadable, Identifiable, MLReader, MLWriter}

class ViveknSentimentModel(override val uid: String) extends AnnotatorModel[ViveknSentimentModel] {

  import com.johnsnowlabs.nlp.AnnotatorType._

  private val config: Config = ConfigFactory.load
  private val importantFeatureRatio = config.getDouble("nlp.viveknSentiment.importantFeaturesRatio")
  private val unimportantFeatureStep = config.getDouble("nlp.viveknSentiment.unimportantFeaturesStepRatio")
  private val featureLimit = config.getInt("nlp.viveknSentiment.featuresLimit")

  override val annotatorType: AnnotatorType = SENTIMENT

  override val requiredAnnotatorTypes: Array[AnnotatorType] = Array(TOKEN, DOCUMENT)

  protected var positive = Map[String, Int]()
  protected var negative = Map[String, Int]()
  protected var features = Array[String]()

  protected val positiveTotals: IntParam = new IntParam(this, "positive_totals", "count of positive words")
  protected val negativeTotals: IntParam = new IntParam(this, "negative_totals", "count of negative words")

  def this() = this(Identifiable.randomUID("VIVEKN"))

  private[vivekn] def setPositive(value: Map[String, Int]) = {positive = value; this}
  private[vivekn] def setNegative(value: Map[String, Int]) = {negative = value; this}
  private[vivekn] def setPositiveTotals(value: Int) = set(positiveTotals, value)
  private[vivekn] def setNegativeTotals(value: Int) = set(negativeTotals, value)
  private[vivekn] def setWords(value: Array[String]) = {
    require(value.nonEmpty, "Word analysis for features cannot be empty. Set prune to false if training is small")
    val currentFeatures = scala.collection.mutable.Set.empty[String]
    val start = (value.length * importantFeatureRatio).ceil.toInt
    val afterStart = {
      if (featureLimit == -1) value.length
      else featureLimit
    }
    val step = (afterStart * unimportantFeatureStep).ceil.toInt
    value.take(start).foreach(currentFeatures.add)
    Range(start, afterStart, step).foreach(k => {
      value.slice(k, k+step).foreach(currentFeatures.add)
    })

    features = currentFeatures.toArray
    this
  }

  def classify(sentence: TokenizedSentence): Boolean = {
    val words = ViveknSentimentApproach.negateSequence(sentence.tokens.toList).intersect(features).distinct
    if (words.isEmpty) return true
    val positiveProbability = words.map(word => scala.math.log((positive.getOrElse(word, 0) + 1.0) / (2.0 * $(positiveTotals)))).sum
    val negativeProbability = words.map(word => scala.math.log((negative.getOrElse(word, 0) + 1.0) / (2.0 * $(negativeTotals)))).sum
    positiveProbability > negativeProbability
  }

  /**
    * Tokens are needed to identify each word in a sentence boundary
    * POS tags are optionally submitted to the model in case they are needed
    * Lemmas are another optional annotator for some models
    * Bounds of sentiment are hardcoded to 0 as they render useless
    * @param annotations Annotations that correspond to inputAnnotationCols generated by previous annotators if any
    * @return any number of annotations processed for every input annotation. Not necessary one to one relationship
    */
  override def annotate(annotations: Seq[Annotation]): Seq[Annotation] = {
    val sentences = Tokenized.unpack(annotations)

    sentences.filter(s => s.indexedTokens.nonEmpty).map(sentence => {
      Annotation(
        annotatorType,
        sentence.indexedTokens.map(t => t.begin).min,
        sentence.indexedTokens.map(t => t.end).max,
        if (classify(sentence)) "positive" else "negative",
        Map.empty[String, String]
      )
    })
  }

  override def write: MLWriter = new ViveknSentimentModel.Writer(this, super.write)
}

object ViveknSentimentModel extends DefaultParamsReadable[ViveknSentimentModel] {
  override def read = new Reader(super.read)

  private val positiveKey = "positive"
  private val negativeKey = "negative"
  private val featuresKey = "features"

  class Reader(baseReader: MLReader[ViveknSentimentModel]) extends MLReader[ViveknSentimentModel] {

    override def load(path: String): ViveknSentimentModel = {
      val helper = SerializationHelper(sparkSession, path)
      val instance = baseReader.load(path)

      val positive = helper.deserializeMap[String, Int](positiveKey)
      val negative = helper.deserializeMap[String, Int](negativeKey)
      val features = helper.deserializeArray[String](featuresKey)

      instance.features = features
      instance
        .setNegative(negative)
        .setPositive(positive)
    }
  }

  class Writer(model: ViveknSentimentModel, baseWriter: MLWriter) extends MLWriter {

    override protected def saveImpl(path: String): Unit = {
      baseWriter.save(path)
      val helper = SerializationHelper(sparkSession, path)

      helper.serializeMap[String, Int](positiveKey, model.positive)
      helper.serializeMap[String, Int](negativeKey, model.negative)
      helper.serializeArray[String](featuresKey, model.features)
    }
  }
}