package com.johnsnowlabs.nlp.annotators.sda.pragmatic

import com.johnsnowlabs.nlp.annotators.common.TokenizedSentence
import com.johnsnowlabs.nlp.util.io.ResourceHelper
import com.johnsnowlabs.nlp.{Annotation, AnnotatorModel}
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.ml.param.Param
import org.apache.spark.ml.util.{DefaultParamsReadable, Identifiable}

/**
  * Created by saif on 12/06/2017.
  */

/**
  * Gives a good or bad score to a sentence based on the approach used
  * @param uid internal uid needed for saving annotator to disk
  * @@ model: Implementation to be applied for sentiment analysis
  */
class SentimentDetectorModel(override val uid: String) extends AnnotatorModel[SentimentDetectorModel] {

  import com.johnsnowlabs.nlp.AnnotatorType._

  val dictPath = new Param[String](this, "dictPath", "path to dictionary for pragmatic sentiment analysis")

  lazy val model: PragmaticScorer = {
    if (get(dictPath).isDefined)
      new PragmaticScorer(SentimentDetectorModel.retrieveSentimentDict($(dictPath)))
    else
      new PragmaticScorer()
  }

  override val annotatorType: AnnotatorType = SENTIMENT

  override val requiredAnnotatorTypes: Array[AnnotatorType] = Array(TOKEN, DOCUMENT)

  def this() = this(Identifiable.randomUID("SENTIMENT"))

  def setDictPath(path: String): this.type = {
    set(dictPath, path)
  }

  def getDictPath: String = {
    $(dictPath)
  }

  /**
    * Tokens are needed to identify each word in a sentence boundary
    * POS tags are optionally submitted to the model in case they are needed
    * Lemmas are another optional annotator for some models
    * Bounds of sentiment are hardcoded to 0 as they render useless
    * @param annotations Annotations that correspond to inputAnnotationCols generated by previous annotators if any
    * @return any number of annotations processed for every input annotation. Not necessary one to one relationship
    */
  override def annotate(annotations: Seq[Annotation]): Seq[Annotation] = {
    val tokens = annotations.filter(_.annotatorType == TOKEN)
    val sentences = annotations.filter(_.annotatorType == DOCUMENT)
    val taggedSentences = sentences.map(sentence => {
      val sentenceTokens = tokens
        .filter(token => token.metadata(Annotation.BEGIN).toInt >= sentence.metadata(Annotation.BEGIN).toInt &&
          token.metadata(Annotation.END).toInt <= sentence.metadata(Annotation.END).toInt)
        .map(_.result).toArray
      TokenizedSentence(sentenceTokens)
    }).toArray
    val score = model.score(taggedSentences)
    Seq(Annotation(
      annotatorType,
      { if (score >= 0) "positive" else "negative"},
      Map.empty[String, String]
    ))
  }

}
object SentimentDetectorModel extends DefaultParamsReadable[SentimentDetectorModel] {

  private val config: Config = ConfigFactory.load

  /**
    * Sentiment dictionaries from compiled sources set in configuration
    * @return Sentiment dictionary
    */
  private[pragmatic] def retrieveSentimentDict(sentFilePath: String = "__default"): Map[String, String] = {
    val filePath = if (sentFilePath == "__default") config.getString("nlp.sentimentDict.file") else sentFilePath
    val sentFormat = config.getString("nlp.sentimentDict.format")
    val sentSeparator = config.getString("nlp.sentimentDict.separator")
    ResourceHelper.parseKeyValueText(filePath, sentFormat, sentSeparator)
  }


}