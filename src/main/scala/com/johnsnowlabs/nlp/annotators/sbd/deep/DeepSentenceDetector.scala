package com.johnsnowlabs.nlp.annotators.sbd.deep

import com.johnsnowlabs.nlp.AnnotatorType.{CHUNK, DOCUMENT, TOKEN}
import com.johnsnowlabs.nlp.annotator.SentenceDetector
import com.johnsnowlabs.nlp.annotators.common.{Sentence, SentenceSplit}
import com.johnsnowlabs.nlp.annotators.sbd.SentenceDetectorParams
import com.johnsnowlabs.nlp.{Annotation, AnnotatorModel}
import org.apache.spark.ml.PipelineModel
import org.apache.spark.ml.param.{BooleanParam, StringArrayParam}
import org.apache.spark.ml.util.{DefaultParamsReadable, Identifiable}
import org.apache.spark.sql.DataFrame

class DeepSentenceDetector(override val uid: String) extends AnnotatorModel[DeepSentenceDetector] with SentenceDetectorParams {

  def this() = this(Identifiable.randomUID("DEEP SENTENCE DETECTOR"))

  /** Annotator reference id. Used to identify elements in metadata or to refer to this annotator type */
  override val inputAnnotatorTypes: Array[AnnotatorType] = Array(DOCUMENT, TOKEN, CHUNK)
  override val outputAnnotatorType: AnnotatorType = DOCUMENT

  val includesPragmaticSegmenter = new BooleanParam(this, "includesPragmaticSegmenter",
    "Whether to include rule-based sentence detector as first filter")

  val endPunctuation = new StringArrayParam(this, "endPunctuation",
    "An array of symbols that deep sentence detector will consider as an end of sentence punctuation")

  def setIncludePragmaticSegmenter(value: Boolean): this.type = set(includesPragmaticSegmenter, value)

  def setEndPunctuation(value: Array[String]): this.type = set(endPunctuation, value)

  setDefault(
    includesPragmaticSegmenter -> false,
    endPunctuation -> Array(".", "!", "?")
  )

  /**
    * takes a document and annotations and produces new annotations of this annotator's annotation type
    *
    * @param annotations Annotations that correspond to inputAnnotationCols generated by previous annotators if any
    * @return any number of annotations processed for every input annotation. Not necessary one to one relationship
    */
  override def annotate(annotations: Seq[Annotation]): Seq[Annotation] = {

    if ($(includesPragmaticSegmenter)) {

      val document = getDocument(annotations)
      val pragmaticSentenceDetector = new SentenceDetector()
        .setUseAbbreviations($(useAbbrevations))
        .setUseCustomBoundsOnly($(useCustomBoundsOnly))
        .setCustomBounds($(customBounds))
      if (get(splitLength).isDefined) pragmaticSentenceDetector.setSplitLength($(splitLength))
      val pragmaticSegmentedSentences = pragmaticSentenceDetector.annotate(document)
      val unpunctuatedSentences = getUnpunctuatedSentences(pragmaticSegmentedSentences)

      if (unpunctuatedSentences.isEmpty) {
        pragmaticSegmentedSentences
      } else {
        getDeepSegmentedSentences(annotations, document.head.result, unpunctuatedSentences, pragmaticSegmentedSentences)
      }

    } else {
      deepSentenceDetector(annotations)
    }

  }

  override protected def afterAnnotate(dataset: DataFrame): DataFrame = {
    import org.apache.spark.sql.functions.{col, explode, array}
    if ($(explodeSentences)) {
      dataset
        .select(dataset.columns.filterNot(_ == getOutputCol).map(col) :+ explode(col(getOutputCol)).as("_tmp"):_*)
        .withColumn(getOutputCol, array(col("_tmp"))
          .as(getOutputCol, dataset.schema.fields.find(_.name == getOutputCol).get.metadata))
        .drop("_tmp")
    }
    else dataset
  }

  def getDocument(annotations: Seq[Annotation]): Seq[Annotation] = {
    val document = annotations.find(annotation => annotation.annotatorType == DOCUMENT) match {
      case Some(d) => Seq(d)
      case None => Seq.empty[Annotation]
    }
    if (document.isEmpty) {
      throw new Exception("Document is empty, please check.")
    } else {
      document
    }
  }

  def getDeepSegmentedSentences(annotations: Seq[Annotation], originalText: String,
                                unpunctuatedSentences: Seq[Annotation], pragmaticSegmentedSentences: Seq[Annotation] ):
  Seq[Annotation] = {

    val validNerEntities = retrieveValidNerEntities(annotations, unpunctuatedSentences)
    if (validNerEntities.nonEmpty && unpunctuatedSentences.length == validNerEntities.length ){
      val deepSegmentedSentences = deepSentenceDetector(originalText, unpunctuatedSentences, validNerEntities)
      val mergedSegmentedSentences = mergeSentenceDetectors(pragmaticSegmentedSentences, deepSegmentedSentences)
      mergedSegmentedSentences
    } else {
      pragmaticSegmentedSentences
    }
  }

  def deepSentenceDetector(annotations: Seq[Annotation]): Seq[Annotation] = {
    val nerEntities = getNerEntities(annotations)
    val sentence = retrieveSentence(annotations)
    segmentSentence(nerEntities, sentence)
  }

  def getNerEntities(annotations: Seq[Annotation]): Seq[Annotation] = {
    val nerEntities = annotations.filter(annotation => annotation.annotatorType == CHUNK)
    if (nerEntities.isEmpty) {
      return getDocument(annotations)
    }
    val tokensHead = annotations.filter(annotation => annotation.annotatorType == TOKEN && annotation.begin == 0)
    if (tokensHead.nonEmpty) {
      val output: Seq[Annotation] = nerEntities.headOption.map(annotation => annotation.begin) match {
        case None => tokensHead
        case Some(s) if s == 0 => nerEntities
        case _ => tokensHead ++: nerEntities
      }
      output
    } else {
      Seq.empty[Annotation]
    }
  }

  def retrieveSentence(annotations: Seq[Annotation]): String = {
    val sentences = SentenceSplit.unpack(annotations)
    val sentence = sentences.map(sentence => sentence.content)
    sentence.mkString(" ")
  }

  def segmentSentence(nerEntities: Seq[Annotation], originalText: String):
  Seq[Annotation] = {
    var sentenceIndex = 0
    nerEntities.flatMap{nerEntity =>
      val segmentedSentence = {
        if (sentenceIndex < nerEntities.length - 1){
          val beginIndex = nerEntity.begin
          val endIndex = nerEntities(sentenceIndex + 1).begin - 1
          val segmentedSentence = originalText.substring(beginIndex, endIndex)
          Sentence(segmentedSentence, beginIndex, endIndex - 1, sentenceIndex)
        } else {
          val beginIndex = nerEntity.begin
          val segmentedSentence = originalText.substring(beginIndex)
          Sentence(segmentedSentence, beginIndex, originalText.length - 1, sentenceIndex)
        }
      }
      var currentStart = segmentedSentence.start
      val annotatedSentenceWithLimit = get(splitLength)
        .map(splitLength => truncateSentence(segmentedSentence.content, splitLength))
        .getOrElse(Array(segmentedSentence.content))
        .map{truncatedSentence => {
          val currentEnd = currentStart + truncatedSentence.length - 1
          val result = Annotation(
            outputAnnotatorType,
            currentStart,
            currentEnd,
            truncatedSentence,
            Map("sentence" -> sentenceIndex.toString)
          )
          /** +1 because of shifting to the next token begin. +1 because of a whitespace jump to next token. */
          currentStart = currentEnd + 2
          sentenceIndex += 1
          result
      }}
      annotatedSentenceWithLimit
    }
  }

  def deepSentenceDetector(originalText: String, unpunctuatedSentences: Seq[Annotation],
                           nerEntities: Seq[Seq[Annotation]]):
  Seq[Annotation] = {
    unpunctuatedSentences.zipWithIndex.flatMap{ case (unpunctuatedSentence, index) =>
      segmentSentence(nerEntities(index), originalText, unpunctuatedSentence)
    }
  }

  def segmentSentence(nerEntities: Seq[Annotation], originalText: String, currentSentence: Annotation):
  Seq[Annotation] = {
    nerEntities.zipWithIndex.map{case (nerEntity, index) =>
      if (index != nerEntities.length - 1){
        val beginIndex = nerEntity.begin
        val endIndex = nerEntities(index + 1).begin - 1
        val segmentedSentence = originalText.substring(beginIndex, endIndex)
        Annotation(outputAnnotatorType, beginIndex, endIndex - 1, segmentedSentence, Map.empty)
      } else {
        val beginIndex = nerEntity.begin
        val endIndex = currentSentence.end
        val segmentedSentence = originalText.substring(beginIndex, endIndex + 1)
        Annotation(outputAnnotatorType, beginIndex, endIndex, segmentedSentence, Map.empty)
      }
    }
  }

  def getUnpunctuatedSentences(pragmaticSegmentedSentences: Seq[Annotation]): Seq[Annotation] = {
    pragmaticSegmentedSentences.filterNot(annotatedSentence =>
      sentenceHasPunctuation(annotatedSentence.result))
  }

  def sentenceHasPunctuation(sentence: String): Boolean = {
    var hasPunctuation = false

    $(endPunctuation).foreach { punctuation =>
      if (sentence.contains(punctuation)) {
        hasPunctuation = true
      }
      if (punctuation == "") {
        hasPunctuation = false
      }
    }

    hasPunctuation
  }

  def retrieveValidNerEntities(annotations: Seq[Annotation], unpunctuatedSentences: Seq[Annotation]):
  Seq[Seq[Annotation]] = {

    def updateIndex(validNerEntities: Seq[Seq[Annotation]]): Seq[Seq[Annotation]] = {
      validNerEntities.map{ validNerEntity =>
          validNerEntity.map(nerEntity =>
            Annotation(nerEntity.annotatorType, nerEntity.begin, nerEntity.end,
              nerEntity.result, nerEntity.metadata)
          )
      }
    }

    val nerEntities = getNerEntities(annotations)

    if (nerEntities.nonEmpty) {
      val validNerEntities = unpunctuatedSentences.map{unpunctuatedSentence =>
        nerEntities.filter{entity =>
          val beginSentence = unpunctuatedSentence.begin
          val endSentence = unpunctuatedSentence.end
          entity.begin >= beginSentence && entity.end <= endSentence
        }
      }.filterNot(_.isEmpty)

      updateIndex(validNerEntities)
    } else {
      Seq[Seq[Annotation]]()
    }
  }

  def mergeSentenceDetectors(pragmaticSegmentedSentences: Seq[Annotation], deepSegmentedSentences: Seq[Annotation]):
  Seq[Annotation] = {

    val mergeSentences = pragmaticSegmentedSentences.foldRight(Seq.empty[Annotation])
      { (pragmaticSegmentedSentence, annotations) =>
        if (sentenceHasPunctuation(pragmaticSegmentedSentence.result)) {
          annotations ++ Seq(pragmaticSegmentedSentence)
        } else {
          annotations
        }
      } ++ deepSegmentedSentences

    mergeSentences.zipWithIndex.map{ case (mergeSentence, sentenceNumber) =>
      Annotation(
        mergeSentence.annotatorType,
        mergeSentence.begin,
        mergeSentence.end,
        mergeSentence.result,
        Map("sentence"->sentenceNumber.toString)
      )
    }
  }

}

object DeepSentenceDetector extends DefaultParamsReadable[DeepSentenceDetector]