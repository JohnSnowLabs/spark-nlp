package com.johnsnowlabs.nlp.annotators.eal

import com.johnsnowlabs.nlp.AnnotatorType.{DOCUMENT, TOKEN}
import com.johnsnowlabs.nlp.annotators.common.SentenceSplit
import com.johnsnowlabs.nlp.{Annotation, AnnotatorModel, ParamsAndFeaturesReadable}
import org.apache.spark.ml.param.{IntParam, Param, StringArrayParam}
import org.apache.spark.ml.util.Identifiable

import scala.collection.mutable.ListBuffer
import scala.util.control.Breaks.{break, breakable}

class ChineseTokenizerModel(override val uid: String) extends AnnotatorModel[ChineseTokenizerModel] {

  def this() = this(Identifiable.randomUID("CHINESE_TOKENIZER"))

  val words: StringArrayParam = new StringArrayParam(this, "words", "Words generated from a document")

  val wordSegmentMethod = new Param[String](this, "wordSegmentMethod", "How to treat a combination of shorter words: LONG, SHORT, ALL")

  val maxWordLength = new IntParam(this, "maxWordLength", "Maximum word length")

  def setWords(value: Array[String]): this.type = set(words, value)

  def setWordSegmentMethod(value: String): this.type = set(wordSegmentMethod, value)

  def setMaxWordLength(value: Int): this.type = set(maxWordLength, value)

  /**
   * takes a document and annotations and produces new annotations of this annotator's annotation type
   *
   * @param annotations Annotations that correspond to inputAnnotationCols generated by previous annotators if any
   * @return any number of annotations processed for every input annotation. Not necessary one to one relationship
   */

  override def annotate(annotations: Seq[Annotation]): Seq[Annotation] = {
    val sentences = SentenceSplit.unpack(annotations)
    val tokenizedSentences = sentences.flatMap{ sentence =>
      var nextBegin = sentence.start
      val annotations = new ListBuffer[Annotation]()
      for (segment <- segmentSentence(sentence.content)) {
          val begin = nextBegin
          val end = nextBegin + segment.length - 1
          annotations += Annotation(TOKEN, begin, end, segment, Map("sentence" -> sentence.index.toString))
          nextBegin = end + 1
      }
      annotations
    }
    tokenizedSentences
  }

  /** Annotator reference id. Used to identify elements in metadata or to refer to this annotator type */
  override val inputAnnotatorTypes: Array[String] = Array(DOCUMENT)
  override val outputAnnotatorType: AnnotatorType = TOKEN

  private lazy val segmentationIndexes = {
    $(wordSegmentMethod) match {
      case "LONG" => List.range($(maxWordLength), 0, -1)
      case "SHORT" => List.range(2, $(maxWordLength) + 1) ++ List(1)
      case "ALL" => List.range(2, $(maxWordLength) + 1)
      case _ => throw new MatchError(s"Invalid WordSegmentMethod parameter.")
    }
  }

  private def segmentSentence(sentence: String): List[String] = {
    var index = 0
    var segmentedSentence = new ListBuffer[String]()
    while (index < sentence.length) {
      breakable {
        if ($(wordSegmentMethod) == "ALL") {
          var count = 1
          for (segmentationIndex <- segmentationIndexes) {
            val segment = sentence.slice(index, segmentationIndex + index)
            if (index + segmentationIndex <= sentence.length && $(words).contains(segment)) {
              segmentedSentence += segment
              if (count == 1) {count = segmentationIndex}
            }
          }
          if (count == 1) { segmentedSentence += sentence(index).toString }
          index = index + count
        } else {
          for (segmentationIndex <- segmentationIndexes) {
            val segment = sentence.slice(index, segmentationIndex + index)
            if (segmentationIndex == 1 || $(words).contains(segment)) {
              segmentedSentence += segment
              index = index + segmentationIndex
              break
            }
          }
        }
      }
    }
    segmentedSentence.toList
  }


}

object ChineseTokenizerModel extends ParamsAndFeaturesReadable[ChineseTokenizerModel]