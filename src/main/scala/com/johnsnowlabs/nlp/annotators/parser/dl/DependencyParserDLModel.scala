/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.johnsnowlabs.nlp.annotators.parser.dl

import com.johnsnowlabs.ml.tensorflow.{TensorResources, TensorflowEmbeddingLookup, TensorflowWrapper}
import com.johnsnowlabs.nlp.AnnotatorType.{DEPENDENCY, DOCUMENT, TOKEN}
import com.johnsnowlabs.nlp.serialization.MapFeature
import com.johnsnowlabs.nlp.{Annotation, AnnotatorModel, AnnotatorType, HasSimpleAnnotate}
import org.apache.spark.ml.util.Identifiable
import org.tensorflow.op.Scope
import org.tensorflow.op.core.{Constant, Slice, Zeros}
import org.tensorflow.op.nn.BlockLSTM
import org.tensorflow.types.{TFloat32, TInt32, TInt64}
import org.tensorflow.{EagerSession, Operand, SavedModelBundle, Tensor}

import scala.language.postfixOps

class DependencyParserDLModel(override val uid: String) extends AnnotatorModel[DependencyParserDLModel]
  with HasSimpleAnnotate[DependencyParserDLModel] {

  def this() = this(Identifiable.randomUID(DEPENDENCY))

  protected val vocabulary: MapFeature[String, Int] = new MapFeature[String, Int](this, "vocabulary to lookup embeddings based on word id")

  def setVocabulary(value: Map[String, Int]): this.type = set(vocabulary, value)

  private lazy val vocabularySize = $$(vocabulary).size

  private lazy val scope = {
    val session = EagerSession.create
    new Scope(session)
  }

  private val firstLstmVariables =  List(("w_first_lstm", "weightMatrix"), ("wig_first_lstm", "weightInputGate"),
    ("wfg_first_lstm", "weightForgetGate"), ("wog_first_lstm", "weightOutputGate"))
  private val nextLstmVariables =  List(("w_next_lstm", "weightMatrix"), ("wig_next_lstm", "weightInputGate"),
    ("wfg_next_lstm", "weightForgetGate"), ("wog_next_lstm", "weightOutputGate"))
  private val headsVariables = List("hidLayerFOH", "hidLayerFOM")

  private lazy val weightsFirstLstm = restoreLstmWeights(firstLstmVariables)
  private lazy val weightsNextLstm = restoreLstmWeights(nextLstmVariables)
  private lazy val weightsHead = restoreHeadsWeights(headsVariables)

  private val EMBEDDINGS_SIZE = 100
  private val SAMPLE_SIZE = 1
  private var timeSteps: Int = 0

  /**
    * takes a document and annotations and produces new annotations of this annotator's annotation type
    *
    * @param annotations Annotations that correspond to inputAnnotationCols generated by previous annotators if any
    * @return any number of annotations processed for every input annotation. Not necessary one to one relationship
    */
  override def annotate(annotations: Seq[Annotation]): Seq[Annotation] = {

    val embeddings = new TensorflowEmbeddingLookup(EMBEDDINGS_SIZE, vocabularySize, scope)
    val embeddingsTable = embeddings.initializeTable()

    val tokens = annotations
      .filter(_.annotatorType == AnnotatorType.TOKEN)
      .toArray

    val sentenceEmbeddings: Array[Operand[TFloat32]] = tokens.map{ token =>
      val wordId: Int = $$(vocabulary).getOrElse(token.result, 0)
      Constant.create(scope, embeddings.lookup(embeddingsTable, Array(wordId)))
    }
    timeSteps = sentenceEmbeddings.length
    val biLstmInputs = getBiLstmInput(sentenceEmbeddings)
    val biLstmOutputs = computeBiLstmOutput(biLstmInputs)
    val resForward = biLstmOutputs(0)
    val resBackward = biLstmOutputs(1)

    val headsFeatures = (0 until timeSteps).toArray.map{ timeStep =>
      val size = Array(1, -1)
      val lstmForward = TensorResources.sliceTensor(scope, resForward, begin = Array(timeStep, 0), size)
      val lstmBackward = TensorResources.sliceTensor(scope, resBackward, begin = Array(timeSteps - timeStep - 1, 0), size)
      val concatLstm = TensorResources.concatTensors(scope, Array(lstmForward, lstmBackward), 1)
      val hidLayerFoh: Operand[TFloat32] = Constant.create(scope, weightsHead("hidLayerFOH"))
      val hidLayerFom: Operand[TFloat32] = Constant.create(scope, weightsHead("hidLayerFOM"))
      val headFov = TensorResources.matmulTensors(scope, concatLstm, hidLayerFoh)
      val modFov = TensorResources.matmulTensors(scope, concatLstm, hidLayerFom)
      Array(headFov, modFov)
    }
    Seq()
  }

  def getBiLstmInput(sentenceEmbeddings: Array[Operand[TFloat32]]): Array[Operand[TFloat32]] = {
    val reverseSentenceEmbeddings: Array[Operand[TFloat32]] = sentenceEmbeddings.map{ wordEmbeddings =>
      TensorResources.reverseTensor(scope, wordEmbeddings, 1)
    }
    val concatSentenceEmbeddings = TensorResources.concatTensors(scope, sentenceEmbeddings, 0)
    val concatReverseSentenceEmbeddings = TensorResources.concatTensors(scope, reverseSentenceEmbeddings, 0)

    val shape = Array(timeSteps, SAMPLE_SIZE, EMBEDDINGS_SIZE)

    val forwardVector = TensorResources.reshapeTensor(scope, concatSentenceEmbeddings, shape)
    val backwardVector = TensorResources.reshapeTensor(scope, concatReverseSentenceEmbeddings, shape)
    Array(forwardVector, backwardVector)
  }

  private def computeBiLstmOutput(biLstmInputs: Array[Operand[TFloat32]]): Array[Operand[TFloat32]] = {
    val (blockLSTMForward, lstmDims) = getLstmOutput(biLstmInputs(0), weightsFirstLstm)
    val (blockLSTMBackward, _) = getLstmOutput(biLstmInputs(1), weightsFirstLstm)

    val nextLstmInputs = computeNextLstmInput(Array(blockLSTMForward, blockLSTMBackward), lstmDims)

    val (blockNextLSTMForward, _) = getLstmOutput(nextLstmInputs(0), weightsNextLstm)
    val (blockNextLSTMBackward, _) = getLstmOutput(nextLstmInputs(1), weightsNextLstm)

    Array(TensorResources.reshapeTensor(scope, blockNextLSTMForward, Array(timeSteps, lstmDims)),
      TensorResources.reshapeTensor(scope, blockNextLSTMBackward, Array(timeSteps, lstmDims)))
  }

  private def computeNextLstmInput(lstmOutputs: Array[Operand[TFloat32]], lstmDims: Int): Array[Operand[TFloat32]] = {
    val blockLSTMForward = lstmOutputs(0)
    val blockLSTMBackward = lstmOutputs(1)
    val resShape = Array(SAMPLE_SIZE, timeSteps, lstmDims)
    val resForward = TensorResources.reshapeTensor(scope, blockLSTMForward, resShape)
    val resBackward = TensorResources.reshapeTensor(scope, blockLSTMBackward, resShape)

    val vecShape = Array(timeSteps, SAMPLE_SIZE, -1)
    val resTensors = Array(resForward, TensorResources.reverseTensor(scope, resBackward, 1))
    val vecForward = TensorResources.reshapeTensor(scope,
      TensorResources.concatTensors(scope, resTensors, 2), vecShape)
    val vecBackward = TensorResources.reverseTensor(scope, vecForward, 0)

    Array(vecForward, vecBackward)
  }

  private def restoreLstmWeights(weightsVariables: List[(String, String)]): Map[String, Tensor[TFloat32]] = {
    val modelPath: String = "src/main/resources/dependency-parser-dl/bi-lstm/"
    val model: SavedModelBundle = TensorflowWrapper.withSafeSavedModelBundleLoader(Array(), savedModelDir = modelPath)

    weightsVariables.map{ weightVariable =>
      var blockName = "FirstBlockLSTMModule"
      if (weightVariable._1.contains("next")) {
        blockName = "NextBlockLSTM"
      }
      val variableName = s"bi_lstm_model/$blockName/${weightVariable._1}/Read/ReadVariableOp"
      val tensor = TensorflowWrapper.restoreVariable(model, modelPath, variableName)
      (weightVariable._2, tensor.expect(TFloat32.DTYPE))
    }.toMap

  }

  private def restoreHeadsWeights(weightsVariables: List[String]): Map[String, Tensor[TFloat32]] = {
    val modelPath: String = "src/main/resources/dependency-parser-dl/heads/"
    val model: SavedModelBundle = TensorflowWrapper.withSafeSavedModelBundleLoader(Array(), savedModelDir = modelPath)

    weightsVariables.map{ weightVariable =>
      val variableName = s"$weightVariable/Read/ReadVariableOp"
      val tensor = TensorflowWrapper.restoreVariable(model, modelPath, variableName)
      (weightVariable, tensor.expect(TFloat32.DTYPE))
    }.toMap

  }

  private def getLstmOutput(inputSequence: Operand[TFloat32], weights: Map[String, Tensor[TFloat32]]):
  (Operand[TFloat32], Int) = {

    val weightMatrix: Operand[TFloat32] = Constant.create(scope, weights("weightMatrix"))
    val weightInputGate: Operand[TFloat32] = Constant.create(scope, weights("weightInputGate"))
    val weightForgetGate: Operand[TFloat32] = Constant.create(scope, weights("weightForgetGate"))
    val weightOutputGate: Operand[TFloat32] = Constant.create(scope, weights("weightOutputGate"))
    val seqLenMax: Operand[TInt64] = Constant.arrayOf(scope, inputSequence.asTensor().shape().asArray()(0))
    val lstmDims = weightInputGate.asTensor().shape().size().toInt

    val shape = Array(SAMPLE_SIZE, lstmDims)
    val biasShape = Array(lstmDims * 4)
    val initialCellState: Operand[TFloat32] = Zeros.create(scope, Constant.vectorOf(scope, shape), TFloat32.DTYPE)
    val initialHiddenState: Operand[TFloat32] = Zeros.create(scope, Constant.vectorOf(scope, shape), TFloat32.DTYPE)
    val bias: Operand[TFloat32] = Zeros.create(scope, Constant.vectorOf(scope, biasShape), TFloat32.DTYPE)


    val blockLSTM = BlockLSTM.create(scope, seqLenMax, inputSequence,
      initialCellState, initialHiddenState, weightMatrix, weightInputGate,
      weightForgetGate, weightOutputGate, bias)

    (blockLSTM.h(), lstmDims)

  }

  /** Annotator reference id. Used to identify elements in metadata or to refer to this annotator type */
  override val inputAnnotatorTypes: Array[String] = Array[String](DOCUMENT, TOKEN)
  override val outputAnnotatorType: AnnotatorType = DEPENDENCY

}
