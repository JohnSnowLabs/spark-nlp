{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v8xIEZ07QpRM",
    "outputId": "03f475f3-1094-48cd-ff44-c30aa3f2c167"
   },
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/util/Load_Model_from_Azure_Storage.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E81LqDJzJIW7",
    "outputId": "967472fc-8ede-44ad-be4e-33a455ae7f36"
   },
   "source": [
    "## Loading Pretrained Models from Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXU_LZZUJI6V"
   },
   "outputs": [],
   "source": [
    "# Only run this Cell when you are using Spark NLP on Google Colab\n",
    "!wget https://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6KvNW4MU5rrF",
    "outputId": "661514cb-e45e-4f3c-f738-ebeda2cc7d47"
   },
   "source": [
    "## Defining Azure Storage URI in cache_pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark NLP you can configure the location to download the pre-trained models. Starting at Spark NLP 5.1.0, you can set a Azure Storage URI, GCP Storage URI or DBFS paths like HDFS or Databricks FS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to see the steps required to use an external Azure Storage URI as `cache_pretrained` folder. To do this, we need to configure the spark session with the required settings for Spark NLP and Spark ML. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark NLP Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cache_folder`: Here you must define your Azure URI that will store Spark NLP pre-trained models. This is defined in the config `spark.jsl.settings.pretrained.cache_folder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark ML Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark ML requires the following configuration to load a model from Azure:\n",
    "\n",
    "\n",
    "1. Azure connector: You need to identify your hadoop version and set the required dependency in `spark.jars.packages`\n",
    "2. Hadoop File System: You also need to setup the Hadoop file system to work with azure storage as file system. This is define in `spark.hadoop.fs.azure`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To integrage with Azure, we need to define STORAGE_ACCOUNT and AZURE_ACCOUNT_KEY variables:\n",
    "1. STORAGE_ACCOUNT: This can be found in Microsoft Azure portal, in Resources look for the Type storage account and check the name that is your storage account.\n",
    "2. AZURE_ACCOUNT_KEY: \n",
    "Check View account access keys in this oficial [Azure documentation](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal)\n",
    "\n",
    "Then you can define this two properties as variables to set those during spark session creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter your Storage Account:\")\n",
    "STORAGE_ACCOUNT = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter your Azure Account Key:\")\n",
    "AZURE_ACCOUNT_KEY = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_hadoop_config = \"spark.hadoop.fs.azure.account.key.\" + STORAGE_ACCOUNT + \".blob.core.windows.net\"\n",
    "cache_folder = \"https://\" + STORAGE_ACCOUNT + \".blob.core.windows.net/test/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "MBaH_B_dnLGp",
    "outputId": "693b664e-8b96-424a-8561-0c284fa7542f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.hadoop.fs.azure.account.key.MY_STORAGE_ACCOUNT.blob.core.windows.net\n",
      "https://MY_STORAGE_ACCOUNT.blob.core.windows.net/test/models\n"
     ]
    }
   ],
   "source": [
    "print(azure_hadoop_config)\n",
    "print(cache_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLNO3Z9r6HgR"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_azure_pkg = \"org.apache.hadoop:hadoop-azure:3.3.4\"\n",
    "azure_storage_pkg = \"com.microsoft.azure:azure-storage:8.6.6\"\n",
    "azure_identity_pkg = \"com.azure:azure-identity:1.9.1\"\n",
    "azure_storage_blob_pkg = \"com.azure:azure-storage-blob:12.22.2\"\n",
    "azure_pkgs = hadoop_azure_pkg + \",\" + azure_storage_pkg + \",\" + azure_identity_pkg + \",\" + azure_storage_blob_pkg\n",
    "\n",
    "#Azure Storage configuration\n",
    "azure_params = {\n",
    "    \"spark.jars.packages\": azure_pkgs,\n",
    "    \"spark.jsl.settings.pretrained.cache_folder\": cache_folder\n",
    "}\n",
    "\n",
    "spark = sparknlp.start(params=azure_params)\n",
    "\n",
    "print(\"Apache Spark version: {}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Hadoop version = {spark.sparkContext._jvm.org.apache.hadoop.util.VersionInfo.getVersion()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer: \n",
    "- Interaction with Azure depends on Spark/Hadoop/Azure implementations, which is out of our scope. Keep in mind that the configuration requirements or formats could change in other releases. \n",
    "- It's important to stand out that `hadoop-azure`, `azure-storage`, `azure_identity` and `azure-storage-blob` packages versions must be compatible. Otherwise, it won't work. The example of this notebook uses Spark 3.4.0 and Hadoop 3.3.4. So, you must modify those versions based on your Hadoop version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eB72Yzg8_Jx"
   },
   "outputs": [],
   "source": [
    "sample_text = \"This is a sentence. This is another sentence\"\n",
    "data_df = spark.createDataFrame([[sample_text]]).toDF(\"text\").cache()\n",
    "\n",
    "empty_df = spark.createDataFrame([[\"\"]]).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRyju8D-6XJ1"
   },
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5G4_BXwOYtC",
    "outputId": "7008d641-aab2-42a4-aa4a-af6e7a0cccb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_detector_dl download started this may take some time.\n",
      "Approximate size to download 354.6 KB\n",
      "[OK!]\n",
      "Elapsed time:  24.047518253326416\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "sentence_detector_dl = SentenceDetectorDLModel() \\\n",
    ".pretrained() \\\n",
    ".setInputCols([\"document\"]) \\\n",
    ".setOutputCol(\"sentence\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Elapsed time: \", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Oq7PEFWe5B7",
    "outputId": "687b7be9-5fc1-43a2-b0e1-7c1174ed306f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_dl download started this may take some time.\n",
      "Approximate size to download 13.6 MB\n",
      "[OK!]\n",
      "Elapsed time:  11.117968082427979\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "ner_model = NerDLModel.pretrained()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Elapsed time: \", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhKPEMb09w6a"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[document_assembler, sentence_detector_dl, tokenizer])\n",
    "pipeline_model = pipeline.fit(empty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CAp_AtrssPj",
    "outputId": "56716c31-344f-4ed5-95c1-c05b1f67d4ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|This is a sentenc...|[{document, 0, 43...|[{document, 0, 18...|[{token, 0, 3, Th...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = pipeline_model.transform(data_df)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XX7puYR73Axk",
    "outputId": "8cb3103e-40f2-4841-8d1f-1fa32b0c28d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_ml download started this may take some time.\n",
      "Approx size to download 9 MB\n",
      "[OK!]\n",
      "Elapsed time:  31.03494954109192\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "start_time = time.time()\n",
    "pipeline_model = PretrainedPipeline('explain_document_ml', lang = 'en')\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Elapsed time: \", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ifb9MFAeK4dC",
    "outputId": "11bf81e4-a3f9-44b1-fa11-0706d67f9806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albert_xlarge_token_classifier_conll03_pipeline download started this may take some time.\n",
      "Approx size to download 196.9 MB\n",
      "[OK!]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ner_chunk': ['John', 'John Snow Labs'],\n",
       " 'token': ['My',\n",
       "  'name',\n",
       "  'is',\n",
       "  'John',\n",
       "  'and',\n",
       "  'I',\n",
       "  'work',\n",
       "  'at',\n",
       "  'John',\n",
       "  'Snow',\n",
       "  'Labs',\n",
       "  '.'],\n",
       " 'sentence': ['My name is John and I work at John Snow Labs.']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = PretrainedPipeline(\"albert_xlarge_token_classifier_conll03_pipeline\", lang = \"en\")\n",
    "pipeline.annotate(\"My name is John and I work at John Snow Labs.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
