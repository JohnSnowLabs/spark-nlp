{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/prediction/english/Load_Model_From_S3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pretrained Models from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-eUrx5szYw9u",
    "outputId": "500e41f0-bcf3-49ff-df59-f1a7a398566c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-08 14:43:43--  https://setup.johnsnowlabs.com/colab.sh\n",
      "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
      "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
      "--2022-09-08 14:43:44--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1191 (1.2K) [text/plain]\n",
      "Saving to: ‘STDOUT’\n",
      "\n",
      "-                     0%[                    ]       0  --.-KB/s               Installing PySpark 3.2.1 and Spark NLP 4.1.0\n",
      "setup Colab for PySpark 3.2.1 and Spark NLP 4.1.0\n",
      "-                   100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-09-08 14:43:45 (37.8 MB/s) - written to stdout [1191/1191]\n",
      "\n",
      "\u001B[K     |████████████████████████████████| 281.4 MB 32 kB/s \n",
      "\u001B[K     |████████████████████████████████| 616 kB 33.3 MB/s \n",
      "\u001B[K     |████████████████████████████████| 198 kB 54.1 MB/s \n",
      "\u001B[?25h  Building wheel for pyspark (setup.py) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!wget https://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining S3 URI in cache_pretrained "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to see the steps required to use an external S3 URI as `cache_pretrained` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark NLP you can configure the location to download the pre-trained models. Before Spark NLP 4.2.0, we can define a local file system, or a distributed file system (DBFS). Starting at 4.2.0, you can also set an S3 URI. To do this, we need to configure the spark session with the required settings for Spark NLP and Spark ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark NLP Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark NLP requires the following configuration:\n",
    "1. `cache_folder`: Here you must define your S3 URI (using s3 or s3a prefix) that will store Spark NLP pre-trained models. This is defined in the config `spark.jsl.settings.pretrained.cache_folder`\n",
    "2. S3 Region: We need the region to upload a file on your S3 bucket. This is defined in the config `spark.jsl.settings.aws.region`\n",
    "3. Spark NLP JAR: Since some custom configurations are needed to use S3 URI in `cache_pretrained`. It is also required to include spark-nlp JAR either as a dependency for our application or during spark session creation. Since we are using a notebook, we will add these packages while creating a spark session in the following config:\n",
    "\n",
    "- `spark.jars.packages` for Maven coordinates or `spark.jar` for FAT JAR\n",
    "4. We recommend also adding the parameters described in creating manually a spark session in requirements section on [Spark NLP documentation](https://github.com/JohnSnowLabs/spark-nlp#requirements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark ML Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration will depend on your S3 bucket and AWS configuration. In this notebook a connection through **Temporary Security Credentials** is showcased. **Please contact your administrator to choose the right setup, as well as, the required keys/tokens.**\n",
    "\n",
    "Spark ML requires the following configuration to load a model from S3 using *Temporary Security Credentials*:\n",
    "\n",
    "1. Authenticating with S3: This is needed to interact with external S3 buckets, and it will require an access key, a secret key, and a session token. Define the values in these configs:\n",
    "\n",
    "- `spark.hadoop.fs.s3a.access.key`\n",
    "- `spark.hadoop.fs.s3a.secret.key`\n",
    "- `spark.hadoop.fs.s3a.session.token`\n",
    "2. Credential Provider: You need to define the Hadoop provider that will handle this connection. Since in this notebook, *Temporary Security Credentials* is used we need to use the provider `TemporaryAWSCredentialsProvider` from `hadoop-aws` package, and set it up in the config below:\n",
    "\n",
    "- `spark.hadoop.fs.s3a.aws.credentials.provider`\n",
    "3. AWS packages: S3A depends upon two JARs, alongside `hadoop-common` and its dependencies, which are `hadoop-aws` and `aws-java-sdk` packages. So, you will need to either add these dependencies in your application or to your spark session. Since we are using a notebook, we will add these packages while creating the spark session in the following config:\n",
    "\n",
    "- `spark.jars.packages`\n",
    "4. AWS File System: Defining S3AFileSystem it's also required for interacting S3 with AWS SDK. Define the value in this config:\n",
    "\n",
    "- `spark.hadoop.fs.s3a.impl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the spark session creation below to see how to define each of the configurations with its values for **Temporary Security Credentials**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter your AWS Access Key:\")\n",
    "MY_ACCESS_KEY = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter your AWS Secret Key:\")\n",
    "MY_SECRET_KEY = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter your AWS Session Key:\")\n",
    "MY_SESSION_KEY = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "XSCAf1NOe7rC",
    "outputId": "12014be5-e174-42c1-ad37-9f97f64652aa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://15cdfdc2e519:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkNLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6c09017750>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkNLP\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"12G\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", MY_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", MY_SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.session.token\", MY_SESSION_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.1.0,org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk:1.11.901\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.jsl.settings.pretrained.cache_folder\", \"s3://my_bucket/my/models/\") \\\n",
    "    .config(\"spark.jsl.settings.aws.region\", \"us-east-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer: \n",
    "- Interaction with S3 depends on Spark/Hadoop/AWS implementations, which is out of our scope. Keep in mind that the configuration requirements or formats could change in other releases. For addidional information and details, we recommend checking their up to date official documentation, like this one from [Hadoop-AWS Integration with AWS](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html)\n",
    "- It's important to stand out that `hadoop-aws` and `aws-java-sdk` package versions must be compatible. Otherwise, it won't work. The example of this notebook uses Hadoop 3.3.1. So, you must modify those versions based on your Hadoop version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQ8jfnOR39DQ",
    "outputId": "6800b159-2ada-4eb0-f8f2-06aaae482435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop version = 3.3.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hadoop version = {spark.sparkContext._jvm.org.apache.hadoop.util.VersionInfo.getVersion()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oz4bRCvRnPWz"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_qiC18IvnhIA",
    "outputId": "2206db7e-2012-4041-b23e-96e04f59c89f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_detector_dl download started this may take some time.\n",
      "Approximate size to download 354.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetectorDLModel.pretrained() \\\n",
    "            .setInputCols([\"document\"]) \\\n",
    "            .setOutputCol(\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iCFm_eIwoA0P"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "            document_assembler,\n",
    "            sentence_detector\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "F_Vin105oH2W"
   },
   "outputs": [],
   "source": [
    "test_df = spark.createDataFrame([[\"This is a simple example. This is another sentence\"]]).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7wPFZJadoD-N"
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-jN9LtwolmW",
    "outputId": "0d676204-78b6-4460-fde3-0a0dfdcb8d5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                              |document                                                                                    |sentence                                                                                                                               |\n",
      "+--------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|This is a simple example. This is another sentence|[{document, 0, 49, This is a simple example. This is another sentence, {sentence -> 0}, []}]|[{document, 0, 24, This is a simple example., {sentence -> 0}, []}, {document, 25, 49,  This is another sentence, {sentence -> 1}, []}]|\n",
      "+--------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(test_df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XvOWCR6EXrss",
    "outputId": "96cda5f0-55e4-442d-a4d3-780201647331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_ml download started this may take some time.\n",
      "Approx size to download 9.2 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "pipeline_model = PretrainedPipeline('explain_document_ml', lang = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tz1Y8DKRX4sS",
    "outputId": "7bf91165-7912-4028-ad23-a229942572d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                              |document                                                                                    |sentence                                                                                                                              |token                                                                                                                                                                                                                                                                                                                                                                                                                                               |spell                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |lemmas                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |stems                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |pos                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+--------------------------------------------------+--------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|This is a simple example. This is another sentence|[{document, 0, 49, This is a simple example. This is another sentence, {sentence -> 0}, []}]|[{document, 0, 24, This is a simple example., {sentence -> 0}, []}, {document, 26, 49, This is another sentence, {sentence -> 1}, []}]|[{token, 0, 3, This, {sentence -> 0}, []}, {token, 5, 6, is, {sentence -> 0}, []}, {token, 8, 8, a, {sentence -> 0}, []}, {token, 10, 15, simple, {sentence -> 0}, []}, {token, 17, 23, example, {sentence -> 0}, []}, {token, 24, 24, ., {sentence -> 0}, []}, {token, 26, 29, This, {sentence -> 1}, []}, {token, 31, 32, is, {sentence -> 1}, []}, {token, 34, 40, another, {sentence -> 1}, []}, {token, 42, 49, sentence, {sentence -> 1}, []}]|[{token, 0, 3, This, {confidence -> 1.0, sentence -> 0}, []}, {token, 5, 6, is, {confidence -> 1.0, sentence -> 0}, []}, {token, 8, 8, a, {confidence -> 1.0, sentence -> 0}, []}, {token, 10, 15, simple, {confidence -> 1.0, sentence -> 0}, []}, {token, 17, 23, example, {confidence -> 1.0, sentence -> 0}, []}, {token, 24, 24, ., {confidence -> 0.0, sentence -> 0}, []}, {token, 26, 29, This, {confidence -> 1.0, sentence -> 1}, []}, {token, 31, 32, is, {confidence -> 1.0, sentence -> 1}, []}, {token, 34, 40, another, {confidence -> 1.0, sentence -> 1}, []}, {token, 42, 49, sentence, {confidence -> 1.0, sentence -> 1}, []}]|[{token, 0, 3, This, {confidence -> 1.0, sentence -> 0}, []}, {token, 5, 6, be, {confidence -> 1.0, sentence -> 0}, []}, {token, 8, 8, a, {confidence -> 1.0, sentence -> 0}, []}, {token, 10, 15, simple, {confidence -> 1.0, sentence -> 0}, []}, {token, 17, 23, example, {confidence -> 1.0, sentence -> 0}, []}, {token, 24, 24, ., {confidence -> 0.0, sentence -> 0}, []}, {token, 26, 29, This, {confidence -> 1.0, sentence -> 1}, []}, {token, 31, 32, be, {confidence -> 1.0, sentence -> 1}, []}, {token, 34, 40, another, {confidence -> 1.0, sentence -> 1}, []}, {token, 42, 49, sentence, {confidence -> 1.0, sentence -> 1}, []}]|[{token, 0, 3, thi, {confidence -> 1.0, sentence -> 0}, []}, {token, 5, 6, i, {confidence -> 1.0, sentence -> 0}, []}, {token, 8, 8, a, {confidence -> 1.0, sentence -> 0}, []}, {token, 10, 15, simpl, {confidence -> 1.0, sentence -> 0}, []}, {token, 17, 23, exampl, {confidence -> 1.0, sentence -> 0}, []}, {token, 24, 24, ., {confidence -> 0.0, sentence -> 0}, []}, {token, 26, 29, thi, {confidence -> 1.0, sentence -> 1}, []}, {token, 31, 32, i, {confidence -> 1.0, sentence -> 1}, []}, {token, 34, 40, anoth, {confidence -> 1.0, sentence -> 1}, []}, {token, 42, 49, sentenc, {confidence -> 1.0, sentence -> 1}, []}]|[{pos, 0, 3, DT, {word -> This, sentence -> 0}, []}, {pos, 5, 6, VBZ, {word -> is, sentence -> 0}, []}, {pos, 8, 8, DT, {word -> a, sentence -> 0}, []}, {pos, 10, 15, JJ, {word -> simple, sentence -> 0}, []}, {pos, 17, 23, NN, {word -> example, sentence -> 0}, []}, {pos, 24, 24, ., {word -> ., sentence -> 0}, []}, {pos, 26, 29, DT, {word -> This, sentence -> 1}, []}, {pos, 31, 32, VBZ, {word -> is, sentence -> 1}, []}, {pos, 34, 40, DT, {word -> another, sentence -> 1}, []}, {pos, 42, 49, NN, {word -> sentence, sentence -> 1}, []}]|\n",
      "+--------------------------------------------------+--------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_model.transform(test_df).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
