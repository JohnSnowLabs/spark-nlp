{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/word2vec/Train_Word2Vec_and_Named_Entity_Recognition.ipynb)\n",
    "\n",
    "# Train NER Model with Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this Cell when you are using Spark NLP on Google Colab\n",
    "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "# let's start Spark with Spark NLP\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/conll2003/eng.train\n",
    "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/conll2003/eng.testa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|EU rejects German...|[{document, 0, 47...|[{document, 0, 47...|[{token, 0, 1, EU...|[{pos, 0, 1, NNP,...|[{named_entity, 0...|\n",
      "|     Peter Blackburn|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 4, Pe...|[{pos, 0, 4, NNP,...|[{named_entity, 0...|\n",
      "| BRUSSELS 1996-08-22|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 7, BR...|[{pos, 0, 7, NNP,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.training import CoNLL\n",
    "\n",
    "training_data = CoNLL().readDataset(spark, './eng.train')\n",
    "\n",
    "training_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Vec = Word2VecApproach()\\\n",
    "    .setInputCols(\"token\")\\\n",
    "    .setOutputCol(\"embeddings\")\\\n",
    "    .setMaxSentenceLength(1000)\\\n",
    "    .setStepSize(0.025)\\\n",
    "    .setMinCount(5)\\\n",
    "    .setVectorSize(100)\\\n",
    "    .setNumPartitions(1)\\\n",
    "    .setMaxIter(1)\\\n",
    "    .setSeed(42)\\\n",
    "    .setStorageRef(\"word2vec_conll03\")\\\n",
    "\n",
    "nerTagger = NerDLApproach()\\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
    "    .setLabelColumn(\"label\")\\\n",
    "    .setOutputCol(\"ner\")\\\n",
    "    .setMaxEpochs(1)\\\n",
    "    .setLr(0.003)\\\n",
    "    .setBatchSize(8)\\\n",
    "    .setRandomSeed(0)\\\n",
    "    .setVerbose(1)\\\n",
    "    .setEvaluationLogExtended(True) \\\n",
    "    .setEnableOutputLogs(True)\\\n",
    "    .setIncludeConfidence(True)\\\n",
    "    .setValidationSplit(0.2)\\\n",
    "    .setOutputLogsPath('ner_logs')  # if not set, logs will be written to ~/annotator_logs\n",
    "#    .setGraphFolder('graphs') >> put your graph file (pb) under this folder if you are using a custom graph generated thru 4.1 NerDL-Graph.ipynb notebook\n",
    "#    .setEnableMemoryOptimizer() >> if you have a limited memory and a large conll file, you can set this True to train batch by batch\n",
    "\n",
    "ner_pipeline = Pipeline(stages=[\n",
    "    word2Vec,\n",
    "    nerTagger\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model = ner_pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\n",
      "-rw-r--r-- 1 root root 1017 20. Feb 18:09 NerDLApproach_00802da54a15.log\n"
     ]
    }
   ],
   "source": [
    "!cd ./ner_logs && ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the selected graph: ner-dl/blstm_10_100_128_120.pb\n",
      "Training started - total epochs: 1 - lr: 0.003 - batch size: 8 - labels: 9 - chars: 84 - training examples: 11239\n",
      "\n",
      "\n",
      "Epoch 1/1 started, lr: 0.003, dataset size: 11239\n",
      "\n",
      "\n",
      "Epoch 1/1 - 48.29s - loss: 4617.783 - batches: 1407\n",
      "Quality on validation dataset (20.0%), validation examples = 2247\n",
      "time to finish evaluation: 3.36s\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "B-LOC\t 1029\t 145\t 376\t 0.87649065\t 0.7323843\t 0.7979837\n",
      "I-ORG\t 381\t 93\t 363\t 0.8037975\t 0.51209676\t 0.6256157\n",
      "I-MISC\t 124\t 121\t 118\t 0.50612247\t 0.5123967\t 0.50924027\n",
      "I-LOC\t 138\t 53\t 77\t 0.7225131\t 0.6418605\t 0.67980295\n",
      "I-PER\t 898\t 237\t 21\t 0.79118943\t 0.97714907\t 0.8743915\n",
      "B-MISC\t 528\t 192\t 190\t 0.73333335\t 0.73537606\t 0.73435324\n",
      "B-ORG\t 771\t 139\t 507\t 0.8472527\t 0.6032864\t 0.7047532\n",
      "B-PER\t 1200\t 630\t 111\t 0.6557377\t 0.9153318\t 0.7640879\n",
      "tp: 5069 fp: 1610 fn: 1763 labels: 8\n",
      "Macro-average\t prec: 0.74205464, rec: 0.7037352, f1: 0.7223871\n",
      "Micro-average\t prec: 0.75894594, rec: 0.7419497, f1: 0.75035155\n"
     ]
    }
   ],
   "source": [
    "!cat ./ner_logs/{nerTagger.uid}.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|CRICKET - LEICEST...|[{document, 0, 64...|[{document, 0, 64...|[{token, 0, 6, CR...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "|   LONDON 1996-08-30|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 5, LO...|[{pos, 0, 5, NNP,...|[{named_entity, 0...|\n",
      "|West Indian all-r...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 3, We...|[{pos, 0, 3, NNP,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.training import CoNLL\n",
    "\n",
    "test_data = CoNLL().readDataset(spark, './eng.testa')\n",
    "\n",
    "test_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ner_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.88      0.71      0.79      1837\n",
      "      B-MISC       0.77      0.74      0.76       922\n",
      "       B-ORG       0.85      0.58      0.69      1341\n",
      "       B-PER       0.66      0.93      0.78      1842\n",
      "       I-LOC       0.70      0.53      0.60       257\n",
      "      I-MISC       0.62      0.59      0.60       346\n",
      "       I-ORG       0.83      0.42      0.56       751\n",
      "       I-PER       0.80      0.96      0.87      1307\n",
      "           O       0.99      0.99      0.99     42759\n",
      "\n",
      "    accuracy                           0.95     51362\n",
      "   macro avg       0.79      0.72      0.74     51362\n",
      "weighted avg       0.95      0.95      0.95     51362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds_df = predictions.select(F.explode(F.arrays_zip(predictions.token.result,\n",
    "                                                     predictions.label.result,\n",
    "                                                     predictions.ner.result)).alias(\"cols\")) \\\n",
    "                      .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "                              F.expr(\"cols['1']\").alias(\"ground_truth\"),\n",
    "                              F.expr(\"cols['2']\").alias(\"prediction\")).toPandas()\n",
    "\n",
    "print (classification_report(preds_df['ground_truth'], preds_df['prediction']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Restore\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotator Models\n",
    "Let's say you would like to only save the trained annotators inside your pipeline so you can load them inside another custom Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Word2VecModel_2ebfbb8d7c3b, NerDLModel_c57cffac70ba]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all we need is to access that stage and save it on disk\n",
    "ner_model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerDLModel_c57cffac70ba\n",
      "Word2VecModel_2ebfbb8d7c3b\n"
     ]
    }
   ],
   "source": [
    "print(ner_model.stages[-1])\n",
    "print(ner_model.stages[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save our NerDLModel - let's mention it was trained by word2vec_conll03 as well\n",
    "ner_model.stages[-1].write().overwrite().save(\"./nerdl_conll03_word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and here is our trained Word2VecModel\n",
    "ner_model.stages[-2].write().overwrite().save(\"./word2vec_conll03_model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you use your saved model within your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
    "\n",
    "token = Tokenizer().setInputCols([\"sentence\"]).setOutputCol(\"token\")\n",
    "\n",
    "word2vecModel = (\n",
    "    Word2VecModel.load(\"./word2vec_conll03_model\")\n",
    "    .setInputCols(\"token\")\n",
    "    .setOutputCol(\"embeddings\")\n",
    ")\n",
    "\n",
    "nerdlModel = (\n",
    "    NerDLModel.load(\"./nerdl_conll03_word2vec_model\")\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"])\n",
    "    .setOutputCol(\"ner\")\n",
    ")\n",
    "\n",
    "ner_prediction_pipeline = Pipeline(\n",
    "    stages=[document, sentence, token, word2vecModel, nerdlModel]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------+\n",
      "|result                                                                                                 |\n",
      "+-------------------------------------------------------------------------------------------------------+\n",
      "|[O, O, O, B-PER, O, O, O, O, B-ORG, O, B-LOC, O]                                                       |\n",
      "|[B-PER, I-PER, O, O, O, O, O, O, O, B-LOC, I-LOC, O, B-PER, I-PER, O, O, O, O, O, O, O, O, B-LOC, O, O]|\n",
      "+-------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# or you can use it via DataFrame\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "dfTest = spark.createDataFrame([\n",
    "    \"My name is John and I am a Doctor in London!\",\n",
    "    \"Peter Parker is a nice persn and lives in New York. Bruce Wayne is also a nice guy and lives in Gotham city.\"\n",
    "], StringType()).toDF(\"text\")\n",
    "\n",
    "ner_prediction_pipeline\\\n",
    "  .fit(dfTest)\\\n",
    "  .transform(dfTest)\\\n",
    "  .select(\"ner.result\")\\\n",
    "  .show(2, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and restore the whole Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='Pipeline_704aa7f63c6f', name='stages', doc='a list of pipeline stages')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ner_prediction_pipeline.write().overwrite().save(\"./ner_conll03_word2vec_pipeline\")\n",
    "# let's load it back and try\n",
    "loadedPipeline = Pipeline.load(\"./ner_conll03_word2vec_pipeline\")\n",
    "loadedPipeline.stages\n",
    "# we have all of our stages inside the loaded pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------+\n",
      "|result                                                                                                 |\n",
      "+-------------------------------------------------------------------------------------------------------+\n",
      "|[O, O, O, B-PER, O, O, O, O, B-ORG, O, B-LOC, O]                                                       |\n",
      "|[B-PER, I-PER, O, O, O, O, O, O, O, B-LOC, I-LOC, O, B-PER, I-PER, O, O, O, O, O, O, O, O, B-LOC, O, O]|\n",
      "+-------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loadedPipeline\\\n",
    "  .fit(dfTest)\\\n",
    "  .transform(dfTest)\\\n",
    "  .select(\"ner.result\")\\\n",
    "  .show(2, False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Train Doc2Vec and Text Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
