{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/doc2vec/Train_Doc2Vec_and_Text_Classification.ipynb)\n",
    "\n",
    "# Document Embeddings with Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell when you are using Spark NLP on Google Colab\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "# let's start Spark with Spark NLP\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-20 15:54:06--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment-corpus/aclimdb/aclimdb_train.csv\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.231.104, 52.217.198.144, 52.216.212.120, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.231.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 33497180 (32M) [text/csv]\n",
      "Saving to: ‘aclimdb_train.csv’\n",
      "\n",
      "aclimdb_train.csv   100%[===================>]  31,95M  14,6MB/s    in 2,2s    \n",
      "\n",
      "2023-02-20 15:54:09 (14,6 MB/s) - ‘aclimdb_train.csv’ saved [33497180/33497180]\n",
      "\n",
      "--2023-02-20 15:54:09--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment-corpus/aclimdb/aclimdb_test.csv\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.67.14, 3.5.19.152, 52.217.93.246, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.67.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 32715164 (31M) [text/csv]\n",
      "Saving to: ‘aclimdb_test.csv’\n",
      "\n",
      "aclimdb_test.csv    100%[===================>]  31,20M  15,5MB/s    in 2,0s    \n",
      "\n",
      "2023-02-20 15:54:12 (15,5 MB/s) - ‘aclimdb_test.csv’ saved [32715164/32715164]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O aclimdb_train.csv https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment-corpus/aclimdb/aclimdb_train.csv\n",
    "!wget -O aclimdb_test.csv https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment-corpus/aclimdb/aclimdb_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                text|   label|\n",
      "+--------------------+--------+\n",
      "|This is an Excell...|positive|\n",
      "|The Sarah Silverm...|positive|\n",
      "|\"Prom Night\" is a...|negative|\n",
      "|So often a band w...|positive|\n",
      "|\"Pet Sematary\" is...|positive|\n",
      "|I watched the fil...|negative|\n",
      "|Boy this movie ha...|negative|\n",
      "|Checking the spoi...|negative|\n",
      "|Despite its rathe...|positive|\n",
      "|Absolute masterpi...|positive|\n",
      "|The tweedy profes...|positive|\n",
      "|A movie best summ...|negative|\n",
      "|Take young, prett...|negative|\n",
      "|For months I've b...|negative|\n",
      "|\"Batman: The Myst...|positive|\n",
      "|Well, it was funn...|negative|\n",
      "|I have seen the s...|positive|\n",
      "|Brainless film ab...|negative|\n",
      "|Leave it to geniu...|negative|\n",
      "|Seven Pounds star...|positive|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDataset = spark.read \\\n",
    "      .option(\"header\", True) \\\n",
    "      .csv(\"aclimdb_train.csv\")\n",
    "\n",
    "testDataset = spark.read \\\n",
    "      .option(\"header\", True) \\\n",
    "      .csv(\"aclimdb_test.csv\")\n",
    "\n",
    "trainDataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords_en download started this may take some time.\n",
      "Approximate size to download 2.9 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document = DocumentAssembler()\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "token = Tokenizer()\\\n",
    "  .setInputCols(\"document\")\\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "norm = Normalizer()\\\n",
    "  .setInputCols([\"token\"])\\\n",
    "  .setOutputCol(\"normalized\")\\\n",
    "  .setLowercase(True)\n",
    "\n",
    "stops = StopWordsCleaner.pretrained()\\\n",
    "  .setInputCols(\"normalized\")\\\n",
    "  .setOutputCol(\"cleanedToken\")\n",
    "\n",
    "doc2Vec = Doc2VecApproach()\\\n",
    "  .setInputCols(\"cleanedToken\")\\\n",
    "  .setOutputCol(\"sentence_embeddings\")\\\n",
    "  .setMaxSentenceLength(1000)\\\n",
    "  .setStepSize(0.025)\\\n",
    "  .setMinCount(5)\\\n",
    "  .setVectorSize(100)\\\n",
    "  .setNumPartitions(1)\\\n",
    "  .setMaxIter(1)\\\n",
    "  .setSeed(42)\\\n",
    "  .setStorageRef(\"doc2vec_aclImdb\")\\\n",
    "\n",
    "sentimentdl = ClassifierDLApproach()\\\n",
    "  .setInputCols([\"sentence_embeddings\"])\\\n",
    "  .setOutputCol(\"class\")\\\n",
    "  .setLabelColumn(\"label\")\\\n",
    "  .setMaxEpochs(5)\\\n",
    "  .setEnableOutputLogs(True)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        token,\n",
    "        norm,\n",
    "        stops,\n",
    "        doc2Vec,\n",
    "        sentimentdl\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(trainDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 100\n",
      "-rw-r--r-- 1 root root 446 20. Feb 15:55 ClassifierDLApproach_97ff5c76d735.log\n",
      "-rw-r--r-- 1 root root 317 10. Feb 16:54 ClassifierMetrics_17606bbb7d1f.log\n",
      "-rw-r--r-- 1 root root 313 10. Feb 16:54 ClassifierMetrics_1a6c515483ae.log\n",
      "-rw-r--r-- 1 root root 323 10. Feb 16:54 ClassifierMetrics_2530315112a8.log\n",
      "-rw-r--r-- 1 root root 314 10. Feb 16:54 ClassifierMetrics_3ccf43933a23.log\n",
      "-rw-r--r-- 1 root root 325 10. Feb 16:54 ClassifierMetrics_55c7e364bf2b.log\n",
      "-rw-r--r-- 1 root root 325 10. Feb 16:54 ClassifierMetrics_9290b613e8d7.log\n",
      "-rw-r--r-- 1 root root 317 10. Feb 16:54 ClassifierMetrics_aa0e2812a3b9.log\n",
      "-rw-r--r-- 1 root root 318 10. Feb 16:54 ClassifierMetrics_ad4cb4a650fa.log\n",
      "-rw-r--r-- 1 root root 312 10. Feb 16:54 ClassifierMetrics_efc7f6345e79.log\n",
      "-rw-r--r-- 1 root root 319 10. Feb 16:54 ClassifierMetrics_f571876aaa09.log\n",
      "-rw-r--r-- 1 root root 320 26. Okt 09:23 NerDL_0f47f69f09e6.log\n",
      "-rw-r--r-- 1 root root 320  2. Aug 2022  NerDL_10e337c8a3ef.log\n",
      "-rw-r--r-- 1 root root 320 12. Jan 17:31 NerDL_18e7b1673dab.log\n",
      "-rw-r--r-- 1 root root 320  2. Aug 2022  NerDL_27f18f749174.log\n",
      "-rw-r--r-- 1 root root 320  2. Aug 2022  NerDL_3ae0321ce66a.log\n",
      "-rw-r--r-- 1 root root 319 26. Okt 09:13 NerDL_568d747656b8.log\n",
      "-rw-r--r-- 1 root root 320 26. Okt 09:03 NerDL_5970e276422f.log\n",
      "-rw-r--r-- 1 root root 320 16. Jan 11:10 NerDL_759a68c3769d.log\n",
      "-rw-r--r-- 1 root root 320  3. Nov 19:22 NerDL_891f9b941985.log\n",
      "-rw-r--r-- 1 root root 320  2. Feb 2022  NerDL_8e8184f259cb.log\n",
      "-rw-r--r-- 1 root root 320 27. Okt 13:02 NerDL_add5b34b2ecb.log\n",
      "-rw-r--r-- 1 root root 320 21. Okt 19:06 NerDL_bc57a96c68c3.log\n",
      "-rw-r--r-- 1 root root 320 12. Jan 16:47 NerDL_ff0a43f20378.log\n",
      "-rw-r--r-- 1 root root 897 10. Feb 16:54 SentimentDLApproach_98dfd2c1fdee.log\n"
     ]
    }
   ],
   "source": [
    "!cd ~/annotator_logs && ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started - epochs: 5 - learning_rate: 0.005 - batch_size: 64 - training_examples: 25000 - classes: 2\n",
      "Epoch 0/5 - 2.27s - loss: 194.4157 - acc: 0.814335 - batches: 391\n",
      "Epoch 1/5 - 1.74s - loss: 186.7701 - acc: 0.8377324 - batches: 391\n",
      "Epoch 2/5 - 1.75s - loss: 184.50777 - acc: 0.8419792 - batches: 391\n",
      "Epoch 3/5 - 1.79s - loss: 182.49121 - acc: 0.8430609 - batches: 391\n",
      "Epoch 4/5 - 1.69s - loss: 180.77087 - acc: 0.8451843 - batches: 391\n"
     ]
    }
   ],
   "source": [
    "!cat ~/annotator_logs/{sentimentdl.uid}.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipelineModel.transform(testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.82      0.84     13143\n",
      "    positive       0.81      0.85      0.83     11857\n",
      "\n",
      "    accuracy                           0.83     25000\n",
      "   macro avg       0.83      0.83      0.83     25000\n",
      "weighted avg       0.83      0.83      0.83     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predsPd = prediction.select('label','text',\"class.result\").toPandas()\n",
    "predsPd['result'] = predsPd['result'].apply(lambda x : x[0])\n",
    "print (classification_report(predsPd['result'], predsPd['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Restore\n",
    "### Pipeline Model\n",
    "\n",
    "It's pretty simple to save and restore an already trained Pipeline which is called `PipelineModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocumentAssembler_eb3006c82ed9,\n",
       " REGEX_TOKENIZER_62be3e2cd631,\n",
       " NORMALIZER_8c22ec321476,\n",
       " STOPWORDS_CLEANER_3e62acb2648b,\n",
       " Doc2VecModel_8e5707c8288a,\n",
       " ClassifierDLModel_9e82f8b9ca8b]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is our PipelineModel after it was trained via .fit()\n",
    "# as you can see we have all the stages inside this PipelineModel\n",
    "pipelineModel.stages\n",
    "# so once you save it on disk, it will include everything next time you load it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel.write().overwrite().save(\"./imdb_classifier_doc2vec_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocumentAssembler_eb3006c82ed9,\n",
       " REGEX_TOKENIZER_62be3e2cd631,\n",
       " NORMALIZER_8c22ec321476,\n",
       " STOPWORDS_CLEANER_3e62acb2648b,\n",
       " Doc2VecModel_8e5707c8288a,\n",
       " ClassifierDLModel_9e82f8b9ca8b]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load it back and try\n",
    "loadedPipelineModel = PipelineModel.load(\"./imdb_classifier_doc2vec_pipeline\")\n",
    "loadedPipelineModel.stages\n",
    "# we have all of our stages inside the loaded pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': ['This movie was really good!'],\n",
       " 'cleanedToken': ['movie', 'good'],\n",
       " 'normalized': ['this', 'movie', 'was', 'really', 'good'],\n",
       " 'sentence_embeddings': ['movie good'],\n",
       " 'token': ['This', 'movie', 'was', 'really', 'good', '!'],\n",
       " 'class': ['positive']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can use it with Spark NLP LightPipeline\n",
    "lp_loadedPipeline = LightPipeline(loadedPipelineModel)\n",
    "\n",
    "lp_loadedPipeline.annotate(\"This movie was really good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|result    |\n",
      "+----------+\n",
      "|[positive]|\n",
      "|[negative]|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# or you can use it via DataFrame\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "dfTest = spark.createDataFrame([\n",
    "    \"This movie is a delight for those of all ages. I have seen it several times and each time I am enchanted by the characters and magic. The cast is outstanding, the special effects delightful, everything most believable.\",\n",
    "    \"This film was to put it simply rubbish. The child actors couldn't act, as can be seen by Harry's supposed surprise on learning he's a wizard. I'm a wizard! is said with such indifference you'd think he's not surprised at all.\"\n",
    "], StringType()).toDF(\"text\")\n",
    "\n",
    "loadedPipelineModel\\\n",
    "  .transform(dfTest)\\\n",
    "  .select(\"class.result\")\\\n",
    "  .show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotator Models\n",
    "Now let's say you would like to only save the trained annotators inside your pipeline so you can load them inside another custom Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocumentAssembler_eb3006c82ed9,\n",
       " REGEX_TOKENIZER_62be3e2cd631,\n",
       " NORMALIZER_8c22ec321476,\n",
       " STOPWORDS_CLEANER_3e62acb2648b,\n",
       " Doc2VecModel_8e5707c8288a,\n",
       " ClassifierDLModel_9e82f8b9ca8b]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all we need is to access that stage and save it on disk\n",
    "pipelineModel.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierDLModel_9e82f8b9ca8b\n",
      "Doc2VecModel_8e5707c8288a\n"
     ]
    }
   ],
   "source": [
    "print(pipelineModel.stages[-1])\n",
    "print(pipelineModel.stages[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save our ClassifierDL - let's mention it was trained by doc2vec_aclImdb as well\n",
    "pipelineModel.stages[-1].write().overwrite().save(\"./classifierdl_doc2vec_aclImdb_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and here is our trained Doc2VecModel\n",
    "pipelineModel.stages[-2].write().overwrite().save(\"./doc2vec_aclImdb_model\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Train Doc2Vec and Text Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
