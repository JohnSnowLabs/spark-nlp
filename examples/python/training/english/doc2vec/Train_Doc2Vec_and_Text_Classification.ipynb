{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77mVF2ES4S01"
   },
   "outputs": [],
   "source": [
    "# This is only to setup PySpark and Spark NLP on Colab\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCiyzqtH4VCC"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "# let's start Spark with Spark NLP\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JSE7xgQc4gTg",
    "outputId": "4a6296be-f211-48b9-816e-55cab2e37426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-21 09:52:29--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment-corpus/aclimdb/aclimdb_train.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.92.54\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.92.54|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 33497180 (32M) [text/csv]\n",
      "Saving to: ‘aclimdb_train.csv’\n",
      "\n",
      "aclimdb_train.csv   100%[===================>]  31.95M  81.6MB/s    in 0.4s    \n",
      "\n",
      "2021-11-21 09:52:29 (81.6 MB/s) - ‘aclimdb_train.csv’ saved [33497180/33497180]\n",
      "\n",
      "--2021-11-21 09:52:30--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment-corpus/aclimdb/aclimdb_test.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.92.54\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.92.54|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 32715164 (31M) [text/csv]\n",
      "Saving to: ‘aclimdb_test.csv’\n",
      "\n",
      "aclimdb_test.csv    100%[===================>]  31.20M  46.9MB/s    in 0.7s    \n",
      "\n",
      "2021-11-21 09:52:30 (46.9 MB/s) - ‘aclimdb_test.csv’ saved [32715164/32715164]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O aclimdb_train.csv https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment-corpus/aclimdb/aclimdb_train.csv\n",
    "!wget -O aclimdb_test.csv https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment-corpus/aclimdb/aclimdb_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VOSCO4hg4jp9",
    "outputId": "9a4ef71b-772a-4242-b947-1b6f09468ebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                text|   label|\n",
      "+--------------------+--------+\n",
      "|This is an Excell...|positive|\n",
      "|The Sarah Silverm...|positive|\n",
      "|\"Prom Night\" is a...|negative|\n",
      "|So often a band w...|positive|\n",
      "|\"Pet Sematary\" is...|positive|\n",
      "|I watched the fil...|negative|\n",
      "|Boy this movie ha...|negative|\n",
      "|Checking the spoi...|negative|\n",
      "|Despite its rathe...|positive|\n",
      "|Absolute masterpi...|positive|\n",
      "|The tweedy profes...|positive|\n",
      "|A movie best summ...|negative|\n",
      "|Take young, prett...|negative|\n",
      "|For months I've b...|negative|\n",
      "|\"Batman: The Myst...|positive|\n",
      "|Well, it was funn...|negative|\n",
      "|I have seen the s...|positive|\n",
      "|Brainless film ab...|negative|\n",
      "|Leave it to geniu...|negative|\n",
      "|Seven Pounds star...|positive|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDataset = spark.read \\\n",
    "      .option(\"header\", True) \\\n",
    "      .csv(\"aclimdb_train.csv\")\n",
    "\n",
    "testDataset = spark.read \\\n",
    "      .option(\"header\", True) \\\n",
    "      .csv(\"aclimdb_test.csv\")\n",
    "\n",
    "trainDataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "M_6wrm1X4nQP"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFLQsOby4rPg",
    "outputId": "10d4508e-9562-4ee0-cfa2-42ed37a3d0a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords_en download started this may take some time.\n",
      "Approximate size to download 2.9 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document = DocumentAssembler()\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "token = Tokenizer()\\\n",
    "  .setInputCols(\"document\")\\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "norm = Normalizer()\\\n",
    "  .setInputCols([\"token\"])\\\n",
    "  .setOutputCol(\"normalized\")\\\n",
    "  .setLowercase(True)\n",
    "\n",
    "stops = StopWordsCleaner.pretrained()\\\n",
    "  .setInputCols(\"normalized\")\\\n",
    "  .setOutputCol(\"cleanedToken\")\n",
    "  \n",
    "doc2Vec = Doc2VecApproach()\\\n",
    "  .setInputCols(\"cleanedToken\")\\\n",
    "  .setOutputCol(\"sentence_embeddings\")\\\n",
    "  .setMaxSentenceLength(1000)\\\n",
    "  .setStepSize(0.025)\\\n",
    "  .setMinCount(5)\\\n",
    "  .setVectorSize(100)\\\n",
    "  .setNumPartitions(1)\\\n",
    "  .setMaxIter(1)\\\n",
    "  .setSeed(42)\\\n",
    "  .setStorageRef(\"doc2vec_aclImdb\")\\\n",
    "\n",
    "sentimentdl = ClassifierDLApproach()\\\n",
    "  .setInputCols([\"sentence_embeddings\"])\\\n",
    "  .setOutputCol(\"class\")\\\n",
    "  .setLabelColumn(\"label\")\\\n",
    "  .setMaxEpochs(5)\\\n",
    "  .setEnableOutputLogs(True)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        token,\n",
    "        norm,\n",
    "        stops,\n",
    "        doc2Vec,\n",
    "        sentimentdl\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZT4dQu328okt"
   },
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(trainDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vTRFsKV92Yz",
    "outputId": "54af004f-47dd-4038-b0b1-c1dd2c09228b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\n",
      "-rw-r--r-- 1 root root 452 Nov 21 09:58 ClassifierDLApproach_b126569e5e91.log\n"
     ]
    }
   ],
   "source": [
    "!cd ~/annotator_logs && ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qzYCO5j3EkAu",
    "outputId": "2f225170-73f1-41d9-de9a-2353e3d8610a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started - epochs: 5 - learning_rate: 0.005 - batch_size: 64 - training_examples: 25000 - classes: 2\n",
      "Epoch 0/5 - 6.51s - loss: 184.16612 - acc: 0.8153926 - batches: 391\n",
      "Epoch 1/5 - 5.91s - loss: 178.30418 - acc: 0.8358334 - batches: 391\n",
      "Epoch 2/5 - 5.65s - loss: 179.25107 - acc: 0.84036857 - batches: 391\n",
      "Epoch 3/5 - 6.31s - loss: 178.86932 - acc: 0.84237176 - batches: 391\n",
      "Epoch 4/5 - 5.80s - loss: 178.13194 - acc: 0.84489584 - batches: 391\n"
     ]
    }
   ],
   "source": [
    "!cat ~/annotator_logs/{sentimentdl.uid}.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NZJuax-nFHTQ"
   },
   "outputs": [],
   "source": [
    "prediction = pipelineModel.transform(testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yyjerNbgFZWg",
    "outputId": "fc7b650b-5e35-4f90-f40b-2deadfd0e049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.80      0.84     13575\n",
      "    positive       0.79      0.86      0.82     11425\n",
      "\n",
      "    accuracy                           0.83     25000\n",
      "   macro avg       0.83      0.83      0.83     25000\n",
      "weighted avg       0.83      0.83      0.83     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predsPd = prediction.select('label','text',\"class.result\").toPandas()\n",
    "predsPd['result'] = predsPd['result'].apply(lambda x : x[0])\n",
    "print (classification_report(predsPd['result'], predsPd['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZT6UH3NJ5heL"
   },
   "source": [
    "## Save and Restore\n",
    "### Pipeline Model\n",
    "\n",
    "It's pretty simple to save and restore an already trained Pipeline which is called `PipelineModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmfetBzV5nUn",
    "outputId": "181a1c0f-8ea1-4aa3-8c78-a4989ddb2920"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocumentAssembler_2f9c0247af19,\n",
       " REGEX_TOKENIZER_1f492672ab16,\n",
       " NORMALIZER_5f6019207ea3,\n",
       " STOPWORDS_CLEANER_3e62acb2648b,\n",
       " Doc2VecModel_7921b49ae1a0,\n",
       " ClassifierDLModel_4fb2630de611]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is our PipelineModel after it was trained via .fit()\n",
    "# as you can see we have all the stages inside this PipelineModel\n",
    "pipelineModel.stages\n",
    "# so once you save it on disk, it will include everything next time you load it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "L1zq3lyO8cOq"
   },
   "outputs": [],
   "source": [
    "pipelineModel.write().overwrite().save(\"./imdb_classifier_doc2vec_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5ZAJIbx8p20",
    "outputId": "bfc48c93-9915-44ee-d63e-f9c2f65cad0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocumentAssembler_2f9c0247af19,\n",
       " REGEX_TOKENIZER_1f492672ab16,\n",
       " NORMALIZER_5f6019207ea3,\n",
       " STOPWORDS_CLEANER_3e62acb2648b,\n",
       " Doc2VecModel_7921b49ae1a0,\n",
       " ClassifierDLModel_4fb2630de611]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load it back and try\n",
    "loadedPipelineModel = PipelineModel.load(\"./imdb_classifier_doc2vec_pipeline\")\n",
    "loadedPipelineModel.stages\n",
    "# we have all of our stages inside the loaded pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m68VFQuG9Dzf",
    "outputId": "328a4d6d-409e-4084-dee4-b685928cd9c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': ['positive'],\n",
       " 'cleanedToken': ['movie', 'good'],\n",
       " 'document': ['This movie was really good!'],\n",
       " 'normalized': ['this', 'movie', 'was', 'really', 'good'],\n",
       " 'sentence_embeddings': ['movie good'],\n",
       " 'token': ['This', 'movie', 'was', 'really', 'good', '!']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can use it with Spark NLP LightPipeline \n",
    "lp_loadedPipeline = LightPipeline(loadedPipelineModel)\n",
    "\n",
    "lp_loadedPipeline.annotate(\"This movie was really good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fOff6Sfr9VP6",
    "outputId": "99923eae-b173-4937-a820-3146e285bba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|result    |\n",
      "+----------+\n",
      "|[positive]|\n",
      "|[negative]|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# or you can use it via DataFrame\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "dfTest = spark.createDataFrame([\n",
    "    \"This movie is a delight for those of all ages. I have seen it several times and each time I am enchanted by the characters and magic. The cast is outstanding, the special effects delightful, everything most believable.\",\n",
    "    \"This film was to put it simply rubbish. The child actors couldn't act, as can be seen by Harry's supposed surprise on learning he's a wizard. I'm a wizard! is said with such indifference you'd think he's not surprised at all.\"\n",
    "], StringType()).toDF(\"text\")\n",
    "\n",
    "loadedPipelineModel\\\n",
    "  .transform(dfTest)\\\n",
    "  .select(\"class.result\")\\\n",
    "  .show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnufdTmL5oyQ"
   },
   "source": [
    "### Annotator Models\n",
    "Now let's say you would like to only save the trained annotators inside your pipeline so you can load them inside another custom Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_dzzYJYQ5pJa",
    "outputId": "83da0eae-3160-4b5f-983b-3101ff277ca3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocumentAssembler_2f9c0247af19,\n",
       " REGEX_TOKENIZER_1f492672ab16,\n",
       " NORMALIZER_5f6019207ea3,\n",
       " STOPWORDS_CLEANER_3e62acb2648b,\n",
       " Doc2VecModel_7921b49ae1a0,\n",
       " ClassifierDLModel_4fb2630de611]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all we need is to access that stage and save it on disk\n",
    "pipelineModel.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0cEyPk298cd",
    "outputId": "518b3aa8-070d-4cf8-e275-11eaa246dbb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifierDLModel_4fb2630de611\n",
      "Doc2VecModel_7921b49ae1a0\n"
     ]
    }
   ],
   "source": [
    "print(pipelineModel.stages[-1])\n",
    "print(pipelineModel.stages[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jM16Elha-Mj3"
   },
   "outputs": [],
   "source": [
    "# let's save our ClassifierDL - let's mention it was trained by doc2vec_aclImdb as well\n",
    "pipelineModel.stages[-1].write().overwrite().save(\"./classifierdl_doc2vec_aclImdb_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "AkFvbdQA-X1T"
   },
   "outputs": [],
   "source": [
    "# and here is our trained Doc2VecModel\n",
    "pipelineModel.stages[-2].write().overwrite().save(\"./doc2vec_aclImdb_model\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Train Doc2Vec and Text Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
