{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/classification/MultiClassifierDL_train_multi_label_E2E_challenge_classifier.ipynb)\n",
    "\n",
    "\n",
    "# Multi-label Text Classification: E2E Challenge using MultiClassifierDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell when you are using Spark NLP on Google Colab\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download our Toxic comments for tarining and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1922k  100 1922k    0     0  1337k      0  0:00:01  0:00:01 --:--:-- 1337k\n"
     ]
    }
   ],
   "source": [
    "!curl -O 'https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/e2e_challenge/e2e_train.snappy.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4.3.1'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark=sparknlp.start()\n",
    "print(\"Spark NLP version\")\n",
    "sparknlp.version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read our Toxi comments datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset, testDataset = spark.read.parquet(\"e2e_train.snappy.parquet\") \\\n",
    "  .randomSplit([0.9, 0.1], seed = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 ref|              labels|\n",
      "+--------------------+--------------------+\n",
      "|'Bibimbap House' ...|[name[Bibimbap Ho...|\n",
      "|'Browns Cambridge...|[name[Browns Camb...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDataset.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are lots of new lines in our comments which we can fix them with `DocumentAssembler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37762\n",
      "4299\n"
     ]
    }
   ],
   "source": [
    "print(trainDataset.cache().count())\n",
    "print(testDataset.cache().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# The actual text is in a column named ref\n",
    "document = DocumentAssembler()\\\n",
    "  .setInputCol(\"ref\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "# Here we use the state-of-the-art Universal Sentence Encoder model from TF Hub\n",
    "embeddings = UniversalSentenceEncoder.pretrained() \\\n",
    "  .setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "# We will use MultiClassifierDL built by using Bidirectional GRU and CNNs inside TensorFlow that supports up to 100 classes\n",
    "# We will use only 5 Epochs but feel free to increase it on your own dataset\n",
    "multiClassifier = MultiClassifierDLApproach()\\\n",
    "  .setInputCols(\"sentence_embeddings\")\\\n",
    "  .setOutputCol(\"category\")\\\n",
    "  .setLabelColumn(\"labels\")\\\n",
    "  .setBatchSize(128)\\\n",
    "  .setMaxEpochs(5)\\\n",
    "  .setLr(1e-3)\\\n",
    "  .setThreshold(0.5)\\\n",
    "  .setShufflePerEpoch(False)\\\n",
    "  .setEnableOutputLogs(True)\\\n",
    "  .setValidationSplit(0.1)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        embeddings,\n",
    "        multiClassifier\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(trainDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 216\n",
      "-rw-r--r-- 1 root root 456 20. Feb 17:41 ClassifierDLApproach_0375e3a8df00.log\n",
      "-rw-r--r-- 1 root root 918 20. Feb 17:38 ClassifierDLApproach_6fdb8a569309.log\n",
      "-rw-r--r-- 1 root root 446 20. Feb 15:55 ClassifierDLApproach_97ff5c76d735.log\n",
      "-rw-r--r-- 1 root root 438 20. Feb 17:38 ClassifierMetrics_09bd6fa2ecf7.log\n",
      "-rw-r--r-- 1 root root 317 10. Feb 16:54 ClassifierMetrics_17606bbb7d1f.log\n",
      "-rw-r--r-- 1 root root 571 20. Feb 17:45 ClassifierMetrics_176ce729caa6.log\n",
      "-rw-r--r-- 1 root root 313 10. Feb 16:54 ClassifierMetrics_1a6c515483ae.log\n",
      "-rw-r--r-- 1 root root 441 20. Feb 17:38 ClassifierMetrics_1e0c8ea78e67.log\n",
      "-rw-r--r-- 1 root root 323 10. Feb 16:54 ClassifierMetrics_2530315112a8.log\n",
      "-rw-r--r-- 1 root root 566 20. Feb 17:45 ClassifierMetrics_26e8744dc78c.log\n",
      "-rw-r--r-- 1 root root 565 20. Feb 17:45 ClassifierMetrics_284f041511fb.log\n",
      "-rw-r--r-- 1 root root 445 20. Feb 17:38 ClassifierMetrics_2b7b458fc84d.log\n",
      "-rw-r--r-- 1 root root 551 20. Feb 17:45 ClassifierMetrics_2fde2811a93c.log\n",
      "-rw-r--r-- 1 root root 133 20. Feb 17:52 ClassifierMetrics_387f03f0b7a0.log\n",
      "-rw-r--r-- 1 root root 314 10. Feb 16:54 ClassifierMetrics_3ccf43933a23.log\n",
      "-rw-r--r-- 1 root root 559 20. Feb 17:45 ClassifierMetrics_49fdfe64394f.log\n",
      "-rw-r--r-- 1 root root 449 20. Feb 17:38 ClassifierMetrics_4a2e4a7dac7c.log\n",
      "-rw-r--r-- 1 root root 325 10. Feb 16:54 ClassifierMetrics_55c7e364bf2b.log\n",
      "-rw-r--r-- 1 root root 128 20. Feb 17:52 ClassifierMetrics_66b22a01b7d3.log\n",
      "-rw-r--r-- 1 root root 555 20. Feb 17:45 ClassifierMetrics_71effbac2282.log\n",
      "-rw-r--r-- 1 root root 426 20. Feb 17:38 ClassifierMetrics_73fa92fe4be8.log\n",
      "-rw-r--r-- 1 root root 433 20. Feb 17:38 ClassifierMetrics_7764aa9b23e3.log\n",
      "-rw-r--r-- 1 root root 127 20. Feb 17:52 ClassifierMetrics_7dc198897be3.log\n",
      "-rw-r--r-- 1 root root 570 20. Feb 17:45 ClassifierMetrics_80808e6b12d1.log\n",
      "-rw-r--r-- 1 root root 445 20. Feb 17:38 ClassifierMetrics_890dcfe0db80.log\n",
      "-rw-r--r-- 1 root root 444 20. Feb 17:38 ClassifierMetrics_8ecc3f83e12d.log\n",
      "-rw-r--r-- 1 root root 325 10. Feb 16:54 ClassifierMetrics_9290b613e8d7.log\n",
      "-rw-r--r-- 1 root root 567 20. Feb 17:45 ClassifierMetrics_9ba6210e2c94.log\n",
      "-rw-r--r-- 1 root root 129 20. Feb 17:52 ClassifierMetrics_a579e188cf6b.log\n",
      "-rw-r--r-- 1 root root 317 10. Feb 16:54 ClassifierMetrics_aa0e2812a3b9.log\n",
      "-rw-r--r-- 1 root root 318 10. Feb 16:54 ClassifierMetrics_ad4cb4a650fa.log\n",
      "-rw-r--r-- 1 root root 129 20. Feb 17:52 ClassifierMetrics_b901376087b3.log\n",
      "-rw-r--r-- 1 root root 564 20. Feb 17:45 ClassifierMetrics_d302c6e17f10.log\n",
      "-rw-r--r-- 1 root root 452 20. Feb 17:38 ClassifierMetrics_e0da6952b2c6.log\n",
      "-rw-r--r-- 1 root root 567 20. Feb 17:45 ClassifierMetrics_e29d5ee5fe87.log\n",
      "-rw-r--r-- 1 root root 312 10. Feb 16:54 ClassifierMetrics_efc7f6345e79.log\n",
      "-rw-r--r-- 1 root root 319 10. Feb 16:54 ClassifierMetrics_f571876aaa09.log\n",
      "-rw-r--r-- 1 root root 436 20. Feb 17:38 ClassifierMetrics_fdc5fa307baf.log\n",
      "-rw-r--r-- 1 root root 922 20. Feb 17:45 MultiClassifierDLApproach_0420b23f4851.log\n",
      "-rw-r--r-- 1 root root 792 20. Feb 17:52 MultiClassifierDLApproach_73f999799c2b.log\n",
      "-rw-r--r-- 1 root root 320 26. Okt 09:23 NerDL_0f47f69f09e6.log\n",
      "-rw-r--r-- 1 root root 320  2. Aug 2022  NerDL_10e337c8a3ef.log\n",
      "-rw-r--r-- 1 root root 320 12. Jan 17:31 NerDL_18e7b1673dab.log\n",
      "-rw-r--r-- 1 root root 320  2. Aug 2022  NerDL_27f18f749174.log\n",
      "-rw-r--r-- 1 root root 320  2. Aug 2022  NerDL_3ae0321ce66a.log\n",
      "-rw-r--r-- 1 root root 319 26. Okt 09:13 NerDL_568d747656b8.log\n",
      "-rw-r--r-- 1 root root 320 26. Okt 09:03 NerDL_5970e276422f.log\n",
      "-rw-r--r-- 1 root root 320 16. Jan 11:10 NerDL_759a68c3769d.log\n",
      "-rw-r--r-- 1 root root 320  3. Nov 19:22 NerDL_891f9b941985.log\n",
      "-rw-r--r-- 1 root root 320  2. Feb 2022  NerDL_8e8184f259cb.log\n",
      "-rw-r--r-- 1 root root 320 27. Okt 13:02 NerDL_add5b34b2ecb.log\n",
      "-rw-r--r-- 1 root root 320 21. Okt 19:06 NerDL_bc57a96c68c3.log\n",
      "-rw-r--r-- 1 root root 320 12. Jan 16:47 NerDL_ff0a43f20378.log\n",
      "-rw-r--r-- 1 root root 897 10. Feb 16:54 SentimentDLApproach_98dfd2c1fdee.log\n"
     ]
    }
   ],
   "source": [
    "!ls -l ~/annotator_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /home/root/annotator_logs/MultiClassifierDLApproach_b80de1f04776.log: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat ~/annotator_logs/MultiClassifierDLApproach_b80de1f04776.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our trained multi-label classifier model to be loaded in our prediction pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel.stages[-1].write().overwrite().save('tmp_multi_classifierDL_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load saved pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"ref\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "use = UniversalSentenceEncoder.pretrained() \\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "multiClassifier = MultiClassifierDLModel.load(\"tmp_multi_classifierDL_model\") \\\n",
    "  .setInputCols([\"sentence_embeddings\"])\\\n",
    "  .setOutputCol(\"category\")\\\n",
    "  .setThreshold(0.5)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        use,\n",
    "        multiClassifier\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use our testing datasets to evaluate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name[Bibimbap House]', 'name[Wildwood]', 'name[Cotto]', 'name[Clowns]', 'near[Burger King]', 'name[The Dumpling Tree]', 'name[The Vaults]', 'near[Crowne Plaza Hotel]', 'name[The Golden Palace]', 'name[The Rice Boat]', 'customer rating[high]', 'near[Avalon]', 'name[Alimentum]', 'near[The Bakers]', 'name[The Waterman]', 'near[Ranch]', 'name[The Olive Grove]', 'name[The Eagle]', 'name[The Wrestlers]', 'eatType[restaurant]', 'near[All Bar One]', 'customer rating[low]', 'near[Café Sicilia]', 'near[Yippee Noodle Bar]', 'food[Indian]', 'eatType[pub]', 'name[Green Man]', 'name[Strada]', 'near[Café Adriatic]', 'eatType[coffee shop]', 'name[Loch Fyne]', 'customer rating[5 out of 5]', 'near[Express by Holiday Inn]', 'food[French]', 'name[The Mill]', 'food[Japanese]', 'name[Travellers Rest Beefeater]', 'name[The Plough]', 'name[Cocum]', 'near[The Six Bells]', 'name[The Phoenix]', 'priceRange[cheap]', 'name[Midsummer House]', 'near[Rainbow Vegetarian Café]', 'near[The Rice Boat]', 'customer rating[1 out of 5]', 'customer rating[3 out of 5]', 'name[The Cricketers]', 'area[riverside]', 'priceRange[£20-25]', 'name[Blue Spice]', 'priceRange[moderate]', 'priceRange[less than £20]', 'priceRange[high]', 'name[The Golden Curry]', 'name[Giraffe]', 'customer rating[average]', 'name[Aromi]', 'name[The Twenty Two]', 'food[Fast food]', 'name[Browns Cambridge]', 'near[Café Rouge]', 'familyFriendly[no]', 'area[city centre]', 'food[Chinese]', 'name[Taste of Cambridge]', 'food[Italian]', 'name[Zizzi]', 'near[Raja Indian Cuisine]', 'priceRange[more than £30]', 'name[The Punter]', 'food[English]', 'near[Clare Hall]', 'near[The Portland Arms]', 'name[The Cambridge Blue]', 'near[The Sorrento]', 'near[Café Brazil]', 'familyFriendly[yes]', 'name[Fitzbillies]']\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "# let's see our labels:\n",
    "print(pipeline.fit(testDataset).stages[2].getClasses())\n",
    "print(len(pipeline.fit(testDataset).stages[2].getClasses()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pipeline.fit(testDataset).transform(testDataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              labels|                 ref|              result|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[name[Alimentum],...|1 out of 5 stars ...|[name[Alimentum],...|\n",
      "|[name[The Punter]...|1 star budget, fa...|[near[Café Sicili...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.select('labels', 'ref', 'category.result').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86       790\n",
      "           1       0.86      0.87      0.86      1774\n",
      "           2       0.68      0.06      0.11       431\n",
      "           3       0.70      0.12      0.20       422\n",
      "           4       0.72      0.25      0.37       525\n",
      "           5       0.78      0.37      0.50       592\n",
      "           6       0.68      0.18      0.29       421\n",
      "           7       0.72      0.21      0.32       512\n",
      "           8       0.99      0.95      0.97      1043\n",
      "           9       0.97      0.88      0.92       660\n",
      "          10       0.84      0.52      0.64       306\n",
      "          11       0.81      0.62      0.70       932\n",
      "          12       0.83      0.80      0.81      1777\n",
      "          13       0.95      0.90      0.92       292\n",
      "          14       0.94      0.50      0.66       411\n",
      "          15       0.93      0.81      0.86       599\n",
      "          16       0.90      0.73      0.80       564\n",
      "          17       0.98      0.89      0.93       487\n",
      "          18       0.93      0.77      0.84       588\n",
      "          19       0.96      0.83      0.89       635\n",
      "          20       0.90      0.78      0.84       175\n",
      "          21       0.83      0.17      0.28        88\n",
      "          22       1.00      0.84      0.91        68\n",
      "          23       0.98      0.92      0.95       143\n",
      "          24       0.97      0.88      0.92       193\n",
      "          25       1.00      0.79      0.88        84\n",
      "          26       0.85      0.49      0.62       103\n",
      "          27       0.95      0.85      0.90       124\n",
      "          28       0.81      0.34      0.47       191\n",
      "          29       0.96      0.79      0.87        68\n",
      "          30       0.97      0.83      0.89       102\n",
      "          31       0.96      0.84      0.89       152\n",
      "          32       0.94      0.68      0.79       124\n",
      "          33       0.94      0.86      0.90       174\n",
      "          34       1.00      0.07      0.12        45\n",
      "          35       0.96      0.89      0.93        76\n",
      "          36       1.00      0.77      0.87        73\n",
      "          37       0.97      0.73      0.83        79\n",
      "          38       0.97      0.97      0.97       202\n",
      "          39       0.99      0.98      0.98       211\n",
      "          40       0.98      0.54      0.70        96\n",
      "          41       0.94      0.80      0.87        82\n",
      "          42       0.97      0.68      0.80       109\n",
      "          43       0.96      0.94      0.95       138\n",
      "          44       0.93      0.54      0.68        70\n",
      "          45       0.98      0.80      0.88       107\n",
      "          46       0.88      0.83      0.85       303\n",
      "          47       0.94      0.57      0.71       126\n",
      "          48       0.94      0.86      0.90       119\n",
      "          49       0.96      0.47      0.63       279\n",
      "          50       0.99      0.90      0.94       171\n",
      "          51       1.00      0.49      0.65        35\n",
      "          52       0.82      0.42      0.56       112\n",
      "          53       0.47      0.12      0.19        77\n",
      "          54       0.92      0.56      0.70       158\n",
      "          55       1.00      0.83      0.91        35\n",
      "          56       1.00      0.93      0.96       146\n",
      "          57       0.95      0.71      0.81        49\n",
      "          58       0.98      0.89      0.94        73\n",
      "          59       0.99      0.98      0.98       212\n",
      "          60       0.95      0.57      0.71       102\n",
      "          61       0.99      0.88      0.93        93\n",
      "          62       0.99      0.93      0.96       120\n",
      "          63       1.00      0.98      0.99       209\n",
      "          64       1.00      0.99      1.00       148\n",
      "          65       1.00      0.98      0.99        62\n",
      "          66       0.95      0.73      0.82        99\n",
      "          67       0.89      0.78      0.83        95\n",
      "          68       0.98      0.97      0.97       157\n",
      "          69       0.79      0.76      0.77       111\n",
      "          70       0.97      0.70      0.82        44\n",
      "          71       0.91      0.63      0.75        93\n",
      "          72       0.99      0.94      0.97       133\n",
      "          73       0.73      0.23      0.35       459\n",
      "          74       0.77      0.07      0.13       559\n",
      "          75       0.64      0.21      0.32       543\n",
      "          76       0.80      0.27      0.40       580\n",
      "          77       0.70      0.11      0.19       426\n",
      "          78       0.72      0.06      0.12       401\n",
      "\n",
      "   micro avg       0.90      0.64      0.74     23167\n",
      "   macro avg       0.90      0.66      0.73     23167\n",
      "weighted avg       0.87      0.64      0.70     23167\n",
      " samples avg       0.90      0.63      0.72     23167\n",
      "\n",
      "F1 micro averaging: 0.7445738686610911\n",
      "ROC:  0.815347330332481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/root/.conda/envs/sparknlp/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "preds_df = preds.select('labels', 'category.result').toPandas()\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "y_true = mlb.fit_transform(preds_df['labels'])\n",
    "y_pred = mlb.fit_transform(preds_df['result'])\n",
    "\n",
    "print(\"Classification report: \\n\", (classification_report(y_true, y_pred)))\n",
    "print(\"F1 micro averaging:\",(f1_score(y_true, y_pred, average='micro')))\n",
    "print(\"ROC: \",(roc_auc_score(y_true, y_pred, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            metadata|\n",
      "+--------------------+\n",
      "|[{name[Alimentum]...|\n",
      "|[{name[Alimentum]...|\n",
      "|[{name[Alimentum]...|\n",
      "|[{name[Alimentum]...|\n",
      "|[{name[Alimentum]...|\n",
      "|[{name[Alimentum]...|\n",
      "|[{name[Alimentum]...|\n",
      "|[{name[Alimentum]...|\n",
      "|[{name[Alimentum]...|\n",
      "|[{name[Alimentum]...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.select(\"category.metadata\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- metadata: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.select(\"category.metadata\").printSchema()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MultiClassifierDL_train_multi_label_E2E_challenge_classifier.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "name": "MultiClassifierDL_Train_multi_label_toxic_classifier",
  "notebookId": 1952370652427552,
  "nteract": {
   "version": "0.21.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
