{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/classification/MultiClassifierDL_train_multi_label_toxic_classifier.ipynb)\n",
    "\n",
    "\n",
    "# Multi-label Text Classification of Toxic Comments using MultiClassifierDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell when you are using Spark NLP on Google Colab\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download our Toxic comments for tarining and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2702k  100 2702k    0     0  1720k      0  0:00:01  0:00:01 --:--:-- 1720k\n"
     ]
    }
   ],
   "source": [
    "!curl -O 'https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/toxic_comments/toxic_train.snappy.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  289k  100  289k    0     0   254k      0  0:00:01  0:00:01 --:--:--  254k\n"
     ]
    }
   ],
   "source": [
    "!curl -O 'https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/toxic_comments/toxic_test.snappy.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4.3.1'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark=sparknlp.start()\n",
    "print(\"Spark NLP version\")\n",
    "sparknlp.version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read our Toxi comments datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = spark.read.parquet(\"toxic_train.snappy.parquet\").repartition(120)\n",
    "testDataset = spark.read.parquet(\"toxic_test.snappy.parquet\").repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-------+\n",
      "|              id|                text| labels|\n",
      "+----------------+--------------------+-------+\n",
      "|e63f1cc4b0b9959f|EAT SHIT HORSE FA...|[toxic]|\n",
      "|ed58abb40640f983|PN News\\nYou mean...|[toxic]|\n",
      "+----------------+--------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDataset.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are lots of new lines in our comments which we can fix them with `DocumentAssembler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14620\n",
      "1605\n"
     ]
    }
   ],
   "source": [
    "print(trainDataset.cache().count())\n",
    "print(testDataset.cache().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Let's use shrink to remove new lines in the comments\n",
    "document = DocumentAssembler()\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"document\")\\\n",
    "  .setCleanupMode(\"shrink\")\n",
    "\n",
    "# Here we use the state-of-the-art Universal Sentence Encoder model from TF Hub\n",
    "embeddings = UniversalSentenceEncoder.pretrained() \\\n",
    "  .setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "# We will use MultiClassifierDL built by using Bidirectional GRU and CNNs inside TensorFlow that supports up to 100 classes\n",
    "# We will use only 5 Epochs but feel free to increase it on your own dataset\n",
    "multiClassifier = MultiClassifierDLApproach()\\\n",
    "  .setInputCols(\"sentence_embeddings\")\\\n",
    "  .setOutputCol(\"category\")\\\n",
    "  .setLabelColumn(\"labels\")\\\n",
    "  .setBatchSize(128)\\\n",
    "  .setMaxEpochs(5)\\\n",
    "  .setLr(1e-3)\\\n",
    "  .setThreshold(0.5)\\\n",
    "  .setShufflePerEpoch(False)\\\n",
    "  .setEnableOutputLogs(True)\\\n",
    "  .setValidationSplit(0.1)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        embeddings,\n",
    "        multiClassifier\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(trainDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 240\n",
      "-rw-r--r-- 1 root root 456 20. Feb 17:41 ClassifierDLApproach_0375e3a8df00.log\n",
      "-rw-r--r-- 1 root root 918 20. Feb 17:38 ClassifierDLApproach_6fdb8a569309.log\n",
      "-rw-r--r-- 1 root root 446 20. Feb 15:55 ClassifierDLApproach_97ff5c76d735.log\n",
      "-rw-r--r-- 1 root root 438 20. Feb 17:38 ClassifierMetrics_09bd6fa2ecf7.log\n",
      "-rw-r--r-- 1 root root 317 10. Feb 16:54 ClassifierMetrics_17606bbb7d1f.log\n",
      "-rw-r--r-- 1 root root 571 20. Feb 17:45 ClassifierMetrics_176ce729caa6.log\n",
      "-rw-r--r-- 1 root root 313 10. Feb 16:54 ClassifierMetrics_1a6c515483ae.log\n",
      "-rw-r--r-- 1 root root 441 20. Feb 17:38 ClassifierMetrics_1e0c8ea78e67.log\n",
      "-rw-r--r-- 1 root root 323 10. Feb 16:54 ClassifierMetrics_2530315112a8.log\n",
      "-rw-r--r-- 1 root root 566 20. Feb 17:45 ClassifierMetrics_26e8744dc78c.log\n",
      "-rw-r--r-- 1 root root 565 20. Feb 17:45 ClassifierMetrics_284f041511fb.log\n",
      "-rw-r--r-- 1 root root 445 20. Feb 17:38 ClassifierMetrics_2b7b458fc84d.log\n",
      "-rw-r--r-- 1 root root 551 20. Feb 17:45 ClassifierMetrics_2fde2811a93c.log\n",
      "-rw-r--r-- 1 root root 133 20. Feb 17:52 ClassifierMetrics_387f03f0b7a0.log\n",
      "-rw-r--r-- 1 root root 314 10. Feb 16:54 ClassifierMetrics_3ccf43933a23.log\n",
      "-rw-r--r-- 1 root root 132 20. Feb 17:59 ClassifierMetrics_41db1fc54c4f.log\n",
      "-rw-r--r-- 1 root root 559 20. Feb 17:45 ClassifierMetrics_49fdfe64394f.log\n",
      "-rw-r--r-- 1 root root 449 20. Feb 17:38 ClassifierMetrics_4a2e4a7dac7c.log\n",
      "-rw-r--r-- 1 root root 126 20. Feb 17:59 ClassifierMetrics_4a623cb68ecc.log\n",
      "-rw-r--r-- 1 root root 325 10. Feb 16:54 ClassifierMetrics_55c7e364bf2b.log\n",
      "-rw-r--r-- 1 root root 128 20. Feb 17:52 ClassifierMetrics_66b22a01b7d3.log\n",
      "-rw-r--r-- 1 root root 129 20. Feb 17:59 ClassifierMetrics_6f4f96da828e.log\n",
      "-rw-r--r-- 1 root root 555 20. Feb 17:45 ClassifierMetrics_71effbac2282.log\n",
      "-rw-r--r-- 1 root root 129 20. Feb 17:59 ClassifierMetrics_73bcd38f71f7.log\n",
      "-rw-r--r-- 1 root root 426 20. Feb 17:38 ClassifierMetrics_73fa92fe4be8.log\n",
      "-rw-r--r-- 1 root root 433 20. Feb 17:38 ClassifierMetrics_7764aa9b23e3.log\n",
      "-rw-r--r-- 1 root root 127 20. Feb 17:52 ClassifierMetrics_7dc198897be3.log\n",
      "-rw-r--r-- 1 root root 570 20. Feb 17:45 ClassifierMetrics_80808e6b12d1.log\n",
      "-rw-r--r-- 1 root root 445 20. Feb 17:38 ClassifierMetrics_890dcfe0db80.log\n",
      "-rw-r--r-- 1 root root 444 20. Feb 17:38 ClassifierMetrics_8ecc3f83e12d.log\n",
      "-rw-r--r-- 1 root root 325 10. Feb 16:54 ClassifierMetrics_9290b613e8d7.log\n",
      "-rw-r--r-- 1 root root 567 20. Feb 17:45 ClassifierMetrics_9ba6210e2c94.log\n",
      "-rw-r--r-- 1 root root 129 20. Feb 17:52 ClassifierMetrics_a579e188cf6b.log\n",
      "-rw-r--r-- 1 root root 317 10. Feb 16:54 ClassifierMetrics_aa0e2812a3b9.log\n",
      "-rw-r--r-- 1 root root 318 10. Feb 16:54 ClassifierMetrics_ad4cb4a650fa.log\n",
      "-rw-r--r-- 1 root root 129 20. Feb 17:52 ClassifierMetrics_b901376087b3.log\n",
      "-rw-r--r-- 1 root root 564 20. Feb 17:45 ClassifierMetrics_d302c6e17f10.log\n",
      "-rw-r--r-- 1 root root 452 20. Feb 17:38 ClassifierMetrics_e0da6952b2c6.log\n",
      "-rw-r--r-- 1 root root 567 20. Feb 17:45 ClassifierMetrics_e29d5ee5fe87.log\n",
      "-rw-r--r-- 1 root root 312 10. Feb 16:54 ClassifierMetrics_efc7f6345e79.log\n",
      "-rw-r--r-- 1 root root 319 10. Feb 16:54 ClassifierMetrics_f571876aaa09.log\n",
      "-rw-r--r-- 1 root root 131 20. Feb 17:59 ClassifierMetrics_fbe1c172154f.log\n",
      "-rw-r--r-- 1 root root 436 20. Feb 17:38 ClassifierMetrics_fdc5fa307baf.log\n",
      "-rw-r--r-- 1 root root 922 20. Feb 17:45 MultiClassifierDLApproach_0420b23f4851.log\n",
      "-rw-r--r-- 1 root root 792 20. Feb 17:52 MultiClassifierDLApproach_73f999799c2b.log\n",
      "-rw-r--r-- 1 root root 792 20. Feb 17:59 MultiClassifierDLApproach_e6ae1c4549a9.log\n",
      "-rw-r--r-- 1 root root 320 26. Okt 09:23 NerDL_0f47f69f09e6.log\n",
      "-rw-r--r-- 1 root root 320  2. Aug 2022  NerDL_10e337c8a3ef.log\n",
      "-rw-r--r-- 1 root root 320 12. Jan 17:31 NerDL_18e7b1673dab.log\n",
      "-rw-r--r-- 1 root root 320  2. Aug 2022  NerDL_27f18f749174.log\n",
      "-rw-r--r-- 1 root root 320  2. Aug 2022  NerDL_3ae0321ce66a.log\n",
      "-rw-r--r-- 1 root root 319 26. Okt 09:13 NerDL_568d747656b8.log\n",
      "-rw-r--r-- 1 root root 320 26. Okt 09:03 NerDL_5970e276422f.log\n",
      "-rw-r--r-- 1 root root 320 16. Jan 11:10 NerDL_759a68c3769d.log\n",
      "-rw-r--r-- 1 root root 320  3. Nov 19:22 NerDL_891f9b941985.log\n",
      "-rw-r--r-- 1 root root 320  2. Feb 2022  NerDL_8e8184f259cb.log\n",
      "-rw-r--r-- 1 root root 320 27. Okt 13:02 NerDL_add5b34b2ecb.log\n",
      "-rw-r--r-- 1 root root 320 21. Okt 19:06 NerDL_bc57a96c68c3.log\n",
      "-rw-r--r-- 1 root root 320 12. Jan 16:47 NerDL_ff0a43f20378.log\n",
      "-rw-r--r-- 1 root root 897 10. Feb 16:54 SentimentDLApproach_98dfd2c1fdee.log\n"
     ]
    }
   ],
   "source": [
    "!ls -l ~/annotator_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /home/root/annotator_logs/MultiClassifierDLApproach_d670b2c2d0df.log: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat ~/annotator_logs/MultiClassifierDLApproach_d670b2c2d0df.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our trained multi-label classifier model to be loaded in our prediction pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel.stages[-1].write().overwrite().save('tmp_multi_classifierDL_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load saved pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "use = UniversalSentenceEncoder.pretrained() \\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "multiClassifier = MultiClassifierDLModel.load(\"tmp_multi_classifierDL_model\") \\\n",
    "  .setInputCols([\"sentence_embeddings\"])\\\n",
    "  .setOutputCol(\"category\")\\\n",
    "  .setThreshold(0.5)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        use,\n",
    "        multiClassifier\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use our testing datasets to evaluate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toxic', 'severe_toxic', 'identity_hate', 'insult', 'obscene', 'threat']\n"
     ]
    }
   ],
   "source": [
    "# let's see our labels:\n",
    "print(pipeline.fit(testDataset).stages[2].getClasses())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pipeline.fit(testDataset).transform(testDataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------------+\n",
      "|          labels|                text|          result|\n",
      "+----------------+--------------------+----------------+\n",
      "|         [toxic]|Vegan \\n\\nWhat in...|         [toxic]|\n",
      "|[toxic, obscene]|Fight Club! F**k ...|[toxic, obscene]|\n",
      "+----------------+--------------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.select('labels','text',\"category.result\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.35      0.42       127\n",
      "           1       0.73      0.62      0.67       761\n",
      "           2       0.79      0.67      0.73       824\n",
      "           3       0.50      0.15      0.23       147\n",
      "           4       0.73      0.38      0.50        50\n",
      "           5       0.94      1.00      0.97      1504\n",
      "\n",
      "   micro avg       0.84      0.77      0.80      3413\n",
      "   macro avg       0.70      0.53      0.59      3413\n",
      "weighted avg       0.82      0.77      0.78      3413\n",
      " samples avg       0.86      0.80      0.79      3413\n",
      "\n",
      "F1 micro averaging: 0.802391537636057\n",
      "ROC:  0.8437377009561553\n"
     ]
    }
   ],
   "source": [
    "preds_df = preds.select('labels', 'category.result').toPandas()\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "y_true = mlb.fit_transform(preds_df['labels'])\n",
    "y_pred = mlb.fit_transform(preds_df['result'])\n",
    "\n",
    "\n",
    "print(\"Classification report: \\n\", (classification_report(y_true, y_pred)))\n",
    "print(\"F1 micro averaging:\",(f1_score(y_true, y_pred, average='micro')))\n",
    "print(\"ROC: \",(roc_auc_score(y_true, y_pred, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|metadata                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{obscene -> 0.07668713, identity_hate -> 0.08003414, toxic -> 0.8547158, insult -> 0.1457338, severe_toxic -> 0.010274887, threat -> 0.0013722777, sentence -> 0}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|[{obscene -> 0.74973583, identity_hate -> 0.026573211, toxic -> 0.9745251, insult -> 0.4271415, severe_toxic -> 0.07580829, threat -> 0.0124256015, sentence -> 0}, {obscene -> 0.74973583, identity_hate -> 0.026573211, toxic -> 0.9745251, insult -> 0.4271415, severe_toxic -> 0.07580829, threat -> 0.0124256015, sentence -> 0}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|[{obscene -> 0.2895946, identity_hate -> 0.017944932, toxic -> 0.88083, insult -> 0.34860942, severe_toxic -> 0.012507945, threat -> 0.0027540624, sentence -> 0}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|[{obscene -> 0.14852577, identity_hate -> 0.13101593, toxic -> 0.9353854, insult -> 0.36898047, severe_toxic -> 0.020003498, threat -> 0.001435101, sentence -> 0}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|[{obscene -> 0.20265803, identity_hate -> 0.0071552694, toxic -> 0.9020696, insult -> 0.20001402, severe_toxic -> 0.014318436, threat -> 0.0016921461, sentence -> 0}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|[{obscene -> 0.2769695, identity_hate -> 0.014545143, toxic -> 0.82669973, insult -> 0.26631594, severe_toxic -> 0.041005254, threat -> 0.038255185, sentence -> 0}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|[{obscene -> 0.99235296, identity_hate -> 0.57809556, toxic -> 0.9924389, insult -> 0.9226622, severe_toxic -> 0.6050372, threat -> 0.058374584, sentence -> 0}, {obscene -> 0.99235296, identity_hate -> 0.57809556, toxic -> 0.9924389, insult -> 0.9226622, severe_toxic -> 0.6050372, threat -> 0.058374584, sentence -> 0}, {obscene -> 0.99235296, identity_hate -> 0.57809556, toxic -> 0.9924389, insult -> 0.9226622, severe_toxic -> 0.6050372, threat -> 0.058374584, sentence -> 0}, {obscene -> 0.99235296, identity_hate -> 0.57809556, toxic -> 0.9924389, insult -> 0.9226622, severe_toxic -> 0.6050372, threat -> 0.058374584, sentence -> 0}, {obscene -> 0.99235296, identity_hate -> 0.57809556, toxic -> 0.9924389, insult -> 0.9226622, severe_toxic -> 0.6050372, threat -> 0.058374584, sentence -> 0}]|\n",
      "|[{obscene -> 0.115439504, identity_hate -> 0.019395113, toxic -> 0.90979856, insult -> 0.1980845, severe_toxic -> 0.007957667, threat -> 0.00770548, sentence -> 0}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|[{obscene -> 0.835811, identity_hate -> 0.0037145615, toxic -> 0.9678079, insult -> 0.5513622, severe_toxic -> 0.030572414, threat -> 3.7184358E-4, sentence -> 0}, {obscene -> 0.835811, identity_hate -> 0.0037145615, toxic -> 0.9678079, insult -> 0.5513622, severe_toxic -> 0.030572414, threat -> 3.7184358E-4, sentence -> 0}, {obscene -> 0.835811, identity_hate -> 0.0037145615, toxic -> 0.9678079, insult -> 0.5513622, severe_toxic -> 0.030572414, threat -> 3.7184358E-4, sentence -> 0}]                                                                                                                                                                                                                                                                                                                       |\n",
      "|[{obscene -> 0.6326457, identity_hate -> 0.006464809, toxic -> 0.94940895, insult -> 0.56411535, severe_toxic -> 0.03255585, threat -> 0.0070275366, sentence -> 0}, {obscene -> 0.6326457, identity_hate -> 0.006464809, toxic -> 0.94940895, insult -> 0.56411535, severe_toxic -> 0.03255585, threat -> 0.0070275366, sentence -> 0}, {obscene -> 0.6326457, identity_hate -> 0.006464809, toxic -> 0.94940895, insult -> 0.56411535, severe_toxic -> 0.03255585, threat -> 0.0070275366, sentence -> 0}]                                                                                                                                                                                                                                                                                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.select(\"category.metadata\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- metadata: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.select(\"category.metadata\").printSchema()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MultiClassifierDL_train_multi_label_toxic_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "name": "MultiClassifierDL_Train_multi_label_toxic_classifier",
  "notebookId": 1952370652427552,
  "nteract": {
   "version": "0.21.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
