{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/dl-ner/ner_logs_GCP.ipynb)\n",
    "\n",
    "\n",
    "# Exporting Logs in GCP with NER training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark NLP you can configure the location to download the logs of training NER models. Starting at Spark NLP 5.1.0, you can set a GCP Storage URI, or Azure Storage URI or DBFS paths like HDFS or Databricks FS.\n",
    "\n",
    "In this notebook, we are going to see the steps required to use an external GCP Storage URI to store the logs of traning an NER model\n",
    "\n",
    "To do this, we need to configure the spark session with the required settings for Spark NLP and Spark ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1cN4RFm0J3g",
    "outputId": "1e71a3ef-7537-4760-f6f8-35bdbbda6bc8"
   },
   "source": [
    "### Spark NLP Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLGkqRmq39ci"
   },
   "source": [
    "`project_id`: We need to know the ProjectId of our GCP Storage. This is defined in `spark.jsl.settings.gcp.project_id`\n",
    "\n",
    "To integrage with GCP, we need to setup Application Default Credentials (ADC) for GCP. You can check how to configure it in the official [GCP documentation](https://cloud.google.com/docs/authentication/provide-credentials-adc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXtSNHh_3_iR",
    "outputId": "07ff761f-cfd5-48cc-badf-805d66411ce0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to the following link in your browser:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=tFHFF9TX2zTmdvgrv8nJCMR5hzJSpO&prompt=consent&access_type=offline&code_challenge=thIesdTdE1baCvLRj3XVbC7hxiMAZpUqFh4gGgFVwpw&code_challenge_method=S256\n",
      "\n",
      "Enter authorization code: 4/0AbUR2VNBbJPmIJ4S1SGDL0fcasgJGz2NBdJczgZhSjdYAipIjBskSuZnBTFt7oX8aD3r1w\n",
      "\n",
      "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\u001b[1;33mWARNING:\u001b[0m \n",
      "Cannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n"
     ]
    }
   ],
   "source": [
    "! gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpBoet7p4Ctl",
    "outputId": "bdbca500-f9bf-4ddd-95ae-9971073b57b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/.config/application_default_credentials.json\n"
     ]
    }
   ],
   "source": [
    "! ls /content/.config/application_default_credentials.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark ML Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark ML requires the following configuration to load a model from GCP using ADC:\n",
    "\n",
    "1. GCP connector: You need to identify your hadoop version and set the required dependency in `spark.jars.packages`\n",
    "2. ADC credentials: After following the instructions to setup ADC, you will have a JSON file that holds your authenticiation information. This file is setup in `spark.hadoop.google.cloud.auth.service.account.json.keyfile`\n",
    "3. Hadoop File System: You also need to setup the Hadoop implementation to work with GCP Storage as file system. This is define in `spark.hadoop.fs.gs.impl`\n",
    "3. Finally, to mitigate conflicts between Spark's dependencies and user dependencies. You must define `spark.driver.userClassPathFirst` as true. You may also need to define `spark.executor.userClassPathFirst` as true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tF2_C8XG5gWs",
    "outputId": "d674c451-c3cc-4876-bb70-45a2c08b0946"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.7/486.7 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.0/199.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# Only run this Cell when you are using Spark NLP on Google Colab\n",
    "!wget https://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at a simple example the spark session creation below to see how to define each of the configurations with its values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter your GCP ProjectId:\")\n",
    "PROJECT_ID = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "import pyspark\n",
    "\n",
    "json_keyfile = \"/content/.config/application_default_credentials.json\"\n",
    "\n",
    "#GCP Storage configuration\n",
    "gcp_params = {\n",
    "    \"spark.jars.packages\": \"com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.8\",\n",
    "    \"spark.hadoop.fs.gs.impl\": \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\",\n",
    "    \"spark.driver.userClassPathFirst\": \"true\",\n",
    "    \"spark.hadoop.google.cloud.auth.service.account.json.keyfile\": json_keyfile,\n",
    "    \"spark.jsl.settings.gcp.project_id\": PROJECT_ID\n",
    "}\n",
    "\n",
    "spark = sparknlp.start(params=gcp_params)\n",
    "\n",
    "print(\"Apache Spark version: {}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPSvZmW84Ht3"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.training import CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NCNZgCM4J43",
    "outputId": "1664f402-417c-4486-d32c-bf51978f1754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|John Smith works ...|[{document, 0, 35...|[{document, 0, 35...|[{token, 0, 3, Jo...|[{pos, 0, 3, NNP,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "training_data = CoNLL().readDataset(spark, './test_ner_dataset.txt')\n",
    "training_data.show(3)\n",
    "\n",
    "embeddings = WordEmbeddingsModel.pretrained(\"glove_100d\")\n",
    "ready_data = embeddings.transform(training_data).cache()\n",
    "\n",
    "ner_tagger = NerDLApproach() \\\n",
    "    .setInputCols(\"sentence\", \"token\", \"embeddings\") \\\n",
    "    .setLabelColumn(\"label\") \\\n",
    "    .setOutputCol(\"ner\") \\\n",
    "    .setMaxEpochs(1) \\\n",
    "    .setMaxEpochs(5) \\\n",
    "    .setRandomSeed(0) \\\n",
    "    .setVerbose(2) \\\n",
    "    .setDropout(0.8) \\\n",
    "    .setBatchSize(18) \\\n",
    "    .setEnableOutputLogs(True) \\\n",
    "    .setOutputLogsPath(\"gs://test-bucket-danilo/logs\")\n",
    "\n",
    "ner_model = ner_tagger.fit(ready_data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
