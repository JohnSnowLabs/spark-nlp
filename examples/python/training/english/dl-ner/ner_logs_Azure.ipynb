{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSJCkjMNypFd",
    "outputId": "3156704e-5e8d-47dd-e9f3-6908cc0c26c7"
   },
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/dl-ner/ner_logs_Azure.ipynb)\n",
    "\n",
    "\n",
    "# Exporting Logs in Azure with NER training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1cN4RFm0J3g",
    "outputId": "00f29343-1b34-4db2-80a1-9cdc116e5f5e"
   },
   "source": [
    "In Spark NLP you can configure the location to download the logs of training NER models. Starting at Spark NLP 5.1.0, you can set a GCP Storage URI, or Azure Storage URI, or DBFS paths like HDFS or Databricks FS.\n",
    "\n",
    "In this notebook, we are going to see the steps required to use an external Azure Storage URI to store the logs of traning an NER model\n",
    "\n",
    "To do this, we need to configure the spark session with the required settings for Spark NLP and Spark ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell when you are using Spark NLP on Google Colab\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLGkqRmq39ci"
   },
   "source": [
    "### Spark NLP Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tF2_C8XG5gWs",
    "outputId": "59cdafca-85bd-401e-eebc-c1c3a8cf7356"
   },
   "source": [
    "`output_logs_path`: Define the Azure Storage path to be set while trainine NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "s65ubm74YeI6",
    "outputId": "7d5ff7e1-a7a5-4fd4-e6da-2dd0190c1f7a"
   },
   "source": [
    "### Spark ML Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark ML requires the following configuration to load a model from Azure:\n",
    "\n",
    "\n",
    "1. Azure connector: You need to identify your hadoop version and set the required dependency in `spark.jars.packages`|\n",
    "2. Hadoop File System: You also need to setup the Hadoop file system to work with azure storage as file system. This is define in `spark.hadoop.fs.azure`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To integrage with Azure, we need to define STORAGE_ACCOUNT and AZURE_ACCOUNT_KEY variables:\n",
    "1. STORAGE_ACCOUNT: This can be found in Microsoft Azure portal, in Resources look for the Type storage account and check the name that is your storage account.\n",
    "2. AZURE_ACCOUNT_KEY: \n",
    "Check View account access keys in this oficial [Azure documentation](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal)\n",
    "\n",
    "Then you can define this two properties as variables to set those during spark session creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter your Storage Account:\")\n",
    "STORAGE_ACCOUNT = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter your Azure Account Key:\")\n",
    "AZURE_ACCOUNT_KEY = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NsCd1K-f4EHm",
    "outputId": "1ecfc6fe-4722-4399-ed5a-4ef196d8f1fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark version: 3.4.0\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "import pyspark\n",
    "\n",
    "azure_hadoop_config = \"spark.hadoop.fs.azure.account.key.\" + STORAGE_ACCOUNT + \".blob.core.windows.net\"\n",
    "\n",
    "hadoop_azure_pkg = \"org.apache.hadoop:hadoop-azure:3.3.4\"\n",
    "azure_storage_pkg = \"com.microsoft.azure:azure-storage:8.6.6\"\n",
    "azure_identity_pkg = \"com.azure:azure-identity:1.9.1\"\n",
    "azure_storage_blob_pkg = \"com.azure:azure-storage-blob:12.22.2\"\n",
    "azure_pkgs = hadoop_azure_pkg + \",\" + azure_storage_pkg + \",\" + azure_identity_pkg + \",\" + azure_storage_blob_pkg\n",
    "\n",
    "\n",
    "#Azure Storage configuration\n",
    "azure_params = {\n",
    "    \"spark.jars.packages\": azure_pkgs,\n",
    "    azure_hadoop_config: AZURE_ACCOUNT_KEY\n",
    "}\n",
    "\n",
    "spark = sparknlp.start(params=azure_params)\n",
    "\n",
    "print(\"Apache Spark version: {}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPSvZmW84Ht3"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.training import CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NCNZgCM4J43",
    "outputId": "4187ad83-2282-4f36-fd3b-d332063e17dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|John Smith works ...|[{document, 0, 35...|[{document, 0, 35...|[{token, 0, 3, Jo...|[{pos, 0, 3, NNP,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "training_data = CoNLL().readDataset(spark, './test_ner_dataset.txt')\n",
    "training_data.show(3)\n",
    "\n",
    "embeddings = WordEmbeddingsModel.pretrained(\"glove_100d\")\n",
    "ready_data = embeddings.transform(training_data).cache()\n",
    "\n",
    "output_logs_path = \"https://\" + STORAGE_ACCOUNT + \".blob.core.windows.net/test/logs\"\n",
    "\n",
    "ner_tagger = NerDLApproach() \\\n",
    "    .setInputCols(\"sentence\", \"token\", \"embeddings\") \\\n",
    "    .setLabelColumn(\"label\") \\\n",
    "    .setOutputCol(\"ner\") \\\n",
    "    .setMaxEpochs(1) \\\n",
    "    .setMaxEpochs(5) \\\n",
    "    .setRandomSeed(0) \\\n",
    "    .setVerbose(2) \\\n",
    "    .setDropout(0.8) \\\n",
    "    .setBatchSize(18) \\\n",
    "    .setEnableOutputLogs(True) \\\n",
    "    .setOutputLogsPath(output_logs_path)\n",
    "\n",
    "ner_model = ner_tagger.fit(ready_data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
