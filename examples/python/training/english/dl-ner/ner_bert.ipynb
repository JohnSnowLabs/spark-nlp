{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/dl-ner/ner_bert.ipynb)\n",
    "\n",
    "# Deep Learning NER with Bert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell when you are using Spark NLP on Google Colab\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we walk-through a LSTM NER model training and prediction. This annotator is implemented on top of TensorFlow.\n",
    "\n",
    "This annotator will take a series of word embedding vectors, training CoNLL dataset, plus a validation dataset. We include our own predefined Tensorflow Graphs, but it will train all layers during fit() stage.\n",
    "\n",
    "DL NER will compute several layers of BI-LSTM in order to auto generate entity extraction, and it will leverage batch-based distributed calls to native TensorFlow libraries during prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Call necessary imports and set the resource folder path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Download CoNLL 2003 data if not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading eng.testa\n",
      "Downloading eng.testb\n"
     ]
    }
   ],
   "source": [
    "# Download CoNLL 2003 Dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "url = \"https://github.com/patverga/torch-ner-nlp-from-scratch/raw/master/data/conll2003/\"\n",
    "file_train=\"eng.train\"\n",
    "file_testa= \"eng.testa\"\n",
    "file_testb= \"eng.testb\"\n",
    "# https://github.com/patverga/torch-ner-nlp-from-scratch/tree/master/data/conll2003\n",
    "if not Path(file_train).is_file():\n",
    "    print(\"Downloading \"+file_train)\n",
    "    urllib.request.urlretrieve(url+file_train, file_train)\n",
    "if not Path(file_testa).is_file():\n",
    "    print(\"Downloading \"+file_testa)\n",
    "    urllib.request.urlretrieve(url+file_testa, file_testa)\n",
    "\n",
    "if not Path(file_testb).is_file():\n",
    "    print(\"Downloading \"+file_testb)\n",
    "    urllib.request.urlretrieve(url+file_testb, file_testb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  4.3.1\n",
      "Apache Spark version:  3.3.0\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Load dataset and cache into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|EU rejects German...|[{document, 0, 47...|[{document, 0, 47...|[{token, 0, 1, EU...|[{pos, 0, 1, NNP,...|[{named_entity, 0...|\n",
      "|     Peter Blackburn|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 4, Pe...|[{pos, 0, 4, NNP,...|[{named_entity, 0...|\n",
      "| BRUSSELS 1996-08-22|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 7, BR...|[{pos, 0, 7, NNP,...|[{named_entity, 0...|\n",
      "|The European Comm...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Germany 's repres...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 6, Ge...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "|\" We do n't suppo...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 0, \",...|[{pos, 0, 0, \", {...|[{named_entity, 0...|\n",
      "|He said further s...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|He said a proposa...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Fischler proposed...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 7, Fi...|[{pos, 0, 7, JJR,...|[{named_entity, 0...|\n",
      "|But Fischler agre...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 2, Bu...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|Spanish Farm Mini...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 6, Sp...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "|                   .|[{document, 0, 0,...|[{document, 0, 0,...|[{token, 0, 0, .,...|[{pos, 0, 0, ., {...|[{named_entity, 0...|\n",
      "|Only France and B...|[{document, 0, 52...|[{document, 0, 52...|[{token, 0, 3, On...|[{pos, 0, 3, RB, ...|[{named_entity, 0...|\n",
      "|The EU 's scienti...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Sheep have long b...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 4, Sh...|[{pos, 0, 4, NNP,...|[{named_entity, 0...|\n",
      "|British farmers d...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 6, Br...|[{pos, 0, 6, JJ, ...|[{named_entity, 0...|\n",
      "|\" What we have to...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 0, \",...|[{pos, 0, 0, \", {...|[{named_entity, 0...|\n",
      "|Bonn has led effo...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 3, Bo...|[{pos, 0, 3, NNP,...|[{named_entity, 0...|\n",
      "|Germany imported ...|[{document, 0, 84...|[{document, 0, 84...|[{token, 0, 6, Ge...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "|It brought in 4,2...|[{document, 0, 82...|[{document, 0, 82...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.training import CoNLL\n",
    "training_data = CoNLL().readDataset(spark, './eng.train')\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Create annotator components with appropriate params and in the right order. The finisher will output only NER. Put everything in Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_bert_L2_768 download started this may take some time.\n",
      "Approximate size to download 139.6 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "bert = BertEmbeddings.pretrained() \\\n",
    " .setInputCols([\"sentence\", \"token\"])\\\n",
    " .setOutputCol(\"bert\")\\\n",
    " .setCaseSensitive(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|EU rejects German...|[{document, 0, 47...|[{document, 0, 47...|[{token, 0, 1, EU...|[{pos, 0, 1, NNP,...|[{named_entity, 0...|\n",
      "|     Peter Blackburn|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 4, Pe...|[{pos, 0, 4, NNP,...|[{named_entity, 0...|\n",
      "| BRUSSELS 1996-08-22|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 7, BR...|[{pos, 0, 7, NNP,...|[{named_entity, 0...|\n",
      "|The European Comm...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Germany 's repres...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 6, Ge...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "|\" We do n't suppo...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 0, \",...|[{pos, 0, 0, \", {...|[{named_entity, 0...|\n",
      "|He said further s...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|He said a proposa...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|Fischler proposed...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 7, Fi...|[{pos, 0, 7, JJR,...|[{named_entity, 0...|\n",
      "|But Fischler agre...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 2, Bu...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
      "|Spanish Farm Mini...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 6, Sp...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "|                   .|[{document, 0, 0,...|[{document, 0, 0,...|[{token, 0, 0, .,...|[{pos, 0, 0, ., {...|[{named_entity, 0...|\n",
      "|Only France and B...|[{document, 0, 52...|[{document, 0, 52...|[{token, 0, 3, On...|[{pos, 0, 3, RB, ...|[{named_entity, 0...|\n",
      "|The EU 's scienti...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Sheep have long b...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 4, Sh...|[{pos, 0, 4, NNP,...|[{named_entity, 0...|\n",
      "|British farmers d...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 6, Br...|[{pos, 0, 6, JJ, ...|[{named_entity, 0...|\n",
      "|\" What we have to...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 0, \",...|[{pos, 0, 0, \", {...|[{named_entity, 0...|\n",
      "|Bonn has led effo...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 3, Bo...|[{pos, 0, 3, NNP,...|[{named_entity, 0...|\n",
      "|Germany imported ...|[{document, 0, 84...|[{document, 0, 84...|[{token, 0, 6, Ge...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "|It brought in 4,2...|[{document, 0, 82...|[{document, 0, 82...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041\n",
      "+--------------------+--------------------+\n",
      "|               token|                bert|\n",
      "+--------------------+--------------------+\n",
      "|[{token, 0, 1, EU...|[{word_embeddings...|\n",
      "|[{token, 0, 4, Pe...|[{word_embeddings...|\n",
      "|[{token, 0, 7, BR...|[{word_embeddings...|\n",
      "|[{token, 0, 2, Th...|[{word_embeddings...|\n",
      "|[{token, 0, 6, Ge...|[{word_embeddings...|\n",
      "|[{token, 0, 0, \",...|[{word_embeddings...|\n",
      "|[{token, 0, 1, He...|[{word_embeddings...|\n",
      "|[{token, 0, 1, He...|[{word_embeddings...|\n",
      "|[{token, 0, 7, Fi...|[{word_embeddings...|\n",
      "|[{token, 0, 2, Bu...|[{word_embeddings...|\n",
      "|[{token, 0, 6, Sp...|[{word_embeddings...|\n",
      "|[{token, 0, 0, .,...|[{word_embeddings...|\n",
      "|[{token, 0, 3, On...|[{word_embeddings...|\n",
      "|[{token, 0, 2, Th...|[{word_embeddings...|\n",
      "|[{token, 0, 4, Sh...|[{word_embeddings...|\n",
      "|[{token, 0, 6, Br...|[{word_embeddings...|\n",
      "|[{token, 0, 0, \",...|[{word_embeddings...|\n",
      "|[{token, 0, 3, Bo...|[{word_embeddings...|\n",
      "|[{token, 0, 6, Ge...|[{word_embeddings...|\n",
      "|[{token, 0, 1, It...|[{word_embeddings...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 16.6 ms, sys: 0 ns, total: 16.6 ms\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# WARNING: Setting benchmark to true is  slow and might crash your system and is not recommended on standardCollab notebooks-- High end hardware and/or GPU required\n",
    "## dataframe.cache() does not solve this. Results must be serialized to disk for maximum efficiency\n",
    "### You might need to restart your driver after this step finishes\n",
    "benchmark = False\n",
    "\n",
    "\n",
    "with_bert_path = \"./with_bert.parquet\"\n",
    "if benchmark == True :\n",
    "  if not Path(with_bert_path).is_dir():\n",
    "    bert.transform(training_data).write.parquet(\"./with_bert.parquet\")\n",
    "    training_with_bert = spark.read.parquet(\"./with_bert.parquet\").cache()\n",
    "else : training_with_bert = bert.transform(training_data)\n",
    "\n",
    "\n",
    "print(training_with_bert.count())\n",
    "training_with_bert.select(\"token\", \"bert\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerTagger = NerDLApproach()\\\n",
    "  .setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    "  .setLabelColumn(\"label\")\\\n",
    "  .setOutputCol(\"ner\")\\\n",
    "  .setMaxEpochs(1)\\\n",
    "  .setRandomSeed(0)\\\n",
    "  .setVerbose(0)\n",
    "\n",
    "converter = NerConverter()\\\n",
    "  .setInputCols([\"document\", \"token\", \"ner\"])\\\n",
    "  .setOutputCol(\"ner_span\")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "    nerTagger,\n",
    "    converter\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Train the pipeline. (This will take some time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting\n",
      "Fitting is ended\n",
      "4.826304197311401\n",
      "CPU times: user 13.5 ms, sys: 0 ns, total: 13.5 ms\n",
      "Wall time: 4.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start = time.time()\n",
    "print(\"Start fitting\")\n",
    "#We have to limit the rows in Collab, otherwise we will encounter exceptions because of RAM limitations\n",
    "model = pipeline.fit(training_with_bert.limit(25))\n",
    "print(\"Fitting is ended\")\n",
    "print (time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Lets predict with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentence')\n",
    "\n",
    "token = Tokenizer()\\\n",
    "    .setInputCols(['sentence'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "prediction_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        sentence,\n",
    "        token,\n",
    "        bert,\n",
    "        model\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|Germany is a nice...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_data = spark.createDataFrame([[\"Germany is a nice place\"]]).toDF(\"text\")\n",
    "prediction_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model = prediction_pipeline.fit(prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('International', 'O')\n",
      "('Business', 'O')\n",
      "('Machines', 'O')\n",
      "('Corporation', 'O')\n",
      "('(', 'O')\n",
      "('IBM', 'O')\n",
      "(')', 'O')\n",
      "('is', 'O')\n",
      "('an', 'O')\n",
      "('American', 'O')\n",
      "('multinational', 'O')\n",
      "('information', 'O')\n",
      "('technology', 'O')\n",
      "('company', 'O')\n",
      "('headquartered', 'O')\n",
      "('in', 'O')\n",
      "('Armonk', 'O')\n",
      "('.', 'O')\n",
      "CPU times: user 28.1 ms, sys: 3.93 ms, total: 32.1 ms\n",
      "Wall time: 678 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lp = LightPipeline(prediction_model)\n",
    "result = lp.annotate(\"International Business Machines Corporation (IBM) is an American multinational information technology company headquartered in Armonk.\")\n",
    "for e in list(zip(result['token'], result['ner'])):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "|                text|            document|            sentence|               token|                bert|                 ner|ner_span|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "|Germany is a nice...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 6, Ge...|[{word_embeddings...|[{named_entity, 0...|      []|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "\n",
      "CPU times: user 11.2 ms, sys: 1.61 ms, total: 12.8 ms\n",
      "Wall time: 819 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This might take 8 minutes. Timing is not lineal\n",
    "\n",
    "prediction_model.transform(prediction_data).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Save both pipeline and single model once trained, on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model.write().overwrite().save(\"./ner_dl_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Load both again, deserialize from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel, Pipeline\n",
    "\n",
    "loaded_prediction_model = PipelineModel.read().load(\"./ner_dl_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Peter', 'O')\n",
      "('is', 'O')\n",
      "('a', 'O')\n",
      "('good', 'O')\n",
      "('person', 'O')\n",
      "('.', 'O')\n",
      "CPU times: user 31.2 ms, sys: 1.59 ms, total: 32.8 ms\n",
      "Wall time: 479 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lp = LightPipeline(loaded_prediction_model)\n",
    "result = lp.annotate(\"Peter is a good person.\")\n",
    "for e in list(zip(result['token'], result['ner']))[:10]:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocumentAssembler_40d21f31b5d3\n",
      "SentenceDetector_e1b0e714c446\n",
      "REGEX_TOKENIZER_a0a1816c8b3c\n",
      "BERT_EMBEDDINGS_e3d4eaf62b32\n",
      "PipelineModel_7b435b373a60\n",
      "[NerDLModel_264694148c20, NerConverter_0d470ddc9080]\n"
     ]
    }
   ],
   "source": [
    "for stage in loaded_prediction_model.stages:\n",
    "    print(stage)\n",
    "print(loaded_prediction_model.stages[-1].stages)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "ner_bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
