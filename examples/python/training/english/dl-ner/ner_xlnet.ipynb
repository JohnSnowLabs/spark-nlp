{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZhJcUl06r8w"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/dl-ner/ner_xlnet.ipynb)\n",
        "\n",
        "## 0. Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22mElNLo6rUI",
        "outputId": "3be40661-9ca2-4b32-deae-6b8b53707d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-23 11:36:39--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://setup.johnsnowlabs.com/colab.sh [following]\n",
            "--2022-12-23 11:36:39--  https://setup.johnsnowlabs.com/colab.sh\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2022-12-23 11:36:40--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1191 (1.2K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-23 11:36:40 (48.1 MB/s) - written to stdout [1191/1191]\n",
            "\n",
            "Installing PySpark 3.2.3 and Spark NLP 4.2.6\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 4.2.6\n"
          ]
        }
      ],
      "source": [
        "# This is only to setup PySpark and Spark NLP on Colab\n",
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_QE6hqA4WHh"
      },
      "source": [
        "# How to train a NER classifier with Xlnet embeddings based on Char CNNs - BiLSTM - CRF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wybDus1P4WHk"
      },
      "source": [
        "## Download the file into the local File System \n",
        "### It is a standard conll2003 format training file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA0QHrLF4WHl",
        "outputId": "d26c536f-3757-4b6c-a150-d511f60efc48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already present.\n"
          ]
        }
      ],
      "source": [
        "# Download CoNLL 2003 Dataset\n",
        "import os\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "download_path = \"./eng.train\"\n",
        "\n",
        "\n",
        "if not Path(download_path).is_file():\n",
        "    print(\"File Not found will downloading it!\")\n",
        "    url = \"https://github.com/patverga/torch-ner-nlp-from-scratch/raw/master/data/conll2003/eng.train\"\n",
        "    urllib.request.urlretrieve(url, download_path)\n",
        "else:\n",
        "    print(\"File already present.\")\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYZhNUVH4WHs"
      },
      "source": [
        "# Read CoNLL Dataset into Spark dataframe and automagically generate features for futures tasks\n",
        "The readDataset method of the CoNLL class handily adds all the features required in the next steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQExmc684WHu",
        "outputId": "fa606ffb-7cea-4a02-ee69-dde713a31945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|            sentence|               token|                 pos|               label|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|EU rejects German...|[{document, 0, 47...|[{document, 0, 47...|[{token, 0, 1, EU...|[{pos, 0, 1, NNP,...|[{named_entity, 0...|\n",
            "|     Peter Blackburn|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 4, Pe...|[{pos, 0, 4, NNP,...|[{named_entity, 0...|\n",
            "| BRUSSELS 1996-08-22|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 7, BR...|[{pos, 0, 7, NNP,...|[{named_entity, 0...|\n",
            "|The European Comm...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
            "|Germany 's repres...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 6, Ge...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
            "|\" We do n't suppo...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 0, \",...|[{pos, 0, 0, \", {...|[{named_entity, 0...|\n",
            "|He said further s...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
            "|He said a proposa...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
            "|Fischler proposed...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 7, Fi...|[{pos, 0, 7, JJR,...|[{named_entity, 0...|\n",
            "|But Fischler agre...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 2, Bu...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|\n",
            "|Spanish Farm Mini...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 6, Sp...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
            "|                   .|[{document, 0, 0,...|[{document, 0, 0,...|[{token, 0, 0, .,...|[{pos, 0, 0, ., {...|[{named_entity, 0...|\n",
            "|Only France and B...|[{document, 0, 52...|[{document, 0, 52...|[{token, 0, 3, On...|[{pos, 0, 3, RB, ...|[{named_entity, 0...|\n",
            "|The EU 's scienti...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
            "|Sheep have long b...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 4, Sh...|[{pos, 0, 4, NNP,...|[{named_entity, 0...|\n",
            "|British farmers d...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 6, Br...|[{pos, 0, 6, JJ, ...|[{named_entity, 0...|\n",
            "|\" What we have to...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 0, \",...|[{pos, 0, 0, \", {...|[{named_entity, 0...|\n",
            "|Bonn has led effo...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 3, Bo...|[{pos, 0, 3, NNP,...|[{named_entity, 0...|\n",
            "|Germany imported ...|[{document, 0, 84...|[{document, 0, 84...|[{token, 0, 6, Ge...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
            "|It brought in 4,2...|[{document, 0, 82...|[{document, 0, 82...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "from sparknlp.training import CoNLL\n",
        "\n",
        "spark = sparknlp.start()\n",
        "training_data = CoNLL().readDataset(spark, './eng.train')\n",
        "training_data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF9dJWoW4WH6"
      },
      "source": [
        "# Define the NER Pipeline \n",
        "\n",
        "### This pipeline defines a pretrained Xlnet component and a trainable NerDLApproach which is based on the Char CNN - BiLSTM - CRF\n",
        "\n",
        "Usually you have to add additional pipeline components before the elmo for the document, sentence and token columns. But Spark NLPs class CoNLL took already care of this for us, awesome!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0xFttkH4WH7",
        "outputId": "531577c6-0b9f-4497-d895-5ea25ee1f570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xlnet_base_cased download started this may take some time.\n",
            "Approximate size to download 417.5 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *\n",
        "\n",
        "# Define the pretrained Xlnet model. \n",
        "xlnet = XlnetEmbeddings.pretrained().setInputCols(\"sentence\", \"token\")\\\n",
        " .setOutputCol(\"xlnet\")\\\n",
        "\n",
        "\n",
        "# Defien the Char CNN - BiLSTM - CRF model. We will feed it the Xlnet tokens \n",
        "nerTagger = NerDLApproach()\\\n",
        "  .setInputCols([\"sentence\", \"token\", \"xlnet\"])\\\n",
        "  .setLabelColumn(\"label\")\\\n",
        "  .setOutputCol(\"ner\")\\\n",
        "  .setMaxEpochs(1)\\\n",
        "  .setRandomSeed(0)\\\n",
        "  .setVerbose(0)\n",
        "\n",
        "# put everything into the pipe\n",
        "pipeline = Pipeline(\n",
        "    stages = [\n",
        "      xlnet ,\n",
        "      nerTagger\n",
        "  ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpcIr8b_4WIB"
      },
      "source": [
        "# Fit the Pipeline and get results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDKsFDRy4WIC",
        "outputId": "64aa76e3-eb75-496b-c871-29798a25b5fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|            sentence|               token|                 pos|               label|               xlnet|                 ner|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|EU rejects German...|[{document, 0, 47...|[{document, 0, 47...|[{token, 0, 1, EU...|[{pos, 0, 1, NNP,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|     Peter Blackburn|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 4, Pe...|[{pos, 0, 4, NNP,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "| BRUSSELS 1996-08-22|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 7, BR...|[{pos, 0, 7, NNP,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|The European Comm...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|Germany 's repres...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 6, Ge...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|\" We do n't suppo...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 0, \",...|[{pos, 0, 0, \", {...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|He said further s...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|He said a proposa...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|Fischler proposed...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 7, Fi...|[{pos, 0, 7, JJR,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|But Fischler agre...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 2, Bu...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ner_df = pipeline.fit(training_data.limit(10)).transform(training_data.limit(10))\n",
        "ner_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFSKuv-x4WIH"
      },
      "source": [
        "### Checkout only result columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObW2xBPn4WII",
        "outputId": "59d2dc76-85f6-45d2-eaab-a69089dc4bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                            |ner                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
            "+------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|EU rejects German call to boycott British lamb .|[{named_entity, 0, 1, O, {word -> EU, sentence -> 0}, []}, {named_entity, 3, 9, O, {word -> rejects, sentence -> 0}, []}, {named_entity, 11, 16, O, {word -> German, sentence -> 0}, []}, {named_entity, 18, 21, O, {word -> call, sentence -> 0}, []}, {named_entity, 23, 24, O, {word -> to, sentence -> 0}, []}, {named_entity, 26, 32, O, {word -> boycott, sentence -> 0}, []}, {named_entity, 34, 40, O, {word -> British, sentence -> 0}, []}, {named_entity, 42, 45, O, {word -> lamb, sentence -> 0}, []}, {named_entity, 47, 47, O, {word -> ., sentence -> 0}, []}]|\n",
            "+------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ner_df.select(*['text', 'ner']).limit(1).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJAr8usHLsmw"
      },
      "source": [
        "## Alternative Albert models \n",
        "\n",
        "checkout https://github.com/JohnSnowLabs/spark-nlp-models for alternative models, the following are available :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAGIS-vS4WIO",
        "outputId": "a32af11d-6577-46c5-e382-c651eec98d1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "albert_embeddings_albert_base_v1 download started this may take some time.\n",
            "Approximate size to download 42.8 MB\n",
            "[OK!]\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|            sentence|               token|                 pos|               label|              albert|                 ner|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|EU rejects German...|[{document, 0, 47...|[{document, 0, 47...|[{token, 0, 1, EU...|[{pos, 0, 1, NNP,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|     Peter Blackburn|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 4, Pe...|[{pos, 0, 4, NNP,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "| BRUSSELS 1996-08-22|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 7, BR...|[{pos, 0, 7, NNP,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|The European Comm...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|Germany 's repres...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 6, Ge...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|\" We do n't suppo...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 0, \",...|[{pos, 0, 0, \", {...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|He said further s...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|He said a proposa...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|Fischler proposed...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 7, Fi...|[{pos, 0, 7, JJR,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|But Fischler agre...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 2, Bu...|[{pos, 0, 2, CC, ...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|Spanish Farm Mini...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 6, Sp...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|                   .|[{document, 0, 0,...|[{document, 0, 0,...|[{token, 0, 0, .,...|[{pos, 0, 0, ., {...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|Only France and B...|[{document, 0, 52...|[{document, 0, 52...|[{token, 0, 3, On...|[{pos, 0, 3, RB, ...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|The EU 's scienti...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|Sheep have long b...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 4, Sh...|[{pos, 0, 4, NNP,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|British farmers d...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 6, Br...|[{pos, 0, 6, JJ, ...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|\" What we have to...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 0, \",...|[{pos, 0, 0, \", {...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|Bonn has led effo...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 3, Bo...|[{pos, 0, 3, NNP,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|Germany imported ...|[{document, 0, 84...|[{document, 0, 84...|[{token, 0, 6, Ge...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "|It brought in 4,2...|[{document, 0, 82...|[{document, 0, 82...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *\n",
        "\n",
        "# Define the pretrained Albert model. \n",
        "albert_variant = 'albert_embeddings_albert_base_v1'\n",
        "albert = AlbertEmbeddings.pretrained(albert_variant).setInputCols(\"sentence\", \"token\")\\\n",
        " .setOutputCol(\"albert\")\\\n",
        "\n",
        "\n",
        "# Define the Char CNN - BiLSTM - CRF model. We will Xlnet it the Albert tokens.\n",
        "nerTagger = NerDLApproach()\\\n",
        "  .setInputCols([\"sentence\", \"token\", \"albert\"])\\\n",
        "  .setLabelColumn(\"label\")\\\n",
        "  .setOutputCol(\"ner\")\\\n",
        "  .setMaxEpochs(1)\\\n",
        "  .setRandomSeed(0)\\\n",
        "  .setVerbose(0)\n",
        "\n",
        "# put everything into the pipe\n",
        "pipeline = Pipeline(\n",
        "    stages = [\n",
        "      albert ,\n",
        "      nerTagger\n",
        "  ])\n",
        "\n",
        "ner_df = pipeline.fit(training_data.limit(10)).transform(training_data.limit(50))\n",
        "ner_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xX4uVtcMVKF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "ner_xlnet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "name": "NER-Tutorial",
    "notebookId": 3359671281044291
  },
  "nbformat": 4,
  "nbformat_minor": 0
}