{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/entity-ruler/EntityRuler.ipynb)\n",
    "\n",
    "# Training EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this Cell when you are using Spark NLP on Google Colab\n",
    "!wget https://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3.1\n",
      "3.3.0\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(sparknlp.version())\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the default configuration (useStorage=true). This parameter tells the annotator to serialize patterns file data with RocksDB storage when saving the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.createDataFrame([[\"Lord Eddard Stark was the head of House Stark. John Snow lives in Winterfell.\"]]).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "|text                                                                         |\n",
      "+-----------------------------------------------------------------------------+\n",
      "|Lord Eddard Stark was the head of House Stark. John Snow lives in Winterfell.|\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EntityRuler no longer needs `Tokenizer` or `RegexTokenizer` annotatos when using keywords patterns(non-regex patterns). It will handle the chunks output based on the patterns defined, as shown in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "keywords = [\n",
    "          {\n",
    "            \"label\": \"PERSON\",\n",
    "            \"patterns\": [\"Jon\", \"John\", \"John Snow\", \"Jon Snow\"]\n",
    "          },\n",
    "          {\n",
    "            \"label\": \"PERSON\",\n",
    "            \"patterns\": [\"Eddard\", \"Eddard Stark\"]\n",
    "          },\n",
    "          {\n",
    "            \"label\": \"LOCATION\",\n",
    "            \"patterns\": [\"Winterfell\"]\n",
    "          },\n",
    "         ]\n",
    "\n",
    "with open('./keywords.json', 'w') as jsonfile:\n",
    "    json.dump(keywords, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a JSON file with the following format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: ./person.json: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! cat ./person.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with keywords, we DON'T need a pipeline with Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "sentence_detector = SentenceDetector().setInputCols(\"document\").setOutputCol(\"sentence\")\n",
    "\n",
    "entity_ruler = EntityRulerApproach() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"entity\") \\\n",
    "    .setPatternsResource(\"./keywords.json\") \\\n",
    "    .setUseStorage(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[document_assembler, sentence_detector, entity_ruler])\n",
    "pipeline_model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|entity                                                                                                                                                                                                        |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{chunk, 5, 16, Eddard Stark, {entity -> PERSON, sentence -> 0}, []}, {chunk, 47, 55, John Snow, {entity -> PERSON, sentence -> 1}, []}, {chunk, 66, 75, Winterfell, {entity -> LOCATION, sentence -> 1}, []}]|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_model.transform(data).select(\"entity\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_pipeline = LightPipeline(pipeline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['document', 'sentence', 'entity'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations = light_pipeline.fullAnnotate(\"Doctor John Snow lives in London, whereas Lord Commander Jon Snow lives in Castle Black\")[0]\n",
    "annotations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Annotation(chunk, 7, 15, John Snow, {'entity': 'PERSON', 'sentence': '0'}, []),\n",
       " Annotation(chunk, 57, 64, Jon Snow, {'entity': 'PERSON', 'sentence': '0'}, [])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.get('entity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define an id field to identify entities and it supports JSON Lines format as the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "            {\n",
    "              \"id\": \"names-with-j\",\n",
    "              \"label\": \"PERSON\",\n",
    "              \"patterns\": [\"Jon\", \"John\", \"John Snow\", \"Jon Snow\"]\n",
    "            },\n",
    "            {\n",
    "              \"id\": \"names-with-e\",\n",
    "              \"label\": \"PERSON\",\n",
    "              \"patterns\": [\"Eddard\", \"Eddard Stark\"]\n",
    "            },\n",
    "            {\n",
    "              \"id\": \"locations\",\n",
    "              \"label\": \"LOCATION\",\n",
    "              \"patterns\": [\"Winterfell\"]\n",
    "            },\n",
    "         ]\n",
    "\n",
    "with open('./keywords.jsonl', 'w') as jsonlfile:\n",
    "    for keyword in keywords:\n",
    "      json.dump(keyword, jsonlfile)\n",
    "      jsonlfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"names-with-j\", \"label\": \"PERSON\", \"patterns\": [\"Jon\", \"John\", \"John Snow\", \"Jon Snow\"]}\n",
      "{\"id\": \"names-with-e\", \"label\": \"PERSON\", \"patterns\": [\"Eddard\", \"Eddard Stark\"]}\n",
      "{\"id\": \"locations\", \"label\": \"LOCATION\", \"patterns\": [\"Winterfell\"]}\n"
     ]
    }
   ],
   "source": [
    "! cat ./keywords.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ruler = EntityRulerApproach() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"entity\") \\\n",
    "    .setPatternsResource(\"./keywords.jsonl\", ReadAs.TEXT, options={\"format\": \"JSONL\"}) \\\n",
    "    .setUseStorage(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|entity                                                                                                                                                                                                                                                                 |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{chunk, 5, 16, Eddard Stark, {entity -> PERSON, sentence -> 0, id -> names-with-e}, []}, {chunk, 47, 55, John Snow, {entity -> PERSON, sentence -> 1, id -> names-with-j}, []}, {chunk, 66, 75, Winterfell, {entity -> LOCATION, sentence -> 1, id -> locations}, []}]|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[document_assembler, sentence_detector, entity_ruler])\n",
    "model = pipeline.fit(data)\n",
    "model.transform(data).select(\"entity\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the CSV file we use the following configuration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./keywords.csv', 'w') as csvfile:\n",
    "    csvfile.write('PERSON|Jon\\n')\n",
    "    csvfile.write('PERSON|John\\n')\n",
    "    csvfile.write('PERSON|John Snow\\n')\n",
    "    csvfile.write('LOCATION|Winterfell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON|Jon\n",
      "PERSON|John\n",
      "PERSON|John Snow\n",
      "LOCATION|Winterfell"
     ]
    }
   ],
   "source": [
    "! cat ./keywords.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ruler_csv = EntityRulerApproach() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"entity\") \\\n",
    "    .setPatternsResource(\"./keywords.csv\", options={\"format\": \"csv\", \"delimiter\": \"\\\\|\"}) \\\n",
    "    .setUseStorage(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_csv = Pipeline(stages=[document_assembler, sentence_detector, entity_ruler_csv])\n",
    "model_csv = pipeline_csv.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|entity                                                                                                                                   |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{chunk, 47, 55, John Snow, {entity -> PERSON, sentence -> 1}, []}, {chunk, 66, 75, Winterfell, {entity -> LOCATION, sentence -> 1}, []}]|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_csv.transform(data).select(\"entity\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with Spark NLP 4.2.0 regex patterns must be defined at a more granular level, with each label. For example we can have the JSON file below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.createDataFrame([[\"The address is 123456 in Winterfell\"]]).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_string = \"\"\"\n",
    "[\n",
    "  {\n",
    "    \"id\": \"id-regex\",\n",
    "    \"label\": \"ID\",\n",
    "    \"patterns\": [\"[0-9]+\"],\n",
    "    \"regex\": true\n",
    "  },\n",
    "  {\n",
    "    \"id\": \"locations-words\",\n",
    "    \"label\": \"LOCATION\",\n",
    "    \"patterns\": [\"Winterfell\"],\n",
    "    \"regex\": false\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "patterns_obj = json.loads(patterns_string)\n",
    "with open('./patterns.json', 'w') as jsonfile:\n",
    "    json.dump(patterns_obj, jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"id\": \"id-regex\", \"label\": \"ID\", \"patterns\": [\"[0-9]+\"], \"regex\": true}, {\"id\": \"locations-words\", \"label\": \"LOCATION\", \"patterns\": [\"Winterfell\"], \"regex\": false}]"
     ]
    }
   ],
   "source": [
    "!cat ./patterns.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining a regex pattern, we need to define Tokenizer annotator in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer().setInputCols(\"sentence\").setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_entity_ruler = EntityRulerApproach() \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"entity\") \\\n",
    "    .setPatternsResource(\"./patterns.json\") \\\n",
    "    .setUseStorage(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, regex_entity_ruler])\n",
    "regex_model = regex_pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|entity                                                                                                                                                                   |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{chunk, 15, 20, 123456, {entity -> ID, id -> id-regex, sentence -> 0}, []}, {chunk, 25, 34, Winterfell, {entity -> LOCATION, sentence -> 0, id -> locations-words}, []}]|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_model.transform(data).select(\"entity\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPAkTh+xRM44+YYHOMG0V7p",
   "name": "EntityRuler.ipynb",
   "provenance": [
    {
     "file_id": "1QgevB5ZVEDJIwt6TapwdzUa0wSgzfOdb",
     "timestamp": 1631717372195
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
