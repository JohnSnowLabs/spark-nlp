{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/training/chinese/word-segmentation/WordSegmenter_train_chinese_segmentation.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Word Segmenter](https://nlp.johnsnowlabs.com/docs/en/annotators#wordsegmenter)\n",
    "\n",
    "Many languages are not whitespace separated and their sentences are a\n",
    "concatenation of many symbols, like Korean, Japanese or Chinese. Without\n",
    "understanding the language, splitting the words into their corresponding tokens\n",
    "is impossible. The WordSegmenter is trained to understand these languages and\n",
    "split them into semantically correct parts.\n",
    "\n",
    "Let's train a custom WordSegmenterModel that will tokenize Chinese words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run this block if you are inside Google Colab to set up Spark NLP otherwise\n",
    "skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version: 4.3.1\n",
      "Apache Spark version: 3.3.0\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "spark = sparknlp.start()\n",
    "\n",
    "\n",
    "print(\"Spark NLP version: {}\".format(sparknlp.version()))\n",
    "print(\"Apache Spark version: {}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train your own model, a training dataset consisting of [Part-Of-Speech\n",
    "tags](https://en.wikipedia.org/wiki/Part-of-speech_tagging) is required. The\n",
    "data has to be loaded into a dataframe, where the column is a Spark NLP\n",
    "Annotation of type `\"POS\"`. This can be set with `setPosColumn`.\n",
    "\n",
    "For this example we will use some sample files parsed from the [Ontonotes 5.0 Dataset](https://github.com/taotao033/conll-formatted-ontonotes-5.0_for_chinese_language). If a full model needs to be trained, the whole dataset needs to be retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/taotao033/conll-formatted-ontonotes-5.0_for_chinese_language/master/onto.train.ner\n",
    "!wget https://raw.githubusercontent.com/taotao033/conll-formatted-ontonotes-5.0_for_chinese_language/master/onto.test.ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark NLP offers helper classes to load this kind of data into Spark DataFrames.\n",
    "The resulting DataFrame will have columns for the word, POS tag and NER Tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.training import CoNLL\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "train = CoNLL(delimiter=\"\\t\").readDataset(spark, \"onto.train.ner\")\n",
    "test = CoNLL(delimiter=\"\\t\").readDataset(spark, \"onto.test.ner\") \\\n",
    "    .withColumn(\"text\", regexp_replace(\"text\", \"\\t\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "Now we will create the parts for the training pipeline. In this case it is\n",
    "rather simple, as we only need to pass the annotations to the\n",
    "WordSegmenterApproach annotator. We set the `posColumn` parameter to the name\n",
    "of the column which was extracted (in this case `\"pos\"`). The resulting output\n",
    "column will be `\"token\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "wordSegmenter = WordSegmenterApproach() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\") \\\n",
    "    .setPosColumn(\"pos\") \\\n",
    "    .setNIterations(5)\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    documentAssembler,\n",
    "    wordSegmenter\n",
    "])\n",
    "\n",
    "pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have trained the model, we can use the resulting pipeline model to\n",
    "transform the test data. Note that this model might not perform well, as it had\n",
    "little data and iterations and only serves to illustrate the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|result                                                                                                                                                                              |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[在, 华, 美, 资, 企, 业, 要, 求, 延, 长, 给, 中, 国, 的, 贸, 易, 最, 惠, 国, 待, 遇]                                                                                                |\n",
      "|[新, 华, 社, 华, 盛, 顿, ４, 月, ２, ０, 日, 电, （, 记, 者, 应, 谦, ）]                                                                                                            |\n",
      "|[美, 国, 商, 会, 中, 国, 分, 会, 近, 日, 派, 出, 一, 个, ２, ５, 人, 组, 成, 的, 代, 表, 团, ，, 在, 华, 盛, 顿, 向, 国, 会, 和, 白, 宫, 展, 开, 为, 期, 一, 周, 的, 游, 说, 活, 动]|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_transformed = pipelineModel.transform(test)\n",
    "test_transformed.select(\"token.result\").show(5, False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "words_segmenter_demo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
