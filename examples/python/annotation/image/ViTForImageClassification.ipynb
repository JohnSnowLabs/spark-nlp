{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8xIEZ07QpRM",
    "outputId": "b5f5db4b-bce4-4b62-883f-3b3e90a3f1cd"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/prediction/english/ViTForImageClassification.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mz6G5fxae3HW"
   },
   "outputs": [],
   "source": [
    "!wget https://setup.johnsnowlabs.com/colab.sh -O - | bash /dev/stdin -p 3.2.1 -s 4.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KvNW4MU5rrF",
    "outputId": "36cf722b-f3a6-4566-8217-615cc58dc549"
   },
   "source": [
    "## ViTForImageClassification Annotator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BshxwBPTe3Hc"
   },
   "source": [
    "In this notebok we are going to classify images using spark-nlp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaN1OWV0NQ5T"
   },
   "source": [
    "### Downloading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jEHkswUjUfaU"
   },
   "outputs": [],
   "source": [
    "!wget -q https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/images/images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "k9F8WstLNXnS"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.unpack_archive(\"images.zip\", \"images\", \"zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Start Spark Session"
   ],
   "metadata": {
    "id": "3a_shOYHfpOn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XLNO3Z9r6HgR"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4JfeD8Rj-as2"
   },
   "outputs": [],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "99AqJEThSBuT"
   },
   "outputs": [],
   "source": [
    "data_df = spark.read.format(\"image\").option(\"dropInvalid\", value = True).load(path=\"/content/images/images/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J86YU794UYEG"
   },
   "source": [
    "### Pipeline with ViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRyju8D-6XJ1",
    "outputId": "ad8658bb-8170-488a-f9a1-680c63ad0f80"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "image_classifier_vit_base_patch16_224 download started this may take some time.\n",
      "Approximate size to download 309.7 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "image_assembler = ImageAssembler() \\\n",
    "            .setInputCol(\"image\") \\\n",
    "            .setOutputCol(\"image_assembler\")\n",
    "\n",
    "image_classifier = ViTForImageClassification \\\n",
    "    .pretrained() \\\n",
    "    .setInputCols(\"image_assembler\") \\\n",
    "    .setOutputCol(\"class\")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    image_assembler,\n",
    "    image_classifier,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XIYjEhW3O_Uc"
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gIZFLaUOPBnd",
    "outputId": "a8cfe0c5-fe6a-4f0b-a4c1-e9cf5d1f22c0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|               image|     image_assembler|               class|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|{file:///content/...|[{image, file:///...|[{category, 0, 5,...|\n",
      "|{file:///content/...|[{image, file:///...|[{category, 0, 11...|\n",
      "|{file:///content/...|[{image, file:///...|[{category, 0, 55...|\n",
      "|{file:///content/...|[{image, file:///...|[{category, 0, 2,...|\n",
      "|{file:///content/...|[{image, file:///...|[{category, 0, 24...|\n",
      "|{file:///content/...|[{image, file:///...|[{category, 0, 14...|\n",
      "|{file:///content/...|[{image, file:///...|[{category, 0, 7,...|\n",
      "|{file:///content/...|[{image, file:///...|[{category, 0, 8,...|\n",
      "|{file:///content/...|[{image, file:///...|[{category, 0, 6,...|\n",
      "|{file:///content/...|[{image, file:///...|[{category, 0, 1,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_df = model.transform(data_df)\n",
    "image_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rfp5MK1UxoNt"
   },
   "source": [
    "### Light Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_6VJPS9xvfV"
   },
   "source": [
    "To use light pipeline in ViT transformer, we need to use the new method `fullAnnotateImage`, which can receive 3 kind of inputs:\n",
    "1. A path to a single image\n",
    "2. A path to a list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XDQ6PrgbSJ8W",
    "outputId": "a2b3159d-f929-429b-d7be-fe119470fea4"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['image_assembler', 'class'])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "light_pipeline = LightPipeline(model)\n",
    "annotations_result = light_pipeline.fullAnnotateImage(\"images/images/hippopotamus.JPEG\")\n",
    "annotations_result[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73PV--LdSU5-",
    "outputId": "4a5f8730-f515-413d-ed0a-010b98d2d844"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "annotator_type: image\n",
      "origin: images/images/hippopotamus.JPEG\n",
      "height: 333\n",
      "width: 500\n",
      "nChannels: 3\n",
      "mode: 16\n",
      "result size: 499500\n",
      "metadata: Map()\n",
      "[Annotation(category, 0, 55, hippopotamus, hippo, river horse, Hippopotamus amphibius, Map(nChannels -> 3, Some(lumbermill, sawmill) -> 7.2882756E-8, Some(beer glass) -> 9.0488925E-8, image -> 0, Some(damselfly) -> 1.9379786E-7, Some(turnstile) -> 6.8434524E-8, Some(cockroach, roach) -> 1.6622849E-7, height -> 333, Some(bulbul) -> 1.6930231E-7, Some(sea snake) -> 8.89582E-8, origin -> images/images/hippopotamus.JPEG, Some(mixing bowl) -> 1.2995402E-7, mode -> 16, None -> 1.3814622E-7, Some(whippet) -> 3.894023E-8, width -> 500, Some(buckle) -> 1.0061492E-7))]\n"
     ]
    }
   ],
   "source": [
    "for result in annotations_result:\n",
    "  image_assembler = result['image_assembler'][0]\n",
    "  print(f\"annotator_type: {image_assembler.annotator_type}\")\n",
    "  print(f\"origin: {image_assembler.origin}\")\n",
    "  print(f\"height: {image_assembler.height}\")\n",
    "  print(f\"width: {image_assembler.width}\")\n",
    "  print(f\"nChannels: {image_assembler.nChannels}\")\n",
    "  print(f\"mode: {image_assembler.mode}\")\n",
    "  print(f\"result size: {str(len(image_assembler.result))}\")\n",
    "  print(f\"metadata: {image_assembler.metadata}\")\n",
    "  print(result['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V37k8GQFySRW"
   },
   "source": [
    "To send a list of images, we just difine a set of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "asf3MZGzyXl5",
    "outputId": "03db32ad-2ac2-4bb9-dd38-7c06c5d6a4b8"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['image_assembler', 'class'])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "images = [\"images/images/bluetick.jpg\", \"images/images/palace.JPEG\", \"images/images/hen.JPEG\"]\n",
    "annotations_result = light_pipeline.fullAnnotateImage(images)\n",
    "annotations_result[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfby3MJlymNV",
    "outputId": "ef63a544-c995-429e-e965-302bc8781851"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Annotation(category, 0, 7, bluetick, Map(nChannels -> 3, Some(lumbermill, sawmill) -> 1.3846728E-6, Some(beer glass) -> 1.1807944E-6, image -> 0, Some(damselfly) -> 3.6875622E-7, Some(turnstile) -> 2.023695E-6, Some(cockroach, roach) -> 6.2982855E-7, height -> 500, Some(bulbul) -> 5.417509E-7, Some(sea snake) -> 5.7421556E-7, origin -> images/images/bluetick.jpg, Some(mixing bowl) -> 5.4001305E-7, mode -> 16, None -> 4.5454306E-7, Some(whippet) -> 1.2101438E-6, width -> 333, Some(buckle) -> 1.1306514E-6))]\n",
      "[Annotation(category, 0, 5, palace, Map(nChannels -> 3, Some(lumbermill, sawmill) -> 6.3918545E-5, Some(beer glass) -> 8.879939E-6, image -> 0, Some(damselfly) -> 9.565577E-6, Some(turnstile) -> 6.315168E-5, Some(cockroach, roach) -> 1.125408E-5, height -> 334, Some(bulbul) -> 3.321073E-5, Some(sea snake) -> 1.0886038E-5, origin -> images/images/palace.JPEG, Some(mixing bowl) -> 2.6202975E-5, mode -> 16, None -> 2.6134943E-5, Some(whippet) -> 1.3805137E-5, width -> 500, Some(buckle) -> 3.121459E-5))]\n",
      "[Annotation(category, 0, 2, hen, Map(nChannels -> 3, Some(lumbermill, sawmill) -> 2.1663836E-5, Some(beer glass) -> 3.062036E-6, image -> 0, Some(damselfly) -> 5.8477954E-6, Some(turnstile) -> 1.8546416E-6, Some(cockroach, roach) -> 2.5356887E-6, height -> 375, Some(bulbul) -> 3.2049334E-6, Some(sea snake) -> 2.8824059E-6, origin -> images/images/hen.JPEG, Some(mixing bowl) -> 6.9148127E-6, mode -> 16, None -> 2.824775E-6, Some(whippet) -> 4.5998115E-7, width -> 500, Some(buckle) -> 1.6334545E-5))]\n"
     ]
    }
   ],
   "source": [
    "for result in annotations_result:\n",
    "  print(result['class'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ViTForImageClassification-LightPipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
