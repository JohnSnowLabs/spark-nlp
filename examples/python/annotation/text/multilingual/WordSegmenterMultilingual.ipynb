{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/multilingual/WordSegmenterMultilingual.ipynb)\n",
    "\n",
    "# Multilingual Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell when you are using Spark NLP on Google Colab\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with multilingual text, we have two options in WordSegmenter:\n",
    "1. Use `setEnableRegexTokenizer` parameter. This is useful for current pretrained models.\n",
    "2. Train a model with multilingual text. This can be useful in case a current model (with `setEnableRegexTokenizer=True`) does not yield good results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `setEnableRegexTokenizer=True` parameter will make WordSegmenter to tokenize latin words based on spaces and apply word segmenter inference **only in non-latin words**. As show in the example below.\n",
    "\n",
    "**Note:** There are 3 parameters to play around for tokenization of latin words. You can check those in our [official documentation](https://sparknlp.org/api/com/johnsnowlabs/nlp/annotators/ws/WordSegmenterModel.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example has a text with Thai and English words. So, we use a WordSegmenter model of Thai language. You can check additional WordSegmenter models in our [official model's page](https://sparknlp.org/models?q=Word+Segmenter).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_text = \"สำหรับฐานลำโพง apple homepod อุปกรณ์เครื่องเสียงยึดขาตั้งไม้แข็งตั้งพื้น speaker stands null\"\n",
    "multilingual_df = spark.createDataFrame([[multilingual_text]]).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordseg_best download started this may take some time.\n",
      "Approximate size to download 79.2 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "\n",
    "word_segmenter = WordSegmenterModel().pretrained(\"wordseg_best\", \"th\") \\\n",
    "      .setInputCols([\"document\"]) \\\n",
    "      .setOutputCol(\"token\") \\\n",
    "      .setEnableRegexTokenizer(True)\n",
    "\n",
    "pipeline = Pipeline(stages=[document_assembler, word_segmenter])\n",
    "result_df = pipeline.fit(multilingual_df).transform(multilingual_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                text|            document|               token|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|สำหรับฐานลำโพง ap...|[{document, 0, 91...|[{token, 0, 8, สำ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|token                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{token, 0, 8, สำหรับฐาน, {sentence -> 0}, []}, {token, 9, 9, ล, {sentence -> 0}, []}, {token, 10, 10, ำ, {sentence -> 0}, []}, {token, 11, 13, โพง, {sentence -> 0}, []}, {token, 15, 19, apple, {sentence -> 0}, []}, {token, 21, 27, homepod, {sentence -> 0}, []}, {token, 29, 35, อุปกรณ์, {sentence -> 0}, []}, {token, 36, 42, เครื่อง, {sentence -> 0}, []}, {token, 43, 47, เสียง, {sentence -> 0}, []}, {token, 48, 50, ยึด, {sentence -> 0}, []}, {token, 51, 52, ขา, {sentence -> 0}, []}, {token, 53, 56, ตั้ง, {sentence -> 0}, []}, {token, 57, 59, ไม้, {sentence -> 0}, []}, {token, 60, 63, แข็ง, {sentence -> 0}, []}, {token, 64, 67, ตั้ง, {sentence -> 0}, []}, {token, 68, 71, พื้น, {sentence -> 0}, []}, {token, 73, 79, speaker, {sentence -> 0}, []}, {token, 81, 86, stands, {sentence -> 0}, []}, {token, 88, 91, null, {sentence -> 0}, []}]|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.select(\"token\").show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Multilingual Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also train our own multilingual model, which will require to build a training file with the required format, as in this example to label each character for English and Thai alike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags legend for the training dataset is the following:\n",
    "- LL: Left Boundary of a word\n",
    "- RR: Right Boundary of a word\n",
    "- MM: Middle character of a word\n",
    "- LR: A single character that can be seen as a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thai_word1 = \"สำ|LL ห|MM รั|MM บ|RR ฐ|LL า|MM น|RR ลำ|LL โ|MM พ|MM ง|RR \"\n",
    "english_words = \"a|LL p|MM p|MM l|MM e|RR h|LL o|MM m|MM e|MM p|MM o|MM d|RR \"\n",
    "thai_word2 = \"อุ|LL ป|MM ก|MM ร|MM ณ์|RR เ|LL ค|MM รื่|MM อ|MM ง|RR เ|LL สี|MM ย|MM ง|RR ยึ|LL ด|RR ข|LLา|RR ตั้|LL ง|RR พื้|LL น|RR \"\n",
    "english_words2 = \"s|LL p|MM e|MM a|MM k|MM e|MM r|RR s|LL t|MM a|MM n|MM d|MM s|RR n|LL u|MM l|MM l|RR\"\n",
    "thai_english_sentence = thai_word1 + english_words + thai_word2 + english_words2\n",
    "\n",
    "with open('./thai_english.txt', 'w') as alphabet_file:\n",
    "    alphabet_file.write(thai_english_sentence + \"\\n\")\n",
    "    alphabet_file.write(thai_english_sentence + \"\\n\")\n",
    "    alphabet_file.write(thai_english_sentence + \"\\n\")\n",
    "    alphabet_file.write(thai_english_sentence + \"\\n\")\n",
    "    alphabet_file.write(thai_english_sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "สำ|LL ห|MM รั|MM บ|RR ฐ|LL า|MM น|RR ลำ|LL โ|MM พ|MM ง|RR a|LL p|MM p|MM l|MM e|RR h|LL o|MM m|MM e|MM p|MM o|MM d|RR อุ|LL ป|MM ก|MM ร|MM ณ์|RR เ|LL ค|MM รื่|MM อ|MM ง|RR เ|LL สี|MM ย|MM ง|RR ยึ|LL ด|RR ข|LLา|RR ตั้|LL ง|RR พื้|LL น|RR s|LL p|MM e|MM a|MM k|MM e|MM r|RR s|LL t|MM a|MM n|MM d|MM s|RR n|LL u|MM l|MM l|RR\n",
      "สำ|LL ห|MM รั|MM บ|RR ฐ|LL า|MM น|RR ลำ|LL โ|MM พ|MM ง|RR a|LL p|MM p|MM l|MM e|RR h|LL o|MM m|MM e|MM p|MM o|MM d|RR อุ|LL ป|MM ก|MM ร|MM ณ์|RR เ|LL ค|MM รื่|MM อ|MM ง|RR เ|LL สี|MM ย|MM ง|RR ยึ|LL ด|RR ข|LLา|RR ตั้|LL ง|RR พื้|LL น|RR s|LL p|MM e|MM a|MM k|MM e|MM r|RR s|LL t|MM a|MM n|MM d|MM s|RR n|LL u|MM l|MM l|RR\n",
      "สำ|LL ห|MM รั|MM บ|RR ฐ|LL า|MM น|RR ลำ|LL โ|MM พ|MM ง|RR a|LL p|MM p|MM l|MM e|RR h|LL o|MM m|MM e|MM p|MM o|MM d|RR อุ|LL ป|MM ก|MM ร|MM ณ์|RR เ|LL ค|MM รื่|MM อ|MM ง|RR เ|LL สี|MM ย|MM ง|RR ยึ|LL ด|RR ข|LLา|RR ตั้|LL ง|RR พื้|LL น|RR s|LL p|MM e|MM a|MM k|MM e|MM r|RR s|LL t|MM a|MM n|MM d|MM s|RR n|LL u|MM l|MM l|RR\n",
      "สำ|LL ห|MM รั|MM บ|RR ฐ|LL า|MM น|RR ลำ|LL โ|MM พ|MM ง|RR a|LL p|MM p|MM l|MM e|RR h|LL o|MM m|MM e|MM p|MM o|MM d|RR อุ|LL ป|MM ก|MM ร|MM ณ์|RR เ|LL ค|MM รื่|MM อ|MM ง|RR เ|LL สี|MM ย|MM ง|RR ยึ|LL ด|RR ข|LLา|RR ตั้|LL ง|RR พื้|LL น|RR s|LL p|MM e|MM a|MM k|MM e|MM r|RR s|LL t|MM a|MM n|MM d|MM s|RR n|LL u|MM l|MM l|RR\n",
      "สำ|LL ห|MM รั|MM บ|RR ฐ|LL า|MM น|RR ลำ|LL โ|MM พ|MM ง|RR a|LL p|MM p|MM l|MM e|RR h|LL o|MM m|MM e|MM p|MM o|MM d|RR อุ|LL ป|MM ก|MM ร|MM ณ์|RR เ|LL ค|MM รื่|MM อ|MM ง|RR เ|LL สี|MM ย|MM ง|RR ยึ|LL ด|RR ข|LLา|RR ตั้|LL ง|RR พื้|LL น|RR s|LL p|MM e|MM a|MM k|MM e|MM r|RR s|LL t|MM a|MM n|MM d|MM s|RR n|LL u|MM l|MM l|RR\n"
     ]
    }
   ],
   "source": [
    "! cat ./thai_english.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                text|            document|                tags|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|สำ ห รั บ ฐ า น ล...|[{document, 0, 13...|[{pos, 0, 1, LL, ...|\n",
      "|สำ ห รั บ ฐ า น ล...|[{document, 0, 13...|[{pos, 0, 1, LL, ...|\n",
      "|สำ ห รั บ ฐ า น ล...|[{document, 0, 13...|[{pos, 0, 1, LL, ...|\n",
      "|สำ ห รั บ ฐ า น ล...|[{document, 0, 13...|[{pos, 0, 1, LL, ...|\n",
      "|สำ ห รั บ ฐ า น ล...|[{document, 0, 13...|[{pos, 0, 1, LL, ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.training import POS\n",
    "\n",
    "train_df = POS().readDataset(spark, \"./thai_english.txt\")\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                text|            document|               token|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|สำหรับฐานลำโพง ap...|[{document, 0, 91...|[{token, 0, 8, สำ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "            .setInputCol(\"text\") \\\n",
    "            .setOutputCol(\"document\")\n",
    "\n",
    "word_segmenter = WordSegmenterApproach() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"token\") \\\n",
    "    .setPosColumn(\"tags\") \\\n",
    "    .setNIterations(5)\n",
    "\n",
    "pipeline = Pipeline(stages=[document_assembler, word_segmenter])\n",
    "\n",
    "result = pipeline.fit(train_df).transform(multilingual_df)\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|token                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{token, 0, 8, สำหรับฐาน, {sentence -> 0}, []}, {token, 9, 9, ล, {sentence -> 0}, []}, {token, 10, 10, ำ, {sentence -> 0}, []}, {token, 11, 13, โพง, {sentence -> 0}, []}, {token, 15, 19, apple, {sentence -> 0}, []}, {token, 21, 27, homepod, {sentence -> 0}, []}, {token, 29, 35, อุปกรณ์, {sentence -> 0}, []}, {token, 36, 42, เครื่อง, {sentence -> 0}, []}, {token, 43, 47, เสียง, {sentence -> 0}, []}, {token, 48, 50, ยึด, {sentence -> 0}, []}, {token, 51, 52, ขา, {sentence -> 0}, []}, {token, 53, 56, ตั้ง, {sentence -> 0}, []}, {token, 57, 59, ไม้, {sentence -> 0}, []}, {token, 60, 63, แข็ง, {sentence -> 0}, []}, {token, 64, 67, ตั้ง, {sentence -> 0}, []}, {token, 68, 71, พื้น, {sentence -> 0}, []}, {token, 73, 79, speaker, {sentence -> 0}, []}, {token, 81, 86, stands, {sentence -> 0}, []}, {token, 88, 91, null, {sentence -> 0}, []}]|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.select(\"token\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
