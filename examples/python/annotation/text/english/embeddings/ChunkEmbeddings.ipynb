{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/example/python/annotation/text/english/embeddings/ChunkEmbeddings.ipynb)\n",
    "\n",
    "\n",
    "# **Chunk Embeddings**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples we look at how to extract embeddings from chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. Colab Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyspark==3.3.0  spark-nlp==4.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  4.2.8\n",
      "Apache Spark version:  3.3.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.34:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb7535958b0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Spark Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -O news_category_test.csv https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------------------------+\n",
      "|category|                                              text|\n",
      "+--------+--------------------------------------------------+\n",
      "|Business|Unions representing workers at Turner   Newall ...|\n",
      "|Sci/Tech| TORONTO, Canada    A second team of rocketeers...|\n",
      "|Sci/Tech| A company founded by a chemistry researcher at...|\n",
      "|Sci/Tech| It's barely dawn when Mike Fitzpatrick starts ...|\n",
      "|Sci/Tech| Southern California's smog fighting agency wen...|\n",
      "|Sci/Tech|\"The British Department for Education and Skill...|\n",
      "|Sci/Tech|\"confessed author of the Netsky and Sasser viru...|\n",
      "|Sci/Tech|\\\\FOAF/LOAF  and bloom filters have a lot of in...|\n",
      "|Sci/Tech|\"Wiltshire Police warns about \"\"phishing\"\" afte...|\n",
      "|Sci/Tech|In its first two years, the UK's dedicated card...|\n",
      "|Sci/Tech| A group of technology companies  including Tex...|\n",
      "|Sci/Tech| Apple Computer Inc.&lt;AAPL.O&gt; on  Tuesday ...|\n",
      "|Sci/Tech| Free Record Shop, a Dutch music  retail chain,...|\n",
      "|Sci/Tech|A giant 100km colony of ants  which has been di...|\n",
      "|Sci/Tech|                      \"Dolphin groups, or \"\"pods\"\"|\n",
      "|Sci/Tech|Tyrannosaurus rex achieved its massive size due...|\n",
      "|Sci/Tech|  Scientists have discovered irregular lumps be...|\n",
      "|Sci/Tech|  ESAs Mars Express has relayed pictures from o...|\n",
      "|Sci/Tech|When did life begin? One evidential clue stems ...|\n",
      "|Sci/Tech|update Earnings per share rise compared with a ...|\n",
      "+--------+--------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "news_df = spark.read\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .csv(\"news_category_test.csv\")\\\n",
    "                .withColumnRenamed(\"description\", \"text\")\n",
    "\n",
    "news_df.show(truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk Embeddings\n",
    "\n",
    "This annotator utilizes `WordEmbeddings` or `BertEmbeddings` to generate chunk embeddings from either `TextMatcher`, `RegexMatcher`, `Chunker`, `NGramGenerator`, or `NerConverter` outputs.\n",
    "\n",
    "`setPoolingStrategy`: Choose how you would like to aggregate Word Embeddings to Sentence Embeddings: `AVERAGE` or `SUM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(category='Business', text=\"Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\"),\n",
       " Row(category='Sci/Tech', text=' TORONTO, Canada    A second team of rocketeers competing for the  #36;10 million Ansari X Prize, a contest for privately funded suborbital space flight, has officially announced the first launch date for its manned rocket.'),\n",
       " Row(category='Sci/Tech', text=' A company founded by a chemistry researcher at the University of Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the building blocks of proteins.')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = ['parent firm', 'economy', 'amino acids']\n",
    "\n",
    "with open ('entities.txt', 'w') as f:\n",
    "    for i in entities:\n",
    "        f.write(i+'\\n')\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "entity_extractor = TextMatcher() \\\n",
    "                      .setInputCols([\"document\",'token'])\\\n",
    "                      .setOutputCol(\"entities\")\\\n",
    "                      .setEntities(\"entities.txt\")\\\n",
    "                      .setCaseSensitive(False)\\\n",
    "                      .setEntityValue('entities')\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[documentAssembler,\n",
    "                               tokenizer,\n",
    "                               entity_extractor])\n",
    "\n",
    "result = nlpPipeline.fit(news_df).transform(news_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(result=['parent firm']), Row(result=[]), Row(result=['amino acids'])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select('entities.result').take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "chunk_embeddings = ChunkEmbeddings() \\\n",
    "                      .setInputCols([\"entities\", \"embeddings\"]) \\\n",
    "                      .setOutputCol(\"chunk_embeddings\") \\\n",
    "                      .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "glove_embeddings = WordEmbeddingsModel.pretrained('glove_100d')\\\n",
    "    .setInputCols([\"document\", \"token\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[documentAssembler,\n",
    "                               tokenizer,\n",
    "                               entity_extractor,\n",
    "                               glove_embeddings,\n",
    "                               chunk_embeddings])\n",
    "\n",
    "result = nlpPipeline.fit(news_df).transform(news_df.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------------------------------------------------------------------------+\n",
      "|   entities|                                                                                    chunk_embeddings|\n",
      "+-----------+----------------------------------------------------------------------------------------------------+\n",
      "|parent firm|[0.45683652, -0.105479494, -0.34525, -0.143924, -0.192452, -0.33616, -0.22334, -0.208185, -0.3673...|\n",
      "|amino acids|[-0.3861, 0.054408997, -0.287795, -0.33318, 0.375065, -0.185539, -0.330525, -0.214415, -0.73892, ...|\n",
      "+-----------+----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = result.select(F.explode(F.arrays_zip(result.entities.result,\n",
    "                                                 result.chunk_embeddings.embeddings)).alias(\"cols\")) \\\n",
    "                  .select(F.expr(\"cols['0']\").alias(\"entities\"),\n",
    "                          F.expr(\"cols['1']\").alias(\"chunk_embeddings\"))\n",
    "\n",
    "result_df.show(truncate=100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "sparknlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8b81146af0a3e5653a315622171ee30f7af15821bda096dcb17032694ac0d21c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
