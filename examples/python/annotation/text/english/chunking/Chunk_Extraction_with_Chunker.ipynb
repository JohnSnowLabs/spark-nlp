{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/chunking/Chunk_Extraction_with_Chunker.ipynb)\n",
    "\n",
    "\n",
    "# **Chunk Extraction with Chunker**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples we look at how to extract chunks from POS tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. Colab Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyspark==3.3.0  spark-nlp==4.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  4.2.8\n",
      "Apache Spark version:  3.3.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.34:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fca34fd4970>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Spark Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -O news_category_test.csv https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------------------------+\n",
      "|category|                                              text|\n",
      "+--------+--------------------------------------------------+\n",
      "|Business|Unions representing workers at Turner   Newall ...|\n",
      "|Sci/Tech| TORONTO, Canada    A second team of rocketeers...|\n",
      "|Sci/Tech| A company founded by a chemistry researcher at...|\n",
      "|Sci/Tech| It's barely dawn when Mike Fitzpatrick starts ...|\n",
      "|Sci/Tech| Southern California's smog fighting agency wen...|\n",
      "|Sci/Tech|\"The British Department for Education and Skill...|\n",
      "|Sci/Tech|\"confessed author of the Netsky and Sasser viru...|\n",
      "|Sci/Tech|\\\\FOAF/LOAF  and bloom filters have a lot of in...|\n",
      "|Sci/Tech|\"Wiltshire Police warns about \"\"phishing\"\" afte...|\n",
      "|Sci/Tech|In its first two years, the UK's dedicated card...|\n",
      "|Sci/Tech| A group of technology companies  including Tex...|\n",
      "|Sci/Tech| Apple Computer Inc.&lt;AAPL.O&gt; on  Tuesday ...|\n",
      "|Sci/Tech| Free Record Shop, a Dutch music  retail chain,...|\n",
      "|Sci/Tech|A giant 100km colony of ants  which has been di...|\n",
      "|Sci/Tech|                      \"Dolphin groups, or \"\"pods\"\"|\n",
      "|Sci/Tech|Tyrannosaurus rex achieved its massive size due...|\n",
      "|Sci/Tech|  Scientists have discovered irregular lumps be...|\n",
      "|Sci/Tech|  ESAs Mars Express has relayed pictures from o...|\n",
      "|Sci/Tech|When did life begin? One evidential clue stems ...|\n",
      "|Sci/Tech|update Earnings per share rise compared with a ...|\n",
      "+--------+--------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "news_df = spark.read\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .csv(\"news_category_test.csv\")\\\n",
    "                .withColumnRenamed(\"description\", \"text\")\n",
    "\n",
    "news_df.show(truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chunker**\n",
    "\n",
    "Meaningful phrase matching. This annotator matches a pattern of part-of-speech tags in order to return meaningful phrases from document\n",
    "\n",
    "> **Output type**: Chunk\n",
    "\n",
    "> **Input types**: Document, POS\n",
    "\n",
    "Functions:\n",
    "\n",
    "üîç`setRegexParsers(patterns)`: A list of regex patterns to match chunks, for example: Array(‚Äú‚ÄπDT‚Ä∫?‚ÄπJJ‚Ä∫*‚ÄπNN‚Ä∫\n",
    "\n",
    "üîç`addRegexParser(patterns)`: adds a pattern to the current list of chunk patterns, for example: ‚Äú‚ÄπDT‚Ä∫?‚ÄπJJ‚Ä∫*‚ÄπNN‚Ä∫‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='Chunker_382083d14c71', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
       " Param(parent='Chunker_382083d14c71', name='inputCols', doc='previous annotations columns, if renamed'): ['document',\n",
       "  'pos'],\n",
       " Param(parent='Chunker_382083d14c71', name='outputCol', doc='output annotation column. can be left default.'): 'chunk',\n",
       " Param(parent='Chunker_382083d14c71', name='regexParsers', doc='an array of grammar based chunk parsers'): ['<NNP>+',\n",
       "  '<DT>?<JJ>*<NN>']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying POS chunker to find a custom pattern\n",
    "\n",
    "chunker = Chunker() \\\n",
    "            .setInputCols([\"document\", \"pos\"]) \\\n",
    "            .setOutputCol(\"chunk\") \\\n",
    "            .setRegexParsers([\"<NNP>+\", \"<DT>?<JJ>*<NN>\"])\n",
    "\n",
    "# NNP: Proper Noun\n",
    "# NN: COmmon Noun\n",
    "# DT: Determinator (e.g. the)\n",
    "# JJ: Adjective\n",
    "\n",
    "chunker.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 3.9 MB\n",
      "[OK!]\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|category|                text|            document|               token|                stem|               lemma|                 pos|               chunk|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Business|Unions representi...|[{document, 0, 12...|[{token, 0, 5, Un...|[{token, 0, 5, un...|[{token, 0, 5, Un...|[{pos, 0, 5, NNP,...|[{chunk, 0, 5, Un...|\n",
      "|Sci/Tech| TORONTO, Canada ...|[{document, 0, 22...|[{token, 1, 7, TO...|[{token, 1, 7, to...|[{token, 1, 7, TO...|[{pos, 1, 7, NNP,...|[{chunk, 1, 7, TO...|\n",
      "|Sci/Tech| A company founde...|[{document, 0, 20...|[{token, 1, 1, A,...|[{token, 1, 1, a,...|[{token, 1, 1, A,...|[{pos, 1, 1, DT, ...|[{chunk, 52, 61, ...|\n",
      "|Sci/Tech| It's barely dawn...|[{document, 0, 26...|[{token, 1, 4, It...|[{token, 1, 4, it...|[{token, 1, 4, It...|[{pos, 1, 4, NNP,...|[{chunk, 1, 4, It...|\n",
      "|Sci/Tech| Southern Califor...|[{document, 0, 17...|[{token, 1, 8, So...|[{token, 1, 8, so...|[{token, 1, 8, So...|[{pos, 1, 8, NNP,...|[{chunk, 1, 21, S...|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "\n",
    "lemmatizer = LemmatizerModel.pretrained('lemma_antbnc', 'en') \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"lemma\")\n",
    "\n",
    "pos = PerceptronModel.pretrained(\"pos_anc\", 'en')\\\n",
    "        .setInputCols(\"document\", \"token\")\\\n",
    "        .setOutputCol(\"pos\")\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[documentAssembler,\n",
    "                              tokenizer,\n",
    "                              stemmer,\n",
    "                              lemmatizer,\n",
    "                              pos,\n",
    "                              chunker])\n",
    "\n",
    "result = nlpPipeline.fit(news_df).transform(news_df.limit(100))\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unions</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Turner   Newall</td>\n",
       "      <td>31</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Federal Mogul</td>\n",
       "      <td>113</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stricken</td>\n",
       "      <td>92</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>parent</td>\n",
       "      <td>101</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>firm</td>\n",
       "      <td>108</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TORONTO</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Canada</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ansari X Prize</td>\n",
       "      <td>82</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A second team</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             chunk  begin  end\n",
       "0           Unions      0    5\n",
       "1  Turner   Newall     31   45\n",
       "2    Federal Mogul    113  125\n",
       "3         stricken     92   99\n",
       "4           parent    101  106\n",
       "5             firm    108  111\n",
       "6          TORONTO      1    7\n",
       "7           Canada     10   15\n",
       "8   Ansari X Prize     82   95\n",
       "9    A second team     20   32"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result.select(F.explode(F.arrays_zip(result.chunk.result,\n",
    "                                                 result.chunk.begin,\n",
    "                                                 result.chunk.end)).alias(\"cols\")) \\\n",
    "                  .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "                          F.expr(\"cols['1']\").alias(\"begin\"),\n",
    "                          F.expr(\"cols['2']\").alias(\"end\")).toPandas()\n",
    "\n",
    "result_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
