{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/annotation/english/coreference-resolution/Coreference_Resolution_SpanBertCorefModel.ipynb)"
   ],
   "metadata": {
    "id": "Dm865JXIqAQ9"
   },
   "id": "Dm865JXIqAQ9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Coreference Resolution with SpanBertCorefModel\n",
    "\n",
    "SpanBertCorefModel is a coreference resolution model that identifies expressions which refer to the same entity in a\n",
    "text. For example, given a sentence \"John told Mary he would like to borrow a book from her.\"\n",
    "the model will link \"he\" to \"John\" and \"her\" to \"Mary\".\n",
    "\n",
    "This example will show how to use a pretrained model."
   ],
   "metadata": {
    "id": "ThzZq5KVsGcw"
   },
   "id": "ThzZq5KVsGcw"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Colab Setup\n",
    "\n",
    "The following cell will install Spark NLP in a Colab notebook. If this notebook is run locally it should be skipped."
   ],
   "metadata": {
    "id": "s5--DnBP3Spa"
   },
   "id": "s5--DnBP3Spa"
  },
  {
   "cell_type": "code",
   "source": [
    "# This is only to setup PySpark and Spark NLP on Colab\n",
    "!wget https://setup.johnsnowlabs.com/colab.sh -O - | bash\n",
    "\n",
    "# to process audio files\n",
    "!pip install -q pyspark librosa"
   ],
   "metadata": {
    "id": "qrCJxuFts9nF"
   },
   "id": "qrCJxuFts9nF",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start a Spark NLP session:"
   ],
   "metadata": {
    "id": "zQ2JdVlT32iX"
   },
   "id": "zQ2JdVlT32iX"
  },
  {
   "cell_type": "code",
   "source": [
    "import sparknlp\n",
    "# let's start Spark with Spark NLP\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(sparknlp.version())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n88cWKtEtD0-",
    "outputId": "8bfbd519-ab36-4c56-a663-d580654912b0"
   },
   "id": "n88cWKtEtD0-",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4.2.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Using a pretrained `SpanBertCorefModel` in a Pipeline"
   ],
   "metadata": {
    "id": "8dEhKuzb3X3E"
   },
   "id": "8dEhKuzb3X3E"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee0af780-5560-45fe-8d57-0ff2eb188b0e",
   "metadata": {
    "id": "ee0af780-5560-45fe-8d57-0ff2eb188b0e"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "SpanBertCorefModel requires `DOCUMENT` and `TOKEN` type annotations. these are extracted first before being fed to the pretrained model for classification."
   ],
   "metadata": {
    "id": "m57FA0xU3_AP"
   },
   "id": "m57FA0xU3_AP"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bf4c5ca-fda9-41b9-aaaf-833bde7ffeef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bf4c5ca-fda9-41b9-aaaf-833bde7ffeef",
    "outputId": "fe4398df-b20a-4800-b26d-9b6d3667e767"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "spanbert_base_coref download started this may take some time.\n",
      "Approximate size to download 540.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentences\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentences\"]) \\\n",
    "    .setOutputCol(\"tokens\")\n",
    "\n",
    "coref = SpanBertCorefModel() \\\n",
    "    .pretrained() \\\n",
    "    .setInputCols([\"sentences\", \"tokens\"]) \\\n",
    "    .setOutputCol(\"corefs\")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    coref\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create some data so we can test the pipeline:"
   ],
   "metadata": {
    "id": "UJTUrmVs4K2R"
   },
   "id": "UJTUrmVs4K2R"
  },
  {
   "cell_type": "code",
   "source": [
    "data = spark.createDataFrame([\n",
    "    [\"John loves Mary because she knows how to treat him. She is also fond of him. John said something to Mary but she didn't respond to him.\"],\n",
    "]).toDF(\"text\")"
   ],
   "metadata": {
    "id": "jid-XQAe39MO"
   },
   "id": "jid-XQAe39MO",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data is then fit to the pipeline and we can extract the coreferences with an example query like so"
   ],
   "metadata": {
    "id": "0nylNATd4RiE"
   },
   "id": "0nylNATd4RiE"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcc442a4-98b1-49a3-9c47-62d42f4daa07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcc442a4-98b1-49a3-9c47-62d42f4daa07",
    "outputId": "1e970acf-d031-440d-efc7-9e30c1474fe3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+------------------------------------------------------------------------------------+\n",
      "|token|metadata                                                                            |\n",
      "+-----+------------------------------------------------------------------------------------+\n",
      "|Mary |{head.sentence -> -1, head -> ROOT, head.begin -> -1, head.end -> -1, sentence -> 0}|\n",
      "|she  |{head.sentence -> 0, head -> Mary, head.begin -> 11, head.end -> 14, sentence -> 0} |\n",
      "|She  |{head.sentence -> 0, head -> Mary, head.begin -> 11, head.end -> 14, sentence -> 1} |\n",
      "|Mary |{head.sentence -> 0, head -> Mary, head.begin -> 11, head.end -> 14, sentence -> 2} |\n",
      "|she  |{head.sentence -> 0, head -> Mary, head.begin -> 11, head.end -> 14, sentence -> 2} |\n",
      "|John |{head.sentence -> -1, head -> ROOT, head.begin -> -1, head.end -> -1, sentence -> 0}|\n",
      "|him  |{head.sentence -> 0, head -> John, head.begin -> 0, head.end -> 3, sentence -> 0}   |\n",
      "|him  |{head.sentence -> 0, head -> John, head.begin -> 0, head.end -> 3, sentence -> 1}   |\n",
      "|John |{head.sentence -> 0, head -> John, head.begin -> 0, head.end -> 3, sentence -> 2}   |\n",
      "|him  |{head.sentence -> 0, head -> John, head.begin -> 0, head.end -> 3, sentence -> 2}   |\n",
      "+-----+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(data)\n",
    "\n",
    "model.transform(data) \\\n",
    "    .selectExpr(\"explode(corefs) AS coref\") \\\n",
    "    .selectExpr(\"coref.result as token\", \"coref.metadata\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "CrFFcdbEwBdt"
   },
   "id": "CrFFcdbEwBdt",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
