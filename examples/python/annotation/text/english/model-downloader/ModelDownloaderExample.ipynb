{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/model-downloader/ModelDownloaderExample.ipynb)\n",
    "\n",
    "# Running Pretrained models\n",
    "\n",
    "In the following example, we walk-through different use cases of some of our Pretrained models and pipelines which could be used off the shelf.\n",
    "\n",
    "There is BasicPipeline which will return tokens, normalized tokens, lemmas and part of speech tags. The AdvancedPipeline will return same as the BasicPipeline plus Stems, Spell Checked tokens and NER entities using the CRF model. All the pipelines and pre trained models are downloaded from internet at run time hence would require internet access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell when you are using Spark NLP on Google Colab\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Call necessary imports and create the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import ResourceDownloader\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  4.3.1\n",
      "Apache Spark version:  3.3.0\n"
     ]
    }
   ],
   "source": [
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create a dummy spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l = [\n",
    "  (1,'To be or not to be'),\n",
    "  (2,'This is it!')\n",
    "]\n",
    "\n",
    "data = spark.createDataFrame(l, ['docID','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. We use predefined BasicPipeline in order to annotate a dataframe with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_ml download started this may take some time.\n",
      "Approx size to download 9.2 MB\n",
      "[OK!]\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|docID|              text|            document|            sentence|               token|               spell|              lemmas|               stems|                 pos|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|To be or not to be|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 1, To...|[{token, 0, 1, To...|[{token, 0, 1, To...|[{token, 0, 1, to...|[{pos, 0, 1, TO, ...|\n",
      "|    2|       This is it!|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 3, Th...|[{token, 0, 3, Th...|[{token, 0, 3, Th...|[{token, 0, 3, th...|[{pos, 0, 3, DT, ...|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download predefined - pipelines\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "explain_document_ml = PretrainedPipeline(\"explain_document_ml\")\n",
    "basic_data = explain_document_ml.transform(data) \n",
    "basic_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also annotate a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': ['This world is made up of good and bad things'],\n",
       " 'spell': ['This',\n",
       "  'world',\n",
       "  'is',\n",
       "  'made',\n",
       "  'up',\n",
       "  'of',\n",
       "  'good',\n",
       "  'and',\n",
       "  'bad',\n",
       "  'things'],\n",
       " 'pos': ['DT', 'NN', 'VBZ', 'VBN', 'RP', 'IN', 'JJ', 'CC', 'JJ', 'NNS'],\n",
       " 'lemmas': ['This',\n",
       "  'world',\n",
       "  'be',\n",
       "  'make',\n",
       "  'up',\n",
       "  'of',\n",
       "  'good',\n",
       "  'and',\n",
       "  'bad',\n",
       "  'thing'],\n",
       " 'token': ['This',\n",
       "  'world',\n",
       "  'is',\n",
       "  'made',\n",
       "  'up',\n",
       "  'of',\n",
       "  'good',\n",
       "  'and',\n",
       "  'bad',\n",
       "  'things'],\n",
       " 'stems': ['thi',\n",
       "  'world',\n",
       "  'i',\n",
       "  'made',\n",
       "  'up',\n",
       "  'of',\n",
       "  'good',\n",
       "  'and',\n",
       "  'bad',\n",
       "  'thing'],\n",
       " 'sentence': ['This world is made up of good and bad things']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# annotat quickly from string\n",
    "explain_document_ml.annotate(\"This world is made up of good and bad things\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Now we intend to use one of the fast pretrained models such as Preceptron model which is a POS model trained with ANC American Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[OK!]\n",
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 3.9 MB\n",
      "[OK!]\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|docID|              text|            document|            sentence|               token|                 pos|     word_embeddings|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|To be or not to be|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 1, To...|[{pos, 0, 1, TO, ...|[{word_embeddings...|\n",
      "|    2|       This is it!|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{word_embeddings...|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "wordEmbeddings = WordEmbeddingsModel.pretrained().setOutputCol(\"word_embeddings\")    \n",
    "\n",
    "# download directly - models\n",
    "pos = PerceptronModel.pretrained() \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"pos\")\n",
    "    \n",
    "advancedPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, pos, wordEmbeddings])\n",
    "\n",
    "output = advancedPipeline.fit(data).transform(data)\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Now we proceed to download a Fast CRF Named Entity Recognitionl which is trained with Glove embeddings. Then, we retrieve the `advancedPipeline` and combine these models to use them appropriately meeting their requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_crf download started this may take some time.\n",
      "Approximate size to download 10.2 MB\n",
      "[OK!]\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|docID|              text|            document|            sentence|               token|                 pos|     word_embeddings|                 ner|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|To be or not to be|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 1, To...|[{pos, 0, 1, TO, ...|[{word_embeddings...|[{named_entity, 0...|\n",
      "|    2|       This is it!|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{word_embeddings...|[{named_entity, 0...|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ner = NerCrfModel.pretrained()\n",
    "ner.setInputCols([\"pos\", \"token\", \"document\", \"word_embeddings\"]).setOutputCol(\"ner\")\n",
    "\n",
    "annotation_data = advancedPipeline.fit(data).transform(data)\n",
    "\n",
    "pos_tagged = pos.transform(annotation_data)\n",
    "ner_tagged = ner.transform(pos_tagged)\n",
    "ner_tagged.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Finally, lets try a pre trained sentiment analysis pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze_sentiment download started this may take some time.\n",
      "Approx size to download 4.9 MB\n",
      "[OK!]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'checked': ['This', 'is', 'a', 'good', 'movie', '!!!'],\n",
       " 'document': ['This is a good movie!!!'],\n",
       " 'sentiment': ['positive'],\n",
       " 'token': ['This', 'is', 'a', 'good', 'movie', '!!!'],\n",
       " 'sentence': ['This is a good movie!!!']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PretrainedPipeline(\"analyze_sentiment\").annotate(\"This is a good movie!!!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ModelDownloaderExample.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
