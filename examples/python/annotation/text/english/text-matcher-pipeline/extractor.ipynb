{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/text-matcher-pipeline/extractor.ipynb\n",
    ")\n",
    "\n",
    "# Simple Text Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell when you are using Spark NLP on Google Colab\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we walk-through our straight forward Text Matcher Annotator.\n",
    "\n",
    "This annotator will take a list of sentences from a text file and look them up in the given target dataset.\n",
    "\n",
    "This annotator is an Annotator Model and hence does not require training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Call necessary imports and set the resource path to read local data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "import time\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  4.3.1\n",
      "Apache Spark version:  3.0.2\n"
     ]
    }
   ],
   "source": [
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create appropriate annotators. We are using Sentence Detection and Tokenizing the sentence. The Finisher will clean the annotations and exclude the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "  .setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "  .setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "extractor = TextMatcher()\\\n",
    "  .setEntities(\"entities.txt\")\\\n",
    "  .setInputCols([\"token\", \"sentence\"])\\\n",
    "  .setOutputCol(\"entites\")\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"entites\"]) \\\n",
    "    .setIncludeMetadata(False) \\\n",
    "    .setCleanAnnotations(True)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "    documentAssembler,\n",
    "    sentenceDetector,\n",
    "    tokenizer,\n",
    "    extractor,\n",
    "    finisher\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Load the input data to be annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/tmp/sentiment.parquet.zip': No such file or directory\n",
      "--2023-02-20 11:50:49--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment.parquet.zip\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.60.24, 52.216.220.80, 54.231.233.48, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.60.24|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 76127532 (73M) [application/zip]\n",
      "Saving to: ‘sentiment.parquet.zip’\n",
      "\n",
      "sentiment.parquet.z 100%[===================>]  72,60M  22,8MB/s    in 3,7s    \n",
      "\n",
      "2023-02-20 11:50:54 (19,6 MB/s) - ‘sentiment.parquet.zip’ saved [76127532/76127532]\n",
      "\n",
      "Archive:  sentiment.parquet.zip\n",
      "   creating: sentiment.parquet/\n",
      "  inflating: sentiment.parquet/.part-00002-08092d15-dd8c-40f9-a1df-641a1a4b1698.snappy.parquet.crc  \n",
      "  inflating: sentiment.parquet/part-00002-08092d15-dd8c-40f9-a1df-641a1a4b1698.snappy.parquet  \n",
      "  inflating: sentiment.parquet/part-00003-08092d15-dd8c-40f9-a1df-641a1a4b1698.snappy.parquet  \n",
      "  inflating: sentiment.parquet/.part-00000-08092d15-dd8c-40f9-a1df-641a1a4b1698.snappy.parquet.crc  \n",
      "  inflating: sentiment.parquet/part-00001-08092d15-dd8c-40f9-a1df-641a1a4b1698.snappy.parquet  \n",
      " extracting: sentiment.parquet/_SUCCESS  \n",
      "  inflating: sentiment.parquet/.part-00003-08092d15-dd8c-40f9-a1df-641a1a4b1698.snappy.parquet.crc  \n",
      "  inflating: sentiment.parquet/part-00000-08092d15-dd8c-40f9-a1df-641a1a4b1698.snappy.parquet  \n",
      "  inflating: sentiment.parquet/.part-00001-08092d15-dd8c-40f9-a1df-641a1a4b1698.snappy.parquet.crc  \n"
     ]
    }
   ],
   "source": [
    "! rm /tmp/sentiment.parquet.zip\n",
    "! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment.parquet.zip \n",
    "! unzip sentiment.parquet.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+\n",
      "|itemid|sentiment|                text|\n",
      "+------+---------+--------------------+\n",
      "|799033|        0|@FrankomQ8 What's...|\n",
      "|799034|        1|@FranKoUK guitar ...|\n",
      "|799035|        0|@frankparenteau u...|\n",
      "|799036|        1|@frankparenteau w...|\n",
      "|799037|        1|@FrankPatris dude...|\n",
      "|799038|        0|@FrankRamblings a...|\n",
      "|799039|        1|@frankroberts  ni...|\n",
      "|799040|        0|@frankroberts ur ...|\n",
      "|799041|        1|@FrankS Breaking ...|\n",
      "|799042|        1|@frankschultelad ...|\n",
      "|799043|        0|@frankshorter Wol...|\n",
      "|799044|        0|@franksting - its...|\n",
      "|799045|        1|@franksting Ha! D...|\n",
      "|799046|        1|@franksting yeah,...|\n",
      "|799047|        1|@franksting yes, ...|\n",
      "|799048|        1|@FrankSylar arn't...|\n",
      "|799049|        1|    @frankules WO ? |\n",
      "|799050|        0|@frankwkelly I'm ...|\n",
      "|799051|        1|@FrankXSalinas Th...|\n",
      "|799052|        1|@frankybhoy93 tha...|\n",
      "+------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark. \\\n",
    "        read. \\\n",
    "        parquet(\"sentiment.parquet\"). \\\n",
    "        limit(1000).cache()\n",
    "data.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Running the fit for sentence detection and tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting\n",
      "Fitting is ended\n"
     ]
    }
   ],
   "source": [
    "print(\"Start fitting\")\n",
    "model = pipeline.fit(data)\n",
    "print(\"Fitting is ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Runing the transform on data to do text matching. It will append a new coloumns with matched entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+----------------+\n",
      "|itemid|sentiment|                text|finished_entites|\n",
      "+------+---------+--------------------+----------------+\n",
      "|799033|        0|@FrankomQ8 What's...|              []|\n",
      "|799034|        1|@FranKoUK guitar ...|[guitar lessons]|\n",
      "|799035|        0|@frankparenteau u...|              []|\n",
      "|799036|        1|@frankparenteau w...|              []|\n",
      "|799037|        1|@FrankPatris dude...|              []|\n",
      "|799038|        0|@FrankRamblings a...|              []|\n",
      "|799039|        1|@frankroberts  ni...|              []|\n",
      "|799040|        0|@frankroberts ur ...|              []|\n",
      "|799041|        1|@FrankS Breaking ...|              []|\n",
      "|799042|        1|@frankschultelad ...|              []|\n",
      "|799043|        0|@frankshorter Wol...|              []|\n",
      "|799044|        0|@franksting - its...|              []|\n",
      "|799045|        1|@franksting Ha! D...|              []|\n",
      "|799046|        1|@franksting yeah,...|              []|\n",
      "|799047|        1|@franksting yes, ...|              []|\n",
      "|799048|        1|@FrankSylar arn't...|              []|\n",
      "|799049|        1|    @frankules WO ? |              []|\n",
      "|799050|        0|@frankwkelly I'm ...|              []|\n",
      "|799051|        1|@FrankXSalinas Th...|              []|\n",
      "|799052|        1|@frankybhoy93 tha...|              []|\n",
      "+------+---------+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+---------+--------------------+----------------+\n",
      "|itemid|sentiment|                text|finished_entites|\n",
      "+------+---------+--------------------+----------------+\n",
      "|799034|        1|@FranKoUK guitar ...|[guitar lessons]|\n",
      "|799065|        0|@Frannyd oh lame....|       [i think]|\n",
      "|799173|        1|i am seriously sl...|       [i think]|\n",
      "|799869|        1|@FrazzleYeah  yea...|       [i think]|\n",
      "|799898|        0|@freakyfudge that...|       [i think]|\n",
      "|799957|        1|@FreddyMallet Hi!...|       [i think]|\n",
      "|800003|        0|@freecloud but i ...|       [i think]|\n",
      "+------+---------+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted = model.transform(data)\n",
    "extracted.show()\n",
    "\n",
    "# filter rows with extracted text\n",
    "extracted\\\n",
    ".filter(\"size(finished_entites) != 0\") \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. The model could be saved locally and reloaded to run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.write().overwrite().save(\"./extractor_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+----------------+\n",
      "|itemid|sentiment|                text|finished_entites|\n",
      "+------+---------+--------------------+----------------+\n",
      "|799034|        1|@FranKoUK guitar ...|[guitar lessons]|\n",
      "|799065|        0|@Frannyd oh lame....|       [i think]|\n",
      "|799173|        1|i am seriously sl...|       [i think]|\n",
      "|799869|        1|@FrazzleYeah  yea...|       [i think]|\n",
      "|799898|        0|@freakyfudge that...|       [i think]|\n",
      "|799957|        1|@FreddyMallet Hi!...|       [i think]|\n",
      "|800003|        0|@freecloud but i ...|       [i think]|\n",
      "+------+---------+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import  Pipeline\n",
    "\n",
    "sameModel = PipelineModel.read().load(\"./extractor_model\")\n",
    "\n",
    "sameModel.transform(data) \\\n",
    ".filter(\"size(finished_entites) != 0\") \\\n",
    ".show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "extractor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
