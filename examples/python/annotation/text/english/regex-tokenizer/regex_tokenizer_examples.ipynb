{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoiHtKZMdIji"
   },
   "source": [
    "# Document Normalizer annotator notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a7c65f64-07d6-4355-97a0-0a371d83116c",
     "showTitle": false,
     "title": ""
    },
    "id": "IR5fSl51dIjk"
   },
   "source": [
    "# Set up Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HMoSFm4YdIjl",
    "outputId": "07d0a493-0513-4d90-862a-d561c506b611",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2022-12-23 14:48:19--  http://setup.johnsnowlabs.com/colab.sh\n",
      "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
      "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://setup.johnsnowlabs.com/colab.sh [following]\n",
      "--2022-12-23 14:48:20--  https://setup.johnsnowlabs.com/colab.sh\n",
      "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
      "--2022-12-23 14:48:20--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1191 (1.2K) [text/plain]\n",
      "Saving to: ‘STDOUT’\n",
      "\n",
      "-                   100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-12-23 14:48:20 (51.4 MB/s) - written to stdout [1191/1191]\n",
      "\n",
      "Installing PySpark 3.2.3 and Spark NLP 4.2.6\n",
      "setup Colab for PySpark 3.2.3 and Spark NLP 4.2.6\n",
      "\u001B[K     |████████████████████████████████| 281.5 MB 52 kB/s \n",
      "\u001B[K     |████████████████████████████████| 453 kB 76.7 MB/s \n",
      "\u001B[K     |████████████████████████████████| 199 kB 64.9 MB/s \n",
      "\u001B[?25h  Building wheel for pyspark (setup.py) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# This is only to setup PySpark and Spark NLP on Colab\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJDSju19dIjl"
   },
   "source": [
    "# Start Spark NLP session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZFg6pYqrdIjl"
   },
   "outputs": [],
   "source": [
    "# Import Spark NLP\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import sparknlp\n",
    "\n",
    "spark =sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1bnQU73ydIjm",
    "outputId": "93785975-8f3d-4029-c6fd-8f390c21a4e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff246741df0>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://598fc1769a2f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5G_onsRFdIjm"
   },
   "source": [
    "# Regex Tokenizer annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b4efb61f-6011-4ba1-a0ad-6c229f69e3d9",
     "showTitle": true,
     "title": "DocumentNormalizer overview and parameters"
    },
    "id": "XfDocvQ7dIjm",
    "outputId": "463d4ed3-9a15-4328-e248-b7b5b897f819",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|1. T1-T2 DATE**[1...|\n",
      "+--------------------+\n",
      "\n",
      "+---------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                               |document                                                                                     |sentence                                                                                     |regexToken                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+---------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1. T1-T2 DATE**[12/24/13] $1.99 () (10/12), ph+ 90%|[{document, 0, 50, 1. T1-T2 DATE**[12/24/13] $1.99 () (10/12), ph+ 90%, {sentence -> 0}, []}]|[{document, 0, 50, 1. T1-T2 DATE**[12/24/13] $1.99 () (10/12), ph+ 90%, {sentence -> 0}, []}]|[{token, 0, 0, 1, {sentence -> 0}, []}, {token, 1, 1, ., {sentence -> 0}, []}, {token, 3, 4, T1, {sentence -> 0}, []}, {token, 5, 5, -, {sentence -> 0}, []}, {token, 6, 7, T2, {sentence -> 0}, []}, {token, 9, 12, DATE, {sentence -> 0}, []}, {token, 13, 13, *, {sentence -> 0}, []}, {token, 14, 14, *, {sentence -> 0}, []}, {token, 15, 15, [, {sentence -> 0}, []}, {token, 16, 23, 12/24/13, {sentence -> 0}, []}, {token, 24, 24, ], {sentence -> 0}, []}, {token, 26, 26, $, {sentence -> 0}, []}, {token, 27, 27, 1, {sentence -> 0}, []}, {token, 28, 28, ., {sentence -> 0}, []}, {token, 29, 30, 99, {sentence -> 0}, []}, {token, 32, 33, (), {sentence -> 0}, []}, {token, 35, 41, (10/12), {sentence -> 0}, []}, {token, 42, 42, ,, {sentence -> 0}, []}, {token, 44, 45, ph, {sentence -> 0}, []}, {token, 46, 46, +, {sentence -> 0}, []}, {token, 48, 49, 90, {sentence -> 0}, []}, {token, 50, 50, %, {sentence -> 0}, []}]|\n",
      "+---------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StringType\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import sparknlp\n",
    "\n",
    "content = \"1. T1-T2 DATE**[12/24/13] $1.99 () (10/12), ph+ 90%\"\n",
    "pattern = \"\\\\s+|(?=[-.:;*+,$&%\\\\[\\\\]])|(?<=[-.:;*+,$&%\\\\[\\\\]])\"\n",
    "\n",
    "df = spark.createDataFrame([content], StringType()).withColumnRenamed(\"value\", \"text\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "sentenceDetector = SentenceDetector() \\\n",
    "      .setInputCols([\"document\"]) \\\n",
    "      .setOutputCol(\"sentence\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer() \\\n",
    "      .setInputCols([\"sentence\"]) \\\n",
    "      .setOutputCol(\"regexToken\") \\\n",
    "      .setPattern(pattern) \\\n",
    "      .setPositionalMask(False)\n",
    "\n",
    "docPatternRemoverPipeline = \\\n",
    "  Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        sentenceDetector,\n",
    "        regexTokenizer])\n",
    "\n",
    "ds = docPatternRemoverPipeline.fit(df).transform(df)\n",
    "\n",
    "ds.show(10, False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "DocumentNormalizer_notebook_doc",
   "notebookOrigID": 3142402907558969,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python [conda env:spknlp270] *",
   "language": "python",
   "name": "conda-env-spknlp270-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
