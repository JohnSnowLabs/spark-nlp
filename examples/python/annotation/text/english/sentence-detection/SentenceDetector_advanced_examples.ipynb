{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/sentence-detection/SentenceDetector_advanced_examples.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sentence Detector](https://sparknlp.org/docs/en/annotators#sentencedetector)\n",
    "\n",
    "Sentence Detector is an annotator that detects sentence boundaries using regular\n",
    "expressions.\n",
    "\n",
    "The following characters are checked as sentence boundaries:\n",
    "\n",
    "1. Lists (\"(i), (ii)\", \"(a), (b)\", \"1., 2.\")\n",
    "2. Numbers\n",
    "3. Abbreviations\n",
    "4. Punctuations\n",
    "5. Multiple Periods\n",
    "6. Geo-Locations/Coordinates (\"N°. 1026.253.553.\")\n",
    "7. Ellipsis (\"...\")\n",
    "8. In-between punctuation\n",
    "9. Quotation marks\n",
    "10. Exclamation Points\n",
    "11. Basic Breakers (\".\", \";\")\n",
    "\n",
    "Let's see how we can customize the annotator to suit specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run this block if you are inside Google Colab to set up Spark NLP otherwise\n",
    "skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version: 4.3.1\n",
      "Apache Spark version: 3.0.2\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "spark = sparknlp.start()\n",
    "\n",
    "\n",
    "print(\"Spark NLP version: {}\".format(sparknlp.version()))\n",
    "print(\"Apache Spark version: {}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example\n",
    "Now we will create the parts for the pipeline. As the SentenceDetector only\n",
    "requires `DOCUMENT` type annotations, the pipeline only requires an additional\n",
    "DocumentAssembler annotator.\n",
    "\n",
    "In this example we assume we have some data that has fixed separators between\n",
    "the sentences and we want to use that separator for detecting the\n",
    "sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+\n",
      "|col                                                          |\n",
      "+-------------------------------------------------------------+\n",
      "|This is a sentence\tThis is another one\tHow about a third one?|\n",
      "+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    documentAssembler,\n",
    "    sentence\n",
    "])\n",
    "\n",
    "data = spark.createDataFrame([\n",
    "    [\"This is a sentence\\tThis is another one\\tHow about a third one?\"]\n",
    "]).toDF(\"text\")\n",
    "\n",
    "result = pipeline.fit(data).transform(data)\n",
    "result.selectExpr(\"explode(sentence.result)\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the sentences are not properly separated by the default settings.\n",
    "We will add the tab character as custom bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|col                   |\n",
      "+----------------------+\n",
      "|This is a sentence    |\n",
      "|This is another one   |\n",
      "|How about a third one?|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\") \\\n",
    "    .setCustomBounds([\"\\t\"])\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    documentAssembler,\n",
    "    sentence\n",
    "])\n",
    "\n",
    "result = pipeline.fit(data).transform(data)\n",
    "result.selectExpr(\"explode(sentence.result)\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example we will see, how we can exclude some characters that might\n",
    "be detected as sentence boundaries and in turn reconstruct the default rules.\n",
    "\n",
    "These rules are taken from the [`PragmaticContentFormatter`](https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/main/scala/com/johnsnowlabs/nlp/annotators/sbd/pragmatic/PragmaticContentFormatter.scala)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = [\n",
    "    \"(\\\\()[a-z]+\\\\)|^[a-z]+\\\\)\",\n",
    "    '\\\\s\\\\d{1,2}\\\\.\\\\s|^\\\\d{1,2}\\\\.\\\\s|\\\\s\\\\d{1,2}\\\\.\\\\)|^\\\\d{1,2}\\\\.\\\\)|\\\\s\\\\-\\\\d{1,2}\\\\.\\\\s|^\\\\-\\\\d{1,2}\\\\.\\\\s|s\\\\-\\\\d{1,2}\\\\.\\\\)|^\\\\-\\\\d{1,2}(.\\\\))'\n",
    "    ]\n",
    "numbers = [\n",
    "    \"(?<=\\\\d)\\\\.(?=\\\\d)\",\n",
    "    \"\\\\.(?=\\\\d)\",\n",
    "    \"(?<=\\\\d)\\\\.(?=\\\\S)\",\n",
    "]\n",
    "special_abbreviations = [\n",
    "    \"\\\\b[a-zA-Z](?:\\\\.[a-zA-Z])+(?:\\\\.(?!\\\\s[A-Z]))*\",\n",
    "    \"(?i)p\\\\.m\\\\.*\",\n",
    "    \"(?i)a\\\\.m\\\\.*\",\n",
    "]\n",
    "abbreviations = [\n",
    "    \"\\\\.(?='s\\\\s)|\\\\.(?='s\\\\$)|\\\\.(?='s\\\\z)\",\n",
    "    \"(?<=Co)\\\\.(?=\\\\sKG)\",\n",
    "    \"(?<=^[A-Z])\\\\.(?=\\\\s)\",\n",
    "    \"(?<=\\\\s[A-Z])\\\\.(?=\\\\s)\",\n",
    "]\n",
    "punctuations = [\"(?<=\\\\S)[!\\\\?]+(?=\\\\s|\\\\z|\\\\$)\"]\n",
    "multiple_periods = [\"(?<=\\\\w)\\\\.(?=\\\\w)\"]\n",
    "geo_locations = [\"(?<=[a-zA-z]°)\\\\.(?=\\\\s*\\\\d+)\"]\n",
    "ellipsis = [\"\\\\.\\\\.\\\\.(?=\\\\s+[A-Z])\", \"(?<=\\\\S)\\\\.{3}(?=\\\\.\\\\s[A-Z])\"]\n",
    "in_between_punctuation = [\n",
    "    \"(?<=\\\\s|^)'[\\\\w\\\\s?!\\\\.,|'\\\\w]+'(?:\\\\W)\",\n",
    "    \"\\\"[\\\\w\\\\s?!\\\\.,]+\\\"\",\n",
    "    \"\\\\[[\\\\w\\\\s?!\\\\.,]+\\\\]\",\n",
    "    \"\\\\([\\\\w\\\\s?!\\\\.,]+\\\\)\",\n",
    "]\n",
    "quotation_marks = [\"\\\\?(?=(\\\\'|\\\\\\\"))\"]\n",
    "exclamation_points = [\n",
    "    \"\\\\!(?=(\\\\'|\\\\\\\"))\",\n",
    "    \"\\\\!(?=\\\\,\\\\s[a-z])\",\n",
    "    \"\\\\!(?=\\\\s[a-z])\",\n",
    "]\n",
    "basic_breakers = [\"\\\\.\", \";\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we do not want to use the basic breakers (so the period and\n",
    "semicolons). So we will not include those regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [\n",
    "    *lists,\n",
    "    *numbers,\n",
    "    *abbreviations,\n",
    "    *special_abbreviations,\n",
    "    *punctuations,\n",
    "    # *multiple_periods,\n",
    "    *geo_locations,\n",
    "    *ellipsis,\n",
    "    *in_between_punctuation,\n",
    "    *quotation_marks,\n",
    "    *exclamation_points,\n",
    "    # *basic_breakers, # Let's skip the basic breakers.\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+\n",
      "|col                                                    |\n",
      "+-------------------------------------------------------+\n",
      "|this.is.one.sentence\n",
      "This is the second one; not broken|\n",
      "+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\") \\\n",
    "    .setCustomBounds(bounds) \\\n",
    "    .setUseCustomBoundsOnly(True)\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    documentAssembler,\n",
    "    sentence\n",
    "])\n",
    "\n",
    "data = spark.createDataFrame([\n",
    "    [\"this.is.one.sentence\\nThis is the second one; not broken\"]\n",
    "]).toDF(\"text\")\n",
    "\n",
    "result = pipeline.fit(data).transform(data)\n",
    "result.selectExpr(\"explode(sentence.result)\").show(5, False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "words_segmenter_demo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
