{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97EiXueJA9cY"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmxL_blSA9ce"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/zero-shot%20text%20classification/DeBertaForZeroShotClassification.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI7yhCibA9cf"
      },
      "source": [
        "## Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4WQLLrIUA9cg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c18bb4-7045-4a55-b0de-c0f3d2a26fba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing PySpark 3.2.3 and Spark NLP 5.3.1\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 5.3.1\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.8/564.8 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S-XJDfUA9ci"
      },
      "source": [
        "# Download MPNetForQuestionAnswering Model and Create Spark NLP Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4uPbdrSA9ci"
      },
      "source": [
        "Lets create a Spark NLP pipeline with the following stages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KzMHa0HdA9ch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811e0b82-e037-47e8-ebf4-940eb584e848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version 5.3.1\n",
            "Apache Spark version: 3.2.3\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "\n",
        "# for GPU training >> sparknlp.start(gpu = True)\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version\", sparknlp.version())\n",
        "print(\"Apache Spark version:\", spark.version)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DeBertaForZeroShotClassification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "DVHludGFMSCk",
        "outputId": "d8931a8b-4e41-4f4e-bf63-2db2f4fcde98"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sparknlp.annotator.classifier_dl.deberta_for_zero_shot_classification.DeBertaForZeroShotClassification"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>sparknlp.annotator.classifier_dl.deberta_for_zero_shot_classification.DeBertaForZeroShotClassification</b><br/>def __init__(classname=&#x27;com.johnsnowlabs.nlp.annotators.classifier.dl.DeBertaForZeroShotClassification&#x27;, java_model=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/sparknlp/annotator/classifier_dl/deberta_for_zero_shot_classification.py</a>DeBertaForZeroShotClassification using a `ModelForSequenceClassification` trained on NLI (natural language\n",
              "inference) tasks. Equivalent of `DeBertaForSequenceClassification` models, but these models don&#x27;t require a hardcoded\n",
              "number of potential classes, they can be chosen at runtime. It usually means it&#x27;s slower but it is much more\n",
              "flexible.\n",
              "Any combination of sequences and labels can be passed and each combination will be posed as a premise/hypothesis\n",
              "pair and passed to the pretrained model.\n",
              "Pretrained models can be loaded with :meth:`.pretrained` of the companion\n",
              "object:\n",
              "&gt;&gt;&gt; sequenceClassifier = DeBertaForZeroShotClassification.pretrained() \\\n",
              "...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\\n",
              "...     .setOutputCol(&quot;label&quot;)\n",
              "The default model is ``&quot;deberta_base_zero_shot_classifier_mnli_anli_v3&quot;``, if no name is\n",
              "provided.\n",
              "For available pretrained models please see the `Models Hub\n",
              "&lt;https://sparknlp.orgtask=Text+Classification&gt;`__.\n",
              "To see which models are compatible and how to import them see\n",
              "`Import Transformers into Spark NLP ðŸš€\n",
              "&lt;https://github.com/JohnSnowLabs/spark-nlp/discussions/5669&gt;`_.\n",
              "====================== ======================\n",
              "Input Annotation types Output Annotation type\n",
              "====================== ======================\n",
              "``DOCUMENT, TOKEN``    ``CATEGORY``\n",
              "====================== ======================\n",
              "Parameters\n",
              "----------\n",
              "batchSize\n",
              "    Batch size. Large values allows faster processing but requires more\n",
              "    memory, by default 8\n",
              "caseSensitive\n",
              "    Whether to ignore case in tokens for embeddings matching, by default\n",
              "    True\n",
              "configProtoBytes\n",
              "    ConfigProto from tensorflow, serialized into byte array.\n",
              "maxSentenceLength\n",
              "    Max sentence length to process, by default 128\n",
              "coalesceSentences\n",
              "    Instead of 1 class per sentence (if inputCols is `sentence`) output 1\n",
              "    class per document by averaging probabilities in all sentences, by\n",
              "    default False\n",
              "activation\n",
              "    Whether to calculate logits via Softmax or Sigmoid, by default\n",
              "    `&quot;softmax&quot;`.\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; import sparknlp\n",
              "&gt;&gt;&gt; from sparknlp.base import *\n",
              "&gt;&gt;&gt; from sparknlp.annotator import *\n",
              "&gt;&gt;&gt; from pyspark.ml import Pipeline\n",
              "&gt;&gt;&gt; documentAssembler = DocumentAssembler() \\\n",
              "...     .setInputCol(&quot;text&quot;) \\\n",
              "...     .setOutputCol(&quot;document&quot;)\n",
              "&gt;&gt;&gt; tokenizer = Tokenizer() \\\n",
              "...     .setInputCols([&quot;document&quot;]) \\\n",
              "...     .setOutputCol(&quot;token&quot;)\n",
              "&gt;&gt;&gt; sequenceClassifier = DeBertaForZeroShotClassification.pretrained() \\\n",
              "...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\\n",
              "...     .setOutputCol(&quot;label&quot;) \\\n",
              "...     .setCaseSensitive(True)\n",
              "&gt;&gt;&gt; pipeline = Pipeline().setStages([\n",
              "...     documentAssembler,\n",
              "...     tokenizer,\n",
              "...     sequenceClassifier\n",
              "... ])\n",
              "&gt;&gt;&gt; data = spark.createDataFrame([[&quot;I loved this movie when I was a child.&quot;, &quot;It was pretty boring.&quot;]]).toDF(&quot;text&quot;)\n",
              "&gt;&gt;&gt; result = pipeline.fit(data).transform(data)\n",
              "&gt;&gt;&gt; result.select(&quot;label.result&quot;).show(truncate=False)\n",
              "+------+\n",
              "|result|\n",
              "+------+\n",
              "|[pos] |\n",
              "|[neg] |\n",
              "+------+</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 19);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ASQ5Ot2NA9ci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a60ad3f-d9de-4640-db73-26e18835fc56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deberta_base_zero_shot_classifier_mnli_anli_v3 download started this may take some time.\n",
            "Approximate size to download 420.7 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol('text') \\\n",
        "    .setOutputCol('document')\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols(['document']) \\\n",
        "    .setOutputCol('token')\n",
        "\n",
        "zeroShotClassifier = DeBertaForZeroShotClassification \\\n",
        "    .pretrained('deberta_base_zero_shot_classifier_mnli_anli_v3', 'en') \\\n",
        "    .setInputCols(['token', 'document']) \\\n",
        "    .setOutputCol('class') \\\n",
        "    .setCaseSensitive(False) \\\n",
        "    .setCandidateLabels([\"urgent\", \"mobile\", \"travel\", \"movie\", \"music\", \"sport\", \"weather\", \"technology\"])\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "        document_assembler,\n",
        "        tokenizer,\n",
        "        zeroShotClassifier\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENiJegX4A9cj"
      },
      "source": [
        "Lets create a dataframe with some queries to be used as input for the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    ['I urgently need advice on handling a sudden job offer decision!'],\n",
        "    [\"My phone screen just went black and won't turn on, I need a fix asap!\"],\n",
        "    [\"I'm about to miss my connecting flight due to a delay, what are my options?\"],\n",
        "    [\"I'm hosting movie night tomorrow and my projector stopped working, help!\"],\n",
        "    ['My playlist got deleted right before the party, how can I recover it quickly?'],\n",
        "    ['The championship game is tonight and my TV signal is out, what can I do?'],\n",
        "    [\"A storm is coming and I'm not prepared, how can I quickly secure my home?\"],\n",
        "    ['My computer got a virus right before my project deadline, I need immediate assistance!']\n",
        "]\n",
        "\n",
        "data = spark.createDataFrame(examples).toDF(\"text\")"
      ],
      "metadata": {
        "id": "AHYb7QbRIGYS"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ3DGJ6CA9cj"
      },
      "source": [
        "display the results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipeline.fit(data).transform(data)\n",
        "result.select(\"text\", \"class.result\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-rN0D7ZCWkZ",
        "outputId": "7e23b769-e48f-4545-d652-29b0ecfe987f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------+--------+\n",
            "|text                                                                                  |result  |\n",
            "+--------------------------------------------------------------------------------------+--------+\n",
            "|I urgently need advice on handling a sudden job offer decision!                       |[mobile]|\n",
            "|My phone screen just went black and won't turn on, I need a fix asap!                 |[travel]|\n",
            "|I'm about to miss my connecting flight due to a delay, what are my options?           |[sport] |\n",
            "|I'm hosting movie night tomorrow and my projector stopped working, help!              |[movie] |\n",
            "|My playlist got deleted right before the party, how can I recover it quickly?         |[mobile]|\n",
            "|The championship game is tonight and my TV signal is out, what can I do?              |[sport] |\n",
            "|A storm is coming and I'm not prepared, how can I quickly secure my home?             |[travel]|\n",
            "|My computer got a virus right before my project deadline, I need immediate assistance!|[travel]|\n",
            "+--------------------------------------------------------------------------------------+--------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:tempspark]",
      "language": "python",
      "name": "conda-env-tempspark-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}