{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb)\n",
    "\n",
    "\n",
    "# **Text Preprocessing with Spark NLP**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples we look at ways to pre-process text in Spark NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. Colab Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyspark==3.3.0  spark-nlp==4.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  4.3.0\n",
      "Apache Spark version:  3.3.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://6c170f26db25:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbaf5c39950>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Spark Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter is a very good person.\n",
      "My life in Russia is very interesting.\n",
      "John and Peter are brothers. However they don't support each other that much.\n",
      "Lucas Nogal Dunbercker is no longer happy. He has a good car though.\n",
      "Europe is very culture rich. There are huge churches! and big houses!\n"
     ]
    }
   ],
   "source": [
    "with open('./sample-sentences-en.txt') as f:\n",
    "  print (f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "|text                                                                         |\n",
      "+-----------------------------------------------------------------------------+\n",
      "|Peter is a very good person.                                                 |\n",
      "|My life in Russia is very interesting.                                       |\n",
      "|John and Peter are brothers. However they don't support each other that much.|\n",
      "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |\n",
      "|Europe is very culture rich. There are huge churches! and big houses!        |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.read.text('./sample-sentences-en.txt').toDF('text')\n",
    "\n",
    "spark_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Document Assembler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark NLP, we have five different transformers that are mainly used for getting the data in or transform the data from one AnnotatorType to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, the DataFrame you have needs to have a column from one of these types if that column will be fed into an annotator; otherwise, you’d need to use one of the Spark NLP transformers. Here is the list of transformers: DocumentAssembler, TokenAssembler, Doc2Chunk, Chunk2Doc, and the Finisher.\n",
    "\n",
    "So, let’s start with DocumentAssembler(), an entry point to Spark NLP annotators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get through the process in Spark NLP, we need to get raw data transformed into Document type at first. \n",
    "\n",
    "DocumentAssembler() is a special transformer that does this for us; it creates the first annotation of type Document which may be used by annotators down the road.\n",
    "\n",
    "DocumentAssembler() comes from sparknlp.base class and has the following settable parameters. See the full list here and the source code here.\n",
    "\n",
    "\n",
    "| Parametre  | Value | Description |\n",
    "| - | - | - |\n",
    "|**setInputCol()***       |String |The name of the column that will be converted. We can specify only one column here. It can read either a String column or an Array.|\n",
    "|**setOutputCol()*** |optional|The name of the column in Document type that is generated. We can specify only one column here. Default is '**document**'.|\n",
    "|**setIdCol()***  |optional|String type column with id information|\n",
    "|**setMetadataCol()*** |optional|Map type column with metadata information.|\n",
    "|**setCleanupMode()***|optional| Cleaning up options|\n",
    "\n",
    "\n",
    "possible values for setCleanupMode :\n",
    "  ```\n",
    "  disabled: Source kept as original. This is a default.\n",
    "  inplace: removes new lines and tabs.\n",
    "  inplace_full: removes new lines and tabs but also those which were converted to strings (i.e. \\n)\n",
    "  shrink: removes new lines and tabs, plus merging multiple spaces and blank lines to a single space.\n",
    "  shrink_full: remove new lines and tabs, including stringified values, plus shrinking spaces and blank lines.\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|Peter is a very g...|\n",
      "|My life in Russia...|\n",
      "|John and Peter ar...|\n",
      "|Lucas Nogal Dunbe...|\n",
      "|Europe is very cu...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "|text                                                                         |\n",
      "+-----------------------------------------------------------------------------+\n",
      "|Peter is a very good person.                                                 |\n",
      "|My life in Russia is very interesting.                                       |\n",
      "|John and Peter are brothers. However they don't support each other that much.|\n",
      "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |\n",
      "|Europe is very culture rich. There are huge churches! and big houses!        |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                         |document                                                                                                               |\n",
      "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|Peter is a very good person.                                                 |[{document, 0, 27, Peter is a very good person., {sentence -> 0}, []}]                                                 |\n",
      "|My life in Russia is very interesting.                                       |[{document, 0, 37, My life in Russia is very interesting., {sentence -> 0}, []}]                                       |\n",
      "|John and Peter are brothers. However they don't support each other that much.|[{document, 0, 76, John and Peter are brothers. However they don't support each other that much., {sentence -> 0}, []}]|\n",
      "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[{document, 0, 67, Lucas Nogal Dunbercker is no longer happy. He has a good car though., {sentence -> 0}, []}]         |\n",
      "|Europe is very culture rich. There are huge churches! and big houses!        |[{document, 0, 68, Europe is very culture rich. There are huge churches! and big houses!, {sentence -> 0}, []}]        |\n",
      "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import *\n",
    "\n",
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\\\n",
    "    .setCleanupMode(\"shrink\")\n",
    "\n",
    "doc_df = documentAssembler.transform(spark_df)\n",
    "\n",
    "doc_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we define DocumentAssembler with desired parameters and then transform the data frame with it. The most important point to pay attention to here is that you need to use a String or String[Array] type column in .setInputCol(). So it doesn’t have to be named as text. You just use the column name as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- document: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------+-----+----+\n",
      "|result                                                                         |begin|end |\n",
      "+-------------------------------------------------------------------------------+-----+----+\n",
      "|[Peter is a very good person.]                                                 |[0]  |[27]|\n",
      "|[My life in Russia is very interesting.]                                       |[0]  |[37]|\n",
      "|[John and Peter are brothers. However they don't support each other that much.]|[0]  |[76]|\n",
      "|[Lucas Nogal Dunbercker is no longer happy. He has a good car though.]         |[0]  |[67]|\n",
      "|[Europe is very culture rich. There are huge churches! and big houses!]        |[0]  |[68]|\n",
      "+-------------------------------------------------------------------------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_df.select('document.result','document.begin','document.end').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new column is in an array of struct type and has the parameters shown above. The annotators and transformers all come with universal metadata that would be filled down the road depending on the annotators being used. Unless you want to append other Spark NLP annotators to DocumentAssembler(), you don’t need to know what all these parameters mean for now. So we will talk about them in the following articles. You can access all these parameters with {column name}.{parameter name}.\n",
    "\n",
    "Let’s print out the first item’s result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(result=['Peter is a very good person.'])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_df.select(\"document.result\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we would like to flatten the document column, we can do as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---+-----------------------------------------------------------------------------+---------------+----------+\n",
      "|annotatorType|begin|end|result                                                                       |metadata       |embeddings|\n",
      "+-------------+-----+---+-----------------------------------------------------------------------------+---------------+----------+\n",
      "|document     |0    |27 |Peter is a very good person.                                                 |{sentence -> 0}|[]        |\n",
      "|document     |0    |37 |My life in Russia is very interesting.                                       |{sentence -> 0}|[]        |\n",
      "|document     |0    |76 |John and Peter are brothers. However they don't support each other that much.|{sentence -> 0}|[]        |\n",
      "|document     |0    |67 |Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |{sentence -> 0}|[]        |\n",
      "|document     |0    |68 |Europe is very culture rich. There are huge churches! and big houses!        |{sentence -> 0}|[]        |\n",
      "+-------------+-----+---+-----------------------------------------------------------------------------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "doc_df.withColumn(\n",
    "    \"tmp\", \n",
    "    F.explode(\"document\"))\\\n",
    "    .select(\"tmp.*\")\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sentence Detector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finds sentence bounds in raw text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parametre  | Value | Description |\n",
    "| - | - | - |\n",
    "|**setCustomBounds()***       |String |Custom sentence separator text e.g. `[\"\\n\"]`|\n",
    "|**setUseCustomOnly()*** |Bool|Use only custom bounds without considering those of Pragmatic Segmenter. Defaults to false. Needs customBounds.|\n",
    "|**setUseAbbreviations***  |Bool| Whether to consider abbreviation strategies for better accuracy but slower performance. Defaults to true.|\n",
    "|**setExplodeSentences*** |Bool|Whether to split sentences into different Dataset rows. Useful for higher parallelism in fat rows. Defaults to false.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import *\n",
    "\n",
    "# we feed the document column coming from Document Assembler\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "      .setInputCols(['document'])\\\n",
    "      .setOutputCol('sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='SentenceDetector_176a5418c081', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
       " Param(parent='SentenceDetector_176a5418c081', name='useAbbreviations', doc='whether to apply abbreviations at sentence detection'): True,\n",
       " Param(parent='SentenceDetector_176a5418c081', name='detectLists', doc='whether detect lists during sentence detection'): True,\n",
       " Param(parent='SentenceDetector_176a5418c081', name='useCustomBoundsOnly', doc='Only utilize custom bounds in sentence detection'): False,\n",
       " Param(parent='SentenceDetector_176a5418c081', name='customBounds', doc='characters used to explicitly mark sentence bounds'): [],\n",
       " Param(parent='SentenceDetector_176a5418c081', name='customBoundsStrategy', doc='How to return matched custom bounds'): 'none',\n",
       " Param(parent='SentenceDetector_176a5418c081', name='explodeSentences', doc='whether to explode each sentence into a different row, for better parallelization. Defaults to false.'): False,\n",
       " Param(parent='SentenceDetector_176a5418c081', name='minLength', doc='Set the minimum allowed length for each sentence.'): 0,\n",
       " Param(parent='SentenceDetector_176a5418c081', name='maxLength', doc='Set the maximum allowed length for each sentence'): 99999,\n",
       " Param(parent='SentenceDetector_176a5418c081', name='inputCols', doc='previous annotations columns, if renamed'): ['document'],\n",
       " Param(parent='SentenceDetector_176a5418c081', name='outputCol', doc='output annotation column. can be left default.'): 'sentences'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenceDetector.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                         |document                                                                                                               |\n",
      "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|Peter is a very good person.                                                 |[{document, 0, 27, Peter is a very good person., {sentence -> 0}, []}]                                                 |\n",
      "|My life in Russia is very interesting.                                       |[{document, 0, 37, My life in Russia is very interesting., {sentence -> 0}, []}]                                       |\n",
      "|John and Peter are brothers. However they don't support each other that much.|[{document, 0, 76, John and Peter are brothers. However they don't support each other that much., {sentence -> 0}, []}]|\n",
      "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[{document, 0, 67, Lucas Nogal Dunbercker is no longer happy. He has a good car though., {sentence -> 0}, []}]         |\n",
      "|Europe is very culture rich. There are huge churches! and big houses!        |[{document, 0, 68, Europe is very culture rich. There are huge churches! and big houses!, {sentence -> 0}, []}]        |\n",
      "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                         |document                                                                                                               |sentences                                                                                                                                                                                          |\n",
      "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Peter is a very good person.                                                 |[{document, 0, 27, Peter is a very good person., {sentence -> 0}, []}]                                                 |[{document, 0, 27, Peter is a very good person., {sentence -> 0}, []}]                                                                                                                             |\n",
      "|My life in Russia is very interesting.                                       |[{document, 0, 37, My life in Russia is very interesting., {sentence -> 0}, []}]                                       |[{document, 0, 37, My life in Russia is very interesting., {sentence -> 0}, []}]                                                                                                                   |\n",
      "|John and Peter are brothers. However they don't support each other that much.|[{document, 0, 76, John and Peter are brothers. However they don't support each other that much., {sentence -> 0}, []}]|[{document, 0, 27, John and Peter are brothers., {sentence -> 0}, []}, {document, 29, 76, However they don't support each other that much., {sentence -> 1}, []}]                                  |\n",
      "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[{document, 0, 67, Lucas Nogal Dunbercker is no longer happy. He has a good car though., {sentence -> 0}, []}]         |[{document, 0, 41, Lucas Nogal Dunbercker is no longer happy., {sentence -> 0}, []}, {document, 43, 67, He has a good car though., {sentence -> 1}, []}]                                           |\n",
      "|Europe is very culture rich. There are huge churches! and big houses!        |[{document, 0, 68, Europe is very culture rich. There are huge churches! and big houses!, {sentence -> 0}, []}]        |[{document, 0, 27, Europe is very culture rich., {sentence -> 0}, []}, {document, 29, 52, There are huge churches!, {sentence -> 1}, []}, {document, 54, 68, and big houses!, {sentence -> 2}, []}]|\n",
      "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_df = sentenceDetector.transform(doc_df)\n",
    "\n",
    "sent_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sentences=[Row(annotatorType='document', begin=0, end=27, result='Peter is a very good person.', metadata={'sentence': '0'}, embeddings=[])]),\n",
       " Row(sentences=[Row(annotatorType='document', begin=0, end=37, result='My life in Russia is very interesting.', metadata={'sentence': '0'}, embeddings=[])]),\n",
       " Row(sentences=[Row(annotatorType='document', begin=0, end=27, result='John and Peter are brothers.', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='document', begin=29, end=76, result=\"However they don't support each other that much.\", metadata={'sentence': '1'}, embeddings=[])])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df.select('sentences').take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(result=['Peter is a very good person.']),\n",
       " Row(result=['My life in Russia is very interesting.']),\n",
       " Row(result=['John and Peter are brothers.', \"However they don't support each other that much.\"])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df.select('sentences.result').take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(metadata=[{'sentence': '0'}]),\n",
       " Row(metadata=[{'sentence': '0'}]),\n",
       " Row(metadata=[{'sentence': '0'}, {'sentence': '1'}])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df.select('sentences.metadata').take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The patient was prescribed 1 capsule of Advil for 5 days. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day. It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text ='The patient was prescribed 1 capsule of Advil for 5 days. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day. It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.'\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                                                                                                                                                                                                                                           |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|The patient was prescribed 1 capsule of Advil for 5 days. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day. It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "\n",
    "spark_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|                                              text|\n",
      "+--------------------------------------------------+\n",
      "|The patient was prescribed 1 capsule of Advil f...|\n",
      "+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                              text|                                          document|                                         sentences|\n",
      "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
      "|The patient was prescribed 1 capsule of Advil f...|[{document, 0, 334, The patient was prescribed ...|[{document, 0, 56, The patient was prescribed 1...|\n",
      "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_df = documentAssembler.transform(spark_df)\n",
    "\n",
    "sent_df = sentenceDetector.transform(doc_df)\n",
    "\n",
    "sent_df.show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(result=['The patient was prescribed 1 capsule of Advil for 5 days.', 'He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day.', 'It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.'])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df.select('sentences.result').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceDetector_176a5418c081"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setExplodeSentences: Whether to split sentences into different Dataset rows. Useful for higher parallelism in fat rows. Defaults to false.\n",
    "\n",
    "sentenceDetector.setExplodeSentences(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                              text|                                          document|                                         sentences|\n",
      "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
      "|The patient was prescribed 1 capsule of Advil f...|[{document, 0, 334, The patient was prescribed ...|[{document, 0, 56, The patient was prescribed 1...|\n",
      "|The patient was prescribed 1 capsule of Advil f...|[{document, 0, 334, The patient was prescribed ...|[{document, 58, 240, He was seen by the endocri...|\n",
      "|The patient was prescribed 1 capsule of Advil f...|[{document, 0, 334, The patient was prescribed ...|[{document, 242, 334, It was determined that al...|\n",
      "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_df = sentenceDetector.transform(doc_df)\n",
    "\n",
    "sent_df.show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|result                                                                                                                                                                                   |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[The patient was prescribed 1 capsule of Advil for 5 days.]                                                                                                                              |\n",
      "|[He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day.]|\n",
      "|[It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.]                                                                                          |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_df.select('sentences.result').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|col                                                                                                                                                                                    |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|The patient was prescribed 1 capsule of Advil for 5 days.                                                                                                                              |\n",
      "|He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day.|\n",
      "|It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.                                                                                          |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "sent_df.select(F.explode('sentences.result')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`.setCustomBounds([r\"\\\\.\", \";\"])`**\n",
    "\n",
    "**`.setCustomBoundsStrategy(\"append\")`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    [\"Peter is a very good person.\"],\n",
    "    [\"My life in Russia is very interesting.\"], \n",
    "    [\"John and Peter are brothers. However; they don't support each other that much.\"],\n",
    "    [\"Lucas Nogal Dunbercker is no longer happy. He has a good car though.\"],\n",
    "    [\"Europe is very culture rich. There are huge churches! and big houses!\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+\n",
      "|text                                                                          |\n",
      "+------------------------------------------------------------------------------+\n",
      "|Peter is a very good person.                                                  |\n",
      "|My life in Russia is very interesting.                                        |\n",
      "|John and Peter are brothers. However; they don't support each other that much.|\n",
      "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.          |\n",
      "|Europe is very culture rich. There are huge churches! and big houses!         |\n",
      "+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.createDataFrame(text).toDF(\"text\")\n",
    "spark_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = documentAssembler.transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                          |document                                                                                                                |sentences                                                                                                                                                                                                   |\n",
      "+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Peter is a very good person.                                                  |[{document, 0, 27, Peter is a very good person., {sentence -> 0}, []}]                                                  |[{document, 0, 27, Peter is a very good person., {sentence -> 0}, []}]                                                                                                                                      |\n",
      "|My life in Russia is very interesting.                                        |[{document, 0, 37, My life in Russia is very interesting., {sentence -> 0}, []}]                                        |[{document, 0, 37, My life in Russia is very interesting., {sentence -> 0}, []}]                                                                                                                            |\n",
      "|John and Peter are brothers. However; they don't support each other that much.|[{document, 0, 77, John and Peter are brothers. However; they don't support each other that much., {sentence -> 0}, []}]|[{document, 0, 27, John and Peter are brothers., {sentence -> 0}, []}, {document, 29, 36, However;, {sentence -> 1}, []}, {document, 38, 77, they don't support each other that much., {sentence -> 2}, []}]|\n",
      "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.          |[{document, 0, 67, Lucas Nogal Dunbercker is no longer happy. He has a good car though., {sentence -> 0}, []}]          |[{document, 0, 41, Lucas Nogal Dunbercker is no longer happy., {sentence -> 0}, []}, {document, 43, 67, He has a good car though., {sentence -> 1}, []}]                                                    |\n",
      "|Europe is very culture rich. There are huge churches! and big houses!         |[{document, 0, 68, Europe is very culture rich. There are huge churches! and big houses!, {sentence -> 0}, []}]         |[{document, 0, 27, Europe is very culture rich., {sentence -> 0}, []}, {document, 29, 52, There are huge churches!, {sentence -> 1}, []}, {document, 54, 68, and big houses!, {sentence -> 2}, []}]         |\n",
      "+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDetector = SentenceDetector()\\\n",
    "      .setInputCols(['document'])\\\n",
    "      .setOutputCol('sentences')\\\n",
    "      .setCustomBounds([r\"\\.\", \";\", \"!\"])\\\n",
    "      .setCustomBoundsStrategy(\"append\")\n",
    "      \n",
    "sent_df = sentenceDetector.transform(doc_df)\n",
    "sent_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(result=['Peter is a very good person.']),\n",
       " Row(result=['My life in Russia is very interesting.']),\n",
       " Row(result=['John and Peter are brothers.', 'However;', \"they don't support each other that much.\"]),\n",
       " Row(result=['Lucas Nogal Dunbercker is no longer happy.', 'He has a good car though.']),\n",
       " Row(result=['Europe is very culture rich.', 'There are huge churches!', 'and big houses!'])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df.select('sentences.result').take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`.setCustomBoundsStrategy(\"prepend\")`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                          |document                                                                                                                |sentences                                                                                                                                                                                                                                                                                                                                    |\n",
      "+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Peter is a very good person.                                                  |[{document, 0, 27, Peter is a very good person., {sentence -> 0}, []}]                                                  |[{document, 0, 26, Peter is a very good person, {sentence -> 0}, []}, {document, 27, 27, ., {sentence -> 1}, []}]                                                                                                                                                                                                                            |\n",
      "|My life in Russia is very interesting.                                        |[{document, 0, 37, My life in Russia is very interesting., {sentence -> 0}, []}]                                        |[{document, 0, 36, My life in Russia is very interesting, {sentence -> 0}, []}, {document, 37, 37, ., {sentence -> 1}, []}]                                                                                                                                                                                                                  |\n",
      "|John and Peter are brothers. However; they don't support each other that much.|[{document, 0, 77, John and Peter are brothers. However; they don't support each other that much., {sentence -> 0}, []}]|[{document, 0, 26, John and Peter are brothers, {sentence -> 0}, []}, {document, 27, 27, ., {sentence -> 1}, []}, {document, 29, 35, However, {sentence -> 2}, []}, {document, 36, 36, ;, {sentence -> 3}, []}, {document, 38, 76, they don't support each other that much, {sentence -> 4}, []}, {document, 77, 77, ., {sentence -> 5}, []}]|\n",
      "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.          |[{document, 0, 67, Lucas Nogal Dunbercker is no longer happy. He has a good car though., {sentence -> 0}, []}]          |[{document, 0, 40, Lucas Nogal Dunbercker is no longer happy, {sentence -> 0}, []}, {document, 41, 41, ., {sentence -> 1}, []}, {document, 43, 66, He has a good car though, {sentence -> 2}, []}, {document, 67, 67, ., {sentence -> 3}, []}]                                                                                               |\n",
      "|Europe is very culture rich. There are huge churches! and big houses!         |[{document, 0, 68, Europe is very culture rich. There are huge churches! and big houses!, {sentence -> 0}, []}]         |[{document, 0, 26, Europe is very culture rich, {sentence -> 0}, []}, {document, 27, 27, ., {sentence -> 1}, []}, {document, 29, 51, There are huge churches, {sentence -> 2}, []}, {document, 52, 52, !, {sentence -> 3}, []}, {document, 54, 67, and big houses, {sentence -> 4}, []}, {document, 68, 68, !, {sentence -> 5}, []}]         |\n",
      "+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDetector = SentenceDetector()\\\n",
    "      .setInputCols(['document'])\\\n",
    "      .setOutputCol('sentences')\\\n",
    "      .setCustomBounds([r\"\\.\", \";\", \"!\"])\\\n",
    "      .setCustomBoundsStrategy(\"prepend\")\n",
    "\n",
    "sent_df = sentenceDetector.transform(doc_df)\n",
    "sent_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(result=['Peter is a very good person', '.']),\n",
       " Row(result=['My life in Russia is very interesting', '.']),\n",
       " Row(result=['John and Peter are brothers', '.', 'However', ';', \"they don't support each other that much\", '.']),\n",
       " Row(result=['Lucas Nogal Dunbercker is no longer happy', '.', 'He has a good car though', '.']),\n",
       " Row(result=['Europe is very culture rich', '.', 'There are huge churches', '!', 'and big houses', '!'])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df.select('sentences.result').take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The separation of the sentences is determined according to the characters we set with custom bound. When we use `append`, sentences are differentiated according to the characters, if `prepend` is used, it also determines the characters as separate sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sentence Detector DL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text ='The patient was prescribed 1 capsule of Advil for 5 days. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day. It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.'\n",
    "\n",
    "spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "\n",
    "doc_df = documentAssembler.transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_detector_dl download started this may take some time.\n",
      "Approximate size to download 354.6 KB\n",
      "[OK!]\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|col                                                                                                                                                                                    |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|The patient was prescribed 1 capsule of Advil for 5 days.                                                                                                                              |\n",
      "|He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day.|\n",
      "|It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.                                                                                          |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentencerDL = SentenceDetectorDLModel\\\n",
    "    .pretrained(\"sentence_detector_dl\", \"en\") \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentences\")\\\n",
    "\n",
    "sent_dl_df = sentencerDL.transform(doc_df)\n",
    "\n",
    "sent_dl_df.select(F.explode('sentences.result')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_detector_dl download started this may take some time.\n",
      "Approximate size to download 354.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documenter = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentences')\\\n",
    "    \n",
    "sentencerDL = SentenceDetectorDLModel\\\n",
    "    .pretrained(\"sentence_detector_dl\", \"en\") \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentences\")\n",
    "\n",
    "\n",
    "sd_pipeline = PipelineModel(stages=[documenter, sentenceDetector])\n",
    "sd_model = LightPipeline(sd_pipeline)\n",
    "\n",
    "\n",
    "# DL version\n",
    "sd_dl_pipeline = PipelineModel(stages=[documenter, sentencerDL])\n",
    "sd_dl_model = LightPipeline(sd_dl_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0\t51\tJohn loves Mary.Mary loves Peter\n",
      "Peter loves Helen .\n",
      "1\t52\t68\tHelen loves John;\n",
      "2\t71\t98\tTotal: four people involved.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"John loves Mary.Mary loves Peter\n",
    "Peter loves Helen .Helen loves John; \n",
    "Total: four people involved.\"\"\"\n",
    "\n",
    "# sd_model\n",
    "for anno in sd_model.fullAnnotate(text)[0][\"sentences\"]:\n",
    "    print(\"{}\\t{}\\t{}\\t{}\".format(\n",
    "        anno.metadata[\"sentence\"], anno.begin, anno.end, anno.result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0\t15\tJohn loves Mary.\n",
      "1\t16\t31\tMary loves Peter\n",
      "2\t33\t51\tPeter loves Helen .\n",
      "3\t52\t68\tHelen loves John;\n",
      "4\t71\t98\tTotal: four people involved.\n"
     ]
    }
   ],
   "source": [
    "# sd_dl_model\n",
    "for anno in sd_dl_model.fullAnnotate(text)[0][\"sentences\"]:\n",
    "    print(\"{}\\t{}\\t{}\\t{}\".format(\n",
    "        anno.metadata[\"sentence\"], anno.begin, anno.end, anno.result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifies tokens with tokenization open standards. It is an **Annotator Approach, so it requires .fit()**.\n",
    "\n",
    "A few rules will help customizing it if defaults do not fit user needs.\n",
    "\n",
    "setExceptions(StringArray): List of tokens to not alter at all. Allows composite tokens like two worded tokens that the user may not want to split.\n",
    "\n",
    "| Parametre  | Value | Description |\n",
    "| - | - | - |\n",
    "|**addException()***       |String |Add a single exception.|\n",
    "|**setExceptionsPath()*** |String|Path to txt file with list of token exceptions.|\n",
    "|**caseSensitiveExceptions***  |Bool| Whether to follow case sensitiveness for matching exceptions in text.|\n",
    "|**contextChars()*** |StringArray|List of 1 character string to rip off from tokens, such as parenthesis or question marks. Ignored if using prefix, infix or suffix patterns.|\n",
    "|**splitChars()*** |StringArray|List of 1 character string to split tokens inside, such as hyphens. Ignored if using infix, prefix or suffix patterns.|\n",
    "|**splitPattern()*** |String|pattern to separate from the inside of tokens. takes priority over splitChars. setTargetPattern: Basic regex rule to identify a candidate for tokenization. Defaults to \\S+ which means anything not a space.|\n",
    "|**setSuffixPattern()*** ||Regex to identify subtokens that are in the end of the token. Regex has to end with \\z and must contain groups (). Each group will become a separate token within the prefix. Defaults to non-letter characters. e.g. quotes or parenthesis|\n",
    "|**setPrefixPattern()*** ||Regex to identify subtokens that come in the beginning of the token. Regex has to start with \\A and must contain groups (). Each group will become a separate token within the prefix. Defaults to non-letter characters. e.g. quotes or parenthesis.|\n",
    "|**addInfixPattern()*** ||Add an extension pattern regex with groups to the top of the rules (will target first, from more specific to the more general).|\n",
    "|**minLength()*** ||Set the minimum allowed legth for each token.|\n",
    "|**maxLength()*** ||Set the maximum allowed legth for each token.|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='Tokenizer_6d8479c7cd62', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
       " Param(parent='Tokenizer_6d8479c7cd62', name='targetPattern', doc='pattern to grab from text as token candidates. Defaults \\\\S+'): '\\\\S+',\n",
       " Param(parent='Tokenizer_6d8479c7cd62', name='contextChars', doc='character list used to separate from token boundaries'): ['.',\n",
       "  ',',\n",
       "  ';',\n",
       "  ':',\n",
       "  '!',\n",
       "  '?',\n",
       "  '*',\n",
       "  '-',\n",
       "  '(',\n",
       "  ')',\n",
       "  '\"',\n",
       "  \"'\"],\n",
       " Param(parent='Tokenizer_6d8479c7cd62', name='caseSensitiveExceptions', doc='Whether to care for case sensitiveness in exceptions'): True,\n",
       " Param(parent='Tokenizer_6d8479c7cd62', name='minLength', doc='Set the minimum allowed length for each token'): 0,\n",
       " Param(parent='Tokenizer_6d8479c7cd62', name='maxLength', doc='Set the maximum allowed length for each token'): 99999,\n",
       " Param(parent='Tokenizer_6d8479c7cd62', name='inputCols', doc='previous annotations columns, if renamed'): ['document'],\n",
       " Param(parent='Tokenizer_6d8479c7cd62', name='outputCol', doc='output annotation column. can be left default.'): 'token'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Peter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail!'\n",
    "\n",
    "spark_df = spark.createDataFrame([[text]]).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                           text|                                                                                            document|                                                                                               token|\n",
      "+-------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|Peter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail!|[{document, 0, 78, Peter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail...|[{token, 0, 4, Peter, {sentence -> 0}, []}, {token, 6, 11, Parker, {sentence -> 0}, []}, {token, ...|\n",
      "+-------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_df = documentAssembler.transform(spark_df)\n",
    "\n",
    "token_df = tokenizer.fit(doc_df).transform(doc_df)\n",
    "\n",
    "token_df.show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(result=['Peter', 'Parker', '(', 'Spiderman', ')', 'is', 'a', 'nice', 'guy', 'and', 'lives', 'in', 'New', 'York', 'but', 'has', 'no', 'e-mail', '!'])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df.select('token.result').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\") \\\n",
    "    .setSplitChars(['-']) \\\n",
    "    .setContextChars(['?', '!']) \\\n",
    "    .addException(\"New York\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(result=['Peter', 'Parker', '(Spiderman)', 'is', 'a', 'nice', 'guy', 'and', 'lives', 'in', 'New York', 'but', 'has', 'no', 'e', 'mail', '!'])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df = tokenizer.fit(doc_df).transform(doc_df)\n",
    "\n",
    "token_df.select('token.result').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                               |document                                                                                     |sentence                                                                                     |regexToken                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "+---------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1. T1-T2 DATE**[12/24/13] $1.99 () (10/12), ph+ 90%|[{document, 0, 50, 1. T1-T2 DATE**[12/24/13] $1.99 () (10/12), ph+ 90%, {sentence -> 0}, []}]|[{document, 0, 50, 1. T1-T2 DATE**[12/24/13] $1.99 () (10/12), ph+ 90%, {sentence -> 0}, []}]|[{token, 0, 0, 1, {sentence -> 0}, []}, {token, 2, 2, ., {sentence -> 0}, []}, {token, 4, 5, T1, {sentence -> 0}, []}, {token, 7, 7, -, {sentence -> 0}, []}, {token, 9, 10, T2, {sentence -> 0}, []}, {token, 12, 15, DATE, {sentence -> 0}, []}, {token, 17, 17, *, {sentence -> 0}, []}, {token, 19, 19, *, {sentence -> 0}, []}, {token, 21, 21, [, {sentence -> 0}, []}, {token, 23, 30, 12/24/13, {sentence -> 0}, []}, {token, 32, 32, ], {sentence -> 0}, []}, {token, 35, 35, $, {sentence -> 0}, []}, {token, 37, 37, 1, {sentence -> 0}, []}, {token, 39, 39, ., {sentence -> 0}, []}, {token, 41, 42, 99, {sentence -> 0}, []}, {token, 44, 45, (), {sentence -> 0}, []}, {token, 47, 53, (10/12), {sentence -> 0}, []}, {token, 55, 55, ,, {sentence -> 0}, []}, {token, 57, 58, ph, {sentence -> 0}, []}, {token, 60, 60, +, {sentence -> 0}, []}, {token, 62, 63, 90, {sentence -> 0}, []}, {token, 65, 65, %, {sentence -> 0}, []}]|\n",
      "+---------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "content = \"1. T1-T2 DATE**[12/24/13] $1.99 () (10/12), ph+ 90%\"\n",
    "pattern = \"\\\\s+|(?=[-.:;*+,$&%\\\\[\\\\]])|(?<=[-.:;*+,$&%\\\\[\\\\]])\"\n",
    "\n",
    "df = spark.createDataFrame([content], StringType()).withColumnRenamed(\"value\", \"text\")\n",
    "\n",
    "documenter = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentence')\n",
    "\n",
    "regexTokenizer = RegexTokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"regexToken\") \\\n",
    "    .setPattern(pattern) \\\n",
    "    .setPositionalMask(False)\n",
    "\n",
    "docPatternRemoverPipeline = Pipeline().setStages([documenter,\n",
    "                                                  sentenceDetector,\n",
    "                                                  regexTokenizer])\n",
    "\n",
    "result = docPatternRemoverPipeline.fit(df).transform(df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c2262da7-f8c4-4f54-b576-dde0c25a3bbf\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regexToken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12/24/13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(10/12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2262da7-f8c4-4f54-b576-dde0c25a3bbf')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-c2262da7-f8c4-4f54-b576-dde0c25a3bbf button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-c2262da7-f8c4-4f54-b576-dde0c25a3bbf');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   regexToken\n",
       "0           1\n",
       "1           .\n",
       "2          T1\n",
       "3           -\n",
       "4          T2\n",
       "5        DATE\n",
       "6           *\n",
       "7           *\n",
       "8           [\n",
       "9    12/24/13\n",
       "10          ]\n",
       "11          $\n",
       "12          1\n",
       "13          .\n",
       "14         99\n",
       "15         ()\n",
       "16    (10/12)\n",
       "17          ,\n",
       "18         ph\n",
       "19          +\n",
       "20         90\n",
       "21          %"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "result_df = result.select(F.explode(result.regexToken.result).alias('regexToken')).toPandas()\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Spark NLP Annotators in Spark ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark NLP provides an easy API to integrate with Spark ML Pipelines and all the Spark NLP annotators and transformers can be used within Spark ML Pipelines. So, it’s better to explain Pipeline concept through Spark ML official documentation.\n",
    "\n",
    "What is a Pipeline anyway? In machine learning, it is common to run a sequence of algorithms to process and learn from data. \n",
    "\n",
    "Apache Spark ML represents such a workflow as a Pipeline, which consists of a sequence of PipelineStages (Transformers and Estimators) to be run in a specific order.\n",
    "\n",
    "In simple terms, a pipeline chains multiple Transformers and Estimators together to specify an ML workflow. We use Pipeline to chain multiple Transformers and Estimators together to specify our machine learning workflow.\n",
    "\n",
    "The figure below is for the training time usage of a Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAtUAAADxCAYAAAAEEUtiAAAgAElEQVR4AeydB1RUVxuu7/3vX5M/McmviaZoLFGxYMcIogioWFAELDQVu9jFAvaGiL13sbfYe8Xee0GNienVAlaKCL53vfvMgQGBIDNUv1lr1gAz58zw7He+/e5vf2fv/wO5CQEhIASEgBAQAkJACAgBIWASgf9j0tFysBAQAkJACAgBISAEhIAQEAIQUy0iEAJCQAgIASEgBISAEBACJhIQU20iQDlcCAgBISAEhIAQEAJCQAiIqRYNCAEhIASEgBAQAkJACAgBEwmIqTYRoBwuBISAEBACQkAICAEhIATEVIsGhIAQEAJCQAgIASEgBISAiQTEVJsIUA4XAkJACAgBISAEhIAQEAJiqkUDQkAICAEhIASEgBAQAkLARAJiqk0EKIcLASEgBISAEBACQkAICAEx1aIBISAEhIAQEAJCQAgIASFgIgEx1SYClMOFgBAQAkJACAgBISAEhICYatGAEBACQkAICAEhIASEgBAwkYCYahMByuFCQAgIASEgBISAEBACQkBMtWhACAgBISAEhIAQEAJCQAiYSEBMtYkA5XAhIASEgBAQAkJACAgBISCmWjQgBISAEBACQkAICAEhIARMJCCm2kSAcrgQEAJCQAgIASEgBISAEBBTLRoQAkJACAgBISAEhIAQEAImEhBTbSJAOVwICAEhIASEgBAQAkJACIipFg0IASEgBISAEBACQkAICAETCYipNhGgHC4EhIAQEAJCQAgIASEgBMRUiwaEgBAQAkJACAgBISAEhICJBMRUmwhQDhcCQkAICAEhIASEgBAQAmKqRQNCQAgIASEgBISAEBACQsBEAmKqTQQohwsBISAEhIAQEAJCQAgIATHVogEhIASEgBAQAkJACAgBIWAiATHVJgKUw4WAEBACQkAICAEhIASEgJhq0YAQEAJCQAgIASEgBISAEDCRgJhqEwHK4UJACAgBISAEhIAQEAJCQEy1aEAICAEhIASEgBAQAkJACJhIQEy1iQDlcCEgBISAEBACQkAICAEhIKZaNCAEhIAQEAJCQAgIASEgBEwkIKbaRIByuBAQAkJACAgBISAEhIAQEFMtGhACQkAICAEhIASEgBAQAiYSEFNtIkA5XAgIASEgBISAEBACQkAIiKkWDQgBISAEhIAQEAJCQAgIARMJiKk2EaAcLgSEgBAQAkJACAgBISAExFSLBoSAEBACQkAICAEhIASEgIkExFSbCFAOFwJCQAgIASEgBISAEBACYqpFA0JACAgBISAEhIAQEAJCwEQCYqpNBCiHCwEhIASEgBAQAkJACAgBMdWiASEgBISAEBACQkAICAEhYCIBMdUmApTDhYAQEAJCQAgIASEgBISAmGrRgBAQAkJACAgBISAEhIAQMJGAmGoTAcrhQkAICAEhIASEgBAQAkJATLVoQAgIASEgBISAEBACQkAImEhATLWJAOVwISAEhIAQEAJCQAgIASEgplo0IASEgBAQAkJACAgBISAETCQgptpEgHK4EBACQkAICAEhIASEgBAQUy0aEAJCQAgIASEgBISAEBACJhIQU20iQDlcCAgBISAEhIAQEAJCQAiIqRYNCAEhIASEgBAQAkJACAgBEwmIqTYRoBwuBISAEBACQkAICAEhIATEVIsGhIAQEAJCQAgIASEgBISAiQTEVJsIUA4XAkJACAgBISAEhIAQEAJiqkUDQkAICAEhIASEgBAQAkLARAJiqk0EKIcLASEgBISAEBACQkAICAEx1aIBISAEhIAQEAJCQAgIASFgIgEx1SYClMOFgBAQAkJACAgBISAEhICYatGAEBACQkAICAEhIASEgBAwkYCYahMByuFCQAgIASEgBISAEBACQkBMtWhACAgBISAEhIAQEAJCQAiYSEBMtYkA5XAhIASEgBAQAkJACAgBISCmWjQgBISAEBACQkAICAEhIARMJCCm2kSAcrgQEAJCQAgIASEgBISAEBBTLRoQAkJACAgBISAEhIAQEAImEhBTbSJAOVwICAEhIASEgBAQAkJACIipFg0IASEgBISAEBACQkAICAETCYipNhGgHC4EhIAQEAJCQAgIASEgBMRU56AGXr4EEhKA+Hi5ZwcDss6LN9FIxr4f5MTvVF678TNnh/7lPTTOeVEjf6VpiRHZ9x3Kq/3IX2lInjcPATHV5uH42md5EQ9EPEzAuUux2L43Clt3yz0rGWzfE4XzV2IVc7LPCzeaoCdPExB+Ow67DkSLRtL5jvA7dPZiLO7eT8DzuJd5wlzT3D199hK3vo3DnoPSvln5/ee5t+2OwpGTMfj51xeIjnmJhDw4AEsZtxgjHkTGK+1nNT85fxS2743GhataP0L2chMCKQmIqU5JJBt+Z6d/6nws7Lwi1b3XiMfoN/qR3LOQQZ+Rj1C/bSSqu0bgwNEYlRnMhqbO9FswYH/7/Qs07/pQfeYuAY/Qf4xoJK3vSZ+Rj9HINxJVXCKwblu0Mk2Zhp8NB9JQ/343Hm16PUTFZhHoPFjaN622Neffybu8cwT8xz3Go8cJeWLwlZYc4168xLEzMbBpHaH6EXNyknOlHmv7jnoMe+8IWLlH4NjZWDXTnFb7yN/fTAJiqrO53WmWmDFt3CESN76JQ3y8llVjJyv3rGVA1lfDn8OudSRu3I7L5pZ/vbf77W48nDtFYuf+aERFi0Yy8t1g+978Nk4NnjgDlJszSU+evkSzTpGYuewZYp9L+2akfc3zmpdqNiMg+DFmLH2GmNi8ma5mCQI1btNKi2XSj2Rt32GsPbK+dvM5HH0iVOLj9SK7vDq/ExBTnc0tzOn8ISGPsXVPNBLyw/xjNvMz9e1Y+rF2WxTa+z9CQi6dvqMZ3HM4Br1GP0ZsHu30TW2nzB7/8uVL7AmLRr/Rj/HoSe4toj96OgY+/R4iJiZvmrrMtk9uOe63P+Ph4BWhDHZu+Uyv8zk4EPMb+girN0W9zmHyWjMSWLzmGboPfWTGM8qp8gMBMdXZ3Iqs+WSG6oefX2TzO8vbkQAzPMxWs1QgKip3mq7o6JeYNO8Jlq1/Jo2WCQI//RaPxu0j8ee93Nm+/JdmhT7D9NCnuTqbngn0eeYQzv609HuIS1fz5hQ+a8Lre0eo2c48Az2ffdDTF2LRqH2kfIfzWbua+u+IqTaV4Gse/8fdBDTyjsQvv+fSNOlr/j957eWcxvvmThyc2kficS7NZLI0YMTkx9i4Q7JQmdEXa5UbekXi9z9zr6kOmf0Uc1c9kw45Mw1shmNY9tGu/0OcOJO7y4TS+lc58K7XKgLf/ijJmbQYZfXfr4Q/Rz2fSMTFyWxTVrPOS+cXU53NrSWmOpuBp3g73VQ3FFOdgkzW/JqQA+tP5QlTPSd3m2qW0fDOGx9jY2Px8OFDPHr0CFFRUYhPpWCdf4uJicGLF5rR4+Pjx4/VPbXXZ43iMnZWMdUZ45Tdr6LWoqOjERcXl6i/tD4DdcjX5dRNTHVOkc/d7yumOpvbR0x1NgNP8XZiqlMAycJf2elduHAh2zs+MdWmN+r333+Pn376SZ2IRjk0NBTt27fH8OHDMXnyZPzyyy+vvMkff/yBuXPn4ptvvlFtfvToUfTq1QsDBgzA/fv3X3l9Tv5BTHVO0k/7vTkQo4YYN9IbiPG5+fPn4/Tp02mfLIufEVOdxYDz6OnFVGdzw5nTVBtnkhhk9N/5L/Fn47v+b/JvzB4av9b49XwuZXbxdc6jn4ufJ+V59OfS+3tqnyu1z6T/P6/7KKY6bWIp2evtZXxEWm3BY1M+d+7cOVSvXl1lOI3PkdU/i6k2jTDbsUmTJvjqq69UnKCBLliwIHr06IGFCxfCz88Pd+7cSXwTve1v3LgBBwcHHDx4EBEREWjQoAEaN26MRYsWqQx34gG54Acx1eZpBLa93v585C3l7/o76X+nvlLe9OeYpbazs8Pq1auTDcZTxpbnz5+jXr16WLJkScpTZdvvYqqzDXWeeiMx1dncXOY01T/++CO+/fZbXL58Gfv27cOVK1cSAxGfu3nzJk6fOo3z58+rKTVO39Lo7N+/H+HXw9U0Lv99/v369esqwxQWFqZG/5GRkSo48vHmjZvqPY4dOwb+/vTpU1y9elWd58yZM6oD1QMlp4nZue7duxd8/Z9//qnOQ5P9+++/48SJE+B78HPrU3zPnj1T5+fn4vl4DM/HrAX/j0OHDqk7j+H5TbmJqU6dHlmz3chbv7Fdrl27pgwROzE+d+TIEdWuNFo8hjc+99133ykzdfjwYfz666948uSJMlMff/wxTp48qV6jnzerH8VUm0b49u3b+PzzzzFhwgRlnjdu2IgPPvhAfafZthcvXlRxgN9Rxo6TJ0/h0KHDKh7Q6OzatUu1ecWKFRESEoKff/o53ayjaZ82c0eLqc4cN/0omuCff/4Zd769o+LGgQMH1CO/99evXVf9EfsfPcZz1urWrVvg6xgPGOPZJ/Cmx38+x37lyy+/xJo1a9SxjDGcNWFc4cwH4w51p5tqzqDk1E1MdU6Rz93vK6Y6m9vHXKaaQS0oKAg2NjYqI9SmTRvUqlULO3bsUCZ27NixqFq1KurUqaOyRTQ9nIa1t7eHq6ureuQon1O7p06dQpUqVdC8eXO0bt1aZQCCg4NVh0kTzCDHOzMINFWzZs1S523RooU6DzNY9+7dU0Fy7dq16jO5uLio53x8fNTUL9+/VatWKgPG93F0dFTmmpmJDRs2qM/esmVLdUzPnj1x9+5dfP311+oYZruYOeMxDMh6MM5M04mpTp0addC7d2907txZdVhkTNbMPLId+DPb1NnZWemJeqPhZqfHQVLDhg3V39lGbm5uapDEY9955x14eHgoLaX+zub/q5hq05hOmTIFb7/9NmrWrInFixertv33v/+t2pVtTePMQTgNNWOQra0t9FhQrlw5bNmyRWWzP/30U/W9Xr9+feJg37RPZr6jxVSbxpLxgX2Mk5OTis18rFChAmbOnKliBGcp2DfRWNNQs19gPGAM4SPjDGc7GP9XrlypNEUN8bjChQurTDWN8/Hjx1Vc4XNNmzZVd8YjMdWmtZ8cnXUExFRnHdtUz2wuU83R+pAhQ1C8eHGV3WXNImsdLS0tVWZx6NCh+Oijj1R2ifVpHNGXLl1aZYqZcWQgq1y5sjJMzABwenfGjBlgXSQzBpy25yM70c8++0ydm1lummp2nMwk8LXMgpcvX15NC/P3MmXKYOLEicoUM3NOU8/AyNpKGnoGxB9++AGjRo1CtWrVVPaTwbJdu3Yqe83zd+/eXXXanALkQIGZatZpDh48GOvWrTOpgxZTnaoslTnes2ePaltmIzkb0b9/f1VHS71wwMNpf84WMOPk6emJPn364LffflMdZZ/efVRb8jkOgqi37du3o0SJEqq9TRkIpf6J0/6rmOq02WTkGZrlsmXLqjbkz/zec8aBbcvvIeMG/8a4wcE4Z8moA5pxZrj5O3VibW0NGmpqifEqN93EVJvWGvw+s//hDAYH3JzdoC74faeR5p3aYHKGMZ9a4MwHZys508FB+JgxY9TAvG7duqr/4HM7d+5U/daqVavUbAiTOYwz7DOoPZryTp06qRlLDu4kU21aO8rR5icgptr8TNM9o7lNNc2oXhLBwPPWW2+pLOGwYcNUZpmZRGYhaVRpuCdNmoSpU6eqgMaOk8aHRlY3uPzwLPFgdnHOnDnKWFtZWSmTy+w46yOZsWZmmjcG1y5duqiMMoNrkSJFlLnia3lnIGTWmVkMZjD53ux8AwMDlVlnyQFNOD9bt27d1MUnPA9XGWAGndkOZkeZEWMG7PfffpdMdboKy9yTbCu2KQcxS5cuVVO7zEBu3rxZdXwcODG7xPbjnSabHd7Zs2dBfXDgROPEO034gwcPVDkANUZjlp03MdWm0WYb8vvKGSTqgqaoaNGiyhDx4kWaJ343eUEZv5v8rvLGmQvGBg7EaZCYkeTMWW4z1PysYqpN0wjjPhM3HEDzZ2aj3d3dlemlZvidZ4zo27cvOFhnrNDr8Jllnjx5ihqY0xQz5tCU8ziWjzDuMKFC3TExNHDgQNVnMO5wJpVJHvZ5YqpNa0M5OmsIiKnOGq5pntXcppqlEgxSvDHDyGlaGh2aagY83jjFRvPNwMZsAescaVI5fccaZppqBjIez9ujh4/U66dNm6ZMNTPMzDwx6LEjbdSokaqjVi8GVNkAp/+Y2aapZtZKv7FzZf0cM1gsGeB76+8/btw4/PrLr+r1y5YtUxdC8b3YMbPshIGZHfTIkSOVIdcHBc+eZn5TFMlU6y3z6iN1wjbhzAGna5ld4uwDa/Y5y9G1a9fE9hs9erTq6Kg1tgt1xBs1QnPOtmONvJjqVznzLyG5eEm9jJpqxgcaKRoh3miMGBvEVKfe5ub8a06vU00jzT7G29s70VRz9qpDhw7q3+T3n4kZZpk5sGIs0VeMYaKHCRuWA7JchM/phptmmYMxzoSyxIgzqAEBAYn9FuMO+wMmisRUm1NRci5zERBTbS6SGTyPuU01p86YWWaQYyaXGSWaGmNTTdPNqTZmBJhF5HQsO0B/f391cQmncWl6mW3khSXMELBsg1lKXsnPIKcHPV6AyCwWAx6DI8tO+NpBgwYhPDxc1cNx+lfPXjCwbty4URk1ZqK5KgDXrmU2mjXezHwxUG7atAkRDyKUmWYtJztsTgXSgPM1vDNTykBKk57Zm5jqtMmxzViyw0wQa6OZiaIeOAPCgQ4zRdQa23D58uVq1oO64ICMGSeacmqLHSmnfZmhKlmypGqv7MxWSqY67TbOyDNsK15kyDbW40HKTPWZM2exbds2VXfNWMIYw0E1Y4OY6oxQNu01ecVU9+vXT5UIspyQ8Zx6Yp/RsWNHNTPJvof9ErVDDfHiR5YXMlPNJA/LSVjXT5POGREmX1g6+DxWW/1Dyj9M05EcbX4CYqrNzzTdM5rbVHMkz/KL8ePHq0z09OnTlaE1NtXsJPW6Ni8vLzXq58WCzEgyE8nA9u6776qaZ56H03bMbPM5dpDGppqGihkGZsFZtuHr66tMNQ01swc0yjRZzIizjlqv2+aKAMxa0sjTRLOsgzW6HADwGAZd/p0/M8jSzDPQ0mAzQ8rzcQDB7DqNW2ZvYqrTJ8fOi7pgDa2efebULmco2Easo2TmiOVCXCeWAyReuMpZEGaQOLjiBbKsuWW9PQ06p4CpJWaxs+Mmpto0ymwnZpw5gKURMi7/oOlhrSyv0+AsFGefWNbFuMHX8xoPMdWm8c/I0bnFVPNCdPYvjBGpZap5XQZjCq+HYfzmTBjjPrXCfofPsa9ijTU1xCQML26mqeY5eZ0Q9UYjzdjDPoRZbhpwnkNMdUbUIq/JTgJiqrOTNgBzm2peuDF79my1KQMvPuRULDtFlnTwoj79xgwBs8vMNjKITZ82XZV0MDvJ4MbswIIFC1TwokniMkZ8jpltBjgGP94YQNmxMnswYsQIZXb1VSD4vg/uP1AXQTIIMkjyohSeh4abyyIxg8njWLerL6tEw8VabX4uGnUuxceAyqWWWELALDvPx/+Px/AzZPYmpjp9ctQJ+XM9YvLnje3K9uc0Lgc1HPxwVoR/05/bunWreo5lRZyFYHvzeGaWONtAA8bXZsdNTLVplNlOHFCxrTkzxfp4Dqo4mGW2kN99fmc5U8VVfTirRE3wokS2N2c2+FrGDV5TkV3t/jr/tdRUvw6tV1/LGMw+Rl9tilrgzAUHYbzx+8+YwOQI4z/1wosPGccZD1g2xtdQG4zpNMfsF6gf9kNM0vA46o2xhmacx3LW88mTp+o56oulaTl1kyX1cop87n5fMdXZ3D7mNtXMUtNI08DQEOk3BiTj3/l3BkLWrPG1fNTNKU01M8LsDGlmjZ/jeRgw9dfyPAyEzBTwPAyMxs/xeb4vz8NyAOPneC7dbPE1emfLR/6un4+v02/8mefi3fgY/fnXfRRTnT4xtgWZp2St/11vC2pCbz+ekb/rbW7cfjwP29z4b+l/AtOfFVNtOkO2F7+/bD9+h/l9Z3vzZ/5Nb0/+znihfz+pAz7Hv+s/m/5pzH8GMdWmM2U7s4150+OD8e/82fh36ia1GMFjqS9dQ/yZ59bjC8/B53jXdcf3NNah6f/N659BTPXrM3sTjhBTnc2tbC5TzYDDrC6vjGZwMeXGrAGv4jelVtmU98/OY8VUZyftnHkvMdU5wz0vvauY6rzUWrnzs4qpzp3tktOfSkx1NrfA/YgEtPJ7iNt3TDPCNNWclmVNMrNCptyYAeCKHcwQ5PcbUV28GosmHSPBusTceIuOeYlZoU+xYEXmVznJjf9Xdn2mb79/geadI3H3vmnfi6z8vItXP0PIPE5jZ+W7yLnTIvAs6iWad3mI67eYiU/rVbn374wRjX0jcTk8/8fs3NoKR8/Gwrnjwzypn9zKND98LjHV2dyKUdEvMW7mE8xZ/gzP43KnqctmJNn6dnFxLzF14VN0H/oo1wbD+PiXOHAsBu0HPMKjJ3mwx8/WFk3+ZjSpqzZFYdjEJ3j6LPd+vy5ciUX9dpF49FjaN3kLZs9vNNOO7SLBJEdevLHvGDjuMSapgVnu1XleZJvRzxw06ykGj3+c0ZfL694QAmKqs7mhExJe4sdf4uHgGYn126Lw8FGCZKuyoQ1Y9kGDOnv5U1i5ReDHn5Nq9rLh7V/7Le49SECrHg8xIOgx/rzL2kXpONODyGzjk6cJ2LgzGjYtI/DNd6ztzb3MYmNfoteIx/Ab9ki1r2Ss02td8z0XE/MS1248h3WrCGzcEZ1nExvU+50fXsDaLQK7D0bj8eMEZNN1wOZrjDx4JtWPPE7AotXPYOcZid//lKmmPNiMWfqRxVRnKd7UT85rO374OR5t+z+EZYsIWDjLPTsYVHaNQNOOkar0JjcbLqqGn+/uvQQMCXmMam4RKN9MNPJXGqncIgIObSNx+kIsnj/PvYaa7cvyrciHCRg74wmqu0nb/lXbmut5S5cIfNkqAqHrnoElIHnZiL6If4nvfnwB9x7Sj5hLHxk5TxXXCDh3isTX37J0KHfHmdQdiPw1KwmIqc5Kuumcm5mpJ88ScO9BPP68J/fsYEDWnG7P7YZalw2zUVHRCbgfEY8/74tG/kojevvmlaw+DR3r+lX7SgzIljh49348Ih7GI/Z53jbUeoxgLOMMjfQj2RcfVZx5kpBrywd1bchjzhAQU50z3OVdhYAQEAJCQAgIASEgBPIRATHV+agx5V8RAkJACAgBISAEhIAQyBkCYqpzhru8qxAQAkJACAgBISAEhEA+IiCmOh81pvwrQkAICAEhIASEgBAQAjlDQEx1znCXdxUCQkAICAEhIASEgBDIRwTEVOejxpR/RQgIASEgBISAEBACQiBnCIipzhnu8q5CQAgIASEgBITAG0TgRUICeJdb/iUgpjr/tq38Z0JACAgBISAEhEAOEuD2MHHxCfj+4VMsvnQHa67/iD+fxiA+L+88lIM8c/tbi6nO7S0kn08ICAEhIASEgBDIcwTiE17i7tMYfHXjJ4w4fBWDwy4hIOwyxh4Lx97vfsfDmDgkiLnOc+2a3gcWU50eHXlOCAgBISAEhIAQEAKvQYBG+VncC+z/7g+MP3EDg2imD11Ofg+7jGlnbuHMr/cR/eKFmOvX4JubXyqmOje3jnw2ISAEhIAQEAJCIE8QoJmOeRGPC79HYMa5rzH4IDPTqRhqg8EeHHYZQw9dwZLL3+HrB0/w/EUCJHGdJ5o6zQ/5Rprqly9fIiGB9wS8iI9HXHw8nr94gdi4OETHPsezmFh1fxodiydRMXgcFYNHz6Lx6Bkfje/ReGR4/klULPh6HhsVE4uY53GIjXuB5y/i1XvEx8er9+OXju8vt5wloDSgdPDXGngcFWvQgHHb6z9TF9Hq+eQaeJ6KBhJEAznb7MnePSMaeBrDGJAyDmhtrsWEpJ8ZJxgv+HqJA8lQ59pfjDXAGJ3UF7xI7AteRwPsD3QNsD9I3he8kL4g1yrBtA/GHv15fALuRDzF0ivfYcihy1qpR8rsdBq/syxk5JFr2HTrZ/z+JBpxCQkQl2Bam+TU0W+EqY6PT0AMzXJ0NB49fYpf/riLr7//CedvfI0dx89i1d7DmLVxJyat2YoRSzei39w16Dl7LbrNWgvf6avhPWUlPCatgMfk1fCYska7T16NNpNWwnPKSrSduhqdZ65Bj1lr0HvOagxauA7jV27FtHU7sHj7AWw8dAJHL13D9Tvf4/tff8X9yId4GhWF6NgYFWTFZGe9/Nlhxjw31sA93P7hJ1y4cRs7T5zF6n1HUmhgLXrNSamBlfCYvCq5BiauhOfkFfBRGlgNv9nJNTB13XYs2r4fG8KO48ila7j67Xf47pckDcTExuJF/AsZaGW9BKBrICo6Go+fPcOvf1IDP+PijW+w68RZrNl3FLMNcWDksk2JcaA748C0VUZxYBU8JqeIA0oDq9Bpxhr4GeLAwIXrELRiC6au24aF2/fhq4PHcfjCVVz55jvc+eUX3I2MxJOoKMRIHMiG1tfeIqUGfvvzvqaBm7ex++Q5rNmvaWDymq1QGpjDvmANqIEO01NowLgvmKj1EYwDnWas1jQwezUGLliLoBWbwTiwcJumgbALV3DlmztJGnim9QVx8VICkG1CMOMbcTWPe89isPXrX7S6aWanUzPPYZcx6MBFDNp/Me3sddhlBB0Px+Ef/sTDmOdgTbbc8haBfGmqmQ1m8IyOjkbEw4e4eO0GVm/bhRHT56Ld4NFo2n0Q6rTvDSsvP9Ro2xtW7f1Rs9MgWHcfDtve42DnHwLHwdPRYNhcNBq1CE3GLYXzhJVoPnENWkxZC5cp6+AyaZ363Tl4ORqPWYKGIxbAMXAW7AdOQd2+wajdYxRqdRmCLzsOhFX7vrDy7ola3j1Qv3N/tO43DL3HTcbMFWux//gp/PjLb3jy9Bmex8Xhpdf+1EcAACAASURBVHyJzPINStJAjNLA5fCbWLtjN0bOmAffwDFo2n1wcg346hoYBtveYzUNBExHw2QaWIHmk1Zr7a80sBbNQtbAOXgFGo9djIbDdQ1MRd1+wajdU9NATWqgXT+lgZpefqjfuZ/SQK+xkzBj2RqlgZ9+/R2Pnz5D7HPRgFkEAKiBihYHYhD58CEu37iFdTv2YNTM+WgfYKQBTz/U8DGKA92SNOAweJqKA04jFxriwAr1vVcxIFkcoAZSxAGlgdEqDtTsOAhW7fuhhndPFXccO/WDe5+h6DF6IqaGrsLeoyfwwy+/4vHTp4h9/lzNaJiLw5t6HiYreFcaiKEGHuHqzVtYv2svxsxeqOJAMz+jOKBroOMgWOsa6B8CB/YFQ+ciuQa0ONCCGpis9wW6BhYm9QX9GQdGo1bXofiyk0EDPj3BOODQqS/c+wxBj9EhmLRoBXYdOobvfvxZaeD58zg1m/qmtl1e+L/Zxzx5/gJHfryr6qZZypGqmWbW+sAl9NtxHt6z96Hp0LnotekoBu2/kOrreR5muqef/RpX/oxEVJwMtvKCHvTPmO9MdcLLBDyLisKtb77F8JApcGzTDuUdmqOErRNK2DXDFw3cYeHsjYquHVG5dXdUa9sHVoYgWqf3ONTznwjHgOlwGj4fjceGKjPtMmU93GZsRss529Bq3k51bzlnJ9xnbYPr9A1oPmkNmgYvVwa8wZDZBmM9HrV7jEStLoGo0b4/qnj2QKWWXVCuWTuUadQapRxboGTdpiht1xQ1ndugfZ9B2Lp7Hx5ERqrMpd5A8vj6BNiRRkVF4+tvvsOoSdNR36M9yjs0Uxooad8Mpeq7w6KpkQZ8+oDG17rbUNShoe4fojTQMJkG1sFtxia4z96GVnOpgV3QNeA2fQNcJq1F0/ErNA0M1TRg1y8YtmpwZdCAV09UatkV5ZtTAx4o5dACJepQA01Qo2lL+PTyx1dbd+JBZIQyAq//n8sROgFdA99+9wNGT56Ohp6+qODYHCXqaHGgVH03WDT1QgXXDlocSNTAMKWBehxYB0yDsQZooBI1oOKAsQY2ahoIXoFGoxehwdA5sB84FdSAGmB3DYSVrz+qpNSAo6aBUnUbo4qTGzz8+mHt5m249+C+aEBvzEw+Gmtgwsy5aOTVERUTNeCML+q7oWyiBrqhqndv1OwwENZdjeMANTDPqC9YB9fphjiQoi9wm65pQA2yRy1Gw6Fz4DCIGhgPWzXA1jRQ1aCBCi6GOODoquLAF3ZNUNXJHW2698OKrzbi3n3RQCabPksPo5mOfhGPG/ceYdrZWwhMz0wfvIQBey6i49KTaD3tMJoM34TS9p1g2awXWgYvRf+dpzH4wMVUzTVXCRl2+AoWXvwWvz6JQuyLeKm3ztKWNc/J852pjouLw4LlK1GmZh18WLE6ilS1wWdW9ihWuyFK2DdH6YYtUa5ZW1Ry76yMLg1vrS4ByvywA3QYNEV1iMw+OwevRIsp6+E+aytaz9sFz8Vh8Fl+RN29Qg+jzYL9aDlnhwqyLjTWQcu1jPXgmbDzn6g6Zxo1BmoGbJr4Ci06qM6cAb14XWcUta6PItVs8ZFlTXxsaQWPzn749vvvkZAQb54WfgPPwszUivUbUKZWHXxYoRoKV7LGp1b1lAZKJmrAB5Zuugb64cvOAWoQxA7QfuDkRA00DV4BNaiamaQB72VJGvBYSA3sTNSA8/jlcOKsxeCZqDdgUjoa8EbpBu4oUdcZxZQG6igNfFqpJly82uPODz+IBkzQLktqVm/chIp1HFDYsjqKVLbGZ1b18DnjgJ0eB3y0OODRAzXa9VNxgANhzjIoDQyZg8ajF6Np8EqDBrYY4sBBeC87nBgHqAEOtGi4tcHVMjQcsRCOATNhTw30GQvr7sPUwK2aT59kcUDXgB4HClvWROEK1dG4lTfufP89Xr6UOJBZGbAv2LJzN6raO+GjitVRuLI1Pqmhx4FmKO3EBIsPLNkXePihejstDtj4jUBdxoEBjAOz0UhpYAVcJq+Hux4HFh1M1hd4LDxg0MBmpQEVB0ZSA7MMGhgHG6WBQajm01dpoKJrB5XgKd3QHSXtmqGYTQMUqVYHugYatfTCt999j4T4F5lFIMeZmQBLSX988AShV75TFxiqJfJSK/VgdvrgJfitPwPPmUfgPilM3WmqS9l1xOe1PFHSth1qegWg3fzN8N91RruoMY1zjT56HRtv/oSI6Fi15rUUhZi5Yc14unxnqu9HRsLezRsflK6IQuWqaaa6pj0+t01pqrsYmepA2PYYqbJKzCxwqk/LUq9Ci2lfwX32drRZsE8F0XarT6H9mtNou/IYvJaEodXcXSqLzSnApspQ6YFUN9XDlKmu5mMw1a6+sHD2UoZKN9UfV9cM1f/KVEIRi8pYuW49Xrx4gZdyqUKmpP7kWRQcWvqgYBlLpQFjU13KoTlKO3FgZWyq++PLzoGaqe4/HvYcWA2jBpbAecIqtJiapAEa6narTqKdkQY44HKbuUVNA2umWtNAvYE0VOM0Q5XKwEozVM2SmeqCZSrj4/JVMWHKNCkFylTrawexVrmJZwcUKltZiwOVbfBpisE1DVUl9y6okmiqqQEaKppqw+A6mQa2oc2CvVocWHUS7decSowDr2iAhipwdgpTTUNlMNU0VE298EV9bWClmeo6KFypFgqWrYwPy1TE5Fmz8VJ2X8u0CiIfPYFn9774X2lDHKjMwbWWYOGMFeOApgGa6h6JppoaYIJFH1w3MiRYOLjm7CQ14LX0MNqt1uPAcXiFhqH1/N1w1+NA8Ao4jVyE+tTAwMkqDth0Hw6WAekaSGaq7TVT/XF1OxSuVBP/K1MZ7xUvi2lz5qm+INMQ5ECzErgf+Ri+E5ai10aWb6SRYTYY44H7LqL9ouNoNeUw3CcdUndjU01jzXtp+46o12M8uq7ehwF7z6deb81seNhlTDx1U5WbPI6V9a3N2rBmPFm+MtUcvT14FgPfmWtRyr45PqleB0Wq1sanNe1RzEY31e7KUOmZapWd6BQAG7+RqNs3SHWCLOHgFC6n8bTsxBZlnpmR8go9BGapmbVuPX8PWs7ehhbTNqL5xNVoMi5UTRc7Dp6hanJte41RJQVWvgNQ1bsXKrXqhgouvrBo6okv6ruieN2m+KxWfdBUf1jRCh+UqoD3PyuBpcuWSSA1QeRPY+PgO+crlHZskagBNVuha0A31e6dtM60bT98qTQwAnX7MkM1SRki1tMnaWBzkgaWaBrwMtIAp34TNTBsPjQNTIRtr7GqPjPZbIVL+0QNlKjbVJlqTQM18X6piihYrBTGjB1rAgE59HFMHHos3oKyDVoaNGCrzVhRA3bNwOwgB1aVlAb8UD1RA8NRp08Q6vlTA7NUOY/zeD0OUAM7oeKAQQOeiw+ijYoD2+Gq4sAaVXvNshFmqllOVqc344A2uK7qbTDVxnGgjqaBT2ioLGvig1IVUeCT4hgXNF4uYDVBypFRsRi0cjcsGnvgsxp2+LhqbYMGGhg0QFPtDUu3jlqmum1f1FRxQNOAFgd0DSxX19G4z9Q0wFnKxL5gSRjaLNiDlnO2Q4sDnLVcaugLdA2MhY2RBtgXVGzBvoAJFjeUsHNWmWqa6o8qahr4z4efIGTiJCkDMkED5j70tz8foGqjrrBs3hutJq1E320n0l7lQ12YeAk9NpyF95yjaDXlkCr/0DPVuqnWH8s16oZmI+aj18Yj6Rr2wEOXMff8Nwi/+0itby1Za3O3smnny3emOiI6DoP3XkfHNcfhGrwQX/r0QKXmPijHrIRTS5Rt1Ep1pnpNNTs5K1/W0Q0DTbBdf5aATEODofPUtJ9mqtbCdfpGlYWgiebdfdYWNd3L8hDNTC3VpnwDZxoyE+Nh4zcKNTsFonpb1lT3hKVRTTXLEGj0afw/rFAdBYqVwj8LvI//vPMuQkNDxVSboOunz19g0L5wdFx7HK4TlqBW216o5OKD8o1awqKhQQPO3qhgqKtnaU4N34HqYqJXNKCM9XK4TF6jTBOnf1lXbawBZrCoAXakTiMWor66YJXZqWClAWbBa7TzR1VP1lR3QfnmbVGWdfUOLvjcxgk0Ux9WqIH3ipfBPwt8gP+++z5Gjx5tAgE5lBcQjTh4Ax3XnoD7pFDUat8HlVu0VRooY6QBZgtZlqXFgQGqnta255jks1ZGGuAAmrMSSRrYqsWBqalrgIM0xgFqoHo7fxUHNA0Yrq1waI5itZ3wSQ07fFhR08C/3i+If771DsaNGyem2gQpR8bEIfjo1+iw5jjcJy+DtW/fRA0wDliwL2AcaNFBJTwYB5gA4UWFxhqoP0S7YJ3XTDAO6BpI6gtSamCZuqhRz1IbX7heoz3r6nsZ4kA7lFXXVrjg89qN8GmNeiq5wjhADfzt//0TwcETxFSboIHMHkqjyiXyWD9tfNNMdReVYS5e2xs1vQLhu3Ar+u86k3pdtGFpvQF7L6LL6lNoEbQVpR06q+N1M238WNzGG5Vd+8Jz+jr0234y7ZKQsEsYfvgqVl//ET89eqY+q/HnlJ9zjkD+M9VRcQjYFw6/rZfQZ+dlDNh5AYO2nEDfZdvQZepieI6YjCb9RsG+eyBsuwTApvNgWHcJRO3uw9TFJHZ9maWaqBnrIbO0C5XGLFalHc0mrFLmiQaq+cRVKovJlUEaj16kVolwDJgB+wETUbfvOENHOlhNKXJqkSa+XFNvfOHoiuK1G+KTarXV1HSBoqXw9oef4F8FCuLvb72N/7z9jphqE78Puqn223YFfXZegf/OixioNLAdXaYuSdKAn5EGugaqmkdeUKRlKqmBqSpb2XDEfDRWGliBZiG6BtagWcjqRA1wZqPhsHnQNDBJzXroZkpdqKo00AnlmJmiBmyd8Gn12ihkURkFipY2aOAD/P2td/Cfdwpg1OjRYqhM0IEy1WE30T1RA1oc6LdiJ7pOWwLvUVPQtP8oOPgNMcSBAEMcoKEahbp9tIuW1eofxnEgaDmS4oCxBpap+mtNA1otNWOJpoEA1GjXX82KsI5fxQFeU6E0YIsPLSrjvWIGDbxfEP94+x38499vi6k2of15KE31hKO30X3rZfQ2xIFBW0+g38qd6Do9FN6jpsK5/2gjDWh9AWuftThADYQY4sDMxL6AJV5pxwGuAsQ4oGmAs5+8UFUNrNv3VwNrS7dOysyXVhpojE+rUwNVlQb++9EnylBTA/9XmepgMdUm6uB1D+feEr89icbWW7/g9oPHamdE3Vwbm2rdDH9RzxeOfSei25r9GLjvfNrm+uAl9N5yAvX7TkK5Rl1BA62fI+VjKbv2sG43DB2X7oT/7rOpl4SwzERteX4d++78jntRsbJizOs2dha8Pt+aatWZ7rqCwXuvYtTBa5h4NBxzT97AopPXMOPoZQTtOYeArSfQe10YOi/dA5/5W9F65ga4TV4FlwnL0GzcYjiPXYSmY5LuTUYvRGPeRy5AoxHz0HDobDQImAbHgZNQr18Q6vQcBZuuAfjStz+qevihkpsvyjf1RJn6LVCyjhOKWdmBF6IVtqiM/5W0wHvFSuLdIp/hrYJF8K8CNFRiqs2h8WSmetcVDKIGwqiB65hDDZzSNDB+7zkEbj2OPusOKg20nb8VbWYlaaB50CKDBhaiaaIWDBoYtdCggVlKAw4GDbAztu4aiJq+/VHN0w+V3TqggrMXytZ3TdJA5VooXK4yCpWywAfUwMdFjTQgptocGtBNtd+2q2pgNWjPVYw0xAFNA9cx8+hljN97HoFbT6AP48CyvWg7f5umgSmGOBDEOLAQTccsTIwHiXFAaWC+URyYbBQHAlUcoAYqGWmgVJ3GWhwwaOB/Jcvj/aJ6HPg40VCJqTZdBVqm+ht0M5jqQXuvYMTBawg5ch2zT97AwlPXMevYZYzfdx5Dtmka6EINLNA04D55FVpMWI7mqWiAemBf0Ggk44CugelwHEgNjEfdXuwLqAF/VPfqgcruHbU40MAV1MDnNevh08Q4UD5ZHOBslZhq09v/dc/A9aYjop8nbi3OZe1GHrmqNnP54eFTtZHbr3+w/EPLVKc0wuUbd0fzkQvUcnnpregxYM85dFyyAza+w1U99efWWm11yvPxd4v6ndFgwFT4rTuIgfvSWoLvElgSMuX0LZz79YFa5k9W5n3d1jff6/O3qd5JU33FYKqvY+7Jm1hy5ibmnrqBKcfCMf7IdYw4eB2D915Dv11X0Gv7RXTfeAZd1h5Fh+X70X7xDnjP24Q2M9ej1dSVcJuwGC5j56PJsJlwGjwZDfzHwb7PKNj5DUHtTv6o1a4XrDy7ooqrLyo6e6K8kyvKODRHqTpOKF6rHopWs8HHljVQuGwlFCpZFu8VK6GZ6kJiqs0naUA31RxY9aWp3kMNXDWY6psIPXsD806FY6pBAyMPXkfAvmvov1vTgJ/SwDF0XKFrYDM8ZhlrYJ5BA5PQwD9IaaAuNdB5gNJADc9uqOrWAZbNPFGhkRvKOjZHKdskDXxCDVhYolDJVztTyVSbRwmaqb4FzlYwS0kNjDhwDSFHw9XAKvTsTYMGbqg4oGnguhYHdjAOnEaXdceNNLApSQMhi+Eybh6aDp+lxYEBQXAwxAFbpYHesDLSQHknN5ShBhgHrO1RrLoNPrG0wkcWlfC/EhZ4L9FUFxFTbZ7mV2dJMtXajJXSwMGrBlPNvuAG5p2+gWnHwhF8JBwjwxgHrqs40HvHRTAOdFUaOID2i3fCex7jwFdaX6A0oPcFU9CAGuhr6AuoAe6DoDTQEZbNvFChsTss6ut9ga4BvS/Q4kABDq4LFVElYGKqzSiEDJzqWWwczv16HzPOfq2WsUu53vS44+HYdvsXXP/xd1Rp3C3NDDOzz9VbD0CbKavRb8epNMs3uDIIn+frqrUagFJ126V9TmsvVGzWEy3GLEKfrcfTXoLvkLbl+eJLd3Dz3mM1CMjAvy4vMTOBN8tUn7qBJWduYd7pJEM14mB4oqHqTVO9iYH0GDqtPIj2S3bCZ57WmbaeugLuIUvQYpweSCehQf+xsO89EnW7BaJ2R5rqnrDy6Iwqru1h6ewF1Zm+YqqrvzGmOkU5mpmlm/bp0jXVp25AGSqDBtiZcmDFztR/91X03nEJfpu1zrTTygPwXbILPvO3qM6UGnALWQIXgwYaBdBUGwZW3YfAthNNdQ9YeXTRNMDOtJFrKqa6er401fHx3Gwj7XbJzmf+2lTfwnylgRuaoTLEAU0Dlw0aOKHigG+opgHP2V+h9bQVcJ+ox4FZaBwwCQ39x2mmOlEDjANdUNXNV8UBDqxeNdXV3whTnZDwMsempNM31Tew5KymgWnHqIHrGHnIEAf2GMeBE+i06iB8Q3fDZ8EWeM7eANUXUANB89XAihpo4D9WS7B0Z4KFcaAnaugaUHHAMLhWCRbdVOt9Qf4y1YwBuSUOZDTmrN1/Gr3XHjCsvpH6Ji7MBgcdvgLnwJkoW78LPrf2StMI0yTX6TJG1VunWb4Rxk1hLqoLE5sNn4uKzj1RonbaJSElavuoJfg8p69F/x2nUzfsvDhy/yUM3nERV3+KyOi/L68zIwEx1cZZSmaoXsNU10/VVCcZqjfdVF+78yhHOlSTTbXSwHHNUKVjqhsHTEwy1d0CNVPdNrmpLv8Gmeo7vzzFwye5Yyc400z1JfhtOo2u6wym2qABz1mvmupGg2mqxyaaat1QaaZaG1ynNNVFq3PG6s0w1THP43Hj+8eIisn+tZYzb6qvaINrFQfSMdXj/tpUc9ZSZar1Gas3wFQ/i36Bew9j89SCsKOnLUeFJn5q9Y2eG46knQ0Ou6yMd/sFW2DbZQxK1W2fprEubu2FMg6d4DRgGrqu2pf2ih5hl1RpR9eVe1VttkWDLuCxqZWD8G+s4a7nF4QOi7YnGwQw+913+3l0CD0Jz+lHcOj6b2a0inKqjBIQUy2mOqNaee3X9Zp6DWfCI7LdWIupfu2mMssBp69HYNraO7j/kNts52zKWky1WZrU5JPQTE9bdwc7TvyBqOjsNdZiqk1uvtc+ATPUv96Lxqglt3A3MjbPZKxHTVmqTGxJWx9Ua+WPNlNW/eXqG9wNkcvq1fAYBGaR0zLBLAmp2LQHWoxdjF4bjqaeYVYXHV6C/+5zaDtvE6x9hyvznNY5mSWn+W4cMFNdINl/1wW1ukib6dxo5hBaTzkspvq11WueA8RUi6k2j5JSOYv3mIvwHX8JNFssDcium5jq7CKd/H2OX36AduMuYezSrzVjnYNzwGKqk7dNTv1GUz1u2ddgLNh+/PdszViLqc7+VudX/sc/nsF92DmMDr2FPyJi8oSx1k21bmK/qNcBNr4jVDY4zfINw66JvTYdVRlubj2eXoaZOyjWaDMQnjMMy+WFXUp1pRAt43wCLSeuQNWW/dXOi/rnSvlIw17JpTdaT90P98naBjNiqrNf98bvKKZaTLWxHsz6s8/Yi2gx5Bx8x1/GiWsPwC1es+Mmpjo7KL/6HjTVXqMvwn3YeYxZeitHjbWY6lfbJyf+optqxgEa623Hf0d0NpWCiKnO/hbXTXXzgLNoNfy8Mta/34/O9cY6pammeaVBZjbYvkcwuq3lcnmpr77BixoH7r+A7mv2q/IN1lsXT2NFD60kpCNsO45UK4CkZ9i5Y2PPDYfRdMgcVZpCA53SVPP3Yl96oNnobYm7Noqpzn7dG7+jmGox1cZ6MOvPuql2Vcb6Eo5duZ8tGWsx1WZtxgyfTDfVNFA01qNDv1a1lfoarxk+kRleKKbaDBDNcArdVDMGJBrrY78hJjbeDGdP/xRiqtPnkxXPGptqtrdurFkSkoMTV3/5r6ZmqnUDSzPLLLTz8HnotelYOuUbl9Wa0r6Ltqosdyk731RNsDLsNt7gDooN/Cej+9oDGHQg7S3PB+w5D9Zb09xzS3P9cxk/iqn+yybOtheIqRZTnWVi0001g6vrUGasL+Ho5ft4kcWlIGKqs6xJ0z2xsalOMta3DMY63UPN/qSYarMjzdQJU5pq6oKzGVuOZr2xFlOdqSYz6aCUplo31qyx/vnP6Fd2KDTpzcx4cHqmWjevLN+o3magqqPuvyvtDVnUcnnbT6LNVC6X559uvXUZh45oNHiGtsEL66rTuA8O48Yxp9BgwDwUt3m1fltMtRnFYOKp8pWpJovI6DgM2X8jcSe1pHWqw9X61K8uqWe0RnEuWP3jv+8WwLJly/LFNuXGplo31u2DLuHwxXtZaqxpqgMNGlDrVKu1yg3rVGdkSb1sXf2jROLmL9z0getU/7fAB3lyN72Uplo31uxQ1WoA2VdWr9YqH3XoFvy2X01jner0ltTLudU//llA21Hxn2+/i5CQkDy/q2Zqploz1hew8dCv4OogWXV7GBOHkGPfoPvW1NapTm9Jvdyx+sff/vEvTJo0KU/tqJiaqdaN9cjFN/HTH1G50lhnxFTr5pqrb9TuOBJcASS9khCWb/TefEzVW1do2iNZvTUvbGS9dKuJK9CX25Gnlak2LJHX46uz8JpzDE4Bq1HC9tU1rcVUZ1UUef3z5jtTzS1G9337J4KO3MLA3drGLqMPXcMk7qh4Stv8hQv+6xt/aGsUGzb+yAFT/U6Rz/DfQkXwfpFPUbWGFYYMHYrbt2/nqUCaluxSmupEYz3uEsIu0FhnTY11XHwCDnz3J8Yf/VppYNCea2pHRaWBk4bNX9JbpzqbTTU1wE0f3i9cBJZVq2LQ4MG4fv16njNUqZlqzVifw6jFNNbPs20KmBo4/P1dtU31wD1aHODmHtxZdY7SwM101qnOKVNdGAU+KowKlSpjwMCBCA8PR0JC1nxH0vrOmvvvaZlq6sJz1AVsCMs6Y82+4MRP9zDx2G0M2nMdjANJu2pqm7/MV5u/pFynOmdM9TuGnVULfFQEFpaW6Nuvv9JAfHzWDTzM3d5pmWrdWI9YlDuN9euYapprVRvt2AkN+k1GtzUH0ikJuaSWveu+9iAceoeo9a25dF/TIbPVzovKlKdzwWLvLefQdsExtJqiXYTYcJCYanNr1tzny3emmvWb0bHP8ev9CKw6dArtpq9A3zX7MW7vWcw6fg2LT99QGeu0TLWfMlQZ2/zlddep5k5qXJ9W7ajILaqLl0aBoqVRqFR5dOjRF+cuXETkw4d4Ece1fjPXmbIT23PqT+w5nbP3/WfvovWIC6qOkgHV+M5SEK4SceDcXdD8mPv28uVLxFADDyKx9uhZ+Exdij5r92HsHmrgOhafDk/cUVHtpKaXAOmbPmSFqa7TGMWttV01tR0VK6l2pwbeK1oaBUtZoE2Hrjh38RIiI6mBF8pUZ6QO8eYPj7Hv7N0cb/P5W75H6xHnk7W13u5uQ89h5KKb6uLF7EhYaxqIUxr46tg5tJ26FL1X78WYPWcx89g1LOZueqe4m57BUBk04L+bOzCm2PzFLOtUuxh2VrWH8TrV/O5/UIIaKIV3i5aCm08HnDp7zhAHXiDhZQIys+Dvo6dxqtQqp+PAzpN/oP+M8FQ1QW3QWK8/8Ati47IqDsThtweR2HTiPDpMX4Feq/Zg9J4zmgYYB4xNtdpR8Rr892SlqW6M4rX0zV+4s6ohDigNlMZ7xUujuWdbnDh1GhERkSoOKA38RZDk6kq3fnic4zGAelu7/2e4BJ5Ntc1ZYz1socFY5/Cym8ZIU5pqXvxXpn53WDgxw5z6BYI019yshbXRLqMXofeW4wgIS72EgyUh/rvOouuqvaqGesDe82kacZZ69Nt5AR2XnkTrqYeTXYAoptq41XLnz/nOVHNUf/nqdXTqOxAV7BqjuE0DWDR0Q+12veA+JAQ956xA4JrdGLn9BEbuOY+hey9i0N7L6L/7MtTWtJvOoEsGd1T8a1Otb1PeGCWsHVG8Zj0Us6qDolb1ULSmA4rWcsRnvFvZoUTNemjQygdL16xHRORDJCQwO/H69iPySRxYYtFmgHTDIAAAIABJREFU5IUcvXuMTN1Q6yZLM9YXse/MXcS9MG+H+vJlAq6F30TXAQGoaNcExazrKw3YtO0J14AJ6Dl7OQJXUwPHNQ3su4hBey4n60y5RTV31UxvR8WMbv5SxrEZvqjTBMVT0UAxa0d8VpMaqIfiVvXg2NIbC5atRMRDaiBjXFbs+VmtrJDTbc4Ok+2qt3HKRxrr4Qs1Y53V4ZDswm99jZ4Bw1GxXhMVB8rWd4W1jx9cB49Hj1nLEbB6J0ZsM2hg70UM3ssLjRgHmKnmrprm2/yFW9V/YdcEJWwc8blxHKiVFAc+qW6HYjXsUK+FB2YtCsXDR9w8KXNZyt/uxaD/zPAcjQG6Hrm8WkotGP9OY73uwC94buY4kBAfj69vf4v+w8egsmMzlLBpiLINWsDapztaDA6C38xlCFi1AyO3HVNxYNg+auCSIQ5cTNJAWjsqvubmL9ymXGnAOqUG7FVfULRmfXxaox4+t7JD3eatMW3+Qk0DGchUMznBtcAZd3XuOfWY1sBab3NexDx0/k38yFKQXGKsUzPVlV2HwHnUVtTuNA2l7dPfQZH11lXc+6vl8mie06qNDgi7BJrmVJ/nxjJ7LqLr6tNoM+NIsiXyuKIH72Kqs7rnMP38+c5UP3z0GG7tuqBIxer4qFJNfFy9DorZNEBJ++Yo69QSFV3aorpnd9h0Ggj73qPRaMhUuIxfjJbT1sBj7mb4LNiJtov2qLvPgu3wmsctqjeg9bTVcJ+0FC2CFsF55Fw0HjIVDQYGw6HvGNTrORx1ugbApqM/vmzXBzW8/VDNozuqeXZF1dbdUKV1V1Ru1QWWbp1QwaU9LJp6oYxTS5Swb4ZitRvhUysHFK5cGx9VtELZWnbYunsP4uMzt1FC5JPnKvujB7Dc/EgD1nbsRew+9YdZjXVUVBRad+iGjyvWwEeWmgaKWtdHSftmBg34oLqHroFRRhpYDY+5m+C9YFeiBtoufFUDLuOpgTloPGQaGmZIA901DbSkBjqjgosvLJp6o4xTK5RyaIbPazfCZzUdUKSyLQpbWqG0VR2ErlqdYUO1ZMdP6ir73NzW+mejsWamihvEZOXtWVQUvLv1xGeVaqKwZU0UqVZHDa5K2jVDmYaMAz6o5tENNh0HoF6vUXAKnAKXoEVoOXU1POZQAzvRdrEWB9ou3JEUB6avgfukZWhBDYzSNTBBxQF7xoFugbDpNEDFASvvHiniQBdDHDBowFnTQAn75ioOUAMqDlha4fOq1li+dj1evsycqf71bjR6TbkGl8D0Da3eLjn92Gbkeaw98ItZ48DjJ0/Qc9AQFQc0DdgqDZQwaKBCc00D1koDI5UGmo9bqDTQhhqYvwNtF+/VdGDQgNqmfBo1sBSuwYa+YKgWBxz7sS8YYegLkjTAWFPdsxuqerAv6ILKKg50QoUWvrAwaKCUQ3N8btsYn1k5oEhVW9UXfGZphRXrvspQX0BTve3Y79BXWcnp9vyr96exDpx3I9cY69RMdZWWw+AavA9uEw6g6YjNqOE5FiXrpL2iBzdk+cLOF7XaDkWXlXvT3kExlYsRBx24hJ4bzsJr9tHEUg/dSBs/iqnOyl7DPOfOd6b6wdMoNO87AoUr1TIYKjsVSIvXaYpS9V1h0dRTGdvKrbuhqndv1OwwENbdh6FO73GwHzAJjoEz4DR8PpoGhaL5xDVwnfYVWs7egjbzdsJjwR54LNwHz4X70Wb+LrScvRWu09bDZdIqOAcvQ+Mxi9Bw2Hw4Dp4OO/8JqNNnHGy6D8OXnQahetu+qOLRQxnrcs18lKHSOtOG+NTKXnWmBS2q4cMyFbBs5SrDhYqZyVTnHVPNwKtnrHed/MNsmaqnMbFoMWAsilSuhQ8raoaKpjq5BtqhUquuSgNWvgNg3W1oMg00HD4fTcYtNWhgPVrO2oLWr2hgdzINNKMGRidpoJ5/SKIGanYajOrt+qGykQbKNqKpZmfqZNCALQqV45SwJcYGByOjBch5yVTrbT50QdYa6yfRsfAIHI9PqtniI8taylQX5UxBnab4wrGFIQ60Q+XWugYGGjQwNjEONBw2D03GMg6sVt9zTQM74LFgtyEO7EOb+cYaWA2lgTGLDXFgBhI14DcMugaqeDIOdIaKA41agXFAaaCmvTJUhcpVR8HSFTB+4sRM19UrUz0175hq6oIZThprc11rEfksGl2DZuDj6tQA44AtNA00Rqn6LWDR2APlXQxxwKs3VBzoOgR1eo9FPf+JcAyYDidqYFxqGtgDz4V7DX3BbrSavRVu07+CyyRqYDma6BoIoAYmanHAz9AXtOsHaqCSeyelAS0OuCQz1YUq1ECBEmUwceq0DF1fk9dMNdubMxgBc2msn+V4xjo9U01T6zbxIFyD96N+/+WpXiioX8So11uXdeysaqh7fnUk7cy0YfOYPtvOo91C1k0nL/UwNtP6z67BO1DZpS9SrlktFyqaxxCb4yz5ylTTgkZEPceAHRfQPDgUNdv1QmlHFxS3baiygaUcWqBMo9Yo16wtLN07q8BWvW0/1dnZdB+JOn3Gw85/MhwDZoCmqvHYUDQLWQWXKevhOn0T3GdtRcs5O9XdffYOuM3cjBZTtUDadPwyOI1chAZD58B+4BTY9ZsA215jYN11qDLuNPCVW3dHhRYdVKa6dAM3MGNSzLoBPqlWGwXLVsbbRYrhPwU+QGhoaKZX/8hLmWoGVt1ktRt3ETtO/I7nZqit5Oof/tsvwHXiMtRs10dp4PPaDVWnVcqRGvDQNODWWQ10aHZpeKy7j0Cd3kGw858EB4MGmoxdAucJK9EimQZ2aBqYtQPuM7eogReNl7EGHAZNhV3/CbDtqWnAqsNAZeATNeDshTIN3dUMyuc2DfFJdVsUtKiK/35cHG8X/AijRo/OsKHKa6ZatfmQcxgyP+uM9ZPYFxi25zJcJ6/El+37oXR9VyRqwMEFZZxaw8LZR5lbDnY56P0ypQYGz1DmuPGYUKUBl8lGcWC2QQMqDmxBi2mMA2vgPH4ZGo1ajAZD50JpoF8I6jAOdBtmiAN9DHGgozZb0dBdmepi1EA1WxQsSw2UwL9MXAEmL5pq6kI31ubYgZUrQQUduAbXKatRq6M/yjRwRXFbJxUHStpTA61QzrmtSnRU8fCD3hdYdxsO216MAxPhoDQwD43HLIFzsBYH3KZv1PoCgwZaMg7M2ooW0zYYaYB9QZIGbHuNTdRANR9qwA8VXTtqmeqGbioOFLNxUjOrhcpVw38/KYm/vfUOgoMn5FtTzfamsR40J+cz1n9lqjVTG4ZmY3akn62u5Zm4jnQJw/bkLmMWod+Ok8lLPsIug1uLd1z2at20bqBTe/SecxS9Nh9XNdwWDbomvpeYanPYYfOcI1+a6oB9N9Bt83n02HIWfmsOoMPcdWg5chocuw3Cl17dUMmtAyq28EWlll1Q1bMnahhMlY0yVeNg5x8C+0HT0GDIbDiNWIDGY5egadByOE9YoUw2jTbvzhOWqyxGo1GL0HDoPGXG7QdOhl3f8bDtORq1ugxBzQ4DwCBaxaM7LN3YkXrhi/ot8HltJ60TLWOJAp8Ux78/KIS/v/0O/v32O2+cqdZNFo31djMY68R1qrdcUBrosTYMHeetR6tR0+HYbTBqeVMDvmr61dK9C6p69VRZZBprm+7DUafXWNj1D4GDQQMNqYEx1MCyRA00n2jQALNS40LReNQiNBiWpIG6fYM1DXQdAiulgb6o3MagAWdvfEGTxww1B1RlLPHOJyXwr/c1Dfzn3ffzvanW23zI/BtZUgrCdapHht1E960X0HPrOVADneZ/hVZjZqJ+9wDU8u6OytSAS3vDAFvXwKAUGpgKx8DZSNLAcjgHa3EgSQMr0DRoKVQcUBqYqQ2sDXHAuusQ1Ow40DBb5afiQDlnxgFNA59Uq6M0UOCTkvj3Bx/h72+/i3++9a5JyyrmVVNNXbA2nxe7mWqsaaonHL2NbpsZB87Bb90hgwZmoYFfoEEDHVQ5ViUmWTi4atcXNTsNgjLWvfU4MBX1A2dpiZYxi7U4YNCA1g+wL6AGQpM0EEgNTIZdPy0O6MkVrS/wgyWz1M7eKK2MfmN8Wr0OCpa1RIHPSuLf//sI/3jnHfzt7//K96aa7c2SsAGzw3M0Y50xU30IzcbuzLCpVtlra17M6IPqrQfAZ84GDNx3HgP3XUS3tWnXTadmpvW/+cw7hkH7L6nSEurZofdEcJMZMdXmMcTmOEs+NNVxCNgXnrhONS9AG3ngCoLDrmLGkSuYdfg8Ju09haEbDqLLgg1oNWExmg6fiQaDJ8NhQAjs/YNRr/8E1OsfAvsBk1W2yTFgpgqqNNk0z7w3GDIX9QNnqlIPZqZ5DMs9bHuO0sy0rz+qefVE5ZadUb6ZN8rUd0EJm/r4rFptFC5XTVv5oVgpbY3iQkVUZurvb72N/7yhpprBlXcuw8etjE3JWCea6m1XoNap3nMZI4w0MPvwBaWBYZvC0HXBRrQOWYKmI2aiQcBkOCoNsP2pg7Q0MNeggTloMGQWHAfPgMOgKYlT/alpoAJLfuq3MGjABkUqVEOh0uXxQXGDBgoWgb5O9X/eKfBGmGq2N8t/mLF+YOYa6+Sbv1xWF6ImaeAqZh++iMn7TmP45kPotnCT0oDziJloOHiKIQ5kTAP1h8zVNBAwXRlplnvY9Q1KjAMsKajmzTjQBeWNNFC0Wm0UqVD9FQ38631tnep//PvtN9ZUUxfMYLIUxBRjnbT5y2X02XlF08D+KxgfdhXTj1zFnCPUwBkM33QI3RZtQpuJS+A8YhYaBkyBw8AQ2LMf6PfXcYAaqB8425BUmarigK4BZaaVBnopDag40KAFStRugCQNVEiKA4a+4B9vv4P/+//+ieDg4HydqdbjPuMAL6zNqXWsTTHVXCnEwqkXitu0TcwcG5eD6D/T/Np0HAeP6WGpXoSoG2c+Nh+7E81GbVVlJ8Z/1001L3TkiiJcRcR79ja4BO1NtkpI6ymHcej6b+bwiHKO1ySQ7021tvkLN/7g+rRc8P8m5p4MV+tUBx2+huEHrmLQnivou/OiymZ0W38cnVcdQvvQPfBZsAWes79SFym6TVoKdYHaqLloPHQGnAZPgqP/ONj3GYW6fkNRu/MA1GrfG1a8IMXdF5bNvFGhkTvKOrrgi7raMkoMoh9b1kDhspVRqGRZvFesJN41rFHM6V4x1QZjPca0jPWrpvoKRh28lkwD805qa5UHHb6OEYkauGSkgcOaBuZvheecDWg1fQ3cJlIDC9FsdJIG6vsHwb7PaNglaqAPrLw0DVRqrmnAon5yDWhL6lVGIS6rSA0Y1qd9E001O1VmqmisIx6Z7+LF5KaahuoKRhy4hhBDHAg9e8uwpF44xh+5jhEHr2Hwnsvou9NIA6sPwzd0L3wWbIV+gZoWB1LTgB4HBsLaty+svLqrOKA00NgdZQ0aKGHN5dRqgxr4yKIy/lfSAu8VNcSBgkUgpjrpwkpTjXWSqTbe/OUaQo6EY/bJm6ovmH/KsGfB4WuJGui38yJ6bj2Pbl8dR+fVR+Cr+gItDrRWF6pqfYEWB2bCafBkJMaBHuwLBqJWe8YBaqADdA3occBYA6ov4LKKxUqiAONAIW1w/aaZan2A3XfGdcPOi6/pZEx8uamm2qFvKBoOXo3yjfsm2+RFN9T6o0WjXnAJ2p3MABubZtZtO/ZbCouGPVG783R1kaTx84mm2rBSCJfdazk57JXziak2URAmHP6GmGoaqusZ3lEx7SX1FsNl3Hw0GcZAOgmJS+p1H4LaHf1Rq11PWHl0QRXXdrBs5oXyTm4o49DcsD6ttkZx4jrVylSXEFOdYg1rPXPhbYKxTt1Um2dHRfeQJYkaSG9JvapuBg00cgWXUytl64TitfR1qqurixELldQ60zfdVOvGOnD+TXBJSHPcMmKq56sNgLhOdThGHgxHwL5r8N991bCk3ulXltTzmMUB9gpQAy1UHJiFRoMnoYH/WDj0GQU7xoFOAxLjgNKAsxcqNHJDGWqgjhOKWydfp1pMdZKJ1r/7xo801lxuLzMZ69RN9VWEHLmO2SrBou2qmbhW+SHurns92dKaalnFxCX1NmuDq6kr4EYNBLEvmIXGAZoGmGAx1kANjy6o6tYelZprGlBxgBpIXKfasGeBIQ686aZab/fe066B5UsZWaPfHLGC5zDFVBf90gOO/ZfDfWIYWozfC7se87Ul+IzqqxNNtVPqppoXQjYash6VXAJQwsYHxb70hHXHaama6gF7L6L7ujNoM40XNr5qqGnCxVSbSxmvfx4x1frGH7uvoFcO7KgomerUO1WutX3k0v1MdaZZaarZmeoDq/RNdXttYCWmOt01ivWOlNO/wStuIzo2c0vIpQx9WWGqPXVTPTG5qW6YpqluD8u/MtUlJFOtayC1R24gdfzKg5TNm6HfzW+qOXO5Aa2nroC7QQNNh6dvqqu4ctZSTHVqbZvW37iGdeTj7Nt9lWIyi6k2rCVNc91s9A5UaTnilXIQlomkzFRz2T7r9pNVrTbNNA04S0pSM9Vtph+B56yjKjPtloahFlOdofCQZS8SUy2mOkOmJ60AmBV/5wWLhy7cy5Sh5jclN5jqKq5iqjOqDa6tG7T8NqKiM7c2e2rRUUx1NHrlsSX1UuqFFywevXw/tebN0N/EVKeesEjJObf8zjXV9RWBXn8x2QxJIs0XmdVUcwm+kIOo021Ohkw166crNhugjLSe0U7LVNNIp2em9VIRyVSn2dRZ/oSYajHVucpUc/vyMBMMNb8xYqrzTmeqDPWy24iK0bZlN1fEE1Odt001DfXhi/cyvKxkaroRU5134gCNPdeuvxsZa1Kbp6aDjPwtN5jqz2t5JJrwtEy1bpr/6lFMdUZaPWteI6ZaTHWuMdUs+TAlQ61/RcRU543OlIZ6/PLbeBZtXkNNHYipzrummrsrmmqoqQEx1XkkDgw9h+ELb+KPiJgcMdTUSp411RPD1Eoh3PXR2GiLqdbdQPY/iqkWU50rTLW5DDW/QmKqc39nyhrqIGaos8BQUwNiqvOmqfYcdUENrF+a4So1MdW5Pw5w5Z8Ri27ijwc5Z6gZL/KcqdYviuy1QF0UKetUZ795TusdxVSLqc5RU01zRUPNzFRmrvBPTdhiqnN3Z8qOlBlqc5d8GGtBTHXeM9Veo3ktxV2zbVktpjp3xwGu7DJy8S38fj9nDTXjRs6a6l2qpjqj5R+uE/bDKWAtyjfuk1guIqbaOPrn7M/51FTfSNz8RVun+vWW1Ou67hg6rTyI9kt2wmfeJnjMWq9d8Z24lFaKJfW6BaZYUs9wkVpGl9Tj+rRv4DrVuqHmxUjx8Qlm+ybQVA/ep2lA2/yF61SbZ0m9jK7+kdkLFf/x1jt46933MTqfblNOQx3Mkg8z11CnFA9N9ciwW/DbdgW91cYfr65T/bpL6mXb6h9vvQPqICgoKNPT4XltR0Vu+kRDbc44QFM94eg36LZV3/zlCkYcNGVJvexd/eNvf/8nJkzIn9uU01CPWpI7DDVjR06aapegPajWZpjaeTHdCxUnH0KryQfxZdsglLRtl2ioeYyY6pQ9QM79nq9MNTE+VoH0NvruvIJ+u64g95pqC7xXzLBOdcEieLtgIXzwURGUK18BmzdvztAuWqnJ5uGTOHQYfxnM+uT0nTWzaV1Zrhvq42Y21GQSFfdCbU/cd9dVpYFBe/OGqX7rg0J4r1BhWJSvgIULF2bYUK3c8zN4gWdOtzeXP2O7ptXm7EgnrLyNp1Hmr6FO+V2gBqac+Bb9dl1Dv13aBk8pN3/JKVNdrLoNuF79RxaVkm/+UqgIdA18UaYMFi9enGENpPz/f7sXgwGzwuE5KmfjAN/ffdj5NDVBrWiGmjNV5htYk8eT2DjMPfNdCg3kHlOdtGeB0eYvBYvgrYKFUKDQR/i8REmEhoZmqC94EZ+AnSf/yPEYwBjUZuSFdNu75bDzucpQUys5aapbTTmETsvC0HTobJRr1A3Fa3srw6wvqddy8iG0nnoYnVecgv/u8+gYuhPW7YahVN32iRvNiKlOGQFz7vd8Z6oTEhLw4Fk09n7zG0IOX0W/zScwbM8FTDh0FXNPXMeSMzfVJjBTj93A+CPhGJED5R/cSU3b+EPbopqL/lvXc8DESVNw8+ZNREdz4fvMLSrE7b0v336Y4/drdx6B9ZGpGSwaL9/xl3DyaoTZO1J+lRJeaho4cOc3TDx8FX03HcfQ3dTAFcw5Ga40MO+0tpOatvGHYdOHxI0/zqDruuNqtsJ3yS74zN8CtfGHYdOHjKxTnX6mugYKW1TSdlQsXgoFPimKAkWKorqNLcZPCMGtW7cQFRWlNJARHfxyNxpXvsn5Nt969Lc0O1Qa6pCV3+BpVFymtf06YTLh5UtEREUj7LvfMeXodRUHhu4+r+LA7BPhWMIdFV9z8xdzZappqpN2VKSh0jTw9ocfo8qXNhgbNB7h4eGIidGmxTOigZRsnkTF4cYPj3M8Dpy/FYnAeTdSjQOMDVw+k6VfNIXmvjEOREZF48gPf2Dqsevov+Ukhuw6j+CwK5h94jq4q+b80zegbf4SjpF/ufmLeTPVanddtasmNVBaxYF3PvoElWvWwqgxY3HlypVEDfwVm4SEl6qMIqdj/6WvH2LfmbtwCTybaptzVZfRobknQ61zzUlTTcPce8s5te14tzX7Ydd9HMo4dIRtl+loOekg2i8+jv47L2Bw2GVwe/KAsMsYuO8C2kxZg6ot+6usdbPR2+VCRb0xc/gxX5rqR4+f4OCx4+jsH4gqzb3QoOcwdJm1AqM2HsCMA2cx/fBlTDx8BePCrmL4/msYvPcq+qe5+cvmZOUfuqFKtqNihso/tO2JP61UE0UqVseHFlXxoUVlfFiuKopY1kB9Vw/MC12GH376CXFx5tlVLoe1pTJQKU21nqE+dY2GOnMDh7/6v2hCHj95giMnTqLrwKGo5NwGjv+/vfMAiyLL2v/z3/+3M7szn7q6ujM6plHXiAlzwASSDICkwYiKilnMGVQURRAUTJhzGNM45qyYwBkQs45p1DGggOTs+z3nVlfTYIuC0MnTz9N0qO6qW7/7cuut26fOGT4Vgxavh9euY1h8/DIWn4nAwtNXMfdUlChfPekIVdO7qqimV1hTPQ6t+spVNeU81faiVH2Ndtaimh6VqK7YsBXKG1GZ6hwNfN+gGdp3d0TQyjV4+OixXmogNPKNmCnL2+dOMzRrqEkfpIGEhESEXryEoROnwbi7C0yHTYZb4Dp4/nxUqQG/XBqIUtHA+xUVewYrKirmKf5iPs77AxUV1Rd/ydEAzVYboxydZNdphLJ1jNG6ix0CV6zC/YcPkZlZdHm7P/Y/U1zLKW7ee/0d5P3Vil7TrytnIqKRkVn0hlqpgcREnL8chtHTZ6GpbS90HDIRAxatxcydRxB4TBoH/E5HYt7pKFGmXFTVPCyNA0N3X4b7jvNwU1ZUfN9Uq6uoaKKoqilVVFRT/IVK1Tdri4qNpHFAPhaUo/GgrjHadLGDX/AK3PnjPjKzMlE8o2Tx9DjNBf35Ihk2k9831ZKhvqP1ixLV7bkumGrJMEcIw9x32S7YzdmKkbsuY9LxCEw6GSEZajLV8v1kJDwOXILl+CDYzDnIplpdx2rhPYMz1Wnp6Zg9fyEq1DNGmZoNUc6oGco3MUHVNuao1bkHmjgOgMWomei9IARDQnZj+OajGLHjLIbvuoChP1+EiKfefAYDNhyD6+qD6L2cBtIdcA7YJKpo2c5ZKZWmneIH8/HzlAdTk0FUnngkmvccIrbRyLYPjLo6o66FA2qZ2aKmaTf8t0M31OhkixqmtqjWoRuqtLVExead8H2j1ihX1xhla9VHp249EHX92if95KcFvRRok/SzrqrBkmeoL10vPkNNDSQz4rdkGSrUIQ00kDRgbILKrTqjZuceMLbvD4tRM9B7vkIDm47kaGAXmalQDNpyVqEBiqsnDZCh2gx73zWQNBAMq8nqNDAKzXu5v6eB2p3tUNO0ex4NdBcaqNTcFOUbtREaKFe7IVqaWSMy6hqys4umumCBOu0zPqzOVNOB1HfzHxqboZabn0HhH8HLUbVhc/y7Fo0DTVFe1oCZHajSHZ1s9/ZZiSErd2GYqgZoHNhxDoO2KMYB8WvFPsU4sFkaB7xXouuMYFhP8YeFPA4MmwqTwTQOyBoYiEa2fWHUzQX1LB1AGqhlJmvARhoHOpEGrFBRqYEmKPNfIzQ0MUVkVBSy3+mXBmT+8qM6U02Gmi5Opmsp0ovJUNP209PTsWr9JlQzboF/0zhQlzTQFlVam6GmmR0a9XCF2fBp6OWzQtLAxiMYsf2MdCzYRRpQGQfWHETvFaQBGge2wMF3LWznShqwmuIP8wnzYDpmFjqQBuhY4KrQgNNANLLLrQFxLOhEx4LcGqjUopNiHGiCMjXqCw1EXI3Sq2MBmerHL5LeM9XOM69g9ro7eKnFtHmyJtU96oypVhhmYaJPRojZa6WJls00PYrZ6t8xck8Yei45A3vf3OXKOaWeul7WzHsGZ6qjY2PRwb4X/lXdSBxM/1O/OSo0bScG0h/bdUWNzj1Qu2tPGNn1RyPnoWjWzwOth0xDhzHzYD41EF3nrILdgk1wWLQdTkt+hvOSXXAO+lk8d1qyAw4B22HvtwW2PmvRbXYIrGYEwXLKIphNmI+OY7zRfqQX2g6djjbuU9Fq0CS0GDBBbMO41wg0dBiCejb9UMvSCdU72ShMdUelqf5X9XooV7MO1m3aIoyhPs1QqJOrqqkmQz3QJxKXbxSvoaZ2xCclo4NDb5SpYSRM9X/qSydWlVqZQdKAHep07Yl6tioaGDwVHcbMVWrA1ncT7P23w3Ex9f0uOOXRQA+/LbDxWYduc0JgPTMIFqSB8ZIG2o2YBZOhM9RoYCQaOio0YOWEGqY2qNrWEjmahVTbAAAgAElEQVSmuglIA9/VNMLseQvw7l3xzOCp66uieC+vqaYD6cItf4BCESgcQ5M30oBlr/4oW6uBYhxoJsYBpQbM7MQ4IGnAHU37eqDV4KloP3ouzKcEouvsENgt2IgeH9CAY+B2kAZs58saCFaMAwvQccxctB85CybD8mpgLMQ44DgERjQOCA3YoqoJaYBOrunEqglK16gnYq19FvrrnQby9nFeUy2fWJ+7+hrpGcV7whDzNh4uw8agdA0jYapzjwPWqGFmh1rWPVHPhsYB0sAYtBo8Be1He6OzUgOb0MN/m2Ic2C3GAXE8WLwTpAFxLFCnAY8cDbQVx4LJaDFwApq5esC4lzwOuKKWlbM4uaITq0ot5JNr0oARSlSqDt+AxcjK0p9fLNSZahoH5uiwoSbN6pqpVmukFaZ64rEIjPnlCvquPAeKt1bNTy0/Z1OddyTS3GuDM9VvktIwOGQfGvRwReXmncTMxA9N26FSK1P82E4aSOtYS6a6oZM7jHuPQosB49HafRpMRs5Gh7ELYDopEBYzVsB6zlrY+G6BXcDPcAjaB+flB/HTyqNwCTmKn1YchuPSA+gRuBu2/tvRzWcDrLxCYD41GJ0m+KODhw/aDvdEy0GThalu3HM4Gti7oU63Pqhp4YBqHbqjShsL/NCsI75v2Ar/rt0QJSvXQMnyFbFu3XqD+OlXNtU0MyUZ6tgiS5eV379IYlomBq/+BQ0dBgoN/NCkLco3aScOWlVN5IOpC4xsXSFpYLSkgSFTC6QBZ4UG7Bfvhp1SA6tgPm0pTCcuEhowUWigaT8PNHYhDQxC3e6kAUdxYlVVaKATvmsgaaBUlRr41w+VMGv2bEXssWbNaH5cP7ZM1VTTgdRvq3YMNbUzPi0DozccRmPnwajSwhSkATEOtJDHAVvUslZowHGIGAea9x+P1rIGPObDdFIALGYsF+NA9wU0DuyEw5J9cFpG48ARuIQcUY4D9ov3wM5/hxgHrGdJGpDHAZMRXopxYCwa9xyhogEHVBMn1/I40BplazdEqSo1UeK7CvD+jOwfH+srTS1XNdXyiXXo1TfFbqhp/2KT0zF1xwkY9xyGKq3MUMH4A+OAjas42TXuNQqkgVaqGpgYAIvpKhpY9DPsg2QNSMcCGgeclh2AUgPzN0LWQM444CUMe9N+qhromzMOtCUN0DjQGmXrNEKpH2viqzLlMH+Br17PVNM4QOE/ujpDLf8f+K/cASOzgcoL/6iiYWPH6ejhc1RpWn/yP40+/sdQs8OAXJk3Krd0gdnYDcrPkbEtSJlyOaY6PyNNyyadiMDYg79hwLoLcFpEZjr37LRsqB0XnkL/oFBcuvtK3j1+1CABgzLVZD9ikjMw5dgNDN4Zht7LdsFqwjw0cxqI+tZOqGfphLpWThCm2qafOLjRQY5mqVq4TUTroTPQbtQcdBwnGWtzGkxnrUbXeRth47tVmGe7RTsh7v47YOO3Hd3mb0YX73WwnLkSZlOWoNMEP7T3mAeT4V5oPWSKGKSlmYnBMLKhmQkXMTNRVcxMdEL5hq3EjOo331XCVyVK4Z/flhBXfBtCPKVsqt3mRyLspmYMNf3viJR6QgPh6L18N6wm+qCZ8yA0sHZGPUtHhQZcxK8GZHJVNdBGRQNmZKqmLYeVUgNbYOu3Xep/hQ7odXehgfWw9CQNBAkN0EkVmSk6QNOBWmjAiWap+6OWVU/UMLUDGXzpJ19JA9+Wr4yvSpbCP0uWhlcBUuppcLzId1OyqaYDqf+2PxCflKGRkyh1jZJS6t3CoJ3hInyny2RftOg5GPWFBuRxwAVGNn1Rv8cgccIjxoGBNA5Mh8nIOeggxoEAmE9bBisveRzYAjtVDfjvhO1CeRxYrxgHSAPSiTVpgIy6pIFRil8q+osxiGZKczTQGmVq1sf/ViANlMZX35SAt7e3Ri7qVMevqN5Tmupp4aBx4HzUm2IN+VBtt5RS7w4G7QhDnxX70GXKQrToOQT1u1I4Tm4N0IQHnfRKGpiA1u6kgdkf1IC6cYCOBV3n0jhAkyuSBtrTODBSGgea9Z8gTt4aOg2BkS1poBdqmPVQaECapSYNfFu+Cr4qWQaUUs/Hx0cvTbXt5DDIhloqPa7aM7r3/K+Xb7B8436YuYxHrQ6uwjTLptrJ77QwqYv2X8fpyIdoZDlYo6aazPT4w79jyNZLItRDNs/qHnstOoPxG8Jw7OozJKUaxrVZuqeW/FtkkKZ68tEbGLbvKkaLlHpRmH7od0zddRrj1+zCEP/VcJ7hiy7jZsF05Ay0HzYNbd2niZCNtsNmoh0NpB7zJGM9cRHMJgfBYtoyWM5cAatZIcJkW89eDSuvVbD0XAGL6csURsofHcb6iO+TMWs5cCKa9RuDxi7DxAx13a69UdPcHtXaWaNSsw74vn5z/LtGPZT64Ud8U648vipVGv/zzbcGZ6oHLbiKsFuaM9Qkd7n4C+UoHn0gCpOOXsP0w6SBMyoaWIgu42YrNDA9Hw34Cw2YT18GC9KAVwio/+lky9prtTDSuTTgMV8cjEkDdKImaUD6laJu1z4qGugoNFC2Rj2UrCRroIykgRKl9NZUU1aXgO33tWqoSQM5xV+iMJpS6h2JwjQxDpzFhHW74b5oDX6auRBdx8+Bmeo44D4d748DpIElOeMAaWDWKqEB1XGg82SFmR47X5yci3FAoQFj8UvVIKhqoHLzjvi+QXOQBkpVqqYYB8rg79+WwN//8a3BmOq56++AxoEL1zRnqEkDcvGXoXQsoFzlR66KcWDaHtLAHgwNWAsXTz90HS+PA3mPBbNUjgX+6Dx5iTjBEseCj40DSg3MREu3SVLYh6yBbn1R09wB1dt3QeUWsgaMFBqogK9KSRr4f/9ff001XUsxd8NdRMelQcORX/k7nnyWpmdk4trth/D0X4dW3YejhYsnevqfgOf2CJy78RLJaZl49uI1jK00ZKoVcdMjdoahz/JzcPA7BfsPzE5TuMeIVZewLfQBXsSmiHA7/fmNM59O0cNFhmuqyVAp81RHYeHZGyKV3urLt7A09Bp8T0ZixqHLGLcnFMO3ncaAdUfRZ+V+/BS0E46LtsJ+4QbY+qxBd+/VIm62i9cKWM9cCsvpwbCYugSdJy2C2URfdBrrgw6jZqPdsOloM2gSWrp6oGmv4WjkMBD1u/dGPStH1OzUHdXadEaVZiao0KA5vq/dEOWq10EZOU91uaIr/qJLGhyz+DquaNhQ0/7LpnqoQgNkqLxOShpYevEm1ly+iWUXVDVwHsO3ncrRQPDPQgM9fGUNrJI0MGsFrD2XwmpGMCxlDUzI0YCJQgMt+kkaaOw4EPVtekkaMM3RQMWGzaWUerIGKlTGN2XpxIpMdQn8U09N9eUbsQjZ9whxidqboZb1n2Oq5eIvUfA8QUWgbmDpxVtSak2FBjwPhWH8XtLAaQxYT+PAr3AJ/hkOAduU44DNXMU4oNRAkFoN5B0HJA0oxgHT7qjeVhoHfmjQHN8px4HqKFmhkjDVX5cua3Cmevmeh7h47Q3SMjR7jYBsqt1lU31YUfxFaOAm6FggjwMzD4Vhwp5QjNh+GgPXH0OfEEkDjh/SwMycccB8cgA6T/SF6Tg6FsyRYumVx4IRaOzohga2fVDPyklcrFy9rTmqNGsHygQlUmuKcYA0UDnXiZW+muq/olNEyMdrPTLU8rhBWYOSU1Jx+kIEZgTtwp6LDxCXlC5+caNlNKOtCVNNcdOj94WLdHqUxzq/UA+3pecRfPAm7v71tlhSU8ps+PHTCBisqRaG6kCe4i8XyFDdwvKLNyDnqfY8eQOTj17H2MPXMOrX3zFs9yUMFhUVj8N19QH0Xi6l1HPy3wj7+athO2c5ukyjioq+MB83B51GeaL90Klo6zYOLfuOgJRGidKp9UQ9S3vUNrVBDRNL/NiqIyo3kYo+SKa6dk7xFwM11TcevtXKz/+qpvq9ioqkgbCbanIUX8fYQzkaoCwwAzceV1TVzEmraL9AoYHpsgZU06nlpNSjDCNCA1aUUs8G/1XRAOUo/r5OA0WucsXB1ABM9aPnydJFidnanyN531Tnrqi4JuwWchV/UYwD48Q4EIFhey6J7A9um0gDlAVIMQ4s2oTcGlgI9Sn1hogsMw2694KRVQ9JA+0sFWkV5TzVDfHvanXwr8rVUbJ8JXFiZWimOi0jS6RYo/z5mr590FSfuY7gC5KpJg3Ieaq9lHmqr2H0gQhxLJDy1UsaoCxAlK/eSdaA93J0mb4EVpPVa0BOqdfQpheMrBXjAGmgFaVXlTQgHQtUir+IXy31d6aa+phCfvRphlqdLqneRWpahjCpZKblW3Gbagr18DjwG9w2XIBzwOkPmmkK/ei7+By8d0Yi/I9opKTrz8WsMktDffyyTLWYpbytYqqvqy3+IplquUx5jqFyKK4y5QZqqlXGIo3+/+Rrqi/exNqwWwpTfQPaKf7S1CBNNVXE03SWjw8J62OmWir8IZ1c+5y5Ds8TdHJNucqjFLnK8+ap3oOCF3/JyVFcy8wGNRSmurJqRUUDN9VkSLQ1DnzMVFMBoBxTfV2l+IusAcpXr5qneg96Bv8M50UbRVpFu7kr0HVGEKyFqZ6DTqO90EFMsFBaRWmChVI3SidWkqkWGlCa6qaKXy0Ny1TT/6S2+vxD40FRvV9cpnrknnARN+2+7RJ6Bp0RoR7qYqbpPRf/M5iwMRxHI54iNjFdZ8bcomKs7+v5Mk21qKR2A/POqDfVIlf1JtlU785V/MXOe4WYnSh48Zcvb6ZaW/8cBTXVUlXN6yqGqrDFX8ajVd/haO4yGPlXVDRMU62t/la33YKZ6hufYKr3FsJUqy/+8iWZanV9o6n3Cm+qVYtAqZrq94u/sKnWVG/qxnai38Shk7OHMktI1VY9oS77h9PCE7D2WJnrgkb6bB3LkbCdeyhXphAK7xi4/oKIm5ZCPdSnyXPyP42hKy9i27kHeBaTVGzF03SDtP62gk21FsqUfynhH9r6t2BTrS3yurNdNtW60xfaagmbam2RN9ztUlGps5evYtBEP9Tp2F+YZlVTTXmjBy49j/Wn7mDh2l8+yVSLGekP5JumZZQir9+Sc1h6+BZuPY1DWmaWwf4SYAjKYVPNptoQdJxrH9hU58LxRb5gU/1FdnuunWZTnQsHvygiAu+ys/HqdSx27D+Nrq5TUK11L3QetwG9A84i4NcbuPY4BgnJ6di05/inm+qF6menKUXerB0RuHjnFZJSM7VyjVIRYftiVsOmmk21wYmdTbXBdWmBd4hNdYGRGdwX2FQbXJfq1A5lZmXh4ZPnCFi1E1NDjiH05kskpGSIWeSMzKzPMtXOfqcxZu1lHLjyJ2IS05ClAxd/6xR8HW4Mm2o21Tosz8I1jU114bgZ0rfYVBtSbxZuX9hUF44bf+vTCVBekJTUNMQlpuSaRS6sqaZQD0qRt+7kPTx9k4T0rCzk5B759HbxJ7VHgE21LpjqsoaZp1pbstZXU/21yFP9Lb4pqZ/FX7TV3+q2q++m2lAqKqrrG029p++mWh8rKmqqb3V9O4Ux1X0Cz8JndxRuPo0TKfJ0JZOSrrPWtfYZlKkmuLEpGZh+/CaG7ZeqaE06chVeoujDdVH8Zc3l28p0ajqT/YNMdeky+Ps3JVCqdBls3LgRhlCmXFtiT0rPxBSFBkSeatLASSr8cR1U/CVvSj1dyv5BGvhXmXKiPLFqflRtsdTX7SZmZGLWqTsYvj8Kow5EYuLhq6L4ywIq/HGBNCClU6N89VJaxY+l1Cvm7B/K4i9SjmI6sfLz89P7MuXa1M/btEz4nvsDQ3+JlCoqiuIv17BAkaf6wyn1tJv9QxwLqKrm199g0aJFelWmXJv9rUvbptCQ7ftPo3qb3rniqtVl/3DxP43x68Nw6e4rvE3WfuEsXeKoj20xOFOdkZWFy0/eIPDSPUw7dgNTjl3FrJPXhaFaJuep1pGUemWqUNGHyijxXQVUql4D5lZWWLkqBE+ePOWB9DP+m2hAC3/6Gksu38fUY9cx+SiZakkDumqqSQPlq/yITmZmWL5iOR49egQqQMC3whHIzM5GxF8xCA67j+nHpXFAOrnWVVMtjQP/qVwZHc3MEBwczBooXNcrv0XHgmsvY7HsykOlBjxP6q6ppoqKJb4rj3KVqqJdp44ICAzA48eP+Vig7FH9eUITIq9j32J24EY0sXZXGmtVU+3kdxruKy5i7+VHiI5PRUYWj/f608MfbqnBmWoSc1pmJuLTUvEqMQXhT99gc8RDrAq7i1Vhd7AunGaqb2LROTlPtTRDNfaQ6uzEObgp81RT8ZedUsJ/NcVfTKmiovsUtB04TiT8z5Wj2NIetaiioqii1RFVmrYFVdOrUM8YlYwao17z1rC2c0Rg8DLcunMHr1+/RkpKshhEeZbyw6L92BJily40kIZXiWn47VkMtl4lDdxByOXbWBd+R6mBXLOUh1WLPoQKDfRfcxB9VqhWUlsDW0WucuvJVFXTWyr64D4FJm4fzlP9XxNrUU2vchNJAz8YGaNSPWPUbtoSVrYO8FschLt37yH69WskJ7MGPtbHH1v+DtI4kJCWhuikVET8FavQgDQOrA+/gxWKcSBHA5SrnMqaR4Iqq4rCH5tOQNbApxR/ERroN0LkKje2V5+nWjkOGBnjh3qNUbOJpIEF/oG4efs2XkVH8zjwsQ7+hOXSOJCFhLR0oYHI57HYHiUfC6RxgDQgV1T0VFZUVD0WqMlTHbBJKv7iXfDiL/9tZ40fW5mKY0HFhs3FsUDSQAtYdO+Bub5+4ljw6tUrHgc+oY91+SMUvpGUnIJrtx9g8CR/1OnQH3WtpDzV/YNCsezIbfwZnQiqOmqoxXJ0uX+Kq20GZ6pVQZFQqcpbSkYmEtPSEJOUinvRMTj38C/suv4nQsLvwe/cbXievImJR65hzIFIDN8TBqk0rVz8RTLVVJr2/YqK3jAdNTPHVPelg+kgUfijkU1vNOjqiPoWPdDYugda2zjBsqcr3MZOwoIlQTh+5hyePX+O2Lg4pKTQRQ58lqrad0X2/N07UWqWNJCQlqrUwPlHz7H7hkIDobk1MGJPGNypkpqqoQr+WZQnJg28b6o90SGPqaYy5Xk10MrGCRYu/TDQYwLmBS7BkROnhAbi4uKQnJzCP/UXWae/v6JMGgcyM5GYnobYpDQxDlx4/EJoYFX4PfiH3obXKXkcuIrhey7DfYeqBmgc+BnOKoaqy/QgWE1aCItxVKreU1TTy22qFRro5oj6ljQO2IM0YP5T3xwNHJc0QOMAa+D9fiuqd+hiL9JAqkIDcSnp+ON1DC4+eo69N59i1ZV78D+v0MDRa/A4GIkRe8OEBgZtPoH+aw+Jk2tRUVHWwIcqKtIES1+poiKNA41tFccCS3txLJA00Aeuo8fD2z8AB44ew5NnzxAbK2mAY2mLqtd1Yz3kQd7GJ+HwmXD0GR8Iz81huP5nLJLTOEWebvRQ0bbCoE21uGyWRlNRKvedKOeZlZ0N+lmQEqgnZ2TiTVIaHsQk4uqLOJx7/Ar7bv6Jzb/fx4rQ6/A7ehk+v56H166TmLr1EMat24vhK7ZhyJJNcPNbi4ELVsJ1XhD6zQmAm3cghs1bjFHzAzHJfxl812zC6p27cfhsKMKuXsODx38iNu4tEpKSkJKaKmKmxeBJzl+0sWg7ltemIKDo//w0EJOchoexiYh8EYezjyQNbIm4jxXnSQNhORrYJmtgu6QB/7UYsGAl+s0LhqvQQACGzQsUGpjovwzzV29EyI7dOHQmFGGRUQoNxCEhkTSQotSAmKWgdvKt+AiI/zMqmS2NA3QSqzoOxKak41FcIq6+pHEgWowDWyIeYOX5G/A7Gg6fA+fhtfsUpm49jPHr92LEyu1wD9qEQf7yOBAMV+9AuHkHYKhSA0uxYPVGrNqxSxoHIqNw/9GfiIlTpwHFOFB8BL7wNRNf6f4hDcQkKzTwIg6hCg1sjXiAkPM34E8a+PWC0MC0bYcxfgMdC0gDm+FGGvANgSuNA96LMXCOpIGRPoGY4L8UPqtoHNiFA6fO4nLEVdx/9BhvYuMQn5goJlQyMjOR/S5bmq3kccAgdUqao4sXE5JSkJiSztUQDbKXpZ0ybFP9CR1HYxiZW4rDlQ6ymUjLyBSz28npGUhOT0dSWjoSU1LwNikJsYmJiE1IQGxCPGLic+5vExORkJyMxOQUJKWkiDQ7qelpoAEzi9LiCOf0CQ3ij2icwMc0kJSWoaKBZPUaeJuAONJAkqyB1BwNZLAGNN6pBdygrIGMbMU4kCGNA6k0FohxQNZAKuKTFBpIzH8cSE5R0QCPAwXsEc1/XNYAzWirHgtIA+JYoBwHUvE2jwZyjgeKcUB5LMjRQLpiHOCZaM33LW+RCWiKwBdvqgsPmoZgmgUv/Br4m/pKQO501oC+9uDnt7tgGpA//fnb5TXoDoE8vZrnpe60k1vCBJiApgiwqdYUad4OE2ACTIAJMAEmwASYgMESYFNtsF3LO8YEmAATYAJMgAkwASagKQJsqjVFmrfDBJgAE2ACTIAJMAEmYLAE2FQbbNfyjjEBJsAEmAATYAJMgAloigCbak2R5u0wASbABJgAE2ACTIAJGCwBNtUG27W8Y0yACTABJsAEmAATYAKaIsCmWlOkeTtMgAkwASbABJhAkRGgQsT3H2Zi5eYkjPeOx5Cpb+HOd51jMHTaW0zzjceG3cl4+hdVkiwyCejcithU61yXcIOYABNgAkyACTCB/Agkp7yDp38C2rnEYIpfArbsScbB4yk4wHedYiD65FgK1u9MgseceDSwicH8ZVRN1DCdNZvq/P5reRkTYAJMgAkwASagUwRSUt/BZlAc3KfF400MlXjnyjs61UEfaExW1jvcuJOBHu6xGOedgPQMw+s3NtUf6Hx+mwkwASbABJgAE9A9AiGbE2HWJxapqe/YUOte93y0RU+fZ6Ftz1gcPpUKQzsfYlP90e7nDzABJsAEmAATYAK6QCA17R1a9IgRsdS6PkOdmZmJ5OTk9+6pqanIyspCWloaMjIyBNbs7GzQnW60X7S8oPtHn6d1y+vUhf5S14bMrHc4fi4VDiPjQP1pSDc21YbUm7wvTIAJMAEmwAQMmMCjJ5moYf5G5/eQTPGJEydgamoKa2trdOnSRdy7du2KSZMm4fHjx5g4cSIOHToEMt+RkZF49OiR2K+EhATxXTLkBbmlp6djzJgx+PXXXwvyNa189v7jTJi7xuL5S8OKrWZTrRU58UaZABNgAkyACTCBghIIvZwK+2FxBf2axj9PRnnbtm0oVaoUNm7ciJ07dyrvZ8+exbNnzxAQEIBz584Jg03G++jRo2K2OjAwED169MDLly+V7ZZnr+XZbOUCQHyHtkez1FZWVli7dq3qYp18Hh2TDRv3OJC5NqQbm2pD6k3eFybABJgAE2ACBkzg9PlUuIzSH1NdrVo1xMfHv9cjFPpBM9NknC9evIgWLVogJCREGOzJkyejTZs2uHLlCmj2mWasaWZ7//794r3Y2FhlqAgtu3r1Kvbt24e7d+/CzMxML0z1m7hs2A2Lw71HbKrfEwe/wQSYABNgAkyACTCB4iagb6a6YsWKwjxHv4oG3WNiYkQs9dOnT0GhIEFBQRg+fDhKlCiBmjVrYsaMGahVq5aY4W7Xrh1evHiBWbNmoUaNGmjdujVq164tQkpoPTQ7vWDBAtA2WrVqJb5PM+P6MFPNprq4/1N4/UyACTABJsAEmAATyIeAvpnqf/zjH2jatClMTEzEvVu3brhz5w5kU00GmGasKWzj2LFjYs8XLlwowj+io6PFsqpVq+LgwYOIi4sT36PZ7/nz54uZ6a+//hpnzpwRFzXevn1bmHM21fkIqJgXcfhHMQPm1TMBJsAEmAATYAJFQ0DfTHWZMmVE2AYZZrqHhoYqzTHNVJMBfvjwISwtLcVyip329fWFnZ0dXr16hd27d6Ny5criAkRPT094eXmhefPmcHV1FestXbq0yBRCdCmmmmKz2VQXjdYKsxY21YWhxt9hAkyACTABJsAENE5A30z1h2KqVWeq8zPVmzdvFuEdc+fOxZIlS8Td29tbXPy4d+9efPfddyIMhDqC4rRpJpxNtcZlqdwgm2olCn7CBJgAE2ACTIAJ6DIBQzTVFP5hYWGBXbt2idlmf39/mJubixlsugixbt26OH78OBITE/H69Wt4eHhg/fr1+OOPP0Ts9fnz55GSkiLCSn788Uc21VoUMJtqLcLnTTMBJsAEmAATYAKfTkCfTDWl0aMLCynvdN7bX3/9JeKmN2zYAIqddnZ2FnHVp06dEqn36tevj9GjR4vUex4eY8WFiDRb7ebmhsaNGyM8PFwUeZk5c6ZYRnHY9vb2qFChAmidun7jCxV1vYe4fUyACTABJsAEmIBBE9AXU03FX27duoU1a9aIsIy8nUJGm2amaSaasniEhYWBZqgvXLiAN2/eiPCO4OBgUPo8ukBx69atoLAPyvbx+++/KysuJiUliZnp2bNni8+QkY+Kisq7OZ17zaZa57qEG8QEmAATYAJMgAl8SQT0xVR/bp+oK1GelZX9wdLlZOL16camWp96i9vKBJgAE2ACTIAJGByBL8VUG1zH5dkhNtV5gPBLJsAEmAATYAJMgAlokgCbak3SLr5tsakuPra8ZibABJgAE2ACTIAJfJQAm+qPItKLD7Cp1otu4kYyASbABJgAE2AChkqgKEw1xR9ToRTVO70nxzFnZ2cjPT1dWVSlMCxpXfI65PXRY1HdMjIyRPvVxVLntyy/7VP76Lvq1qn6Pdo3yon9OfvDplqVKD9nAkyACTABJsAEmICGCRSFqY6IiMC4ceMwatQocR8zZgxmzJiB06dPi3zPlG2Dqhc+efJUabQLupuUlYNS4FHqvOfPn4vMHbTeorqFhISIlHuXLl3KtUoyxQEBAU/bhCYAAAv8SURBVJg4caLYbq6FH3lBmUYoNR+VUc/vRvsxZcoUPHnypNB82FTnR5iXMQEmwASYABNgAkygmAkUhanes2cPqEjKhAkT4OPjI8yvg4MDGjRogDNnzogCK5Qj+vHjx4U2jTExMWjUqBGuX78uck2TiS9KU03t/dvf/oahQ4fmauOLFy9QqVIllCtXTqT0K0h3kEnu0KEDzp49m2udedfx7NkzUKVISgdY2NlqNtV5qfJrJsAEmAATYAJMgAlokEBRmGrKD03FVWhGlioRJicn488//4SJiQkGDx6Mly9f4rfffhMmmGacqXIhmVWqXHjx4kXQe/KNQiVoOVU8JANN66Ib5Zo2MjLCtWvXxHootzRVRIyNjQNVUKQZ7HPnzomc0vS+fKPnN2/exMmTJ4WppxASdTc7OzthnqlMOe0D3cjg0glDmTJlhKm+ceOGeJ/CXGhfqY30KLeRFtL2aOae2kJtbN26tZixpxAPmvUmLtSWyMhIsd/0PpVYr1y5sniPTbVArPzDFRWVKPgJE2ACTIAJMAEmoMsEispU0yzyw4cPlbsaHx8Pa2truLi44N69e2LGlgwo3Y2NjUUZcapY2LRpUzE7TEaVDGVgYKBY3rVrV2FIaeaYTLdsqqkQC62vU6dOwpgfPHhQzGBTGfJ27dqJ59OnTxdVF6kEOYWkkLm3srJC8+bNRWEX2TQrGwuATLWrqytKlCghzDIty0jPQK9evURoCxlrMtVkmmn9VIWxY8eOovri+PHjhdGnfZ4zZ47YjqmpqWh/1apVhakmQ02Fa9q0aQNLS0u0aNECAwcOFN+jGW021aq9kfOcTXUOC37GBJgAE2ACTIAJ6DCBojLVZB7Xrl2L/fv3Y8eOHaL8N4VN/Prrr2KGuF69emIWmUxx2bJlQeXAabaaZmxr1qwpPkfPy5cvj7179+LVq1e4ffu2MMm//PJLLlNNM880M06zvjRLTt+hyoc0Wx0UFCQMPH2G2tO+fXtRXZFmy6ldZP7v37//XpgFmWqKBaeZdXd3d9Fj9J3q1avj2LFjKFWqlGg/lT2nUBd6pPaHhoaKMBdaN4V5kFmmGWxaRrHY9FmKLadt0gnEtm3bxDLat1q1amHFihXiZIRNtfp/EjbV6rnwu0yACTABJsAEmICOESgqU01hExSeQTOxNDPcpUsXkBmmWWaKGZZNNYVvkFGlWW2amaZwD5qVnjp1qjCc33zzjbiokS7wmz9/vvgezQRHR0eL9ZMpz2uqyazSNiiUgowtGWkqT07muGXLlmL2mNZHF0uWLFlShF/kzcghm2oy0KVLlxYhIFu2bAHNOFN5dDLVFNZB66H35JAViuvu378/PDw8hKF3cnISs8/UFjLStH0y1WT+K1SogEmTJol1yPtGM+EU7sKmWv0/Bptq9Vz4XSbABJgAE2ACTEDHCBSVqa5bt66YdaYwDbqokEIhKOSBzGVeU00ztPQZ+UZGlMI0Vq5cCTLnFF4xa9YseHl5iSwimzZtEhc7kmnPa6p3794NMzMzYbppfWSm6eJAiml2dnYWJn/atGlifWSqJ0+eLOKy88Yuy6aawjvoosRDhw7B0dERZKwpZIVMNcVIe3t7w8bGRqT3o+0lJCRg2LBhGDBgAGbPno3evXsrDTeFdVBICplqyi5CJxM0G077RndqF4WE0AWcbKplNeR+ZFOdmwe/YgJMgAkwASbABHSUQFGZ6rwx1aq7m9dUV6xYUZhfmi0mUyrHOlPoCJlXMrEU9/z27VthsCm0guKjP2SqKZ6altNNNtWUGo8MNMVtk2ml9dHs+IgRI8T6P2SqqU0jR46Era0tvv/+e3FCIJtqCk/ZvHmzCD2h0BC6UciJhYWFMNRk8CnW+9HDR2JZWFiYCA0hU33ixAkR5nLlyhVxYSOddJAJpxAVahebaoHsvT9sqt9Dwm8wASbABJgAE2ACukjg7KVUOAz/vHzPFNpQEFNN4RUUE02ZNehiPQqRoNzTlEWDsmVQ+AgtI3NLBpwyh6heqKga/kFGVp2pplllMrV0USRtg2KuaQab1k/xzjSDrnqTZ6rJVFOcNKXXozR79Fo21ZSNhL5LJwG0zX379ilnw+/evStivCnshb5HJwI0S02z3mSqyUTTNmi/aRmFgVDM+dGjR4tkpvp1TDZs3OPwx+NM1d3S++dsqvW+C3kHmAATYAJMgAl8GQTuPchEPes3n7Wz4eHhYlZYni3OuzKacaa4aAqHkGOqKdSDQiYouwfFFJN5JaNL5pnCNPr16ycudqR1Z2ZmipAKKpBC5ptmh+k5FVehmV+KT5ZjnCkzCIWN0Ow0hZ9Qxo6xY8eK9cltoG3lvS1evBgUZkIz2JSJZMiQIeJiRGoTbZMMPj3Sd+mR1kVtpMcHDx6I92kZxVFTfDXt24YNG0SIB5lyWi/FX1Meb8oy4ubmJkJUqJIihcJQ+Mvn5PF++CQLFq6xePr8/X3Lu6/69JpNtT71FreVCTABJsAEmMAXTCAp+R1+NH+DV9GFL/lNZpIMbN6QChkrvS8vp9leyvYhh2SoK89NuaQpXIOWyQaYzC2tg16rrk/etjzzTK/JhMttode0PjLK9Ci/L7dNfpTXLb9W3TZ9R/W7tC3VddI25Jv8WdoerZPaIi+X94GW0V31fdX1y+v61Eeq1n7hShosB8QhManw/fip29Pk59hUa5I2b4sJMAEmwASYABMoNIF32e8wKyAB/ca/BXJHRBR6nfl9kUI3KBOInK0jv8/ysk8jEBOXjV6j47BmWxLyRLV82gp0+FNsqnW4c7hpTIAJMAEmwASYQG4CiUnv0KFPLBYsTUBSSvHOdFJBFTmMIncr+FVBCZCBfvU6C3ODEjFo8lukpGjgrKigjfzMz7Op/kyA/HUmwASYABNgAkxAcwQoLCE+IRvDZ8SjU99YBKxJxLGzqQiPTCv6e0QawuleHOv+QtYZFpGGwydTsGBpIlo6xGDSvHgkJBqeoab/ADbVmhsHeEtMgAkwASbABJhAERGg2Nzfr6VjzpIEOA6LhWmfGL7rIAOzvjHoMzYOfisTcfc+5QLHe9lMikgSWl8Nm2qtdwE3gAkwASbABJgAE2ACTEDfCbCp1vce5PYzASbABJgAE2ACTIAJaJ0Am2qtdwE3gAkwASbABJgAE2ACTEDfCbCp1vce5PYzASbABJgAE2ACTIAJaJ0Am2qtdwE3gAkwASbABJgAE2ACTEDfCbCp1vce5PYzASbABJgAE2ACTIAJaJ0Am2qtdwE3gAkwASbABJgAE2ACTEDfCbCp1vce5PYzASbABJgAE2ACTIAJaJ0Am2qtdwE3gAkwASbABJgAE2ACTEDfCbCp1vce5PYzASbABJgAE2ACTIAJaJ0Am2qtdwE3gAkwASbABJgAE2ACTEDfCbCp1vce5PYzASbABJgAE2ACTIAJaJ0Am2qtdwE3gAkwASbABJgAE2ACTEDfCbCp1vce5PYzASbABJgAE2ACTIAJaJ0Am2qtdwE3gAkwASbABJgAE2ACTEDfCbCp1vce5PYzASbABJgAE2ACTIAJaJ0Am2qtdwE3gAkwASbABJgAE2ACTEDfCbCp1vce5PYzASbABJgAE2ACTIAJaJ0Am2qtdwE3gAkwASbABJgAE2ACTEDfCbCp1vce5PYzASbABJgAE2ACTIAJaJ0Am2qtdwE3gAkwASbABJgAE2ACTEDfCbCp1vce5PYzASbABJgAE2ACTIAJaJ0Am2qtdwE3gAkwASbABJgAE2ACTEDfCbCp1vce5PYzASbABJgAE2ACTIAJaJ0Am2qtdwE3gAkwASbABJgAE2ACTEDfCbCp1vce5PYzASbABJgAE2ACTIAJaJ0Am2qtdwE3gAkwASbABJgAE2ACTEDfCbCp1vce5PYzASbABJgAE2ACTIAJaJ0Am2qtdwE3gAkwASbABJgAE2ACTEDfCbCp1vce5PYzASbABJgAE2ACTIAJaJ0Am2qtdwE3gAkwASbABJgAE2ACTEDfCbCp1vce5PYzASbABJgAE2ACTIAJaJ0Am2qtdwE3gAkwASbABJgAE2ACTEDfCbCp1vce5PYzASbABJgAE2ACTIAJaJ3A/wHKoC4zF0g+9AAAAABJRU5ErkJggg==)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. That is, the data are passed through the fitted pipeline in order. Each stage’s transform() method updates the dataset and passes it to the next stage. With the help of Pipelines, we can ensure that training and test data go through identical feature processing steps.\n",
    "\n",
    "Now let’s see how this can be done in Spark NLP using Annotators and Transformers. Assume that we have the following steps that need to be applied one by one on a data frame.\n",
    "\n",
    "- Split text into sentences\n",
    "- Tokenize\n",
    "\n",
    "And here is how we code this pipeline up in Spark NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentences')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentences\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[documentAssembler, \n",
    "                               sentenceDetector,\n",
    "                               tokenizer])\n",
    "\n",
    "spark_df = spark.read.text('./sample-sentences-en.txt').toDF('text')\n",
    "\n",
    "pipelineModel = nlpPipeline.fit(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "|text                                                                         |\n",
      "+-----------------------------------------------------------------------------+\n",
      "|Peter is a very good person.                                                 |\n",
      "|My life in Russia is very interesting.                                       |\n",
      "|John and Peter are brothers. However they don't support each other that much.|\n",
      "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |\n",
      "|Europe is very culture rich. There are huge churches! and big houses!        |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.read.text('./sample-sentences-en.txt').toDF('text')\n",
    "\n",
    "spark_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipelineModel.transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|                                    text|                                document|                               sentences|                                   token|\n",
      "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|            Peter is a very good person.|[{document, 0, 27, Peter is a very go...|[{document, 0, 27, Peter is a very go...|[{token, 0, 4, Peter, {sentence -> 0}...|\n",
      "|  My life in Russia is very interesting.|[{document, 0, 37, My life in Russia ...|[{document, 0, 37, My life in Russia ...|[{token, 0, 1, My, {sentence -> 0}, [...|\n",
      "|John and Peter are brothers. However ...|[{document, 0, 76, John and Peter are...|[{document, 0, 27, John and Peter are...|[{token, 0, 3, John, {sentence -> 0},...|\n",
      "|Lucas Nogal Dunbercker is no longer h...|[{document, 0, 67, Lucas Nogal Dunber...|[{document, 0, 41, Lucas Nogal Dunber...|[{token, 0, 4, Lucas, {sentence -> 0}...|\n",
      "|Europe is very culture rich. There ar...|[{document, 0, 68, Europe is very cul...|[{document, 0, 27, Europe is very cul...|[{token, 0, 5, Europe, {sentence -> 0...|\n",
      "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(truncate=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- document: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- sentences: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- token: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(result=['Peter is a very good person.']),\n",
       " Row(result=['My life in Russia is very interesting.']),\n",
       " Row(result=['John and Peter are brothers.', \"However they don't support each other that much.\"])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select('sentences.result').take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(token=[Row(annotatorType='token', begin=0, end=3, result='John', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=5, end=7, result='and', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=9, end=13, result='Peter', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=15, end=17, result='are', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=19, end=26, result='brothers', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=27, end=27, result='.', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=29, end=35, result='However', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=37, end=40, result='they', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=42, end=46, result=\"don't\", metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=48, end=54, result='support', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=56, end=59, result='each', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=61, end=65, result='other', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=67, end=70, result='that', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=72, end=75, result='much', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=76, end=76, result='.', metadata={'sentence': '1'}, embeddings=[])])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select('token').take(3)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary\n",
    "\n",
    "`setCleanupPatterns(patterns)`: Regular expressions list for normalization, defaults [^A-Za-z]\n",
    "\n",
    "`setLowercase(value)`: lowercase tokens, default false\n",
    "\n",
    "`setSlangDictionary(path)`: txt file with delimited words to be transformed into something else\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "    \n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\")\\\n",
    "    .setLowercase(True)\\\n",
    "    .setCleanupPatterns([\"[^\\w\\d\\s]\"]) # remove punctuations (keep alphanumeric chars)\n",
    "    # if we don't set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z])\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[documentAssembler, \n",
    "                               tokenizer,\n",
    "                               normalizer])\n",
    "\n",
    "result = nlpPipeline.fit(spark_df).transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocumentAssembler_d4926309c3ee,\n",
       " REGEX_TOKENIZER_a4789e51e51c,\n",
       " NORMALIZER_81bbdb7b0bdb]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlpPipeline.fit(spark_df).stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|                                    text|                                document|                                   token|                              normalized|\n",
      "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|            Peter is a very good person.|[{document, 0, 27, Peter is a very go...|[{token, 0, 4, Peter, {sentence -> 0}...|[{token, 0, 4, peter, {sentence -> 0}...|\n",
      "|  My life in Russia is very interesting.|[{document, 0, 37, My life in Russia ...|[{token, 0, 1, My, {sentence -> 0}, [...|[{token, 0, 1, my, {sentence -> 0}, [...|\n",
      "|John and Peter are brothers. However ...|[{document, 0, 76, John and Peter are...|[{token, 0, 3, John, {sentence -> 0},...|[{token, 0, 3, john, {sentence -> 0},...|\n",
      "|Lucas Nogal Dunbercker is no longer h...|[{document, 0, 67, Lucas Nogal Dunber...|[{token, 0, 4, Lucas, {sentence -> 0}...|[{token, 0, 4, lucas, {sentence -> 0}...|\n",
      "|Europe is very culture rich. There ar...|[{document, 0, 68, Europe is very cul...|[{token, 0, 5, Europe, {sentence -> 0...|[{token, 0, 5, europe, {sentence -> 0...|\n",
      "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(truncate=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(token=[Row(annotatorType='token', begin=0, end=4, result='Peter', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=6, end=7, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=9, end=9, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=11, end=14, result='very', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=16, end=19, result='good', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=21, end=26, result='person', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=27, end=27, result='.', metadata={'sentence': '0'}, embeddings=[])]),\n",
       " Row(token=[Row(annotatorType='token', begin=0, end=1, result='My', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=3, end=6, result='life', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=8, end=9, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=11, end=16, result='Russia', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=18, end=19, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=21, end=24, result='very', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=26, end=36, result='interesting', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=37, end=37, result='.', metadata={'sentence': '0'}, embeddings=[])])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select('token').take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(result=['peter', 'is', 'a', 'very', 'good', 'person']),\n",
       " Row(result=['my', 'life', 'in', 'russia', 'is', 'very', 'interesting'])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select('normalized.result').take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(normalized=[Row(annotatorType='token', begin=0, end=4, result='peter', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=6, end=7, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=9, end=9, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=11, end=14, result='very', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=16, end=19, result='good', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=21, end=26, result='person', metadata={'sentence': '0'}, embeddings=[])]),\n",
       " Row(normalized=[Row(annotatorType='token', begin=0, end=1, result='my', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=3, end=6, result='life', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=8, end=9, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=11, end=16, result='russia', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=18, end=19, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=21, end=24, result='very', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=26, end=36, result='interesting', metadata={'sentence': '0'}, embeddings=[])])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select('normalized').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DocumentNormalizer is an annotator that can be used after the DocumentAssembler to narmalize documents once that they have been processed and indexed .\n",
    "It takes in input annotated documents of type Array AnnotatorType.DOCUMENT and gives as output annotated document of type AnnotatorType.DOCUMENT .\n",
    "\n",
    "Parameters are:  \n",
    "\n",
    "| Parametre   | Description |\n",
    "| - | - |\n",
    "|**inputCol**      |input column name string which targets a column of type Array(AnnotatorType.DOCUMENT).|\n",
    "|**outputCol**      |output column name string which targets a column of type AnnotatorType.DOCUMENT.|\n",
    "|**action** |action string to perform applying regex patterns, i.e. (clean | extract). Default is \"clean\".|\n",
    "|**cleanupPatterns**  |normalization regex patterns which match will be removed from document. Default is \"<[^>]*>\" (e.g., it removes all HTML tags).|\n",
    "|**replacement** |replacement string to apply when regexes match. Default is \" \".|\n",
    "|**lowercase** |whether to convert strings to lowercase. Default is False.|\n",
    "|**removalPolicy** |removalPolicy to remove patterns from text with a given policy. Valid policy values are: \"all\", \"pretty_all\", \"first\", \"pretty_first\". Defaults is \"pretty_all\". |\n",
    "|**encoding** |file encoding to apply on normalized documents. Supported encodings are: UTF_8, UTF_16, US_ASCII, ISO-8859-1, UTF-16BE, UTF-16LE. Default is \"UTF-8\".|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "  <div id=\"theworldsgreatest\" class='my-right my-hide-small my-wide toptext' style=\"font-family:'Segoe UI',Arial,sans-serif\">\n",
    "    THE WORLD'S LARGEST WEB DEVELOPER SITE\n",
    "    <h1 style=\"font-size:300%;\">THE WORLD'S LARGEST WEB DEVELOPER SITE</h1>\n",
    "    <p style=\"font-size:160%;\">Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum..</p>\n",
    "  </div>\n",
    "\n",
    "</div>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|\\n  <div id=\"theworldsgreatest\" class='my-right my-hide-small my-wide toptext' style=\"font-family:'Segoe UI',Arial,sans-serif\">\\n    THE WORLD'S LARGEST WEB DEVELOPER SITE\\n    <h1 style=\"font-size:300%;\">THE WORLD'S LARGEST WEB DEVELOPER SITE</h1>\\n    <p style=\"font-size:160%;\">Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum..</p>\\n  </div>\\n\\n</div>|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "\n",
    "spark_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='DocumentNormalizer_99e443833d62', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
       " Param(parent='DocumentNormalizer_99e443833d62', name='action', doc='action to perform applying regex patterns on text'): 'clean',\n",
       " Param(parent='DocumentNormalizer_99e443833d62', name='patterns', doc='normalization regex patterns which match will be removed from document. Defaults is <[^>]*>'): ['<[^>]*>'],\n",
       " Param(parent='DocumentNormalizer_99e443833d62', name='replacement', doc='replacement string to apply when regexes match'): ' ',\n",
       " Param(parent='DocumentNormalizer_99e443833d62', name='lowercase', doc='whether to convert strings to lowercase'): False,\n",
       " Param(parent='DocumentNormalizer_99e443833d62', name='policy', doc='policy to remove pattern from text'): 'pretty_all',\n",
       " Param(parent='DocumentNormalizer_99e443833d62', name='encoding', doc='file encoding to apply on normalized documents'): 'UTF-8',\n",
       " Param(parent='DocumentNormalizer_99e443833d62', name='inputCols', doc='previous annotations columns, if renamed'): ['document'],\n",
       " Param(parent='DocumentNormalizer_99e443833d62', name='outputCol', doc='output annotation column. can be left default.'): 'normalizedDocument'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentNormalizer = DocumentNormalizer() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"normalizedDocument\")\n",
    "\n",
    "documentNormalizer.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "#default\n",
    "cleanUpPatterns = [\"<[^>]*>\"]\n",
    "\n",
    "documentNormalizer = DocumentNormalizer() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"normalizedDocument\") \\\n",
    "    .setAction(\"clean\") \\\n",
    "    .setPatterns(cleanUpPatterns) \\\n",
    "    .setReplacement(\" \") \\\n",
    "    .setPolicy(\"pretty_all\") \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "docPatternRemoverPipeline = Pipeline() \\\n",
    "    .setStages([documentAssembler,\n",
    "                documentNormalizer])\n",
    "    \n",
    "pipelineModel = docPatternRemoverPipeline.fit(spark_df).transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|result                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[ the world's largest web developer site the world's largest web developer site lorem ipsum is simply dummy text of the printing and typesetting industry. lorem ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. it has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. it was popularised in the 1960s with the release of letraset sheets containing lorem ipsum passages, and more recently with desktop publishing software like aldus pagemaker including versions of lorem ipsum..]|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineModel.select('normalizedDocument.result').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " for more examples : https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/annotation/english/document-normalizer/document_normalizer_notebook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords Cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This annotator excludes from a sequence of strings (e.g. the output of a Tokenizer, Normalizer, Lemmatizer, and Stemmer) and drops all the stop words from the input sequences.\n",
    "\n",
    "**Functions**:\n",
    "\n",
    "| Parametre   | Description |\n",
    "| - | - |\n",
    "|**setStopWords**      |The words to be filtered out. Array[String]|\n",
    "|**setCaseSensitive**      |Whether to do a case sensitive comparison over the stop words.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "    .setInputCols(\"token\")\\\n",
    "    .setOutputCol(\"cleanTokens\")\\\n",
    "    .setCaseSensitive(False)\\\n",
    "    #.setStopWords([\"no\", \"without\"]) (e.g. read a list of words from a txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now',\n",
       " \"i'll\",\n",
       " \"you'll\",\n",
       " \"he'll\",\n",
       " \"she'll\",\n",
       " \"we'll\",\n",
       " \"they'll\",\n",
       " \"i'd\",\n",
       " \"you'd\",\n",
       " \"he'd\",\n",
       " \"she'd\",\n",
       " \"we'd\",\n",
       " \"they'd\",\n",
       " \"i'm\",\n",
       " \"you're\",\n",
       " \"he's\",\n",
       " \"she's\",\n",
       " \"it's\",\n",
       " \"we're\",\n",
       " \"they're\",\n",
       " \"i've\",\n",
       " \"we've\",\n",
       " \"you've\",\n",
       " \"they've\",\n",
       " \"isn't\",\n",
       " \"aren't\",\n",
       " \"wasn't\",\n",
       " \"weren't\",\n",
       " \"haven't\",\n",
       " \"hasn't\",\n",
       " \"hadn't\",\n",
       " \"don't\",\n",
       " \"doesn't\",\n",
       " \"didn't\",\n",
       " \"won't\",\n",
       " \"wouldn't\",\n",
       " \"shan't\",\n",
       " \"shouldn't\",\n",
       " \"mustn't\",\n",
       " \"can't\",\n",
       " \"couldn't\",\n",
       " 'cannot',\n",
       " 'could',\n",
       " \"here's\",\n",
       " \"how's\",\n",
       " \"let's\",\n",
       " 'ought',\n",
       " \"that's\",\n",
       " \"there's\",\n",
       " \"what's\",\n",
       " \"when's\",\n",
       " \"where's\",\n",
       " \"who's\",\n",
       " \"why's\",\n",
       " 'would']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_cleaner.getStopWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|                                    text|                                document|                                   token|                             cleanTokens|\n",
      "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|            Peter is a very good person.|[{document, 0, 27, Peter is a very go...|[{token, 0, 4, Peter, {sentence -> 0}...|[{token, 0, 4, Peter, {sentence -> 0}...|\n",
      "|  My life in Russia is very interesting.|[{document, 0, 37, My life in Russia ...|[{token, 0, 1, My, {sentence -> 0}, [...|[{token, 3, 6, life, {sentence -> 0},...|\n",
      "|John and Peter are brothers. However ...|[{document, 0, 76, John and Peter are...|[{token, 0, 3, John, {sentence -> 0},...|[{token, 0, 3, John, {sentence -> 0},...|\n",
      "|Lucas Nogal Dunbercker is no longer h...|[{document, 0, 67, Lucas Nogal Dunber...|[{token, 0, 4, Lucas, {sentence -> 0}...|[{token, 0, 4, Lucas, {sentence -> 0}...|\n",
      "|Europe is very culture rich. There ar...|[{document, 0, 68, Europe is very cul...|[{token, 0, 5, Europe, {sentence -> 0...|[{token, 0, 5, Europe, {sentence -> 0...|\n",
      "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[documentAssembler, \n",
    "                               tokenizer,\n",
    "                               stopwords_cleaner])\n",
    " \n",
    "spark_df = spark.read.text('./sample-sentences-en.txt').toDF('text')\n",
    "\n",
    "result = nlpPipeline.fit(spark_df).transform(spark_df)\n",
    "\n",
    "result.show(truncate=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(result=['Peter', 'good', 'person', '.'])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select('cleanTokens.result').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|           sentences|               token|          normalized|         cleanTokens|          clean_text|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Peter is a very g...|[{document, 0, 27...|[{document, 0, 27...|[{token, 0, 4, Pe...|[{token, 0, 4, Pe...|[{token, 0, 4, Pe...|[{document, 0, 16...|\n",
      "|My life in Russia...|[{document, 0, 37...|[{document, 0, 37...|[{token, 0, 1, My...|[{token, 0, 1, My...|[{token, 3, 6, li...|[{document, 0, 22...|\n",
      "|John and Peter ar...|[{document, 0, 76...|[{document, 0, 27...|[{token, 0, 3, Jo...|[{token, 0, 3, Jo...|[{token, 0, 3, Jo...|[{document, 0, 18...|\n",
      "|Lucas Nogal Dunbe...|[{document, 0, 67...|[{document, 0, 41...|[{token, 0, 4, Lu...|[{token, 0, 4, Lu...|[{token, 0, 4, Lu...|[{document, 0, 34...|\n",
      "|Europe is very cu...|[{document, 0, 68...|[{document, 0, 27...|[{token, 0, 5, Eu...|[{token, 0, 5, Eu...|[{token, 0, 5, Eu...|[{document, 0, 18...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentences')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentences\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\")\\\n",
    "    .setLowercase(False)\\\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "    .setInputCols(\"normalized\")\\\n",
    "    .setOutputCol(\"cleanTokens\")\\\n",
    "    .setCaseSensitive(False)\\\n",
    "\n",
    "tokenassembler = TokenAssembler()\\\n",
    "    .setInputCols([\"sentences\", \"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"clean_text\")\n",
    "\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[documentAssembler,\n",
    "                               sentenceDetector,\n",
    "                               tokenizer,\n",
    "                               normalizer,\n",
    "                               stopwords_cleaner,\n",
    "                               tokenassembler])\n",
    "\n",
    "result = nlpPipeline.fit(spark_df).transform(spark_df)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(clean_text=[Row(annotatorType='document', begin=0, end=16, result='Peter good person', metadata={'sentence': '0'}, embeddings=[])])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we use TokenAssembler().setPreservePosition(True), the original borders will be preserved (dropped & unwanted chars will be replaced by spaces)\n",
    "\n",
    "result.select('clean_text').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+-----------------------------------+\n",
      "|text                                                                         |clean_text                         |\n",
      "+-----------------------------------------------------------------------------+-----------------------------------+\n",
      "|Peter is a very good person.                                                 |Peter good person                  |\n",
      "|My life in Russia is very interesting.                                       |life Russia interesting            |\n",
      "|John and Peter are brothers. However they don't support each other that much.|John Peter brothers                |\n",
      "|John and Peter are brothers. However they don't support each other that much.|However dont support much          |\n",
      "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |Lucas Nogal Dunbercker longer happy|\n",
      "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |good car though                    |\n",
      "|Europe is very culture rich. There are huge churches! and big houses!        |Europe culture rich                |\n",
      "|Europe is very culture rich. There are huge churches! and big houses!        |huge churches                      |\n",
      "|Europe is very culture rich. There are huge churches! and big houses!        |big houses                         |\n",
      "+-----------------------------------------------------------------------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select('text', F.explode(result.clean_text.result).alias('clean_text')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-37e2fb9a-d0d2-4d12-a401-92aca68caa1b\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Peter is a very good person.</td>\n",
       "      <td>Peter good person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My life in Russia is very interesting.</td>\n",
       "      <td>life Russia interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John and Peter are brothers. However they don'...</td>\n",
       "      <td>John Peter brothers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>John and Peter are brothers. However they don'...</td>\n",
       "      <td>However dont support much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lucas Nogal Dunbercker is no longer happy. He ...</td>\n",
       "      <td>Lucas Nogal Dunbercker longer happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lucas Nogal Dunbercker is no longer happy. He ...</td>\n",
       "      <td>good car though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Europe is very culture rich. There are huge ch...</td>\n",
       "      <td>Europe culture rich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Europe is very culture rich. There are huge ch...</td>\n",
       "      <td>huge churches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Europe is very culture rich. There are huge ch...</td>\n",
       "      <td>big houses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37e2fb9a-d0d2-4d12-a401-92aca68caa1b')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-37e2fb9a-d0d2-4d12-a401-92aca68caa1b button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-37e2fb9a-d0d2-4d12-a401-92aca68caa1b');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                       Peter is a very good person.   \n",
       "1             My life in Russia is very interesting.   \n",
       "2  John and Peter are brothers. However they don'...   \n",
       "3  John and Peter are brothers. However they don'...   \n",
       "4  Lucas Nogal Dunbercker is no longer happy. He ...   \n",
       "5  Lucas Nogal Dunbercker is no longer happy. He ...   \n",
       "6  Europe is very culture rich. There are huge ch...   \n",
       "7  Europe is very culture rich. There are huge ch...   \n",
       "8  Europe is very culture rich. There are huge ch...   \n",
       "\n",
       "                            clean_text  \n",
       "0                    Peter good person  \n",
       "1              life Russia interesting  \n",
       "2                  John Peter brothers  \n",
       "3            However dont support much  \n",
       "4  Lucas Nogal Dunbercker longer happy  \n",
       "5                      good car though  \n",
       "6                  Europe culture rich  \n",
       "7                        huge churches  \n",
       "8                           big houses  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select('text', F.explode(result.clean_text.result).alias('clean_text')).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------------------------------+--------+\n",
      "|begin|end|result                             |sentence|\n",
      "+-----+---+-----------------------------------+--------+\n",
      "|0    |16 |Peter good person                  |0       |\n",
      "|0    |22 |life Russia interesting            |0       |\n",
      "|0    |18 |John Peter brothers                |0       |\n",
      "|29   |53 |However dont support much          |1       |\n",
      "|0    |34 |Lucas Nogal Dunbercker longer happy|0       |\n",
      "|43   |57 |good car though                    |1       |\n",
      "|0    |18 |Europe culture rich                |0       |\n",
      "|29   |41 |huge churches                      |1       |\n",
      "|54   |63 |big houses                         |2       |\n",
      "+-----+---+-----------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "result.withColumn(\n",
    "    \"tmp\", \n",
    "    F.explode(\"clean_text\")) \\\n",
    "    .select(\"tmp.*\").select(\"begin\",\"end\",\"result\",\"metadata.sentence\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+-----------------------------------------------------+\n",
      "|text                                                                         |result                                               |\n",
      "+-----------------------------------------------------------------------------+-----------------------------------------------------+\n",
      "|Peter is a very good person.                                                 |[Peter good person]                                  |\n",
      "|My life in Russia is very interesting.                                       |[life Russia interesting]                            |\n",
      "|John and Peter are brothers. However they don't support each other that much.|[John Peter brothers However dont support much]      |\n",
      "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[Lucas Nogal Dunbercker longer happy good car though]|\n",
      "|Europe is very culture rich. There are huge churches! and big houses!        |[Europe culture rich huge churches big houses]       |\n",
      "+-----------------------------------------------------------------------------+-----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if we hadn't used Sentence Detector, this would be what we got. (tokenizer gets document instead of sentences column)\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "tokenassembler = TokenAssembler()\\\n",
    "    .setInputCols([\"document\", \"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"clean_text\")\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[documentAssembler,\n",
    "                               tokenizer,\n",
    "                               normalizer,\n",
    "                               stopwords_cleaner,\n",
    "                               tokenassembler])\n",
    "\n",
    "result = nlpPipeline.fit(spark_df).transform(spark_df)\n",
    "result.select('text', 'clean_text.result').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------------------------------------------------+--------+\n",
      "|begin|end|result                                             |sentence|\n",
      "+-----+---+---------------------------------------------------+--------+\n",
      "|0    |16 |Peter good person                                  |0       |\n",
      "|0    |22 |life Russia interesting                            |0       |\n",
      "|0    |44 |John Peter brothers However dont support much      |0       |\n",
      "|0    |50 |Lucas Nogal Dunbercker longer happy good car though|0       |\n",
      "|0    |43 |Europe culture rich huge churches big houses       |0       |\n",
      "+-----+---+---------------------------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.withColumn(\n",
    "    \"tmp\", \n",
    "    F.explode(\"clean_text\")) \\\n",
    "    .select(\"tmp.*\").select(\"begin\",\"end\",\"result\",\"metadata.sentence\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:**\n",
    "\n",
    "If you have some other steps & annotators in your pipeline that will need to use the tokens from cleaned text (assembled tokens), you will need to tokenize the processed text again as the original text is probably changed completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns hard-stems out of words with the objective of retrieving the meaningful part of the word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"stem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|                                    text|                                document|                                   token|                                    stem|\n",
      "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|            Peter is a very good person.|[{document, 0, 27, Peter is a very go...|[{token, 0, 4, Peter, {sentence -> 0}...|[{token, 0, 4, peter, {sentence -> 0}...|\n",
      "|  My life in Russia is very interesting.|[{document, 0, 37, My life in Russia ...|[{token, 0, 1, My, {sentence -> 0}, [...|[{token, 0, 1, my, {sentence -> 0}, [...|\n",
      "|John and Peter are brothers. However ...|[{document, 0, 76, John and Peter are...|[{token, 0, 3, John, {sentence -> 0},...|[{token, 0, 3, john, {sentence -> 0},...|\n",
      "|Lucas Nogal Dunbercker is no longer h...|[{document, 0, 67, Lucas Nogal Dunber...|[{token, 0, 4, Lucas, {sentence -> 0}...|[{token, 0, 4, luca, {sentence -> 0},...|\n",
      "|Europe is very culture rich. There ar...|[{document, 0, 68, Europe is very cul...|[{token, 0, 5, Europe, {sentence -> 0...|[{token, 0, 5, europ, {sentence -> 0}...|\n",
      "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[documentAssembler, \n",
    "                               tokenizer,\n",
    "                               stemmer])\n",
    "\n",
    "result = nlpPipeline.fit(spark_df).transform(spark_df)\n",
    "\n",
    "result.show(truncate=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------+\n",
      "|result                                                                                     |\n",
      "+-------------------------------------------------------------------------------------------+\n",
      "|[peter, i, a, veri, good, person, .]                                                       |\n",
      "|[my, life, in, russia, i, veri, interest, .]                                               |\n",
      "|[john, and, peter, ar, brother, ., howev, thei, don't, support, each, other, that, much, .]|\n",
      "|[luca, nogal, dunberck, i, no, longer, happi, ., he, ha, a, good, car, though, .]          |\n",
      "|[europ, i, veri, cultur, rich, ., there, ar, huge, church, !, and, big, hous, !]           |\n",
      "+-------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select('stem.result').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using PySpark 3.1.2 or below, You should use this format.\n",
    "``` \n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "result_df = result.select(F.explode(F.arrays_zip(\"token.result\", \"stem.result\")).alias(\"cols\")) \\\n",
    "                  .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "                          F.expr(\"cols['1']\").alias(\"stem\")).toPandas()\n",
    "\n",
    "result_df.head(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-9be894b8-aa85-4d04-b664-eea8f89bfdb3\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Peter</td>\n",
       "      <td>peter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>very</td>\n",
       "      <td>veri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>person</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>My</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>life</td>\n",
       "      <td>life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9be894b8-aa85-4d04-b664-eea8f89bfdb3')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-9be894b8-aa85-4d04-b664-eea8f89bfdb3 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-9be894b8-aa85-4d04-b664-eea8f89bfdb3');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "    token    stem\n",
       "0   Peter   peter\n",
       "1      is       i\n",
       "2       a       a\n",
       "3    very    veri\n",
       "4    good    good\n",
       "5  person  person\n",
       "6       .       .\n",
       "7      My      my\n",
       "8    life    life\n",
       "9      in      in"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "result_df = result.select(F.explode(F.arrays_zip(result.token.result, \n",
    "                                                 result.stem.result)).alias(\"cols\")) \\\n",
    "                  .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "                          F.expr(\"cols['1']\").alias(\"stem\")).toPandas()\n",
    "\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieves lemmas out of words with the objective of returning a base dictionary word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = Lemmatizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"lemma\") \\\n",
    "    .setDictionary(\"./AntBNC_lemmas_ver_001.txt\", value_delimiter =\"\\t\", key_delimiter = \"->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='Lemmatizer_d115c22dc86c', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
       " Param(parent='Lemmatizer_d115c22dc86c', name='formCol', doc='Column that correspends to CoNLLU(formCol=) output'): 'form',\n",
       " Param(parent='Lemmatizer_d115c22dc86c', name='lemmaCol', doc='Column that correspends to CoNLLU(lemmaCol=) output'): 'lemma',\n",
       " Param(parent='Lemmatizer_d115c22dc86c', name='inputCols', doc='previous annotations columns, if renamed'): ['token'],\n",
       " Param(parent='Lemmatizer_d115c22dc86c', name='outputCol', doc='output annotation column. can be left default.'): 'lemma',\n",
       " Param(parent='Lemmatizer_d115c22dc86c', name='dictionary', doc=\"lemmatizer external dictionary. needs 'keyDelimiter' and 'valueDelimiter' in options for parsing target text\"): JavaObject id=o3141}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|               token|                stem|               lemma|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Peter is a very g...|[{document, 0, 27...|[{token, 0, 4, Pe...|[{token, 0, 4, pe...|[{token, 0, 4, Pe...|\n",
      "|My life in Russia...|[{document, 0, 37...|[{token, 0, 1, My...|[{token, 0, 1, my...|[{token, 0, 1, My...|\n",
      "|John and Peter ar...|[{document, 0, 76...|[{token, 0, 3, Jo...|[{token, 0, 3, jo...|[{token, 0, 3, Jo...|\n",
      "|Lucas Nogal Dunbe...|[{document, 0, 67...|[{token, 0, 4, Lu...|[{token, 0, 4, lu...|[{token, 0, 4, Lu...|\n",
      "|Europe is very cu...|[{document, 0, 68...|[{token, 0, 5, Eu...|[{token, 0, 5, eu...|[{token, 0, 5, Eu...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[documentAssembler, \n",
    "                               tokenizer,\n",
    "                               stemmer,\n",
    "                               lemmatizer])\n",
    "\n",
    "result = nlpPipeline.fit(spark_df).transform(spark_df)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------+\n",
      "|result                                                                                       |\n",
      "+---------------------------------------------------------------------------------------------+\n",
      "|[Peter, be, a, very, good, person, .]                                                        |\n",
      "|[My, life, in, Russia, be, very, interest, .]                                                |\n",
      "|[John, and, Peter, be, brother, ., However, they, don't, support, each, other, that, much, .]|\n",
      "|[Lucas, Nogal, Dunbercker, be, no, long, happy, ., He, have, a, good, car, though, .]        |\n",
      "|[Europe, be, very, culture, rich, ., There, be, huge, church, !, and, big, house, !]         |\n",
      "+---------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select('lemma.result').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-cc160e8b-7ab5-49fa-bf19-24586b1abdbf\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>stem</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Peter</td>\n",
       "      <td>peter</td>\n",
       "      <td>Peter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>i</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>very</td>\n",
       "      <td>veri</td>\n",
       "      <td>very</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>person</td>\n",
       "      <td>person</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>My</td>\n",
       "      <td>my</td>\n",
       "      <td>My</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>life</td>\n",
       "      <td>life</td>\n",
       "      <td>life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc160e8b-7ab5-49fa-bf19-24586b1abdbf')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-cc160e8b-7ab5-49fa-bf19-24586b1abdbf button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-cc160e8b-7ab5-49fa-bf19-24586b1abdbf');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "    token    stem   lemma\n",
       "0   Peter   peter   Peter\n",
       "1      is       i      be\n",
       "2       a       a       a\n",
       "3    very    veri    very\n",
       "4    good    good    good\n",
       "5  person  person  person\n",
       "6       .       .       .\n",
       "7      My      my      My\n",
       "8    life    life    life\n",
       "9      in      in      in"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result.select(F.explode(F.arrays_zip(result.token.result, \n",
    "                                                 result.stem.result, \n",
    "                                                 result.lemma.result)).alias(\"cols\")) \\\n",
    "                  .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "                          F.expr(\"cols['1']\").alias(\"stem\"),\n",
    "                          F.expr(\"cols['2']\").alias(\"lemma\")).toPandas()\n",
    "\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGram Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGramGenerator annotator takes as input a sequence of strings (e.g. the output of a `Tokenizer`, `Normalizer`, `Stemmer`, `Lemmatizer`, and `StopWordsCleaner`). \n",
    "\n",
    "The parameter n is used to determine the number of terms in each n-gram. The output will consist of a sequence of n-grams where each n-gram is represented by a space-delimited string of n consecutive words with annotatorType `CHUNK` same as the Chunker annotator.\n",
    "\n",
    "Functions:\n",
    "\n",
    "`setN:` number elements per n-gram (>=1)\n",
    "\n",
    "`setEnableCumulative:` whether to calculate just the actual n-grams or all n-grams from 1 through n\n",
    "\n",
    "`setDelimiter:` Glue character used to join the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                                                                                  result|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                    [Peter, is, a, very, good, person, ., Peter_is, is_a, a_very, very_good, good_person, person_., Peter_is_a, is_a_very, a_very_good, very_good_person, good_person_.]|\n",
      "|[My, life, in, Russia, is, very, interesting, ., My_life, life_in, in_Russia, Russia_is, is_very, very_interesting, interesting_., My_life_in, life_in_Russia, in_Russia_is, Russia_is_very, is_very_...|\n",
      "|[John, and, Peter, are, brothers, ., However, they, don't, support, each, other, that, much, ., John_and, and_Peter, Peter_are, are_brothers, brothers_., ._However, However_they, they_don't, don't_...|\n",
      "|[Lucas, Nogal, Dunbercker, is, no, longer, happy, ., He, has, a, good, car, though, ., Lucas_Nogal, Nogal_Dunbercker, Dunbercker_is, is_no, no_longer, longer_happy, happy_., ._He, He_has, has_a, a_...|\n",
      "|[Europe, is, very, culture, rich, ., There, are, huge, churches, !, and, big, houses, !, Europe_is, is_very, very_culture, culture_rich, rich_., ._There, There_are, are_huge, huge_churches, churche...|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngrams_cum = NGramGenerator() \\\n",
    "                .setInputCols([\"token\"]) \\\n",
    "                .setOutputCol(\"ngrams\") \\\n",
    "                .setN(3) \\\n",
    "                .setEnableCumulative(True)\\\n",
    "                .setDelimiter(\"_\") # Default is space\n",
    "    \n",
    "# .setN(3) means, take bigrams and trigrams.\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[documentAssembler, \n",
    "                               tokenizer,\n",
    "                               ngrams_cum])\n",
    "\n",
    "result = nlpPipeline.fit(spark_df).transform(spark_df)\n",
    "result.select('ngrams.result').show(truncate=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                                                                                  result|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                   [Peter_is_a, is_a_very, a_very_good, very_good_person, good_person_.]|\n",
      "|                                                                                                     [My_life_in, life_in_Russia, in_Russia_is, Russia_is_very, is_very_interesting, very_interesting_.]|\n",
      "|[John_and_Peter, and_Peter_are, Peter_are_brothers, are_brothers_., brothers_._However, ._However_they, However_they_don't, they_don't_support, don't_support_each, support_each_other, each_other_th...|\n",
      "|   [Lucas_Nogal_Dunbercker, Nogal_Dunbercker_is, Dunbercker_is_no, is_no_longer, no_longer_happy, longer_happy_., happy_._He, ._He_has, He_has_a, has_a_good, a_good_car, good_car_though, car_though_.]|\n",
      "|[Europe_is_very, is_very_culture, very_culture_rich, culture_rich_., rich_._There, ._There_are, There_are_huge, are_huge_churches, huge_churches_!, churches_!_and, !_and_big, and_big_houses, big_ho...|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngrams_nonCum = NGramGenerator() \\\n",
    "                  .setInputCols([\"token\"]) \\\n",
    "                  .setOutputCol(\"ngrams_v2\") \\\n",
    "                  .setN(3) \\\n",
    "                  .setEnableCumulative(False)\\\n",
    "                  .setDelimiter(\"_\") # Default is space\n",
    "    \n",
    "ngrams_nonCum.transform(result).select('ngrams_v2.result').show(truncate=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotator to match entire phrases (by token) provided in a file against a Document\n",
    "\n",
    "Functions:\n",
    "\n",
    "`setEntities(path, format, options)`: Provides a file with phrases to match. Default: Looks up path in configuration.\n",
    "\n",
    "`path`: a path to a file that contains the entities in the specified format.\n",
    "\n",
    "`readAs`: the format of the file, can be one of {ReadAs.LINE_BY_LINE, ReadAs.SPARK_DATASET}. Defaults to LINE_BY_LINE.\n",
    "\n",
    "`options`: a map of additional parameters. Defaults to {“format”: “text”}.\n",
    "\n",
    "`entityValue` : Value for the entity metadata field to indicate which chunk comes from which textMatcher when there are multiple textMatchers. \n",
    "\n",
    "`mergeOverlapping` : whether to merge overlapping matched chunks. Defaults false\n",
    "\n",
    "`caseSensitive` : whether to match regardless of case. Defaults true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='TextMatcher_06501f2ac688', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
       " Param(parent='TextMatcher_06501f2ac688', name='caseSensitive', doc='whether to match regardless of case. Defaults true'): True,\n",
       " Param(parent='TextMatcher_06501f2ac688', name='mergeOverlapping', doc='whether to merge overlapping matched chunks. Defaults false'): False,\n",
       " Param(parent='TextMatcher_06501f2ac688', name='inputCols', doc='previous annotations columns, if renamed'): ['document',\n",
       "  'token'],\n",
       " Param(parent='TextMatcher_06501f2ac688', name='outputCol', doc='output annotation column. can be left default.'): 'matched_entities'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_extractor = TextMatcher() \\\n",
    "    .setInputCols([\"document\",'token'])\\\n",
    "    .setOutputCol(\"matched_entities\")\\\n",
    "\n",
    "entity_extractor.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_train.csv\n",
    "\n",
    "news_df = spark.read \\\n",
    "            .option(\"header\", True) \\\n",
    "            .csv(\"news_category_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------------------------+\n",
      "|category|                                       description|\n",
      "+--------+--------------------------------------------------+\n",
      "|Business| Short sellers, Wall Street's dwindling band of...|\n",
      "|Business| Private investment firm Carlyle Group, which h...|\n",
      "|Business| Soaring crude prices plus worries about the ec...|\n",
      "|Business| Authorities have halted oil export flows from ...|\n",
      "|Business| Tearaway world oil prices, toppling records an...|\n",
      "+--------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the target entities to txt file \n",
    "\n",
    "entities = ['Wall Street', 'USD', 'stock', 'NYSE']\n",
    "with open ('financial_entities.txt', 'w') as f:\n",
    "    for i in entities:\n",
    "        f.write(i+'\\n')\n",
    "\n",
    "\n",
    "entities = ['soccer', 'world cup', 'Messi', 'FC Barcelona']\n",
    "with open ('sport_entities.txt', 'w') as f:\n",
    "    for i in entities:\n",
    "        f.write(i+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"description\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "financial_entity_extractor = TextMatcher() \\\n",
    "    .setInputCols([\"document\",'token'])\\\n",
    "    .setOutputCol(\"financial_entities\")\\\n",
    "    .setEntities(\"financial_entities.txt\")\\\n",
    "    .setCaseSensitive(False)\\\n",
    "    .setEntityValue('financial_entity')\n",
    "\n",
    "sport_entity_extractor = TextMatcher() \\\n",
    "    .setInputCols([\"document\",'token'])\\\n",
    "    .setOutputCol(\"sport_entities\")\\\n",
    "    .setEntities(\"sport_entities.txt\")\\\n",
    "    .setCaseSensitive(False)\\\n",
    "    .setEntityValue('sport_entity')\n",
    "\n",
    "nlpPipeline = Pipeline(\n",
    "    stages=[\n",
    "        documentAssembler, \n",
    "        tokenizer,\n",
    "        financial_entity_extractor,\n",
    "        sport_entity_extractor\n",
    "        ])\n",
    "\n",
    "result = nlpPipeline.fit(news_df).transform(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(result=[], result=[]),\n",
       " Row(result=[], result=[]),\n",
       " Row(result=['stock'], result=[])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select('financial_entities.result','sport_entities.result').take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+----------------------------------+-------------------+\n",
      "|                                                                  text|                 financial_matches|      sport_matches|\n",
      "+----------------------------------------------------------------------+----------------------------------+-------------------+\n",
      "|\"Company launched the biggest electronic auction of stock in Wall S...|              [stock, Wall Street]|                 []|\n",
      "|Google, Inc. significantly cut the expected share price for its ini...|                    [stock, stock]|                 []|\n",
      "|Google, Inc. significantly cut the expected share price this mornin...|                    [stock, stock]|                 []|\n",
      "| Shares of Air Canada  (AC.TO) fell by more than half on Wednesday,...|                    [Stock, stock]|                 []|\n",
      "|Stock prices are lower in moderate trading. The Dow Jones Industria...|                    [Stock, Stock]|                 []|\n",
      "|The bad news just keeps pouring in for mutual fund manager Janus Ca...|                      [NYSE, NYSE]|                 []|\n",
      "|  Shaun Wright Phillips scored in his international debut as Englan...|                                []|[soccer, World Cup]|\n",
      "|NEWCASTLE, ENGLAND - England deservedly beat Ukraine 3-0 today in t...|                                []|[soccer, World Cup]|\n",
      "|MONTREAL (Reuters) - Shares of Air Canada (AC.TO: Quote, Profile, R...|                    [Stock, stock]|                 []|\n",
      "|\"SAN JOSE, California - On the cusp of its voyage into public tradi...|[stock, Wall Street, stock, Stock]|                 []|\n",
      "|\"Shortly before noon today, Google Inc. stock began trading under t...|                    [stock, stock]|                 []|\n",
      "|roundup Plus: EA to take World Cup soccer to Xbox...IBM chalks up t...|                                []|[World Cup, soccer]|\n",
      "|The U.S. Securities and Exchange Commission yesterday approved Goog...|                    [stock, stock]|                 []|\n",
      "|After a bumpy ride toward becoming a publicly traded company, Googl...|                    [stock, stock]|                 []|\n",
      "|In the most highly anticipated Wall Street debut since the heady da...|              [Wall Street, stock]|                 []|\n",
      "|NEW YORK Despite voluble skepticism among investors, Google #39;s s...|                    [stock, stock]|                 []|\n",
      "|If only the rest of my investments worked out this way. One week ag...|                    [stock, stock]|                 []|\n",
      "| U.S. stocks to watch: GOOGLE INC. (GOOG.O) Google shares jumped 18...|                    [stock, stock]|                 []|\n",
      "|\" U.S. stocks to watch: GOOGLE INC.  &lt;A HREF=\"\"http://www.invest...|                    [stock, stock]|                 []|\n",
      "|roundup Plus: KDE updates Linux desktop...EA to take World Cup socc...|                                []|[World Cup, soccer]|\n",
      "+----------------------------------------------------------------------+----------------------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select('description','financial_entities.result','sport_entities.result')\\\n",
    "      .toDF('text','financial_matches','sport_matches').filter((F.size('financial_matches')>1) | (F.size('sport_matches')>1))\\\n",
    "      .show(truncate=70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-eef9f07c-c428-4aac-9509-a98c101c94fd\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clinical_entities</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stock</td>\n",
       "      <td>112</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stock</td>\n",
       "      <td>114</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stock</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stock</td>\n",
       "      <td>126</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stock</td>\n",
       "      <td>188</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stock</td>\n",
       "      <td>52</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wall Street</td>\n",
       "      <td>61</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stock</td>\n",
       "      <td>70</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stock</td>\n",
       "      <td>143</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stock</td>\n",
       "      <td>294</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eef9f07c-c428-4aac-9509-a98c101c94fd')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-eef9f07c-c428-4aac-9509-a98c101c94fd button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-eef9f07c-c428-4aac-9509-a98c101c94fd');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "  clinical_entities  begin  end\n",
       "0             stock    112  116\n",
       "1             stock    114  118\n",
       "2             stock     45   49\n",
       "3             stock    126  130\n",
       "4             stock    188  192\n",
       "5             stock     52   56\n",
       "6       Wall Street     61   71\n",
       "7             stock     70   74\n",
       "8             stock    143  147\n",
       "9             stock    294  298"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result.select(F.explode(F.arrays_zip(result.financial_entities.result, \n",
    "                                                 result.financial_entities.begin, \n",
    "                                                 result.financial_entities.end)).alias(\"cols\")) \\\n",
    "                  .select(F.expr(\"cols['0']\").alias(\"clinical_entities\"),\n",
    "                          F.expr(\"cols['1']\").alias(\"begin\"),\n",
    "                          F.expr(\"cols['2']\").alias(\"end\")).toPandas()\n",
    "\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegexMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|                                              text|\n",
      "+--------------------------------------------------+\n",
      "|The human KCNJ9 (Kir 3.3, GIRK3) is a member of...|\n",
      "|BACKGROUND: At present, it is one of the most i...|\n",
      "|OBJECTIVE: To investigate the relationship betw...|\n",
      "|Combined EEG/fMRI recording has been used to lo...|\n",
      "|Kohlschutter syndrome is a rare neurodegenerati...|\n",
      "|Statistical analysis of neuroimages is commonly...|\n",
      "|The synthetic DOX-LNA conjugate was characteriz...|\n",
      "|Our objective was to compare three different me...|\n",
      "|We conducted a phase II study to assess the eff...|\n",
      "|\"Monomeric sarcosine oxidase (MSOX) is a flavoe...|\n",
      "|We presented the tachinid fly Exorista japonica...|\n",
      "|The literature dealing with the water conductin...|\n",
      "|A novel approach to synthesize chitosan-O-isopr...|\n",
      "|An HPLC-ESI-MS-MS method has been developed for...|\n",
      "|The localizing and lateralizing values of eye a...|\n",
      "|OBJECTIVE: To evaluate the effectiveness and ac...|\n",
      "|For the construction of new combinatorial libra...|\n",
      "|We report the results of a screen for genetic a...|\n",
      "|Intraparenchymal pericatheter cyst is rarely re...|\n",
      "|It is known that patients with Klinefelter's sy...|\n",
      "+--------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -q\thttps://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/pubmed/pubmed-sample.csv\n",
    "\n",
    "pubMedDF = spark.read\\\n",
    "              .option(\"header\", \"true\")\\\n",
    "              .csv(\"./pubmed-sample.csv\")\\\n",
    "              .filter(\"AB IS NOT null\")\\\n",
    "              .withColumnRenamed(\"AB\", \"text\")\\\n",
    "              .drop(\"TI\")\n",
    "\n",
    "pubMedDF.show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = '''\n",
    "renal\\s\\w+, started with 'renal'\n",
    "cardiac\\s\\w+, started with 'cardiac'\n",
    "\\w*ly\\b, ending with 'ly'\n",
    "\\S*\\d+\\S*, match any word that contains numbers\n",
    "(\\d+).?(\\d*)\\s*(mg|ml|g), match medication metrics\n",
    "'''\n",
    "\n",
    "with open('regex_rules.txt', 'w') as f:\n",
    "    \n",
    "    f.write(rules)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RegexMatcher_5f6b3c0b4b06', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
       " Param(parent='RegexMatcher_5f6b3c0b4b06', name='strategy', doc='MATCH_FIRST|MATCH_ALL|MATCH_COMPLETE'): 'MATCH_ALL'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RegexMatcher().extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(result=['inwardly', 'family', 'spansapproximately', 'byapproximately', 'approximately', 'respectively', 'poly', 'KCNJ9', '3.3,', 'GIRK3)', 'KCNJ9', '1q21-23', '7.6', '2.2', '2.6', 'identified14', 'aVal366Ala', '8', 'KCNJ9', 'KCNJ9', '9 g']),\n",
       " Row(result=['previously', 'previously', 'intravenously', 'previously', '25', 'mg/m(2)', '1', '8', 'a3', '50', '20.0%', '(10', '50;', '95%', 'interval,10.0-33.7%).', '58.0%', '[10', '18', '50].', '(50%', '115.0', '17.3%', '52).', '25 mg']),\n",
       " Row(result=['renal failure', 'cardiac surgery', 'cardiac surgery', 'cardiac surgical', 'early', 'statistically', 'analy', '1995', '2005', '=9796).', '2.9', '11years).', '11.3%', '1105),', '7.2%', '30%', '0.0001),', '1.55,95%', '1.42-1.70,', '0.0001).'])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "regex_matcher = RegexMatcher()\\\n",
    "    .setInputCols('document')\\\n",
    "    .setStrategy(\"MATCH_ALL\")\\\n",
    "    .setOutputCol(\"regex_matches\")\\\n",
    "    .setExternalRules(path='./regex_rules.txt', delimiter=',')\n",
    "    \n",
    "\n",
    "nlpPipeline = Pipeline(\n",
    "    stages=[\n",
    "        documentAssembler, \n",
    "        regex_matcher\n",
    "        ])\n",
    "\n",
    "match_df = nlpPipeline.fit(pubMedDF).transform(pubMedDF)\n",
    "match_df.select('regex_matches.result').take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+----------------------------------------------------------------------+\n",
      "|                                                                  text|                                                               matches|\n",
      "+----------------------------------------------------------------------+----------------------------------------------------------------------+\n",
      "|The human KCNJ9 (Kir 3.3, GIRK3) is a member of the G-protein-activ...|[inwardly, family, spansapproximately, byapproximately, approximate...|\n",
      "|BACKGROUND: At present, it is one of the most important issues for ...|[previously, previously, intravenously, previously, 25, mg/m(2), 1,...|\n",
      "|OBJECTIVE: To investigate the relationship between preoperative atr...|[renal failure, cardiac surgery, cardiac surgery, cardiac surgical,...|\n",
      "|Combined EEG/fMRI recording has been used to localize the generator...|[normally, significantly, effectively, analy, only, considerably, 2...|\n",
      "|Statistical analysis of neuroimages is commonly approached with int...|[analy, commonly, overly, normally, thatsuccessfully, recently, ana...|\n",
      "|The synthetic DOX-LNA conjugate was characterized by proton nuclear...|                                             [wasanaly, substantially]|\n",
      "|Our objective was to compare three different methods of blood press...|[daily, only, Conversely, Hourly, hourly, Hourly, hourly, hourly, h...|\n",
      "|We conducted a phase II study to assess the efficacy and tolerabili...|[analy, respectively, generally, 5-fluorouracil, (5-FU)-, 5-FU-base...|\n",
      "|\"Monomeric sarcosine oxidase (MSOX) is a flavoenzyme that catalyzes...|[cataly, methylgly, gly, ethylgly, dimethylgly, spectrally, practic...|\n",
      "|We presented the tachinid fly Exorista japonica with moving host mo...|                                             [fly, fly, fly, fly, fly]|\n",
      "|The literature dealing with the water conducting properties of sapw...|                               [generally, mathematically, especially]|\n",
      "|A novel approach to synthesize chitosan-O-isopropyl-5'-O-d4T monoph...|[efficiently, poly, chitosan-O-isopropyl-5'-O-d4T, Chitosan-d4T, 1....|\n",
      "|An HPLC-ESI-MS-MS method has been developed for the quantitative de...|[chromatographically, respectively, successfully, C18, (n=5), 95.0%...|\n",
      "|The localizing and lateralizing values of eye and head ictal deviat...|                                                        [early, early]|\n",
      "|OBJECTIVE: To evaluate the effectiveness and acceptability of expec...|[weekly, respectively, theanaly, 2006, 2007,, 2, 66, 1), 30patients...|\n",
      "|We report the results of a screen for genetic association with urin...|[poly, threepoly, significantly, analy, actually, anextremely, only...|\n",
      "|Intraparenchymal pericatheter cyst is rarely reported. Obstruction ...|                                  [rarely, possibly, unusually, Early]|\n",
      "|PURPOSE: To compare the effectiveness, potential advantages and com...|[analy, comparatively, wassignificantly, respectively, a7-year, 155...|\n",
      "|We have demonstrated a new type of all-optical 2 x 2 switch by usin...|[approximately, fully, approximately, approximately, approximately,...|\n",
      "|Physalis peruviana (PP) is a widely used medicinal herb for treatin...|[widely, (20,, 40,, 60,, 80, 95%, 100, 95%, (82.3%), onFeCl2-ascorb...|\n",
      "+----------------------------------------------------------------------+----------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "match_df.select('text','regex_matches.result')\\\n",
    "        .toDF('text','matches').filter(F.size('matches')>1)\\\n",
    "        .show(truncate=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiDateMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract exact & normalize dates from relative date-time phrases. The default anchor date will be the date the code is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='MultiDateMatcher_45d507efaadb', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
       " Param(parent='MultiDateMatcher_45d507efaadb', name='inputFormats', doc='input formats list of patterns to match'): [''],\n",
       " Param(parent='MultiDateMatcher_45d507efaadb', name='outputFormat', doc='desired output format for dates extracted'): 'yyyy/MM/dd',\n",
       " Param(parent='MultiDateMatcher_45d507efaadb', name='readMonthFirst', doc='Whether to parse july 07/05/2015 or as 05/07/2015'): True,\n",
       " Param(parent='MultiDateMatcher_45d507efaadb', name='defaultDayWhenMissing', doc='which day to set when it is missing from parsed input'): 1}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiDateMatcher().extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|result                  |\n",
      "+------------------------+\n",
      "|[2022/10/11, 2022/10/03]|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "date_matcher = MultiDateMatcher() \\\n",
    "    .setInputCols('document') \\\n",
    "    .setOutputCol(\"date\")\\\n",
    "    .setOutputFormat(\"yyyy/MM/dd\")\\\n",
    "    .setSourceLanguage(\"en\")\n",
    "\n",
    "\n",
    "date_pipeline = PipelineModel(\n",
    "    stages=[\n",
    "        documentAssembler, \n",
    "        date_matcher\n",
    "        ])\n",
    "\n",
    "sample_df = spark.createDataFrame([['I saw him yesterday and he told me that he will visit us next week']]).toDF(\"text\")\n",
    "\n",
    "result = date_pipeline.transform(sample_df)\n",
    "result.select('date.result').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the Input Format and Output Format to specific format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|result      |\n",
      "+------------+\n",
      "|[2022/05/21]|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "date_matcher = MultiDateMatcher() \\\n",
    "    .setInputCols('document') \\\n",
    "    .setOutputCol(\"date\")\\\n",
    "    .setInputFormats([\"dd/MM/yyyy\"])\\\n",
    "    .setOutputFormat(\"yyyy/MM/dd\")\\\n",
    "    .setSourceLanguage(\"en\")\n",
    "\n",
    "date_pipeline = PipelineModel(\n",
    "    stages=[\n",
    "        documentAssembler, \n",
    "        date_matcher\n",
    "        ])\n",
    "\n",
    "sample_df = spark.createDataFrame([[\"the last payment date of this invoice is 21/05/2022\"]]).toDF(\"text\")\n",
    "\n",
    "result = date_pipeline.transform(sample_df)\n",
    "result.select('date.result').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning with UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------+-----------------------+\n",
      "|text                                                                                          |cleaned                |\n",
      "+----------------------------------------------------------------------------------------------+-----------------------+\n",
      "|<h1 style=\"color: #5e9ca0;\">Have a great <span  style=\"color: #2b2301;\">birth</span> day!</h1>|Have a great birth day!|\n",
      "+----------------------------------------------------------------------------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = '<h1 style=\"color: #5e9ca0;\">Have a great <span  style=\"color: #2b2301;\">birth</span> day!</h1>'\n",
    "\n",
    "text_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "\n",
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "clean_text = lambda s: re.sub(r'<[^>]*>', '', s)\n",
    "\n",
    "text_df.withColumn('cleaned', udf(clean_text, StringType())('text')).select('text','cleaned').show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_not_alnum_count = lambda s: len([i for i in s if not i.isalnum() and i!=' '])\n",
    "\n",
    "find_not_alnum_count(\"it's your birth day!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '<h1 style=\"color: #5e9ca0;\">Have a great <span  style=\"color: #2b2301;\">birth</span> day!</h1>'\n",
    "\n",
    "find_not_alnum_count(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------+-------+\n",
      "|text                                                                                          |cleaned|\n",
      "+----------------------------------------------------------------------------------------------+-------+\n",
      "|<h1 style=\"color: #5e9ca0;\">Have a great <span  style=\"color: #2b2301;\">birth</span> day!</h1>|23     |\n",
      "+----------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.withColumn('cleaned', udf(find_not_alnum_count, IntegerType())('text')).select('text','cleaned').show(truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Finisher:*** Once we have our NLP pipeline ready to go, we might want to use our annotation results somewhere else where it is easy to use. The Finisher outputs annotation(s) values into a string.\n",
    "\n",
    "If we just want the desired output column in the final dataframe, we can use Finisher to drop previous stages in the final output and get the `result` from the process.\n",
    "\n",
    "This is very handy when you want to use the output from Spark NLP annotator as an input to another Spark ML transformer.\n",
    "\n",
    "**Settable parameters are:**\n",
    "\n",
    "| Parametre   | Description |\n",
    "| - | - |\n",
    "|**setInputCols**      |input column name string which targets a column of type Array|\n",
    "|**setOutputCols**      |output column name string which targets a column of type AnnotatorType|\n",
    "|**setCleanAnnotations(True)**|Whether to remove intermediate annotations|\n",
    "|**setValueSplitSymbol(“#”)**|split values within an annotation character|\n",
    "|**setAnnotationSplitSymbol(“@”)**|split values between annotations character|\n",
    "|**setIncludeMetadata(False)**|Whether to include metadata keys. Sometimes useful in some annotations.|\n",
    "|**setOutputAsArray(False)**| Whether to output as Array. Useful as input for other Spark transformers.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                              text|                            finished_regex_matches|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|The human KCNJ9 (Kir 3.3, GIRK3) is a member of...|[inwardly, family, spansapproximately, byapprox...|\n",
      "|BACKGROUND: At present, it is one of the most i...|[previously, previously, intravenously, previou...|\n",
      "|OBJECTIVE: To investigate the relationship betw...|[renal failure, cardiac surgery, cardiac surger...|\n",
      "|Combined EEG/fMRI recording has been used to lo...|[normally, significantly, effectively, analy, o...|\n",
      "|Kohlschutter syndrome is a rare neurodegenerati...|                                          [family]|\n",
      "|Statistical analysis of neuroimages is commonly...|[analy, commonly, overly, normally, thatsuccess...|\n",
      "|The synthetic DOX-LNA conjugate was characteriz...|                         [wasanaly, substantially]|\n",
      "|Our objective was to compare three different me...|[daily, only, Conversely, Hourly, hourly, Hourl...|\n",
      "|We conducted a phase II study to assess the eff...|[analy, respectively, generally, 5-fluorouracil...|\n",
      "|\"Monomeric sarcosine oxidase (MSOX) is a flavoe...|[cataly, methylgly, gly, ethylgly, dimethylgly,...|\n",
      "|We presented the tachinid fly Exorista japonica...|                         [fly, fly, fly, fly, fly]|\n",
      "|The literature dealing with the water conductin...|           [generally, mathematically, especially]|\n",
      "|A novel approach to synthesize chitosan-O-isopr...|[efficiently, poly, chitosan-O-isopropyl-5'-O-d...|\n",
      "|An HPLC-ESI-MS-MS method has been developed for...|[chromatographically, respectively, successfull...|\n",
      "|The localizing and lateralizing values of eye a...|                                    [early, early]|\n",
      "|OBJECTIVE: To evaluate the effectiveness and ac...|[weekly, respectively, theanaly, 2006, 2007,, 2...|\n",
      "|For the construction of new combinatorial libra...|                                           [newly]|\n",
      "|We report the results of a screen for genetic a...|[poly, threepoly, significantly, analy, actuall...|\n",
      "|Intraparenchymal pericatheter cyst is rarely re...|              [rarely, possibly, unusually, Early]|\n",
      "|It is known that patients with Klinefelter's sy...|                                                []|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"regex_matches\"]) \\\n",
    "    .setIncludeMetadata(False) # set to False to remove metadata\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[documentAssembler, \n",
    "                               regex_matcher,\n",
    "                               finisher])\n",
    "\n",
    "match_df = nlpPipeline.fit(pubMedDF).transform(pubMedDF)\n",
    "match_df.show(truncate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- finished_regex_matches: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "match_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                              text|                            finished_regex_matches|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|The human KCNJ9 (Kir 3.3, GIRK3) is a member of...|[inwardly, family, spansapproximately, byapprox...|\n",
      "|BACKGROUND: At present, it is one of the most i...|[previously, previously, intravenously, previou...|\n",
      "|OBJECTIVE: To investigate the relationship betw...|[renal failure, cardiac surgery, cardiac surger...|\n",
      "|Combined EEG/fMRI recording has been used to lo...|[normally, significantly, effectively, analy, o...|\n",
      "|Statistical analysis of neuroimages is commonly...|[analy, commonly, overly, normally, thatsuccess...|\n",
      "|Our objective was to compare three different me...|[daily, only, Conversely, Hourly, hourly, Hourl...|\n",
      "|We conducted a phase II study to assess the eff...|[analy, respectively, generally, 5-fluorouracil...|\n",
      "|\"Monomeric sarcosine oxidase (MSOX) is a flavoe...|[cataly, methylgly, gly, ethylgly, dimethylgly,...|\n",
      "|We presented the tachinid fly Exorista japonica...|                         [fly, fly, fly, fly, fly]|\n",
      "|The literature dealing with the water conductin...|           [generally, mathematically, especially]|\n",
      "|A novel approach to synthesize chitosan-O-isopr...|[efficiently, poly, chitosan-O-isopropyl-5'-O-d...|\n",
      "|An HPLC-ESI-MS-MS method has been developed for...|[chromatographically, respectively, successfull...|\n",
      "|OBJECTIVE: To evaluate the effectiveness and ac...|[weekly, respectively, theanaly, 2006, 2007,, 2...|\n",
      "|We report the results of a screen for genetic a...|[poly, threepoly, significantly, analy, actuall...|\n",
      "|Intraparenchymal pericatheter cyst is rarely re...|              [rarely, possibly, unusually, Early]|\n",
      "|PURPOSE: To compare the effectiveness, potentia...|[analy, comparatively, wassignificantly, respec...|\n",
      "|We have demonstrated a new type of all-optical ...|[approximately, fully, approximately, approxima...|\n",
      "|Physalis peruviana (PP) is a widely used medici...|[widely, (20,, 40,, 60,, 80, 95%, 100, 95%, (82...|\n",
      "|We report the discovery of a series of substitu...|[highly, potentially, highly, respectively, tub...|\n",
      "|The purpose of this study was to identify and c...|[family, Nearly, only, 43, 10, 44%, 32%, 64%, 4...|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "match_df.filter(F.size('finished_regex_matches')>2).show(truncate = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightPipeline\n",
    "\n",
    "https://medium.com/spark-nlp/spark-nlp-101-lightpipeline-a544e93f20f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightPipelines are Spark NLP specific Pipelines, equivalent to Spark ML Pipeline, but meant to deal with smaller amounts of data. They’re useful working with small datasets, debugging results, or when running either training or prediction from an API that serves one-off requests.\n",
    "\n",
    "Spark NLP LightPipelines are Spark ML pipelines converted into a single machine but the multi-threaded task, becoming more than 10x times faster for smaller amounts of data (small is relative, but 50k sentences are roughly a good maximum). To use them, we simply plug in a trained (fitted) pipeline and then annotate a plain text. We don't even need to convert the input text to DataFrame in order to feed it into a pipeline that's accepting DataFrame as an input in the first place. This feature would be quite useful when it comes to getting a prediction for a few lines of text from a trained ML model.\n",
    "\n",
    " **It is nearly 10x faster than using Spark ML Pipeline**\n",
    "\n",
    "`LightPipeline(someTrainedPipeline).annotate(someStringOrArray)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|               token|                stem|               lemma|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Peter is a very g...|[{document, 0, 27...|[{token, 0, 4, Pe...|[{token, 0, 4, pe...|[{token, 0, 4, Pe...|\n",
      "|My life in Russia...|[{document, 0, 37...|[{token, 0, 1, My...|[{token, 0, 1, my...|[{token, 0, 1, My...|\n",
      "|John and Peter ar...|[{document, 0, 76...|[{token, 0, 3, Jo...|[{token, 0, 3, jo...|[{token, 0, 3, Jo...|\n",
      "|Lucas Nogal Dunbe...|[{document, 0, 67...|[{token, 0, 4, Lu...|[{token, 0, 4, lu...|[{token, 0, 4, Lu...|\n",
      "|Europe is very cu...|[{document, 0, 68...|[{token, 0, 5, Eu...|[{token, 0, 5, eu...|[{token, 0, 5, Eu...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "\n",
    "nlpPipeline = Pipeline(\n",
    "    stages=[\n",
    "        documentAssembler, \n",
    "        tokenizer,\n",
    "        stemmer,\n",
    "        lemmatizer\n",
    "        ])\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "\n",
    "pipelineModel = nlpPipeline.fit(empty_df)\n",
    "nlpPipeline.fit(empty_df).transform(spark_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import LightPipeline\n",
    "\n",
    "light_model = LightPipeline(pipelineModel)\n",
    "light_result = light_model.annotate(\"John and Peter are brothers. However they don't support each other that much.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['document', 'token', 'stem', 'lemma'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light_result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'john', 'John'),\n",
       " ('and', 'and', 'and'),\n",
       " ('Peter', 'peter', 'Peter'),\n",
       " ('are', 'ar', 'be'),\n",
       " ('brothers', 'brother', 'brother'),\n",
       " ('.', '.', '.'),\n",
       " ('However', 'howev', 'However'),\n",
       " ('they', 'thei', 'they'),\n",
       " (\"don't\", \"don't\", \"don't\"),\n",
       " ('support', 'support', 'support'),\n",
       " ('each', 'each', 'each'),\n",
       " ('other', 'other', 'other'),\n",
       " ('that', 'that', 'that'),\n",
       " ('much', 'much', 'much'),\n",
       " ('.', '.', '.')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(light_result['token'], light_result['stem'], light_result['lemma']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_result = light_model.fullAnnotate(\"John and Peter are brothers. However they don't support each other that much.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'document': [Annotation(document, 0, 76, John and Peter are brothers. However they don't support each other that much., {})],\n",
       "  'token': [Annotation(token, 0, 3, John, {'sentence': '0'}),\n",
       "   Annotation(token, 5, 7, and, {'sentence': '0'}),\n",
       "   Annotation(token, 9, 13, Peter, {'sentence': '0'}),\n",
       "   Annotation(token, 15, 17, are, {'sentence': '0'}),\n",
       "   Annotation(token, 19, 26, brothers, {'sentence': '0'}),\n",
       "   Annotation(token, 27, 27, ., {'sentence': '0'}),\n",
       "   Annotation(token, 29, 35, However, {'sentence': '0'}),\n",
       "   Annotation(token, 37, 40, they, {'sentence': '0'}),\n",
       "   Annotation(token, 42, 46, don't, {'sentence': '0'}),\n",
       "   Annotation(token, 48, 54, support, {'sentence': '0'}),\n",
       "   Annotation(token, 56, 59, each, {'sentence': '0'}),\n",
       "   Annotation(token, 61, 65, other, {'sentence': '0'}),\n",
       "   Annotation(token, 67, 70, that, {'sentence': '0'}),\n",
       "   Annotation(token, 72, 75, much, {'sentence': '0'}),\n",
       "   Annotation(token, 76, 76, ., {'sentence': '0'})],\n",
       "  'stem': [Annotation(token, 0, 3, john, {'sentence': '0'}),\n",
       "   Annotation(token, 5, 7, and, {'sentence': '0'}),\n",
       "   Annotation(token, 9, 13, peter, {'sentence': '0'}),\n",
       "   Annotation(token, 15, 17, ar, {'sentence': '0'}),\n",
       "   Annotation(token, 19, 26, brother, {'sentence': '0'}),\n",
       "   Annotation(token, 27, 27, ., {'sentence': '0'}),\n",
       "   Annotation(token, 29, 35, howev, {'sentence': '0'}),\n",
       "   Annotation(token, 37, 40, thei, {'sentence': '0'}),\n",
       "   Annotation(token, 42, 46, don't, {'sentence': '0'}),\n",
       "   Annotation(token, 48, 54, support, {'sentence': '0'}),\n",
       "   Annotation(token, 56, 59, each, {'sentence': '0'}),\n",
       "   Annotation(token, 61, 65, other, {'sentence': '0'}),\n",
       "   Annotation(token, 67, 70, that, {'sentence': '0'}),\n",
       "   Annotation(token, 72, 75, much, {'sentence': '0'}),\n",
       "   Annotation(token, 76, 76, ., {'sentence': '0'})],\n",
       "  'lemma': [Annotation(token, 0, 3, John, {'sentence': '0'}),\n",
       "   Annotation(token, 5, 7, and, {'sentence': '0'}),\n",
       "   Annotation(token, 9, 13, Peter, {'sentence': '0'}),\n",
       "   Annotation(token, 15, 17, be, {'sentence': '0'}),\n",
       "   Annotation(token, 19, 26, brother, {'sentence': '0'}),\n",
       "   Annotation(token, 27, 27, ., {'sentence': '0'}),\n",
       "   Annotation(token, 29, 35, However, {'sentence': '0'}),\n",
       "   Annotation(token, 37, 40, they, {'sentence': '0'}),\n",
       "   Annotation(token, 42, 46, don't, {'sentence': '0'}),\n",
       "   Annotation(token, 48, 54, support, {'sentence': '0'}),\n",
       "   Annotation(token, 56, 59, each, {'sentence': '0'}),\n",
       "   Annotation(token, 61, 65, other, {'sentence': '0'}),\n",
       "   Annotation(token, 67, 70, that, {'sentence': '0'}),\n",
       "   Annotation(token, 72, 75, much, {'sentence': '0'}),\n",
       "   Annotation(token, 76, 76, ., {'sentence': '0'})]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'document': ['How did serfdom develop in and then leave Russia ?'],\n",
       "  'token': ['How',\n",
       "   'did',\n",
       "   'serfdom',\n",
       "   'develop',\n",
       "   'in',\n",
       "   'and',\n",
       "   'then',\n",
       "   'leave',\n",
       "   'Russia',\n",
       "   '?'],\n",
       "  'stem': ['how',\n",
       "   'did',\n",
       "   'serfdom',\n",
       "   'develop',\n",
       "   'in',\n",
       "   'and',\n",
       "   'then',\n",
       "   'leav',\n",
       "   'russia',\n",
       "   '?'],\n",
       "  'lemma': ['How',\n",
       "   'do',\n",
       "   'serfdom',\n",
       "   'develop',\n",
       "   'in',\n",
       "   'and',\n",
       "   'then',\n",
       "   'leave',\n",
       "   'Russia',\n",
       "   '?']},\n",
       " {'document': ['There will be some exciting breakthroughs in NLP this year.'],\n",
       "  'token': ['There',\n",
       "   'will',\n",
       "   'be',\n",
       "   'some',\n",
       "   'exciting',\n",
       "   'breakthroughs',\n",
       "   'in',\n",
       "   'NLP',\n",
       "   'this',\n",
       "   'year',\n",
       "   '.'],\n",
       "  'stem': ['there',\n",
       "   'will',\n",
       "   'be',\n",
       "   'some',\n",
       "   'excit',\n",
       "   'breakthrough',\n",
       "   'in',\n",
       "   'nlp',\n",
       "   'thi',\n",
       "   'year',\n",
       "   '.'],\n",
       "  'lemma': ['There',\n",
       "   'will',\n",
       "   'be',\n",
       "   'some',\n",
       "   'exciting',\n",
       "   'breakthrough',\n",
       "   'in',\n",
       "   'NLP',\n",
       "   'this',\n",
       "   'year',\n",
       "   '.']}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list= [\"How did serfdom develop in and then leave Russia ?\",\n",
    "\"There will be some exciting breakthroughs in NLP this year.\"]\n",
    "\n",
    "light_model.annotate(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**important note:** When you use Finisher in your pipeline, regardless of setting `cleanAnnotations` to False or True, LigtPipeline will only return the finished columns."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
