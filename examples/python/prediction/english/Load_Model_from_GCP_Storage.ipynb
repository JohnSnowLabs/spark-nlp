{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/prediction/english/Load_Model_From_GCP_Storage.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfdkWg6LThJP"
   },
   "source": [
    "## Loading Pretrained Models from GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GmZvE5oTku4"
   },
   "outputs": [],
   "source": [
    "!wget https://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r44X4OKlToLC"
   },
   "source": [
    "## Defining GCP Storage URI in cache_pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cy14aeXATt0S"
   },
   "source": [
    "In this notebook, we are going to see the steps required to use an external GCP Storage URI as cache_pretrained folder\n",
    "\n",
    "In Spark NLP you can configure the location to download the pre-trained models. Starting at Spark NLP 4.2.4, you can set a GCP Storage URI. To do this, we need to configure the spark session with the required settings for Spark NLP and Spark ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKVkaiTaULve"
   },
   "source": [
    "### Spark NLP Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0DgEpU7UhBw"
   },
   "source": [
    "\n",
    "\n",
    "1. `cache_folder`: Here you must define your GCP storage URI (using gs prefix) that will store Spark NLP pre-trained models. This is defined in the config spark.jsl.settings.pretrained.cache_folder\n",
    "2. `project_id`: We need to know the ProjectId of our GCP Storage. This is defined in `spark.jsl.settings.gcp`\n",
    "\n",
    "To integrage with GCP, we need to setup Application Default Credentials (ADC) for GCP. You can check how to configure it in the official [GCP documentation](https://cloud.google.com/docs/authentication/provide-credentials-adc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdluR0wzVVM_"
   },
   "source": [
    "### Spark ML Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUeUonSiVkQj"
   },
   "source": [
    "Spark ML requires the following configuration to load a model from GCP using ADC:\n",
    "\n",
    "\n",
    "\n",
    "1. GCP connector: You need to identify your hadoop versio and set the required dependency in `spark.jars.packages`\n",
    "2. ADC credentials: After following the instructions to setup ADC, you will have a JSON file that holds your authenticiation information. This file is setup in `spark.hadoop.google.cloud.auth.service.account.json.keyfile`\n",
    "3. Hadoop File System: You also need to setup the Hadoop implementation to work with GCP Storage as file system. This is define in `spark.hadoop.fs.gs.impl`\n",
    "3. Finally, to mitigate conflicts between Spark's dependencies and user dependencies. You must define `spark.driver.userClassPathFirst` as true. You may also need to define `spark.executor.userClassPathFirst` as true.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEQKV1IRYhg0"
   },
   "source": [
    "Now, let's take a look at a simple ecxample the spark session creation below to see how to define each of the configurations with its values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4JfeD8Rj-as2",
    "outputId": "437ae866-f63e-43e0-b898-0860e3b19b7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark version: 3.2.1\n"
     ]
    }
   ],
   "source": [
    "mport pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#GCP Storage configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkNLP\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"12G\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.jars\", \"./sparknlp.jar\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.2.4,com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.8\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.driver.userClassPathFirst\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/content/.config/application_default_credentials.json\") \\\n",
    "    .config(\"spark.jsl.settings.gcp.project_id\", \"my_project_id\") \\\n",
    "    .config(\"spark.jsl.settings.pretrained.cache_folder\", \"gs://my-bucket/models\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Apache Spark version: {}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting at spark-nlp 4.3.0, if you have control over spark session creation. You can also use sparknlp.start() with params argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"spark.jars.packages\": \"com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.8\",\n",
    "    \"spark.hadoop.fs.gs.impl\": \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\",\n",
    "    \"spark.driver.userClassPathFirst\", \"true\",\n",
    "    \"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/content/.config/application_default_credentials.json\",\n",
    "    \"spark.jsl.settings.gcp.project_id\", \"my_project_id\",\n",
    "    \"spark.jsl.settings.pretrained.cache_folder\", \"gs://my-bucket/models\"\n",
    "}\n",
    "\n",
    "spark = sparknlp.start(params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLNO3Z9r6HgR"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eB72Yzg8_Jx"
   },
   "outputs": [],
   "source": [
    "sample_text = \"This is a sentence. This is another sentence\"\n",
    "data_df = spark.createDataFrame([[sample_text]]).toDF(\"text\").cache()\n",
    "\n",
    "empty_df = spark.createDataFrame([[\"\"]]).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRyju8D-6XJ1"
   },
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5G4_BXwOYtC",
    "outputId": "7f15118f-6c8e-46c0-c432-48de09bd72b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_detector_dl download started this may take some time.\n",
      "Approximate size to download 354.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "sentence_detector_dl = SentenceDetectorDLModel() \\\n",
    ".pretrained() \\\n",
    ".setInputCols([\"document\"]) \\\n",
    ".setOutputCol(\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhKPEMb09w6a"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[document_assembler, sentence_detector_dl, tokenizer])\n",
    "pipeline_model = pipeline.fit(empty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CAp_AtrssPj",
    "outputId": "4d579436-d3e5-429d-dabb-0d321dca1f0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|This is a sentenc...|[{document, 0, 43...|[{document, 0, 18...|[{token, 0, 3, Th...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = pipeline_model.transform(data_df)\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
