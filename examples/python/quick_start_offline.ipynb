{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "## This notebok provides set of commands to install Spark NLP for offline usage. It contains 4 sections:\n",
    "1) Download all dependencies for Spark NLP\n",
    "\n",
    "2) Download all dependencies for Spark NLP (enterprise/licensed)\n",
    "\n",
    "3) Download all dependencies for Spark NLP OCR\n",
    "\n",
    "4) Download all models/embeddings for offline usage\n",
    "\n",
    "5) Example of NER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Download all dependencies for Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['PUBLIC_VERSION', 'JSL_VERSION', 'SECRET', 'SPARK_NLP_LICENSE', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY', 'SPARK_OCR_LICENSE', 'SPARK_OCR_SECRET'])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('workshop_license_keys_365.json') as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "license_keys.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['SPARK_NLP_LICENSE'] = license_keys['SPARK_NLP_LICENSE']\n",
    "os.environ['AWS_ACCESS_KEY_ID']= license_keys['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = license_keys['AWS_SECRET_ACCESS_KEY']\n",
    "os.environ['JSL_OCR_LICENSE'] = license_keys['SPARK_OCR_LICENSE']\n",
    "\n",
    "version = license_keys['PUBLIC_VERSION']\n",
    "jsl_version = license_keys['JSL_VERSION']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! apt-get update -qq\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_265\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_265-8u265-b01-0ubuntu2~18.04-b01)\n",
      "OpenJDK 64-Bit Server VM (build 25.265-b01, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 215.7MB 65kB/s \n",
      "\u001b[K     |████████████████████████████████| 204kB 44.9MB/s \n",
      "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install --ignore-installed -q pyspark==2.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark                       2.4.4          \n"
     ]
    }
   ],
   "source": [
    "!pip list | grep spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark-nlp jar\n",
    "!wget -q https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.7.3.jar\n",
    "\n",
    "# spark-nlp wheel\n",
    "!wget -q https://github.com/JohnSnowLabs/spark-nlp/archive/2.7.3.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvf spark-nlp-2.7.3.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for spark-nlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q spark-nlp-2.7.3/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Download all dependencies for Spark NLP (enterprise/licensed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here you need to enter your AWS KEY and AWS SECRET KEY.\n",
    "# As a region enter \"ohio\"\n",
    "# As a language enter \"en\"\n",
    "!aws configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsl_secret = license_keys['SECRET']\n",
    "jsl_jar = jsl_version+'.jar'\n",
    "jsl_tar = jsl_version+'.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark nlp JSL wheel\n",
    "!sudo aws s3 cp --region us-east-2 s3://pypi.johnsnowlabs.com/$jsl_secret/spark-nlp-jsl-$jsl_jar spark-nlp-jsl-$jsl_jar\n",
    "!sudo aws s3 cp --region us-east-2 s3://pypi.johnsnowlabs.com/$secret/spark-nlp-jsl/spark-nlp-jsl-$jsl_tar spark-nlp-jsl-$jsl_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvf spark-nlp-jsl-$jsl_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q /content/spark-nlp-jsl-$jsl_version/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark                       2.4.4          \n",
      "spark-nlp                     2.6.0          \n",
      "spark-nlp-jsl                 2.6.0          \n"
     ]
    }
   ],
   "source": [
    "!pip list | grep spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Download all dependencies for Spark NLP OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_secret = license_keys['SPARK_OCR_SECRET']\n",
    "ocr_version = ocr_secret.split('-')[0]\n",
    "ocr_jar = ocr_version+'.spark24.jar'ocr_tar = ocr_version+'.spark24.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://pypi.johnsnowlabs.com/$ocr_secret/jars/spark-ocr-assembly-$ocr_jar!wget -q https://pypi.johnsnowlabs.com/$ocr_secret/spark-ocr/spark-ocr-$ocr_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack wheel OCR\n",
    "!tar -xvf /content/spark-ocr-$ocr_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q /content/spark-ocr-$ocr_version/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark                       2.4.4          \n",
      "spark-nlp                     2.6.0          \n",
      "spark-nlp-jsl                 2.6.0          \n",
      "spark-ocr                     1.5.0          \n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "!pip list | grep spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation completed. Let's download models using AWS keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Download all models/embeddings for offline usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will download >100 GB of Spark NLP models to your local disk\n",
    "# !sudo aws s3 cp --region us-east-2 s3://auxdata.johnsnowlabs.com/public/models/ public_models/ --recursive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code also will download >100 GB of clinical embeddings from Spark NLP models\n",
    "# !sudo aws s3 cp --region us-east-2 s3://auxdata.johnsnowlabs.com/clinical/models/ clinical_models/ --recursive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://auxdata.johnsnowlabs.com/public/models/ner_dl_bert_base_cased_en_2.6.0_2.4_1599550960441.zip to public_models/ner_dl_bert_base_cased_en_2.6.0_2.4_1599550960441.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/ner_dl_bert_base_cased_en_2.4.0_2.4_1583223672963.zip to public_models/ner_dl_bert_base_cased_en_2.4.0_2.4_1583223672963.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/ner_dl_bert_en_2.4.3_2.4_1584624951079.zip to public_models/ner_dl_bert_en_2.4.3_2.4_1584624951079.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/ner_dl_bert_en_2.4.0_2.4_1583223672963.zip to public_models/ner_dl_bert_en_2.4.0_2.4_1583223672963.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/ner_dl_bert_contrib_en_2.0.2_2.4_1556650375261.zip to public_models/ner_dl_bert_contrib_en_2.0.2_2.4_1556650375261.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/ner_dl_bert_en_2.2.0_2.4_1567854461249.zip to public_models/ner_dl_bert_en_2.2.0_2.4_1567854461249.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/ner_dl_contrib_de_2.0.8_2.4_1561234357155.zip to public_models/ner_dl_contrib_de_2.0.8_2.4_1561234357155.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/ner_dl_bert_en_2.0.2_2.4_1558809068913.zip to public_models/ner_dl_bert_en_2.0.2_2.4_1558809068913.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/ner_dl_en_2.4.0_2.4_1580251789753.zip to public_models/ner_dl_en_2.4.0_2.4_1580251789753.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/ner_dl_contrib_it_2.0.8_2.4_1560344573823.zip to public_models/ner_dl_contrib_it_2.0.8_2.4_1560344573823.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/ner_dl_contrib_fr_2.0.2_2.4_1558826556431.zip to public_models/ner_dl_contrib_fr_2.0.2_2.4_1558826556431.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/ner_dl_sentence_en_2.4.0_2.4_1580252313303.zip to public_models/ner_dl_sentence_en_2.4.0_2.4_1580252313303.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/ner_dl_contrib_en_2.0.2_2.4_1556501490317.zip to public_models/ner_dl_contrib_en_2.0.2_2.4_1556501490317.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/ner_dl_bert_en_2.6.0_2.4_1599550979101.zip to public_models/ner_dl_bert_en_2.6.0_2.4_1599550979101.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/ner_dl_sentence_en_2.0.2_2.4_1556666842347.zip to public_models/ner_dl_sentence_en_2.0.2_2.4_1556666842347.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/ner_dl_en_2.4.3_2.4_1584624950746.zip to public_models/ner_dl_en_2.4.3_2.4_1584624950746.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/ner_dl_en_2.0.2_2.4_1558802205173.zip to public_models/ner_dl_en_2.0.2_2.4_1558802205173.zip\n"
     ]
    }
   ],
   "source": [
    "# For example purposes let's download only subset for NER and glove\n",
    "!sudo aws s3 cp --region us-east-2 s3://auxdata.johnsnowlabs.com/public/models/ public_models/ --recursive --exclude \"*\" --include \"ner_dl*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_6B_100_xx_2.4.0_2.4_1579690037117.zip to public_models/glove_6B_100_xx_2.4.0_2.4_1579690037117.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_100d_en_2.0.0_2.4_1553028251278.zip to public_models/glove_100d_en_2.0.0_2.4_1553028251278.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_100d_en_2.0.2_2.4_1556534397055.zip to public_models/glove_100d_en_2.0.2_2.4_1556534397055.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_100d_en_2.4.0_2.4_1579690104032.zip to public_models/glove_100d_en_2.4.0_2.4_1579690104032.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_6B_300_xx_2.4.0_2.4_1579698630432.zip to public_models/glove_6B_300_xx_2.4.0_2.4_1579698630432.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_6B_300_xx_2.0.2_2.4_1559059806004.zip to public_models/glove_6B_300_xx_2.0.2_2.4_1559059806004.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_6B_300_xx_2.1.0_2.4_1564760779318.zip to public_models/glove_6B_300_xx_2.1.0_2.4_1564760779318.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_840B_300_xx_2.0.2_2.4_1558645003344.zip to public_models/glove_840B_300_xx_2.0.2_2.4_1558645003344.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_840B_300_xx_2.4.0_2.4_1579698926752.zip to public_models/glove_840B_300_xx_2.4.0_2.4_1579698926752.zip\n"
     ]
    }
   ],
   "source": [
    "!sudo aws s3 cp --region us-east-2 s3://auxdata.johnsnowlabs.com/public/models/ public_models/ --recursive --exclude \"*\" --include \"glove*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo aws s3 cp --region us-east-2 s3://auxdata.johnsnowlabs.com/clinical/models/ clinical_models/ --recursive --exclude \"*\" --include \"embeddings_clinical*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Example on NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q /content/public_models/ner_dl_en_2.4.3_2.4_1584624950746.zip -d ner_dl_glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q /content/public_models/glove_100d_en_2.4.0_2.4_1579690104032.zip -d glove_embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_local_path = 'ner_dl_glove'\n",
    "embeddings_local_path = 'glove_embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_nlp_jar_path = \"/content/spark-nlp-assembly-\"+version+\".jar\"\n",
    "spark_nlp_internal = \"/content/spark-nlp-jsl-\"+jsl_jar\n",
    "spark_nlp_jar_path = spark_nlp_jar_path+\",\"+spark_nlp_internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"Spark NLP Licensed\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"10G\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "        .config(\"spark.jars\", spark_nlp_jar_path)\n",
    "    return builder.getOrCreate()\n",
    "\n",
    "spark = start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "# ner_dl model is trained with glove_100d. So we use the same embeddings in the pipeline\n",
    "glove_embeddings = WordEmbeddingsModel.load(embeddings_local_path).\\\n",
    "  setInputCols([\"document\", 'token']).\\\n",
    "  setOutputCol(\"embeddings\")\n",
    "\n",
    "# NER model trained on i2b2 (sampled from MIMIC) dataset\n",
    "public_ner = NerDLModel.load(ner_local_path) \\\n",
    "  .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\n",
    "  .setOutputCol(\"ner\")\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[\n",
    " documentAssembler, \n",
    " tokenizer,\n",
    " glove_embeddings,\n",
    " public_ner\n",
    " ])\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "\n",
    "pipelineModel = nlpPipeline.fit(empty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+-------------------------------------+\n",
      "|result                                  |result                               |\n",
      "+----------------------------------------+-------------------------------------+\n",
      "|[Peter, Parker, lives, in, New, York, .]|[B-PER, I-PER, O, O, B-LOC, I-LOC, O]|\n",
      "+----------------------------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([['Peter Parker lives in New York.']]).toDF(\"text\")\n",
    "\n",
    "result = pipelineModel.transform(df)\n",
    "\n",
    "result.select('token.result','ner.result').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Peter', 'B-PER'),\n",
       " ('Parker', 'I-PER'),\n",
       " ('lives', 'O'),\n",
       " ('in', 'O'),\n",
       " ('New', 'B-LOC'),\n",
       " ('York', 'I-LOC'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light_model = LightPipeline(pipelineModel)\n",
    "\n",
    "text = 'Peter Parker lives in New York.'\n",
    "\n",
    "light_result = light_model.annotate(text)\n",
    "\n",
    "list(zip(light_result['token'], light_result['ner']))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SparkNLP_offline_installation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
