{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/reader/SparkNLP_Word_Reader_Demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzcU5p2gdak9"
   },
   "source": [
    "# Introducing Word reader in SparkNLP\n",
    "This notebook showcases the newly added  `sparknlp.read().doc()` method in Spark NLP that parses Word documents content from both local and distributed file systems into a Spark DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFOFhaEedalB"
   },
   "source": [
    "## Setup and Initialization\n",
    "Let's keep in mind a few things before we start ðŸ˜Š\n",
    "\n",
    "Support for reading Word files was introduced in Spark NLP 5.5.2. Please make sure you have upgraded to the latest Spark NLP release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's install and setup Spark NLP in Google Colab\n",
    "- This part is pretty easy via our simple script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For local files example we will download a couple of Word files from Spark NLP Github repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ya8qZe00dalC",
    "outputId": "f6800bce-c101-47e3-8030-cf1a0b758183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-11 02:43:35--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/feature/SPARKNLP-1094-Adding-support-to-read-Word-files-v2/src/test/resources/reader/doc/contains-pictures.docx\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 95087 (93K) [application/octet-stream]\n",
      "Saving to: â€˜word-files/contains-pictures.docxâ€™\n",
      "\n",
      "contains-pictures.d 100%[===================>]  92.86K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2024-12-11 02:43:35 (2.47 MB/s) - â€˜word-files/contains-pictures.docxâ€™ saved [95087/95087]\n",
      "\n",
      "--2024-12-11 02:43:36--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/feature/SPARKNLP-1094-Adding-support-to-read-Word-files-v2/src/test/resources/reader/doc/fake_table.docx\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12392 (12K) [application/octet-stream]\n",
      "Saving to: â€˜word-files/fake_table.docxâ€™\n",
      "\n",
      "fake_table.docx     100%[===================>]  12.10K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-12-11 02:43:36 (24.7 MB/s) - â€˜word-files/fake_table.docxâ€™ saved [12392/12392]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir word-files\n",
    "!wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/reader/doc/contains-pictures.docx -P word-files\n",
    "!wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/reader/doc/fake_table.docx -P word-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZLpFt7qcWoC",
    "outputId": "6e5ce0b8-383a-481c-9b7b-d4250d385f25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 112K\n",
      "-rw-r--r-- 1 root root 93K Dec 11 02:43 contains-pictures.docx\n",
      "-rw-r--r-- 1 root root 13K Dec 11 02:43 fake_table.docx\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./word-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TII6UaLqcZw4"
   },
   "source": [
    "## Parsing Word document from Local Files\n",
    "Use the `doc()` method to parse email content from local directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_3GKYbmScehR",
    "outputId": "24941880-c772-4b4e-dd0d-349fe8ea31c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "doc_df = sparknlp.read().doc(\"./word-files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eKOYqIigmlmh",
    "outputId": "1a3ec3b7-b49d-420b-cdaf-e4682b4f66e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 doc|\n",
      "+--------------------+\n",
      "|[{Table, Header C...|\n",
      "|[{Header, An inli...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_df.select(\"doc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IoC1eqPPcmqN",
    "outputId": "b994396c-b670-49af-8bb9-b5e6ff44e8fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- doc: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- elementType: string (nullable = true)\n",
      " |    |    |-- content: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtzqE1P8cpXE"
   },
   "source": [
    "You can also use DFS file systems like:\n",
    "- Databricks: `dbfs://`\n",
    "- HDFS: `hdfs://`\n",
    "- Microsoft Fabric OneLake: `abfss://`"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
