{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzcU5p2gdak9"
      },
      "source": [
        "# Introducing PartitionTransformer in SparkNLP\n",
        "Spark NLP Readers and `Partition` help build structured inputs for your downstream NLP tasks.â€‹\n",
        "\n",
        "The new `PartitionTransformer` makes your current Spark NLP workflow smoother by allowing to reuse your pipelines seamlessly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrvHhiTAdfGd",
        "outputId": "3f74b003-a469-4ea5-ade7-36fc48261c5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mjV3NcQ8eA52"
      },
      "outputs": [],
      "source": [
        "!cp drive/MyDrive/JSL/sparknlp/sparknlp.jar .\n",
        "!cp drive/MyDrive/JSL/sparknlp/spark_nlp-6.0.1-py2.py3-none-any.whl ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qjPeDjvfCpA",
        "outputId": "d8808470-edc0-466d-d0ff-53c83322fde2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./spark_nlp-6.0.1-py2.py3-none-any.whl\n",
            "Installing collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-6.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install spark_nlp-6.0.1-py2.py3-none-any.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DczWop6QeE8F",
        "outputId": "6f428637-88fe-42a9-be7a-1dfe889c57d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apache Spark version: 3.5.1\n"
          ]
        }
      ],
      "source": [
        "# import sparknlp\n",
        "# # let's start Spark with Spark NLP\n",
        "# spark = sparknlp.start()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkNLP\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"12G\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
        "    .config(\"spark.jars\", \"./sparknlp.jar\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "print(\"Apache Spark version: {}\".format(spark.version))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y_JC9AmJtYr"
      },
      "source": [
        "Creating File"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Initialization\n",
        "Let's keep in mind a few things before we start ðŸ˜Š\n",
        "\n",
        "Support for **PartitionTransformer** was introduced in Spark NLP 6.0.2 Please make sure you have upgraded to the latest Spark NLP release.\n",
        "\n",
        "For local files example we will download different files from Spark NLP Github repo:"
      ],
      "metadata": {
        "id": "JWHyJJBdkSAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading HTML files"
      ],
      "metadata": {
        "id": "CAg4inaqkU8J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bo7s-jZVrE7W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ce20a85-f3f1-4e93-9a7d-da60a415e6cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-24 14:57:07--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/feature/SPARKNLP-1174-Adding-PartitionTransformer/src/test/resources/reader/html/example-10k.html\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2456707 (2.3M) [text/plain]\n",
            "Saving to: â€˜html-files/example-10k.htmlâ€™\n",
            "\n",
            "example-10k.html    100%[===================>]   2.34M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-05-24 14:57:08 (31.6 MB/s) - â€˜html-files/example-10k.htmlâ€™ saved [2456707/2456707]\n",
            "\n",
            "--2025-05-24 14:57:08--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/feature/SPARKNLP-1174-Adding-PartitionTransformer/src/test/resources/reader/html/fake-html.html\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 665 [text/plain]\n",
            "Saving to: â€˜html-files/fake-html.htmlâ€™\n",
            "\n",
            "fake-html.html      100%[===================>]     665  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-24 14:57:08 (38.0 MB/s) - â€˜html-files/fake-html.htmlâ€™ saved [665/665]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir html-files\n",
        "!wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/feature/SPARKNLP-1174-Adding-PartitionTransformer/src/test/resources/reader/html/example-10k.html -P html-files\n",
        "!wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/feature/SPARKNLP-1174-Adding-PartitionTransformer/src/test/resources/reader/html/fake-html.html -P html-files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoFI66NAdalE"
      },
      "source": [
        "## Partitioning Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nluIcWMbM_rx"
      },
      "source": [
        "`PartitionTransformer` outpus a different schema than `Partition`, here we can expect our common Annotation schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWnypHRwXruC",
        "outputId": "b82b20f4-cb27-43ab-8ed3-e4e5b12acee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|                path|             content|                text|           partition|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|file:/content/htm...|<?xml  version=\"1...|[{Title, UNITED S...|[{document, 0, 12...|\n",
            "|file:/content/htm...|<!DOCTYPE html>\\n...|[{Title, My First...|[{document, 0, 15...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.partition.partition_transformer import *\n",
        "\n",
        "empty_df = spark.createDataFrame([], \"string\").toDF(\"text\")\n",
        "\n",
        "partition_transformer = PartitionTransformer() \\\n",
        "    .setInputCols([\"text\"]) \\\n",
        "    .setContentType(\"text/html\") \\\n",
        "    .setContentPath(\"./html-files\") \\\n",
        "    .setOutputCol(\"partition\")\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    partition_transformer\n",
        "])\n",
        "\n",
        "pipeline_model = pipeline.fit(empty_df)\n",
        "result_df = pipeline_model.transform(empty_df)\n",
        "\n",
        "result_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.printSchema()"
      ],
      "metadata": {
        "id": "EFMhyfnc_g1V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04f2d7c-aff3-4bb4-93c4-0007151211a7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- path: string (nullable = true)\n",
            " |-- content: string (nullable = true)\n",
            " |-- text: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- elementType: string (nullable = true)\n",
            " |    |    |-- content: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |-- partition: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBNYByJ5Bqq6"
      },
      "source": [
        "You can integrate `PartitionTransformer` directly into your existing Spark NLP pipelines.â€‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "W7LLHf_0BrtQ"
      },
      "outputs": [],
      "source": [
        "text = (\n",
        "    \"The big brown fox\\n\"\n",
        "    \"was walking down the lane.\\n\"\n",
        "    \"\\n\"\n",
        "    \"At the end of the lane,\\n\"\n",
        "    \"the fox met a bear.\"\n",
        ")\n",
        "\n",
        "testDataSet = spark.createDataFrame(\n",
        "    [(text,)],\n",
        "    [\"text\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from sparknlp import DocumentAssembler\n",
        "\n",
        "emptyDataSet = spark.createDataFrame([], testDataSet.schema)\n",
        "\n",
        "documentAssembler = DocumentAssembler() \\\n",
        "            .setInputCol(\"text\") \\\n",
        "            .setOutputCol(\"document\")\n",
        "\n",
        "partition = PartitionTransformer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"partition\") \\\n",
        "    .setGroupBrokenParagraphs(True)\n",
        "\n",
        "pipeline = Pipeline(stages=[documentAssembler, partition])\n",
        "pipelineModel = pipeline.fit(emptyDataSet)"
      ],
      "metadata": {
        "id": "gPsYuhgOlg4G"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultDf = pipelineModel.transform(testDataSet)\n",
        "resultDf.select(\"partition\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR9-8vDtlq5o",
        "outputId": "4f56f1d1-152f-4fbc-b426-acdea9b7952b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|partition                                                                                                                                                                    |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[{document, 0, 43, The big brown fox was walking down the lane., {paragraph -> 0}, []}, {document, 0, 42, At the end of the lane, the fox met a bear., {paragraph -> 0}, []}]|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}