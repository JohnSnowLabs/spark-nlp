{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/data-preprocessing/SparkNLP_Reader2Image_Demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quSlGrh2X0Ar"
   },
   "source": [
    "# Introducing Reader2Image in SparkNLP\n",
    "\n",
    "This notebook showcases the newly added `Reader2Image` annotator in Spark NLP. It provides a streamlined and user-friendly interface for reading image files and integrating them with VLM annotators in Spark NLP. The annotator is useful for preprocessing data in NLP pipelines that rely on information contained within images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvycj4qAObCw",
    "outputId": "46be2c16-710c-4642-fda9-f3532d51dfb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "# let's start Spark with Spark NLP with GPU enabled. If you don't have GPUs available remove this parameter.\n",
    "spark = sparknlp.start()\n",
    "print(sparknlp.version())\n",
    "\n",
    "print(\"Apache Spark version: {}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXtXmyJFYfGG"
   },
   "source": [
    "To illustrate the use of this reader, let’s define an HTML document containing image data and display a preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "id": "6ZUkBA7rZ1lp",
    "outputId": "9db16c69-c198-47cc-cd15-adf06625a1fe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html>\n",
       "<head>\n",
       "    <title>Image Parsing Test</title>\n",
       "</head>\n",
       "<body>\n",
       "<h1>Test Images</h1>\n",
       "\n",
       "<!-- Base64 inline PNG -->\n",
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUA\n",
       "  AAAFCAYAAACNbyblAAAAHElEQVQI12P4\n",
       "  //8/w38GIAXDIBKE0DHxgljNBAAO9TXL0Y4OHwAAAABJRU5ErkJggg==\"\n",
       "     alt=\"Base64 Red Dot\" width=\"5\" height=\"5\">\n",
       "\n",
       "<!-- External image -->\n",
       "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a7/React-icon.svg/1024px-React-icon.svg.png\"\n",
       "     alt=\"React Logo\" width=\"50\" height=\"50\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "html_code = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Image Parsing Test</title>\n",
    "</head>\n",
    "<body>\n",
    "<h1>Test Images</h1>\n",
    "\n",
    "<!-- Base64 inline PNG -->\n",
    "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUA\n",
    "  AAAFCAYAAACNbyblAAAAHElEQVQI12P4\n",
    "  //8/w38GIAXDIBKE0DHxgljNBAAO9TXL0Y4OHwAAAABJRU5ErkJggg==\"\n",
    "     alt=\"Base64 Red Dot\" width=\"5\" height=\"5\">\n",
    "\n",
    "<!-- External image -->\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a7/React-icon.svg/1024px-React-icon.svg.png\"\n",
    "     alt=\"React Logo\" width=\"50\" height=\"50\">\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KhznNBIYx0m"
   },
   "source": [
    "As you can see in the image above, we have two files: a small red dot and an atom. We expect a VLM model to generate descriptions of these images for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MTnevAlxaXB5"
   },
   "outputs": [],
   "source": [
    "with open(\"example-images.html\", \"w\") as f:\n",
    "    f.write(html_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4JOsiklDVTgd"
   },
   "outputs": [],
   "source": [
    "empty_df = spark.createDataFrame([], \"string\").toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZwclDzKVVX_",
    "outputId": "18b3ee6a-281f-423e-ddf0-ddbea5332f85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+---------+\n",
      "|           fileName|               image|exception|\n",
      "+-------------------+--------------------+---------+\n",
      "|example-images.html|[{image, example-...|     NULL|\n",
      "|example-images.html|[{image, example-...|     NULL|\n",
      "+-------------------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.reader.reader2image import Reader2Image\n",
    "\n",
    "reader2image = Reader2Image() \\\n",
    "    .setContentType(\"text/html\") \\\n",
    "    .setContentPath(\"./example-images.html\") \\\n",
    "    .setOutputCol(\"image\")\n",
    "\n",
    "pipeline = Pipeline(stages=[reader2image])\n",
    "model = pipeline.fit(empty_df)\n",
    "\n",
    "image_df = model.transform(empty_df)\n",
    "image_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDRjLx9gZLVK"
   },
   "source": [
    "For this example, we will use the `Qwen2VLTransformer`. Let’s add a text prompt column for VQA (Vision Question Answering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4tGr69PYVpwS"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "prompt_df = image_df.withColumn(\n",
    "    \"text\",\n",
    "    lit(\n",
    "        \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "        \"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\"\n",
    "        \"Describe this image.<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i7vMR6AHVt_w",
    "outputId": "fa524ea7-12fe-4179-d770-1f442add9899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+---------+--------------------+\n",
      "|           fileName|               image|exception|                text|\n",
      "+-------------------+--------------------+---------+--------------------+\n",
      "|example-images.html|[{image, example-...|     NULL|<|im_start|>syste...|\n",
      "|example-images.html|[{image, example-...|     NULL|<|im_start|>syste...|\n",
      "+-------------------+--------------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufF265kuV0-7",
    "outputId": "fd06edf7-8718-425c-a23a-9a2986e6f315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen2_vl_2b_instruct_int4 download started this may take some time.\n",
      "Approximate size to download 1.4 GB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import Qwen2VLTransformer\n",
    "\n",
    "visualQAClassifier = (\n",
    "    Qwen2VLTransformer.pretrained()\n",
    "    .setInputCols(\"image\")\n",
    "    .setOutputCol(\"answer\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jiAReBePWJtq"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages([visualQAClassifier])\n",
    "result_df = pipeline.fit(prompt_df).transform(prompt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XiAw_vbVWqlN",
    "outputId": "a7aa778f-b16b-47fd-ad56-d4d97c3f5f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|origin               |result                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "+---------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[example-images.html]|[The image is a simple, solid-colored background with a gradient effect. The colors blend smoothly from a lighter yellow at the top to a darker yellow at the bottom. The gradient effect creates a subtle visual effect, giving the impression of a gradient background.]                                                                                                                                                                                                                                          |\n",
      "|[example-images.html]|[The image depicts a stylized representation of an atom. The atom is composed of three main parts: the nucleus, which is the central core of the atom, and two electron shells, which are the outer shells around the nucleus. electron can orbit. The electron shells are depicted as concentric circles, with the nucleus at the center and the electron shells extending outward. The color scheme is primarily pink and red, with the nucleus being a lighter pink and the electron shells being a darker pink.]|\n",
      "+---------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.select(\"image.origin\", \"answer.result\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnsyx37VZlUm"
   },
   "source": [
    "Voilà! As you can see above, we have accurate descriptions of the images generated by `Qwen2VLTransformer`."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
