{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzcU5p2gdak9"
   },
   "source": [
    "# Introducing Chunking in Partition Transformer in SparkNLP\n",
    "This notebook demonstrates how to use **Spark NLP's PartitionTransformer** for\n",
    " chunking of documents, enabling efficient text segmentation.\n",
    "\n",
    "We further showcase a practical application of this chunking strategy in the context of **Retrieval-Augmented Generation (RAG)**.\n",
    "\n",
    "We can use this powerful method to enhance the performance of large language models by supplying context-relevant information from a knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3y_JC9AmJtYr"
   },
   "source": [
    "Creating Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bo7s-jZVrE7W"
   },
   "outputs": [],
   "source": [
    "!echo -e \"Introduction: RAG stands for Retrieval-Augmented Generation. Why RAG? It improves factual accuracy and adds fresh or private data to LLMs. Chunking: Breaks documents into pieces so they can be embedded. Semantic Chunking: Focus on respecting document structure like sections. Summary: RAG is powerful when paired with good chunking!\" > rag_intro.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lkJ-P8-50Nhy"
   },
   "outputs": [],
   "source": [
    "!echo -e \"Tomatoes grow best in warm weather with plenty of sun. It's important to water them regularly and use nutrient-rich soil. They are typically planted after the last frost and harvested in late summer.\" > tomatoes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ay-nZLk_J0C4",
    "outputId": "983de5e8-7ee8-434f-c4e2-7e742f97f189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction: RAG stands for Retrieval-Augmented Generation. Why RAG? It improves factual accuracy and adds fresh or private data to LLMs. Chunking: Breaks documents into pieces so they can be embedded. Semantic Chunking: Focus on respecting document structure like sections. Summary: RAG is powerful when paired with good chunking!\n"
     ]
    }
   ],
   "source": [
    "!cat rag_intro.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YmCMs_uU0Qkm",
    "outputId": "55c22d57-c1ff-4628-b410-9ef322820dec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomatoes grow best in warm weather with plenty of sun. It's important to water them regularly and use nutrient-rich soil. They are typically planted after the last frost and harvested in late summer.\n"
     ]
    }
   ],
   "source": [
    "!cat tomatoes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FpiTDDMx0Rx-"
   },
   "outputs": [],
   "source": [
    "!mkdir txt-data\n",
    "!cp rag_intro.txt txt-data/rag_intro.txt\n",
    "!cp tomatoes.txt txt-data/tomatoes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell when you are using Spark NLP on Google Colab\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spark NLP\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoFI66NAdalE"
   },
   "source": [
    "## Partitioning Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nluIcWMbM_rx"
   },
   "source": [
    "Partition Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mWnypHRwXruC",
    "outputId": "a2a8e50b-dcf2-423b-94fe-1c61fa7deda2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                path|             content|                text|              chunks|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|file:/content/txt...|Tomatoes grow bes...|[{NarrativeText, ...|[{document, 0, 19...|\n",
      "|file:/content/txt...|Introduction: RAG...|[{NarrativeText, ...|[{document, 0, 33...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.partition.partition_transformer import *\n",
    "\n",
    "empty_df = spark.createDataFrame([], \"string\").toDF(\"text\")\n",
    "\n",
    "partition_transformer = PartitionTransformer() \\\n",
    "    .setInputCols([\"text\"]) \\\n",
    "    .setContentType(\"text/plain\") \\\n",
    "    .setContentPath(\"./txt-data\") \\\n",
    "    .setOutputCol(\"chunks\") \\\n",
    "    .setChunkingStrategy(\"basic\") \\\n",
    "    .setMaxCharacters(140)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    partition_transformer\n",
    "])\n",
    "\n",
    "pipeline_model = pipeline.fit(empty_df)\n",
    "result_df = pipeline_model.transform(empty_df)\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EFMhyfnc_g1V",
    "outputId": "57befaf7-91af-40b3-acca-9b67c623543a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|chunks                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{document, 0, 198, Tomatoes grow best in warm weather with plenty of sun. It's important to water them regularly and use nutrient-rich soil. They are typically planted after the last frost and harvested in late summer., {paragraph -> 0}, []}]                                                                                                                                     |\n",
      "|[{document, 0, 331, Introduction: RAG stands for Retrieval-Augmented Generation. Why RAG? It improves factual accuracy and adds fresh or private data to LLMs. Chunking: Breaks documents into pieces so they can be embedded. Semantic Chunking: Focus on respecting document structure like sections. Summary: RAG is powerful when paired with good chunking!, {paragraph -> 0}, []}]|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.select(\"chunks\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBNYByJ5Bqq6"
   },
   "source": [
    "RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7LLHf_0BrtQ",
    "outputId": "2e6e3577-044b-4c01-84a2-6d80d73e2a58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_bert_L2_768 download started this may take some time.\n",
      "Approximate size to download 135.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"chunks\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "bert_embeddings = BertEmbeddings.pretrained() \\\n",
    "    .setInputCols([\"chunks\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "sentence_embeddings = SentenceEmbeddings() \\\n",
    "    .setInputCols([\"chunks\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"sentence_embeddings\") \\\n",
    "    .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "finisher = EmbeddingsFinisher().setInputCols([\"sentence_embeddings\"]).setOutputCols([\"finished_sentence_embeddings\"]).setOutputAsVector(True)\n",
    "\n",
    "rag_pipeline = Pipeline(stages=[\n",
    "    partition_transformer,\n",
    "    tokenizer,\n",
    "    bert_embeddings,\n",
    "    sentence_embeddings,\n",
    "    finisher\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sY3fW-93CL2J"
   },
   "source": [
    "Embed a Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "LR0E1EdjCEjS"
   },
   "outputs": [],
   "source": [
    "rag_model = rag_pipeline.fit(empty_df)\n",
    "kb_df = rag_model.transform(empty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKfZCpqLl5WZ",
    "outputId": "38e64d2b-ab95-4beb-e6fb-c4b0ce65d654"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------------+\n",
      "|                path|             content|                text|              chunks|               token|          embeddings| sentence_embeddings|finished_sentence_embeddings|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------------+\n",
      "|file:/content/txt...|Tomatoes grow bes...|[{NarrativeText, ...|[{document, 0, 19...|[{token, 0, 7, To...|[{word_embeddings...|[{sentence_embedd...|        [[0.6935687065124...|\n",
      "|file:/content/txt...|Introduction: RAG...|[{NarrativeText, ...|[{document, 0, 33...|[{token, 0, 11, I...|[{word_embeddings...|[{sentence_embedd...|        [[0.5774036645889...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kb_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IhJqVfU2HJj",
    "outputId": "9ac8bb5f-cc84-40fe-b181-95083baa3c25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|chunks                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{document, 0, 198, Tomatoes grow best in warm weather with plenty of sun. It's important to water them regularly and use nutrient-rich soil. They are typically planted after the last frost and harvested in late summer., {paragraph -> 0}, []}]                                                                                                                                     |\n",
      "|[{document, 0, 331, Introduction: RAG stands for Retrieval-Augmented Generation. Why RAG? It improves factual accuracy and adds fresh or private data to LLMs. Chunking: Breaks documents into pieces so they can be embedded. Semantic Chunking: Focus on respecting document structure like sections. Summary: RAG is powerful when paired with good chunking!, {paragraph -> 0}, []}]|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kb_df.select(\"chunks\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6d9Za6jbdqF"
   },
   "source": [
    "Preparing the output of a Spark NLP RAG pipeline by aligning each chunk of text with its embedding vector,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OZsD7pfZm0br"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import posexplode, monotonically_increasing_id\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "kb_df = kb_df.withColumn(\"doc_id\", monotonically_increasing_id())\n",
    "exploded_chunks = kb_df.selectExpr(\"doc_id\", \"chunks.result as chunks\") \\\n",
    "                       .select(posexplode(\"chunks\").alias(\"pos\", \"chunk_text\"), \"doc_id\")\n",
    "\n",
    "exploded_vectors = kb_df.selectExpr(\"doc_id\", \"finished_sentence_embeddings as vectors\") \\\n",
    "                        .select(posexplode(\"vectors\").alias(\"pos\", \"vector\"), \"doc_id\")\n",
    "\n",
    "aligned_df = exploded_chunks.join(exploded_vectors, on=[\"doc_id\", \"pos\"]).select(\"doc_id\", \"chunk_text\", \"vector\")\n",
    "\n",
    "aligned_df = aligned_df.withColumn(\"vector\", vector_to_array(\"vector\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uMelNiiiHfrU",
    "outputId": "aa123f33-2522-458a-905c-cd66266f25cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "|doc_id|          chunk_text|              vector|\n",
      "+------+--------------------+--------------------+\n",
      "|     0|Tomatoes grow bes...|[0.69356870651245...|\n",
      "|     1|Introduction: RAG...|[0.57740366458892...|\n",
      "+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aligned_df_clean = aligned_df.select(\"doc_id\", \"chunk_text\", \"vector\").cache()\n",
    "aligned_df_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuyM3NdN4ttf"
   },
   "source": [
    "Query Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1HVp-g34z6g",
    "outputId": "b67ec752-65d8-497e-ea90-4eabd72eaadd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_bert_L2_768 download started this may take some time.\n",
      "Approximate size to download 135.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "bert_embeddings = BertEmbeddings.pretrained() \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "sentence_embeddings = SentenceEmbeddings() \\\n",
    "    .setInputCols([\"sentence\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"sentence_embeddings\") \\\n",
    "    .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "query_pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    bert_embeddings,\n",
    "    sentence_embeddings,\n",
    "    finisher\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Numk3cjdoRI3"
   },
   "outputs": [],
   "source": [
    "query = \"What is semantic chunking?\"\n",
    "query_df = spark.createDataFrame([[query]]).toDF(\"text\")\n",
    "query_model = query_pipeline.fit(query_df)\n",
    "# query_model = rag_pipeline.fit(query_df)\n",
    "query_result = query_model.transform(query_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kv_mpg-n4cvi",
    "outputId": "28f4bcfc-3292-4fba-f274-59090bf423ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------------+\n",
      "|                text|            document|            sentence|               token|          embeddings| sentence_embeddings|finished_sentence_embeddings|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------------+\n",
      "|What is semantic ...|[{document, 0, 25...|[{document, 0, 25...|[{token, 0, 3, Wh...|[{word_embeddings...|[{sentence_embedd...|        [[0.3536282181739...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "JqfkKYkXoYd8"
   },
   "outputs": [],
   "source": [
    "query_vector = query_result.select(\"finished_sentence_embeddings\").first()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "LvP5QoaSoEZv"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import FloatType\n",
    "import numpy as np\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    v1, v2 = np.array(vec1), np.array(vec2)\n",
    "    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "\n",
    "# Register UDF\n",
    "cosine_sim_udf = udf(lambda v: cosine_sim(v, query_vector), FloatType())\n",
    "\n",
    "# Add similarity score to each chunk\n",
    "scored_chunks = aligned_df_clean.withColumn(\"similarity\", cosine_sim_udf(col(\"vector\"))) \\\n",
    "                          .orderBy(col(\"similarity\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__Db-4tpJz6N",
    "outputId": "55bf0969-b9fa-4fda-feea-6fd75a9e8804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+----------+\n",
      "|doc_id|          chunk_text|              vector|similarity|\n",
      "+------+--------------------+--------------------+----------+\n",
      "|     1|Introduction: RAG...|[0.57740366458892...|0.61944675|\n",
      "|     0|Tomatoes grow bes...|[0.69356870651245...| 0.2762234|\n",
      "+------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scored_chunks.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
