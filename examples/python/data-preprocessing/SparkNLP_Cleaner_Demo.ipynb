{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/reader/SparkNLP_Cleaner_Demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b585db2-ed1b-4417-b38a-033812c206c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "tzcU5p2gdak9"
   },
   "source": [
    "# Introducing Cleaner in SparkNLP\n",
    "This notebook showcases the newly added  `Cleaner()` annotator in Spark NLP to remove unnecessary or undesirable content from datasets, such as bullets, dashes, and non-ASCII characters, enhancing data consistency and readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68382b5d-51f1-44fc-a913-16b92e44d1ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DczWop6QeE8F",
    "outputId": "ac97c962-bad5-4d71-d823-da1c67580219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n",
      "Apache Spark version: 3.4.1\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "# let's start Spark with Spark NLP\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Apache Spark version: {}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c84cecef-45dc-4169-986c-30c9a6e42377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "RFOFhaEedalB"
   },
   "source": [
    "## Setup and Initialization\n",
    "Let's keep in mind a few things before we start ðŸ˜Š\n",
    "\n",
    "Support for reading html files was introduced in Spark NLP 6.0.0. Please make sure you have upgraded to the latest Spark NLP release.\n",
    "We simple need to import the cleaners components to use `Cleaner` annotator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "596ffcc0-90fb-4bfd-8840-88be66f7bb6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "stirVdLP-ASE"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator.cleaners import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c528b73-797c-40fe-a0a9-5e9b1d72f4fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "EoFI66NAdalE"
   },
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a29210a-143a-4fcd-a62f-9b3403f8d3c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "BjAsd5Gs8drv"
   },
   "source": [
    "Clean a string with bytes to output a string with human visible characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f27952c-611f-47c7-8d7a-6e9075270eea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "bAkMjJ1vdalE"
   },
   "outputs": [],
   "source": [
    "data = \"Hello Ã°\\\\x9f\\\\x98\\\\x80\"\n",
    "data_set = spark.createDataFrame([[data]]).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bd0e20a-aae0-46fe-89e0-b4020b7f618d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnxOTj_Uf3a0",
    "outputId": "cc841020-4e5e-4b64-e6fc-ed82cee5dce3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|cleaned                          |\n",
      "+---------------------------------+\n",
      "|[{chunk, 0, 8, Hello ðŸ˜€, {}, []}]|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "\n",
    "cleaner = Cleaner() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"cleaned\") \\\n",
    "    .setCleanerMode(\"bytes_string_to_string\")\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    document_assembler,\n",
    "    cleaner\n",
    "])\n",
    "\n",
    "model = pipeline.fit(data_set)\n",
    "result = model.transform(data_set)\n",
    "result.select(\"cleaned\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a729dcd7-b8bf-4356-96e2-199c0576dd5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dpohooB0_yOa"
   },
   "source": [
    "Cleaning special characters from a screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bcd1ac3-8b9e-4b8c-84f6-031753f3e205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "OC_PElzuAKZw"
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"â— An excellent point!\",\n",
    "    \"ITEM 1A:     RISK-FACTORS\"\n",
    "]\n",
    "\n",
    "data_set = spark.createDataFrame(data, \"string\").toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f78cc6f-95bc-434e-af72-f315c8f531a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ESl4yUL_2WR",
    "outputId": "a22fa5dd-09d8-4b40-e84e-adc5cc047696"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|cleaned                                        |\n",
      "+-----------------------------------------------+\n",
      "|[{chunk, 0, 19, An excellent point!, {}, []}]  |\n",
      "|[{chunk, 0, 21, ITEM 1A: RISK FACTORS, {}, []}]|\n",
      "+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaner = Cleaner() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"cleaned\") \\\n",
    "    .setCleanerMode(\"clean\") \\\n",
    "    .setBullets(True) \\\n",
    "    .setExtraWhitespace(True) \\\n",
    "    .setDashes(True)\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    document_assembler,\n",
    "    cleaner\n",
    "])\n",
    "\n",
    "model = pipeline.fit(data_set)\n",
    "result = model.transform(data_set)\n",
    "result.select(\"cleaned\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3676aa02-945e-486d-8026-0f56a2ecb0ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Hqm_ttjEAUaH"
   },
   "source": [
    "Clean non-ascii characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d502021d-9668-4bb6-9b3a-262c9958aea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "WB0bI47xAlIr"
   },
   "outputs": [],
   "source": [
    "data = [\"\\\\x88This text contains Â®non-ascii characters!â—\"]\n",
    "data_set = spark.createDataFrame(data, \"string\").toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ef30555-9c58-492c-af19-94a4062514b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YykeYZltAXQX",
    "outputId": "edd53be2-df90-4e77-dd18-930fcadfe5d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|cleaned                                                           |\n",
      "+------------------------------------------------------------------+\n",
      "|[{chunk, 0, 40, This text contains non-ascii characters!, {}, []}]|\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaner = Cleaner() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"cleaned\") \\\n",
    "    .setCleanerMode(\"clean_non_ascii_chars\")\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    document_assembler,\n",
    "    cleaner\n",
    "])\n",
    "\n",
    "model = pipeline.fit(data_set)\n",
    "result = model.transform(data_set)\n",
    "result.select(\"cleaned\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a0c55c6-6ce7-4673-aac8-81ad8fc341ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "YPeqQL-UA17w"
   },
   "source": [
    "Cleaning alphanumeric bullets from the beginning of a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4b6bb1a-31e7-4a54-b887-5ebb44afbee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "10_a1O9cA4Tk"
   },
   "outputs": [],
   "source": [
    "data = [(\"1.1 This is a very important point\",),\n",
    "        (\"a.1 This is a very important point\",),\n",
    "        (\"1.4.2 This is a very important point\",)]\n",
    "\n",
    "data_set = spark.createDataFrame(data).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52d71553-6f73-4eb9-9621-3e222ce10490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JbOmybPLA_nV",
    "outputId": "e53a283d-c4c4-471d-b1ef-0df2cf9bc9d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+\n",
      "|cleaned                                                 |\n",
      "+--------------------------------------------------------+\n",
      "|[{chunk, 0, 30, This is a very important point, {}, []}]|\n",
      "|[{chunk, 0, 30, This is a very important point, {}, []}]|\n",
      "|[{chunk, 0, 30, This is a very important point, {}, []}]|\n",
      "+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaner = Cleaner() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"cleaned\") \\\n",
    "    .setCleanerMode(\"clean_ordered_bullets\")\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    document_assembler,\n",
    "    cleaner\n",
    "])\n",
    "\n",
    "model = pipeline.fit(data_set)\n",
    "result = model.transform(data_set)\n",
    "result.select(\"cleaned\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a297a9ad-c715-4360-b85d-2183a08b6d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "EV4Wpr_qBFm1"
   },
   "source": [
    "Clean postfix from a text based on a pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6647dcf-e29b-48a3-81c3-b135b4e07950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "UQxqmsFgBTw7"
   },
   "outputs": [],
   "source": [
    "data = [\"The end! END\"]\n",
    "\n",
    "data_set = spark.createDataFrame(data, \"string\").toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0359b9bd-b3d0-4eb3-8d0a-a5c2f72a04c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AK_kwa4SBHZL",
    "outputId": "a50cef0f-8be6-4139-8dad-52ffc4933322"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|cleaned                          |\n",
      "+---------------------------------+\n",
      "|[{chunk, 0, 8, The end!, {}, []}]|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaner = Cleaner() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"cleaned\") \\\n",
    "    .setCleanerMode(\"clean_postfix\") \\\n",
    "    .setCleanPrefixPattern(\"(END|STOP)\")\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    document_assembler,\n",
    "    cleaner\n",
    "])\n",
    "\n",
    "model = pipeline.fit(data_set)\n",
    "result = model.transform(data_set)\n",
    "result.select(\"cleaned\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff4acecb-0da2-43c2-a911-def8ddade7da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "w9bBC9ebBgvi"
   },
   "source": [
    "Clean prefix from a text based on a pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "100dd0cd-9430-4c27-aaf6-f0efa79b8328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "nDfwOWkEBjv4"
   },
   "outputs": [],
   "source": [
    "data = [\"SUMMARY: This is the best summary of all time!\"]\n",
    "\n",
    "data_set = spark.createDataFrame(data, \"string\").toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12358399-be76-4312-9a74-b34752b07dc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qaVxWBT-C9eS",
    "outputId": "73bb7cb7-36d1-4168-9f3f-adecbb61b615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|cleaned                                                        |\n",
      "+---------------------------------------------------------------+\n",
      "|[{chunk, 0, 37, This is the best summary of all time!, {}, []}]|\n",
      "+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaner = Cleaner() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"cleaned\") \\\n",
    "    .setCleanerMode(\"clean_prefix\") \\\n",
    "    .setCleanPrefixPattern(\"(SUMMARY|DESCRIPTION):\")\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    document_assembler,\n",
    "    cleaner\n",
    "])\n",
    "\n",
    "model = pipeline.fit(data_set)\n",
    "result = model.transform(data_set)\n",
    "result.select(\"cleaned\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c0b2fa-e0c1-4b99-ba32-3b736de782a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ZJBz2_ZTGL82"
   },
   "source": [
    "Cleaning unicode characters from a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4652c85c-56de-4f2c-8586-905bd792d20c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "iGZEspw1GR6Q"
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"\\x93A lovely quote!\\x94\",\n",
    "    \"\\x91A lovely quote!\\x92\",\n",
    "    \"\"\"\\u201CA lovely quote!\\u201D â€” with a dash\"\"\"\n",
    "]\n",
    "\n",
    "data_set = spark.createDataFrame(data, \"string\").toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fa14989-2657-4b52-9b29-e6f6aa340a02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mm0FrFtBGqBQ",
    "outputId": "49697b57-2fa7-4407-93d3-6bb1e7aa2941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+\n",
      "|cleaned                                                  |\n",
      "+---------------------------------------------------------+\n",
      "|[{chunk, 0, 17, â€œA lovely quote!â€, {}, []}]              |\n",
      "|[{chunk, 0, 17, â€˜A lovely quote!â€™, {}, []}]              |\n",
      "|[{chunk, 0, 31, ?A lovely quote!? ? with a dash, {}, []}]|\n",
      "+---------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaner = Cleaner() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"cleaned\") \\\n",
    "    .setCleanerMode(\"replace_unicode_characters\")\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    document_assembler,\n",
    "    cleaner\n",
    "])\n",
    "\n",
    "model = pipeline.fit(data_set)\n",
    "result = model.transform(data_set)\n",
    "result.select(\"cleaned\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d945d2d-c426-49ce-b755-12f0c497c38e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NdV4paKp6fwM"
   },
   "source": [
    "### Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b882f749-fc63-498a-a111-efae9455b12f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "UGMZ5puuKzcP"
   },
   "source": [
    "You can use `Cleaner` annotator to even translate a text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5119cc76-cc42-475c-b8f5-26b5811e0596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "7GuykSrsK04V"
   },
   "outputs": [],
   "source": [
    "data = [\"This should go to French\"]\n",
    "data_set = spark.createDataFrame(data, \"string\").toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1342379-a8a3-4ffc-bc9c-64a3a36d6504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yX1no37ALAPO",
    "outputId": "9b1cf6c0-2640-474a-a933-1428c1ae40c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opus_mt_en_fr download started this may take some time.\n",
      "Approximate size to download 378.7 MB\n",
      "\r",
      "[ | ]\r",
      "[ / ]\r",
      "[ â€” ]\r",
      "[ \\ ]\r",
      "[ | ]\r",
      "[ / ]\r",
      "[ â€” ]\r",
      "[ \\ ]\r",
      "[ | ]\r",
      "[ / ]\r",
      "[ â€” ]\r",
      "[ \\ ]\r",
      "[ | ]\r",
      "[ / ]\r",
      "[ â€” ]\r",
      "[ \\ ]\r",
      "[ | ]\r",
      "[ / ]\r",
      "[ â€” ]\r",
      "[ \\ ]\r",
      "[ | ]\r",
      "[ / ]\r",
      "[ â€” ]\r",
      "[ \\ ]\r",
      "[ | ]\r",
      "[OK!]\n",
      "+-----------------------------------------------------------------------+\n",
      "|cleaned                                                                |\n",
      "+-----------------------------------------------------------------------+\n",
      "|[{document, 0, 28, Ã‡a devrait aller en franÃ§ais., {sentence -> 0}, []}]|\n",
      "+-----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaner = Cleaner() \\\n",
    "    .pretrained() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"cleaned\")\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    document_assembler,\n",
    "    cleaner\n",
    "])\n",
    "\n",
    "model = pipeline.fit(data_set)\n",
    "result = model.transform(data_set)\n",
    "result.select(\"cleaned\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SparkNLP_Cleaner_Demo",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
