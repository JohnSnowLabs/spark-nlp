{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cac0728",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/llama.cpp/GGUFRankingFinisher_for_AutoGGUFReranker.ipynb)\n",
    "\n",
    "# GGUFRankingFinisher for AutoGGUFReranker\n",
    "\n",
    "This notebook will show you how to use the `GGUFRankingFinisher` to post-process the relevance scores produced by the AutoGGUFReranker.\n",
    "\n",
    "Let's keep in mind a few things before we start ðŸ˜Š\n",
    "\n",
    "- `AutoGGUFReranker` was introduced in `Spark NLP 6.1.2`, enabling efficient and quantized reranking of documents with LLMs. Please make sure you have upgraded to the latest Spark NLP release.\n",
    "- `GGUFRankingFinisher` was introduced in `Spark NLP 6.1.3`, to post-process the document rankings\n",
    "\n",
    "`GGUFRankingFinisher` for `AutoGGUFReranker` outputs that provides ranking capabilities\n",
    "including top-k selection, sorting by relevance score, and score normalization.\n",
    "\n",
    "This finisher processes the output of AutoGGUFReranker, which contains documents with\n",
    "relevance scores in their metadata. It provides several options for post-processing:\n",
    "\n",
    "- Top-k selection: Select only the top k documents by relevance score\n",
    "- Score thresholding: Filter documents by minimum relevance score\n",
    "- Min-max scaling: Normalize relevance scores to 0-1 range\n",
    "- Sorting: Sort documents by relevance score in descending order\n",
    "- Ranking: Add rank information to document metadata\n",
    "\n",
    "The finisher preserves the document annotation structure while adding ranking information\n",
    "to the metadata and optionally filtering/sorting the documents.\n",
    "\n",
    "## Spark NLP Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8568c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only execute this if you are on Google Colab\n",
    "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631ac47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.1.3\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "# let's start Spark with Spark NLP with GPU enabled. If you don't have GPUs available remove this parameter.\n",
    "spark = sparknlp.start(gpu=True)\n",
    "print(sparknlp.version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c81114",
   "metadata": {},
   "source": [
    "## Producing Document Rankings\n",
    "\n",
    "Let's start by producing some document ranking. We first define a suitable pipeline and run it on some data. The relevance scores will then be in the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d781eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 'libjllama.so' to '/tmp/libjllama.so'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no                                      \n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3070, compute capability 8.6, VMM: yes\n",
      "[Stage 1:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-------------------------------------------+\n",
      "|result                           |reranked_document.metadata[relevance_score]|\n",
      "+---------------------------------+-------------------------------------------+\n",
      "|A man is eating food.            |7.023443                                   |\n",
      "|A man is eating a piece of bread.|2.1200795                                  |\n",
      "|The girl is carrying a baby.     |-10.790537                                 |\n",
      "|A man is riding a horse.         |-8.433026                                  |\n",
      "|A young girl is playing violin.  |-10.778883                                 |\n",
      "+---------------------------------+-------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "\n",
    "auto_gguf_model = (\n",
    "    AutoGGUFReranker.loadSavedModel(\n",
    "        \"/home/ducha/Workspace/scala/spark-nlp-release/tmp_autogguf_reranker/bge-reranker-v2-m3-q4_k_m.gguf\",\n",
    "        spark,\n",
    "    )\n",
    "    .setInputCols(\"document\")\n",
    "    .setOutputCol(\"reranked_documents\")\n",
    "    .setQuery(\"A man is eating pasta.\")\n",
    "    .setDisableLog(True)\n",
    ")\n",
    "\n",
    "pipeline = Pipeline().setStages([document_assembler, auto_gguf_model])\n",
    "\n",
    "data = spark.createDataFrame(\n",
    "    [\n",
    "        [\"A man is eating food.\"],\n",
    "        [\"A man is eating a piece of bread.\"],\n",
    "        [\"The girl is carrying a baby.\"],\n",
    "        [\"A man is riding a horse.\"],\n",
    "        [\"A young girl is playing violin.\"],\n",
    "    ]\n",
    ").toDF(\"text\")\n",
    "\n",
    "result = pipeline.fit(data).transform(data)\n",
    "\n",
    "\n",
    "# Verify results contain relevance scores\n",
    "result.selectExpr(\"explode(reranked_documents) as reranked_document\").selectExpr(\n",
    "    \"reranked_document.result\", \"reranked_document.metadata['relevance_score']\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c3bdd0",
   "metadata": {},
   "source": [
    "# Post-Processing Ranking with `GGUFRankingFinisher`\n",
    "\n",
    "Let's now use the `GGUFRankingFinisher` to post-process and sort our results. For this the annotator will\n",
    "\n",
    "1. automatically sort\n",
    "2. only choose the top 3 results\n",
    "3. scale the relevance scores to be between $[0, 1]$, available as `scaled_score` in the metadata\n",
    "4. set a minimum relevance score after rescaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e9051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "\n",
    "finisher = (\n",
    "    GGUFRankingFinisher()\n",
    "    .setInputCols(\"reranked_documents\")\n",
    "    .setOutputCol(\"finished_reranked_documents\")\n",
    "    .setTopK(3)\n",
    "    .setMinRelevanceScore(0.3)\n",
    "    .setMinMaxScaling(True)\n",
    ")\n",
    "finisher_result = finisher.transform(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4836a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|finished_reranked_documents                                                                                                                                 |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{document, 0, 20, A man is eating food., {sentence -> 0, query -> A man is eating pasta., relevance_score -> 1.0000162790005285, rank -> 1}, []}            |\n",
      "|{document, 0, 32, A man is eating a piece of bread., {sentence -> 0, query -> A man is eating pasta., relevance_score -> 0.7246697769085113, rank -> 2}, []}|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finisher_result.selectExpr(\n",
    "    \"explode(finished_reranked_documents) as finished_reranked_documents\"\n",
    ").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
