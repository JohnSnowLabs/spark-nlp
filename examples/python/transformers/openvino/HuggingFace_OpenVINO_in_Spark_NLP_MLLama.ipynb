{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/openvino/HuggingFace_OpenVINO_in_Spark_NLP_MLLama.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import OpenVINO MLLama models from HuggingFace 🤗 into Spark NLP 🚀\n",
    "\n",
    "This notebook provides a detailed walkthrough on optimizing and importing MLLama models from HuggingFace  for use in Spark NLP, with [Intel OpenVINO toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html). The focus is on converting the model to the OpenVINO format and applying precision optimizations (INT8 and INT4), to enhance the performance and efficiency on CPU platforms using [Optimum Intel](https://huggingface.co/docs/optimum/main/en/intel/inference).\n",
    "\n",
    "Let's keep in mind a few things before we start 😊\n",
    "\n",
    "- OpenVINO support was introduced in  `Spark NLP 5.4.0`, enabling high performance CPU inference for models. So please make sure you have upgraded to the latest Spark NLP release.\n",
    "- Model quantization is a computationally expensive process, so it is recommended to use a runtime with more than 32GB memory for exporting the quantized model from HuggingFace.\n",
    "- You can import MLLama models via `MLLama`. These models are usually under `Text Generation` category and have `MLLama` in their labels.\n",
    "- Reference: [MLLama](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md)\n",
    "- Some [example models](https://huggingface.co/models?search=MLLama)\n",
    "- Openvino export taken from [Openvino Notebooks](https://github.com/openvinotoolkit/openvino_notebooks/tree/b4a0791/notebooks/mllama-3.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Export and Save the HuggingFace model\n",
    "\n",
    "- Let's install `transformers` and `openvino` packages with other dependencies. You don't need `openvino` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
    "- We lock `transformers` on version `4.41.2`. This doesn't mean it won't work with the future release, but we wanted you to know which versions have been tested successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"torch>=2.1\" \"torchvision\" \"Pillow\" \"tqdm\" \"datasets>=2.14.6\" \"gradio>=4.36\" \"nncf>=2.14.0\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q \"transformers>=4.45\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -Uq \"openvino>=2024.5.0\"\n",
    "%pip install -q --upgrade ipywidgets\n",
    "\n",
    "utility_files = [\"notebook_utils.py\", \"cmd_helper.py\"]\n",
    "\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(\"ov_mllama_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/b4a0791/notebooks/mllama-3.2/ov_mllama_helper.py\")\n",
    "    open(\"ov_mllama_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "if not Path(\"gradio_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/b4a0791/notebooks/mllama-3.2/gradio_helper.py\")\n",
    "    open(\"gradio_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "if not Path(\"ov_mllama_compression.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/b4a0791/notebooks/mllama-3.2/ov_mllama_compression.py\")\n",
    "    open(\"ov_mllama_compression.py\", \"w\").write(r.text)\n",
    "\n",
    "if not Path(\"data_preprocessing.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/b4a0791/notebooks/mllama-3.2/data_preprocessing.py\")\n",
    "    open(\"data_preprocessing\", \"w\").write(r.text)\n",
    "\n",
    "if not Path(\"notebook_utils.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/b4a0791/utils/notebook_utils.py\")\n",
    "    open(\"notebook_utils.py\", \"w\").write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Convert the model to OpenVino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prabod/anaconda3/envs/mllama/lib/python3.9/importlib/util.py:245: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  self.__spec__.loader.exec_module(self)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from ov_mllama_helper import convert_mllama\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "model_dir = Path(model_id.split(\"/\")[-1]) / \"OV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc77113413684c39ba2b488d2504c7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "device = device_widget(\"CPU\", exclude=[\"NPU\"])\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('Llama-3.2-11B-Vision-Instruct/OV')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_mllama(model_id, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ov_mllama_compression import compress\n",
    "from ov_mllama_compression import compression_widgets_helper\n",
    "\n",
    "compression_scenario, compress_args = compression_widgets_helper()\n",
    "\n",
    "compression_scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_kwargs = {key: value.value for key, value in compress_args.items()}\n",
    "\n",
    "language_model_path = compress(model_dir, **compression_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ov_mllama_compression import vision_encoder_selection_widget\n",
    "\n",
    "vision_encoder_options = vision_encoder_selection_widget(device.value)\n",
    "\n",
    "vision_encoder_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "import nncf\n",
    "import openvino as ov\n",
    "import gc\n",
    "\n",
    "from data_preprocessing import prepare_dataset_vision\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_dir)\n",
    "core = ov.Core()\n",
    "\n",
    "fp_vision_encoder_path = model_dir / \"openvino_vision_encoder.xml\"\n",
    "int8_vision_encoder_path = model_dir / fp_vision_encoder_path.name.replace(\".xml\", \"_int8.xml\")\n",
    "int8_wc_vision_encoder_path = model_dir / fp_vision_encoder_path.name.replace(\".xml\", \"_int8_wc.xml\")\n",
    "\n",
    "\n",
    "if vision_encoder_options.value == \"INT8 quantization\":\n",
    "    if not int8_vision_encoder_path.exists():\n",
    "        calibration_data = prepare_dataset_vision(processor, 100)\n",
    "        ov_model = core.read_model(fp_vision_encoder_path)\n",
    "        calibration_dataset = nncf.Dataset(calibration_data)\n",
    "        quantized_model = nncf.quantize(\n",
    "            model=ov_model,\n",
    "            calibration_dataset=calibration_dataset,\n",
    "            model_type=nncf.ModelType.TRANSFORMER,\n",
    "            advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=0.6),\n",
    "        )\n",
    "        ov.save_model(quantized_model, int8_vision_encoder_path)\n",
    "        del quantized_model\n",
    "        del ov_model\n",
    "        del calibration_dataset\n",
    "        del calibration_data\n",
    "        gc.collect()\n",
    "\n",
    "    vision_encoder_path = int8_vision_encoder_path\n",
    "elif vision_encoder_options.value == \"INT8 weights compression\":\n",
    "    if not int8_wc_vision_encoder_path.exists():\n",
    "        ov_model = core.read_model(fp_vision_encoder_path)\n",
    "        compressed_model = nncf.compress_weights(ov_model)\n",
    "        ov.save_model(compressed_model, int8_wc_vision_encoder_path)\n",
    "    vision_encoder_path = int8_wc_vision_encoder_path\n",
    "else:\n",
    "    vision_encoder_path = fp_vision_encoder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "question = \"What is unusual on this image?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": question}]},\n",
    "]\n",
    "text = processor.tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "url = \"https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/d5fbbd1a-d484-415c-88cb-9986625b7b11\"\n",
    "raw_image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=text, images=[raw_image], return_tensors=\"pt\")\n",
    "\n",
    "pixel_values = inputs[\"pixel_values\"]\n",
    "aspect_ratio_ids = inputs[\"aspect_ratio_ids\"]\n",
    "aspect_ratio_mask = inputs[\"aspect_ratio_mask\"]\n",
    "\n",
    "image_inputs = {\n",
    "    \"pixel_values\": pixel_values,\n",
    "    \"aspect_ratio_ids\": aspect_ratio_ids,\n",
    "    \"aspect_ratio_mask\": aspect_ratio_mask,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "from pathlib import Path\n",
    "core = ov.Core()\n",
    "\n",
    "IMAGE_ENCODER_NAME = \"openvino_vision_encoder.xml\"\n",
    "\n",
    "image_encoder = core.compile_model(model_path / IMAGE_ENCODER_NAME,\"CPU\")\n",
    "cross_attn_outputs = [key.get_any_name() for key in image_encoder.outputs if \"cross_attn_key_values\" in key.get_any_name()]\n",
    "\n",
    "\n",
    "image_request = image_encoder.create_infer_request()\n",
    "image_request.start_async([pixel_values, aspect_ratio_ids, aspect_ratio_mask], share_inputs=True)\n",
    "image_request.wait()\n",
    "cross_attn_key_values = [image_request.get_tensor(name) for name in cross_attn_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class PreprocessingMasks(torch.nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        cross_attention_mask,\n",
    "        attention_mask,\n",
    "        current_input_ids,\n",
    "        num_vision_tokens,\n",
    "        past_cross_attn_kv_length\n",
    "    ):\n",
    "        dtype=torch.float32\n",
    "        batch_size, text_total_length, *_ = cross_attention_mask.shape\n",
    "        cross_attention_mask = cross_attention_mask.repeat_interleave(num_vision_tokens, dim=3)\n",
    "        cross_attention_mask = cross_attention_mask.view(batch_size, text_total_length, -1)\n",
    "        cross_attention_mask = cross_attention_mask.unsqueeze(1)\n",
    "\n",
    "        inverted_cross_attn_mask = (1.0 - cross_attention_mask).to(dtype)\n",
    "        cross_attention_mask = inverted_cross_attn_mask.masked_fill(inverted_cross_attn_mask.to(torch.bool), torch.finfo(dtype).min)\n",
    "\n",
    "        # apply full-row bias, which return 4D tensor of shape [B, H, S1, 1] where value is 0 if the a full row in cross attn mask's\n",
    "        # last dimension contains negative infinity values, otherwise it's 1\n",
    "        negative_inf_value = torch.finfo(dtype).min\n",
    "        full_text_row_masked_out_mask = (cross_attention_mask != negative_inf_value).any(dim=-1).type_as(cross_attention_mask)[..., None]\n",
    "        cross_attention_mask *= full_text_row_masked_out_mask\n",
    "\n",
    "        # if first_pass > 0:\n",
    "        # past_cross_attn_kv_length = cross_attn_key_values[0].shape[-2]\n",
    "        past_cross_attn_mask = torch.zeros((*cross_attention_mask.shape[:-1], past_cross_attn_kv_length), dtype=dtype)\n",
    "        # concatenate both on image-seq-length dimension\n",
    "        cross_attention_mask_second_pass = torch.cat([past_cross_attn_mask, cross_attention_mask], dim=-1)\n",
    "        cache_position = (attention_mask.long().cumsum(-1) - 1)[:, -current_input_ids.shape[1] :][0]\n",
    "\n",
    "        cross_attention_mask_second_pass = cross_attention_mask_second_pass[:, :, cache_position]\n",
    "\n",
    "        cross_attention_mask = cross_attention_mask[:, :, cache_position]\n",
    "        full_text_row_masked_out_mask = full_text_row_masked_out_mask[:, :, cache_position]\n",
    "\n",
    "        return {\n",
    "            \"cache_position\": cache_position.to(torch.int32),\n",
    "            \"cross_attention_mask_first_pass\": cross_attention_mask.to(dtype),\n",
    "            \"cross_attention_mask_second_pass\": cross_attention_mask_second_pass.to(dtype),\n",
    "            \"full_text_row_masked_out_mask\": full_text_row_masked_out_mask.to(dtype),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_masks = PreprocessingMasks()\n",
    "cross_attention_mask = inputs[\"cross_attention_mask\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "current_input_ids = inputs[\"input_ids\"]\n",
    "first_pass = torch.tensor(1)\n",
    "num_vision_tokens = torch.tensor((config.vision_config.image_size // config.vision_config.patch_size) ** 2 + 1)\n",
    "past_cross_attn_kv_length = torch.tensor(cross_attn_key_values[0].shape[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "\n",
    "ov_model_preprocessing_masks = ov.convert_model(\n",
    "    preprocessing_masks,\n",
    "    example_input={\n",
    "        \"cross_attention_mask\": cross_attention_mask,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"current_input_ids\": current_input_ids,\n",
    "        \"num_vision_tokens\": num_vision_tokens,\n",
    "        \"past_cross_attn_kv_length\": past_cross_attn_kv_length,\n",
    "    }\n",
    ")\n",
    "\n",
    "ov.save_model(ov_model_preprocessing_masks,model_path/\"openvino_reshape_model.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load openvino models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE_MODEL_NAME = \"llm_int4_asym_r10_gs64_max_activation_variance_awq_scale_all_layers.xml\"\n",
    "LANGUAGE_MODEL_NAME_1 = \"openvino_language_model.xml\"\n",
    "IMAGE_ENCODER_NAME = \"openvino_vision_encoder.xml\"\n",
    "PREPROCESSING_MASKS_NAME = \"openvino_reshape_model.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "import gc\n",
    "\n",
    "core = ov.Core()\n",
    "model_path = model_dir\n",
    "\n",
    "language_model = core.read_model(model_path / LANGUAGE_MODEL_NAME)\n",
    "compiled_language_model = core.compile_model(language_model, \"CPU\")\n",
    "request = compiled_language_model.create_infer_request()\n",
    "\n",
    "image_encoder = core.compile_model(model_path / IMAGE_ENCODER_NAME,\"CPU\")\n",
    "preprocessing_masks = core.compile_model(model_path / PREPROCESSING_MASKS_NAME,\"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⌛ Check if all models are converted\n",
      "✅ All models are converted. You can find results in /mnt/research/Projects/ModelZoo/LLAMA-3.2-VI/Llama-3.2-11B-Vision-Instruct/OV\n"
     ]
    }
   ],
   "source": [
    "# check if all the models are converted\n",
    "\n",
    "print(\"⌛ Check if all models are converted\")\n",
    "language_model_path = model_dir / LANGUAGE_MODEL_NAME\n",
    "# language_model_path_1 = model_dir / LANGUAGE_MODEL_NAME_1\n",
    "image_encoder_path = model_dir / IMAGE_ENCODER_NAME\n",
    "preprocessing_masks_path = model_dir / PREPROCESSING_MASKS_NAME\n",
    "\n",
    "if all(\n",
    "    [\n",
    "        language_model_path.exists(),\n",
    "        # language_model_path_1.exists(),\n",
    "        image_encoder_path.exists(),\n",
    "        preprocessing_masks_path.exists(),\n",
    "    ]\n",
    "):\n",
    "    print(f\"✅ All models are converted. You can find results in {model_dir}\")\n",
    "else:\n",
    "    print(\"❌ Not all models are converted. Please check the conversion process\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Copy assets to the assets folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_dir = model_dir / \"assets\"\n",
    "assets_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# copy all the assets to the assets directory (json files, vocab files, etc.)\n",
    "\n",
    "import shutil\n",
    "\n",
    "# copy all json files\n",
    "\n",
    "for file in model_dir.glob(\"*.json\"):\n",
    "    shutil.copy(file, assets_dir)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 31G\n",
      "drwxrwxr-x 2 prabod prabod 4.0K Jan 15 03:09 assets\n",
      "-rw-rw-r-- 1 prabod prabod 5.0K Dec 12 01:53 chat_template.json\n",
      "-rw-rw-r-- 1 prabod prabod 5.0K Jan 15 03:06 config.json\n",
      "-rw-rw-r-- 1 prabod prabod  210 Dec 12 01:53 generation_config.json\n",
      "-rw-rw-r-- 1 prabod prabod 4.9G Jan 23 01:10 llm_int4_asym_r10_gs64_max_activation_variance_all_layers.bin\n",
      "-rw-rw-r-- 1 prabod prabod 3.9M Jan 23 01:10 llm_int4_asym_r10_gs64_max_activation_variance_all_layers.xml\n",
      "-rw-rw-r-- 1 prabod prabod 4.9G Dec 12 04:28 llm_int4_asym_r10_gs64_max_activation_variance_awq_scale_all_layers.bin\n",
      "-rw-rw-r-- 1 prabod prabod 3.9M Dec 12 04:28 llm_int4_asym_r10_gs64_max_activation_variance_awq_scale_all_layers.xml\n",
      "-rw-rw-r-- 1 prabod prabod  19G Dec 12 01:55 openvino_language_model.bin\n",
      "-rw-rw-r-- 1 prabod prabod 3.0M Dec 12 01:55 openvino_language_model.xml\n",
      "-rw-rw-r-- 1 prabod prabod   92 Jan 22 05:14 openvino_reshape_model.bin\n",
      "-rw-rw-r-- 1 prabod prabod  37K Jan 22 05:14 openvino_reshape_model.xml\n",
      "-rw-rw-r-- 1 prabod prabod 1.8G Dec 12 01:54 openvino_vision_encoder.bin\n",
      "-rw-rw-r-- 1 prabod prabod 924M Dec 12 08:15 openvino_vision_encoder_int8.bin\n",
      "-rw-rw-r-- 1 prabod prabod 2.5M Dec 12 08:15 openvino_vision_encoder_int8.xml\n",
      "-rw-rw-r-- 1 prabod prabod 1.6M Dec 12 01:54 openvino_vision_encoder.xml\n",
      "-rw-rw-r-- 1 prabod prabod   92 Jan 13 07:25 preprocessing_masks.bin\n",
      "-rw-rw-r-- 1 prabod prabod  37K Jan 13 07:25 preprocessing_masks.xml\n",
      "-rw-rw-r-- 1 prabod prabod  477 Dec 12 01:53 preprocessor_config.json\n",
      "-rw-rw-r-- 1 prabod prabod  454 Dec 12 01:53 special_tokens_map.json\n",
      "-rw-rw-r-- 1 prabod prabod  55K Dec 12 01:53 tokenizer_config.json\n",
      "-rw-rw-r-- 1 prabod prabod  17M Dec 12 01:53 tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 17M\n",
      "-rw-rw-r-- 1 prabod prabod 5.0K Jan 14 08:10  chat_template.json\n",
      "-rw-rw-r-- 1 prabod prabod 5.0K Jan 15 03:09 'config copy.json'\n",
      "-rw-rw-r-- 1 prabod prabod 5.0K Jan 15 03:09  config.json\n",
      "-rw-rw-r-- 1 prabod prabod  210 Jan 14 08:10  generation_config.json\n",
      "-rw-rw-r-- 1 prabod prabod  477 Jan 14 08:10  preprocessor_config.json\n",
      "-rw-rw-r-- 1 prabod prabod  454 Jan 14 08:10  special_tokens_map.json\n",
      "-rw-rw-r-- 1 prabod prabod  55K Jan 14 08:10  tokenizer_config.json\n",
      "-rw-rw-r-- 1 prabod prabod  17M Jan 14 08:10  tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {model_dir / \"assets\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import and Save MLLama in Spark NLP\n",
    "\n",
    "- Let's install and setup Spark NLP in Google Colab\n",
    "- This part is pretty easy via our simple script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start Spark with Spark NLP included via our simple `start()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/07 09:56:55 WARN Utils: Your hostname, minotaur resolves to a loopback address: 127.0.1.1; using 192.168.1.4 instead (on interface eno1)\n",
      "24/11/07 09:56:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/11/07 09:56:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "# let's start Spark with Spark NLP\n",
    "spark = sparknlp.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/14 02:49:23 WARN NativeLibrary: Failed to load library null: java.lang.UnsatisfiedLinkError: Can't load library: /tmp/openvino-native8030791226413631526/libtbb.so.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/prabod/spark/jars/spark-core_2.12-3.3.2.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "imageClassifier = MLLamaForMultimodal.loadSavedModel(str(model_path),spark) \\\n",
    "            .setInputCols(\"image_assembler\") \\\n",
    "            .setOutputCol(\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "imageClassifier.write().overwrite().save(\"file:///tmp/MLLama_spark_nlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6.8G\n",
      "drwxr-xr-x  4 prabod prabod 4.0K Feb 14 02:51 .\n",
      "drwxr-xr-x 13 prabod root   4.0K Feb 14 02:50 ..\n",
      "drwxr-xr-x  6 prabod prabod 4.0K Feb 14 02:50 fields\n",
      "-rw-r--r--  1 prabod prabod 4.9G Feb 14 02:51 llm_int4_asym_r10_gs64_max_activation_variance_awq_scale_all_layers.xml\n",
      "-rw-r--r--  1 prabod prabod  40M Feb 14 02:51 .llm_int4_asym_r10_gs64_max_activation_variance_awq_scale_all_layers.xml.crc\n",
      "drwxr-xr-x  2 prabod prabod 4.0K Feb 14 02:50 metadata\n",
      "-rw-r--r--  1 prabod prabod  37K Feb 14 02:51 openvino_reshape_model.xml\n",
      "-rw-r--r--  1 prabod prabod  304 Feb 14 02:51 .openvino_reshape_model.xml.crc\n",
      "-rw-r--r--  1 prabod prabod 1.8G Feb 14 02:51 openvino_vision_encoder.xml\n",
      "-rw-r--r--  1 prabod prabod  15M Feb 14 02:51 .openvino_vision_encoder.xml.crc\n"
     ]
    }
   ],
   "source": [
    "!ls -lah /tmp/MLLama_spark_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml import Pipeline\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# download two images to test into ./images folder\n",
    "\n",
    "url1 = \"https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/d5fbbd1a-d484-415c-88cb-9986625b7b11\"\n",
    "url2 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "\n",
    "Path(\"images\").mkdir(exist_ok=True)\n",
    "\n",
    "!wget -q -O images/image1.jpg {url1}\n",
    "!wget -q -O images/image2.jpg {url2}\n",
    "\n",
    "\n",
    "\n",
    "images_path = \"file://\" + os.getcwd() + \"/images/\"\n",
    "image_df = spark.read.format(\"image\").load(\n",
    "    path=images_path\n",
    ")\n",
    "\n",
    "test_df = image_df.withColumn(\"text\", lit(\"<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|image|>What is unusual on this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"))\n",
    "\n",
    "image_assembler = ImageAssembler().setInputCol(\"image\").setOutputCol(\"image_assembler\")\n",
    "\n",
    "imageClassifier = MLLamaForMultimodal.load(\"file:///tmp/MLLama_spark_nlp\")\\\n",
    "            .setMaxOutputLength(50) \\\n",
    "            .setInputCols(\"image_assembler\") \\\n",
    "            .setOutputCol(\"answer\")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "            stages=[\n",
    "                image_assembler,\n",
    "                imageClassifier,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "model = pipeline.fit(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path: /home/prabod/Projects/spark-nlp/examples/python/transformers/openvino/images/image1.jpg\n",
      "[Annotation(document, 0, 208, This image depicts a cat lying in a box, on a carpet. The image features a cat lying in a box placed on a carpet. The image features a cat lying in a box placed on a carpet. The image features a cat lying in a, Map(), [])]\n"
     ]
    }
   ],
   "source": [
    "light_pipeline = LightPipeline(model)\n",
    "image_path = os.getcwd() + \"/images/\" + \"image1.jpg\"\n",
    "print(\"image_path: \" + image_path)\n",
    "annotations_result = light_pipeline.fullAnnotateImage(\n",
    "    image_path,\n",
    "    \"<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|image|>What is unusual on this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    ")\n",
    "\n",
    "for result in annotations_result:\n",
    "    print(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
