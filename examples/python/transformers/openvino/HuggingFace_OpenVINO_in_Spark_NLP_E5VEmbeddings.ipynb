{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9c512f5",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/openvino/HuggingFace_OpenVINO_in_Spark_NLP_E5V.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860a240a",
   "metadata": {},
   "source": [
    "# Import OpenVINO E5V models from HuggingFace ðŸ¤— into Spark NLP ðŸš€\n",
    "\n",
    "This notebook provides a detailed walkthrough on optimizing and importing E5V models from HuggingFace  for use in Spark NLP, with [Intel OpenVINO toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html). The focus is on converting the model to the OpenVINO format and applying precision optimizations (INT8 and INT4), to enhance the performance and efficiency on CPU platforms using [Optimum Intel](https://huggingface.co/docs/optimum/main/en/intel/inference).\n",
    "\n",
    "Let's keep in mind a few things before we start ðŸ˜Š\n",
    "\n",
    "- OpenVINO support was introduced in  `Spark NLP 5.4.0`, enabling high performance CPU inference for models. So please make sure you have upgraded to the latest Spark NLP release.\n",
    "- Model quantization is a computationally expensive process, so it is recommended to use a runtime with more than 32GB memory for exporting the quantized model from HuggingFace.\n",
    "- You can import E5V models via `E5V`. These models are usually under `Text Generation` category and have `E5V` in their labels.\n",
    "- Reference: [E5V](https://huggingface.co/docs/transformers/model_doc/llama#transformers.E5V)\n",
    "- Some [example models](https://huggingface.co/models?search=E5V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a6911",
   "metadata": {},
   "source": [
    "## 1. Export and Save the HuggingFace model\n",
    "\n",
    "- Let's install `transformers` and `openvino` packages with other dependencies. You don't need `openvino` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
    "- We lock `transformers` on version `4.41.2`. This doesn't mean it won't work with the future release, but we wanted you to know which versions have been tested successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "902635c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529ad224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install OpenVINO and NNCF for model optimization\n",
    "import platform\n",
    "\n",
    "%pip install -q \"einops\" \"torch>2.1\" \"torchvision\" \"matplotlib>=3.4\" \"timm>=0.9.8\" \"transformers==4.41.2\" \"pillow\" \"gradio>=4.19\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q -U --pre \"openvino>=2025.0\" \"openvino-tokenizers>=2025.0\" \"openvino-genai>=2025.0\" --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
    "%pip install -q \"accelerate\" \"nncf>=2.14.0\" \"git+https://github.com/huggingface/optimum-intel.git\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    %pip install -q \"numpy<2.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/American_Eskimo_Dog.jpg/360px-American_Eskimo_Dog.jpg -O dog.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1623528",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"royokong/e5-v\"\n",
    "output_dir = f\"./models/int4/{model_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46678a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prabod/anaconda3/envs/e5v/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:20<00:00, 20.18s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(model_id)\n",
    "image_encoder_model, input_embedding_model, language_model = None, None, None\n",
    "\n",
    "\n",
    "class ImageEncoder(torch.nn.Module):\n",
    "    def __init__(self, config, vision_tower, multi_modal_projector):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vision_tower = vision_tower\n",
    "        self.multi_modal_projector = multi_modal_projector\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        batch_size, num_patches, num_channels, height, width = pixel_values.shape\n",
    "        reshaped_pixel_values = pixel_values.view(\n",
    "            batch_size * num_patches, num_channels, height, width\n",
    "        )\n",
    "        image_features = self.vision_tower(\n",
    "            reshaped_pixel_values, output_hidden_states=True\n",
    "        )\n",
    "        selected_image_feature = image_features.hidden_states[\n",
    "            self.config.vision_feature_layer\n",
    "        ]\n",
    "        if self.config.vision_feature_select_strategy == \"default\":\n",
    "            selected_image_feature = selected_image_feature[:, 1:]\n",
    "        elif self.config.vision_feature_select_strategy == \"full\":\n",
    "            selected_image_feature = selected_image_feature\n",
    "        image_features = self.multi_modal_projector(selected_image_feature)\n",
    "        return image_features\n",
    "\n",
    "\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    model_id, low_cpu_mem_usage=True\n",
    ")\n",
    "model.config.save_pretrained(output_dir)\n",
    "image_encoder_model = ImageEncoder(\n",
    "    model.config, model.vision_tower, model.multi_modal_projector\n",
    ")\n",
    "input_embedding_model = input_embedding_model = model.get_input_embeddings()\n",
    "language_model = model.language_model\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1908bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "from pathlib import Path\n",
    "\n",
    "core = ov.Core()\n",
    "device = \"CPU\"\n",
    "# Load the model and convert it to OpenVINO format\n",
    "output_dir = f\"./models/int4/{model_id}\"\n",
    "output_dir = Path(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2341d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_ENCODER_PATH = output_dir / \"openvino_vision_embeddings_model.xml\"\n",
    "LANGUAGE_MODEL_PATH = output_dir / \"openvino_language_model.xml\"\n",
    "INPUT_EMBEDDING_PATH = output_dir / \"openvino_text_embeddings_model.xml\"\n",
    "\n",
    "IMAGE_PACKER_PATH = output_dir / \"openvino_image_packer.xml\"\n",
    "MULTIMODAL_MERGER_PATH = output_dir / \"openvino_multimodal_merger.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a0e77cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prabod/anaconda3/envs/e5v/lib/python3.11/site-packages/transformers/modeling_utils.py:4481: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "/home/prabod/anaconda3/envs/e5v/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:276: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/home/prabod/anaconda3/envs/e5v/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:316: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7397"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import openvino as ov\n",
    "import gc\n",
    "\n",
    "\n",
    "def cleanup_torchscript_cache():\n",
    "    \"\"\"\n",
    "    Helper for removing cached model representation\n",
    "    \"\"\"\n",
    "    torch._C._jit_clear_class_registry()\n",
    "    torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "    torch.jit._state._clear_class_state()\n",
    "\n",
    "\n",
    "if not IMAGE_ENCODER_PATH.exists():\n",
    "    ov_image_encoder = ov.convert_model(\n",
    "        image_encoder_model, example_input=torch.zeros((1, 5, 3, 336, 336))\n",
    "    )\n",
    "    ov.save_model(ov_image_encoder, IMAGE_ENCODER_PATH)\n",
    "    del ov_image_encoder\n",
    "    cleanup_torchscript_cache()\n",
    "\n",
    "del image_encoder_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0147d547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_input = None\n",
    "\n",
    "llm_input = input_embedding_model(torch.ones((2, 2), dtype=torch.int64))\n",
    "\n",
    "if not INPUT_EMBEDDING_PATH.exists():\n",
    "    ov_input_embeddings_model = ov.convert_model(\n",
    "        input_embedding_model, example_input=torch.ones((2, 2), dtype=torch.int64)\n",
    "    )\n",
    "    ov.save_model(ov_input_embeddings_model, INPUT_EMBEDDING_PATH)\n",
    "    del ov_input_embeddings_model\n",
    "    cleanup_torchscript_cache()\n",
    "\n",
    "del input_embedding_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18b0be05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prabod/anaconda3/envs/e5v/lib/python3.11/site-packages/openvino/runtime/__init__.py:10: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple, List\n",
    "from openvino.runtime import opset13\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def model_has_state(ov_model: ov.Model):\n",
    "    return len(ov_model.get_sinks()) > 0\n",
    "\n",
    "\n",
    "def model_has_input_output_name(ov_model: ov.Model, name: str):\n",
    "    \"\"\"\n",
    "    Helper function for checking that model has specified input or output name\n",
    "\n",
    "    Parameters:\n",
    "      ov_model (ov.Model):\n",
    "      name (str):\n",
    "          name of input or output\n",
    "\n",
    "    Returns:\n",
    "      True if input or output with requested name exists else False\n",
    "    \"\"\"\n",
    "    return name in sum(\n",
    "        [list(t.get_names()) for t in ov_model.inputs + ov_model.outputs], []\n",
    "    )\n",
    "\n",
    "\n",
    "def fuse_cache_reorder(\n",
    "    ov_model: ov.Model,\n",
    "    not_kv_inputs: List[str],\n",
    "    key_value_input_names: List[str],\n",
    "    gather_dim: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fuses reored_cache during generate cycle into ov.Model. Used with stateful models, because we can not modify model state directly.\n",
    "\n",
    "    Adds a new beam_idx parameter and Gather op per each kv-cache input in a given model.\n",
    "    Should be run before make_stateful. Implements optimumum's _reorder_cache\n",
    "    inside the model in the beginning of each iteration.\n",
    "    Gather works along given gather_dim dimension that may vary from model to model.\n",
    "    KV-cache inputs are identified based on names in key_value_input_names.\n",
    "    Append the new beam_idx parameter to not_kv_inputs.\n",
    "\n",
    "    Parameters:\n",
    "      ov_model (`ov.Model`):\n",
    "          openvino model for processing\n",
    "      not_kv_inputs (`List[str]`):\n",
    "          list of input nodes in model that not related to past key values\n",
    "      key_value_input_names (`List[str]`):\n",
    "          list of names for key value input layers\n",
    "      gather_dim (int):\n",
    "          dimension for gathering cache during reorder pass\n",
    "    \"\"\"\n",
    "\n",
    "    if model_has_input_output_name(ov_model, \"beam_idx\"):\n",
    "        raise ValueError(\"Model already has fused cache\")\n",
    "    input_batch = ov_model.input(\"inputs_embeds\").get_partial_shape()[0]\n",
    "    beam_idx = opset13.parameter(\n",
    "        name=\"beam_idx\", dtype=ov.Type.i32, shape=ov.PartialShape([input_batch])\n",
    "    )\n",
    "    beam_idx.output(0).get_tensor().add_names({\"beam_idx\"})  # why list is not accepted?\n",
    "    ov_model.add_parameters([beam_idx])\n",
    "    not_kv_inputs.append(ov_model.inputs[-1])\n",
    "    # Go over all cache parameters and fuse _reorder_cache with indices provided by the new parameter beam_idx\n",
    "    for input_name in key_value_input_names:\n",
    "        parameter_output_port = ov_model.input(input_name)\n",
    "        consumers = parameter_output_port.get_target_inputs()\n",
    "        gather = opset13.gather(\n",
    "            parameter_output_port, beam_idx, opset13.constant(gather_dim)\n",
    "        )\n",
    "        for consumer in consumers:\n",
    "            consumer.replace_source_output(gather.output(0))\n",
    "    ov_model.validate_nodes_and_infer_types()\n",
    "\n",
    "\n",
    "def build_state_initializer(ov_model: ov.Model, batch_dim: int):\n",
    "    \"\"\"\n",
    "    Build initialization ShapeOf Expression for all ReadValue ops\n",
    "\n",
    "    Parameters:\n",
    "      ov_model (ov.Model):\n",
    "          openvino model\n",
    "      batch_dim (int):\n",
    "          index of dimension corresponding to batch size\n",
    "    \"\"\"\n",
    "    input_ids = ov_model.input(\"inputs_embeds\")\n",
    "    batch = opset13.gather(\n",
    "        opset13.shape_of(input_ids, output_type=\"i64\"),\n",
    "        opset13.constant([0]),\n",
    "        opset13.constant(0),\n",
    "    )\n",
    "    for op in ov_model.get_ops():\n",
    "        if op.get_type_name() == \"ReadValue\":\n",
    "            dims = [dim.min_length for dim in list(op.get_output_partial_shape(0))]\n",
    "            dims[batch_dim] = batch\n",
    "            dims = [\n",
    "                (\n",
    "                    opset13.constant(np.array([dim], dtype=np.int64))\n",
    "                    if isinstance(dim, int)\n",
    "                    else dim\n",
    "                )\n",
    "                for dim in dims\n",
    "            ]\n",
    "            shape = opset13.concat(dims, axis=0)\n",
    "            broadcast = opset13.broadcast(\n",
    "                opset13.constant(0.0, dtype=op.get_output_element_type(0)), shape\n",
    "            )\n",
    "            op.set_arguments([broadcast])\n",
    "    ov_model.validate_nodes_and_infer_types()\n",
    "\n",
    "\n",
    "def make_stateful(\n",
    "    ov_model: ov.Model,\n",
    "    not_kv_inputs: List[str],\n",
    "    key_value_input_names: List[str],\n",
    "    key_value_output_names: List[str],\n",
    "    batch_dim: int,\n",
    "    num_attention_heads: int,\n",
    "    num_beams_and_batch: int = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Hides kv-cache inputs and outputs inside the model as variables.\n",
    "\n",
    "    Parameters:\n",
    "        ov_model (ov.Model):\n",
    "            openvino model\n",
    "        not_kv_inputs (`List[str]`):\n",
    "            list of input nodes in model that not related to past key values\n",
    "        key_value_input_names (`List[str]`):\n",
    "            list of names for key value input layers\n",
    "        key_value_output_names (`List[str]`):\n",
    "            list of names for key value input layers\n",
    "        batch_dim (int):\n",
    "            index of batch dimension in key value layers\n",
    "        num_attention_heads (int):\n",
    "            number of attention heads for batch dimension initialization\n",
    "        num_beams_an_batch (int):\n",
    "            precalculated number of beams and batch for shapes initialization\n",
    "    \"\"\"\n",
    "    from openvino._offline_transformations import apply_make_stateful_transformation\n",
    "\n",
    "    input_output_map = {}\n",
    "\n",
    "    if num_beams_and_batch is not None:\n",
    "        # Set batch size for input_ids and attention mask to avoid dynamic dimension got propagated from the end of the model back to ReadValue\n",
    "        for input in not_kv_inputs:\n",
    "            shape = input.get_partial_shape()\n",
    "            if shape.rank.get_length() <= 2:  # == 1 for beam_index\n",
    "                shape[0] = num_beams_and_batch\n",
    "                input.get_node().set_partial_shape(shape)\n",
    "    for kv_name_pair in zip(key_value_input_names, key_value_output_names):\n",
    "        input_output_map[kv_name_pair[0]] = kv_name_pair[1]\n",
    "        if num_beams_and_batch is not None:\n",
    "            input = ov_model.input(kv_name_pair[0])\n",
    "            shape = input.get_partial_shape()\n",
    "            shape[batch_dim] = num_beams_and_batch * num_attention_heads\n",
    "            input.get_node().set_partial_shape(shape)\n",
    "\n",
    "    if num_beams_and_batch is not None:\n",
    "        # Re-validation model if shapes are altered above\n",
    "        ov_model.validate_nodes_and_infer_types()\n",
    "\n",
    "    apply_make_stateful_transformation(ov_model, input_output_map)\n",
    "    if num_beams_and_batch is None:\n",
    "        build_state_initializer(ov_model, batch_dim)\n",
    "\n",
    "\n",
    "def patch_stateful(ov_model):\n",
    "    key_value_input_names = [key.get_any_name() for key in ov_model.inputs[2:-1]]\n",
    "    key_value_output_names = [key.get_any_name() for key in ov_model.outputs[1:]]\n",
    "    not_kv_inputs = [\n",
    "        input\n",
    "        for input in ov_model.inputs\n",
    "        if not any(name in key_value_input_names for name in input.get_names())\n",
    "    ]\n",
    "    if not key_value_input_names or not key_value_output_names:\n",
    "        return\n",
    "    batch_dim = 0\n",
    "    num_attention_heads = 1\n",
    "\n",
    "    fuse_cache_reorder(ov_model, not_kv_inputs, key_value_input_names, batch_dim)\n",
    "    make_stateful(\n",
    "        ov_model,\n",
    "        not_kv_inputs,\n",
    "        key_value_input_names,\n",
    "        key_value_output_names,\n",
    "        batch_dim,\n",
    "        num_attention_heads,\n",
    "        None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cd69acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.00s/it]\n",
      "/home/prabod/anaconda3/envs/e5v/lib/python3.11/site-packages/transformers/modeling_utils.py:4481: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "/home/prabod/anaconda3/envs/e5v/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1060: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "/home/prabod/anaconda3/envs/e5v/lib/python3.11/site-packages/torch/jit/_trace.py:165: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  if a.grad is not None:\n"
     ]
    }
   ],
   "source": [
    "import types\n",
    "\n",
    "make_stateful_model = False\n",
    "core = ov.Core()\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    model_id, low_cpu_mem_usage=True\n",
    ")\n",
    "language_model = model.language_model\n",
    "if not LANGUAGE_MODEL_PATH.exists() or True:\n",
    "\n",
    "    def forward_wrap(\n",
    "        self,\n",
    "        attention_mask,\n",
    "        position_ids=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "    ):\n",
    "        result = self._orig_forward(\n",
    "            input_ids=None,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        return result[\"hidden_states\"][-1][:, -1, :]\n",
    "\n",
    "    model_inputs = [\"attention_mask\", \"position_ids\"]\n",
    "    model_outputs = [\"last_hidden_state\"]\n",
    "    model_inputs.append(\"inputs_embeds\")\n",
    "    language_model.config.torchscript = True\n",
    "    position_ids = torch.tensor([[2, 3], [2, 3]])\n",
    "    language_model._orig_forward = language_model.forward\n",
    "    language_model.forward = types.MethodType(forward_wrap, language_model)\n",
    "    ov_model = ov.convert_model(\n",
    "        language_model,\n",
    "        example_input={\n",
    "            \"inputs_embeds\": llm_input,\n",
    "            \"attention_mask\": torch.ones((2, 4)),\n",
    "            \"position_ids\": position_ids,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    for input, input_name in zip(ov_model.inputs, model_inputs):\n",
    "        input.get_tensor().set_names({input_name})\n",
    "\n",
    "    for output, output_name in zip(ov_model.outputs, model_outputs):\n",
    "        output.get_tensor().set_names({output_name})\n",
    "    if make_stateful_model:\n",
    "        patch_stateful(ov_model)\n",
    "    ov.save_model(ov_model, LANGUAGE_MODEL_PATH)\n",
    "    del ov_model\n",
    "    cleanup_torchscript_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49838499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:nncf:NNCF provides best results with torch==2.6.*, while current torch version is 2.7.0+cpu. If you encounter issues, consider switching to torch==2.6.*\n",
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”‘\n",
      "â”‚ Weight compression mode   â”‚ % all parameters (layers)   â”‚ % ratio-defining parameters (layers)   â”‚\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¿â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¿â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¥\n",
      "â”‚ int8_asym                 â”‚ 1% (1 / 224)                â”‚ 0% (0 / 223)                           â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ int4_asym                 â”‚ 99% (223 / 224)             â”‚ 100% (223 / 223)                       â”‚\n",
      "â”•â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”™\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/prabod/anaconda3/envs/e5v/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" \n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/prabod/anaconda3/envs/e5v/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" \n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nncf\n",
    "\n",
    "compression_configuration = {\n",
    "    \"mode\": nncf.CompressWeightsMode.INT4_ASYM,\n",
    "    \"group_size\": 64,\n",
    "    \"ratio\": 1.0,\n",
    "}\n",
    "LANGUAGE_MODEL_PATH_INT4 = (\n",
    "    LANGUAGE_MODEL_PATH.parent / LANGUAGE_MODEL_PATH.name.replace(\".xml\", \"-int4.xml\")\n",
    ")\n",
    "ov_model = core.read_model(LANGUAGE_MODEL_PATH)\n",
    "ov_model_compressed = nncf.compress_weights(ov_model, **compression_configuration)\n",
    "ov.save_model(ov_model_compressed, LANGUAGE_MODEL_PATH_INT4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "695c2fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class UnpadImage(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UnpadImage, self).__init__()\n",
    "\n",
    "    def forward(self, tensor, original_size, current_size):\n",
    "        \"\"\"\n",
    "        Unpads an image tensor to its original size based on the current size.\n",
    "        Args:\n",
    "            tensor (torch.Tensor): The input image tensor of shape (C, H, W).\n",
    "            original_size (torch.Tensor): The original size of the image tensor as (H, W).\n",
    "            current_size (torch.Tensor): The current size of the image tensor as (H, W).\n",
    "        \"\"\"\n",
    "        # tensor: (C, H, W)\n",
    "        original_size = original_size.to(torch.float32)\n",
    "        original_height, original_width = original_size[0], original_size[1]\n",
    "        current_height, current_width = current_size[0], current_size[1]\n",
    "\n",
    "        original_aspect_ratio = original_width / original_height\n",
    "        current_aspect_ratio = current_width / current_height\n",
    "\n",
    "        # Comparison\n",
    "        condition = original_aspect_ratio > current_aspect_ratio\n",
    "\n",
    "        # Branch 1: vertical padding\n",
    "        scale_factor_1 = current_width.float() / original_width.float()\n",
    "        new_height = (original_height.float() * scale_factor_1).int()\n",
    "        pad_top = ((current_height.float() - new_height) / 2).floor().long()\n",
    "\n",
    "        # Branch 2: horizontal padding\n",
    "        scale_factor_2 = current_height.float() / original_height.float()\n",
    "        new_width = (original_width.float() * scale_factor_2).int()\n",
    "        pad_left = ((current_width.float() - new_width) / 2).floor().long()\n",
    "\n",
    "        zero = torch.zeros(1, dtype=pad_top.dtype, device=tensor.device).squeeze(0)\n",
    "\n",
    "        # Use torch.where to conditionally compute slicing\n",
    "        y_start = torch.where(condition, pad_top, zero)\n",
    "        y_end = torch.where(condition, current_height - pad_top, current_height)\n",
    "\n",
    "        x_start = torch.where(condition, zero, pad_left)\n",
    "        x_end = torch.where(condition, current_width - pad_left, current_width)\n",
    "        out = tensor[:, y_start.int() : y_end.int(), x_start.int() : x_end.int()]\n",
    "        return out  # Remove batch dimension if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba325001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class PackImageFeatures(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.unpad_image = UnpadImage()\n",
    "        self.height = config.vision_config.image_size // config.vision_config.patch_size\n",
    "        self.width = config.vision_config.image_size // config.vision_config.patch_size\n",
    "\n",
    "    def forward(self, image_feature, image_sizes, num_patch_height, num_patch_width):\n",
    "        # we image features is a single image features, so we can remove the loop\n",
    "        base_image_features = image_feature[0]\n",
    "        features = image_feature[1:]  # Skip the first token\n",
    "        features = (\n",
    "            features.view(\n",
    "                num_patch_height, num_patch_width, self.height, self.width, -1\n",
    "            )\n",
    "            .permute(4, 0, 2, 1, 3)\n",
    "            .contiguous()\n",
    "            .flatten(1, 2)\n",
    "            .flatten(2, 3)\n",
    "        )\n",
    "        features = self.unpad_image(\n",
    "            features, image_sizes[0], torch._shape_as_tensor(features)[1:3]\n",
    "        )\n",
    "        features = features.flatten(1, 2).transpose(0, 1)\n",
    "        features = torch.cat([base_image_features, features], dim=0)\n",
    "        return features.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f911f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MergeInputWithImageFeatures(nn.Module):\n",
    "    def __init__(self, pad_token_id=0, image_token_index=0):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.image_token_index = image_token_index\n",
    "\n",
    "    def forward(self, image_features, inputs_embeds, input_ids, attention_mask):\n",
    "        num_images, num_image_patches, embed_dim = image_features.shape\n",
    "        batch_size, sequence_length = input_ids.shape\n",
    "\n",
    "        # left_padding = torch.sum(input_ids[:, -1] == self.pad_token_id) == 0  # Removed, not needed now\n",
    "\n",
    "        special_image_token_mask = input_ids == self.image_token_index  # [B, S]\n",
    "        num_special_image_tokens = special_image_token_mask.sum(dim=-1)  # [B]\n",
    "\n",
    "        max_embed_dim = (\n",
    "            num_special_image_tokens.max() * (num_image_patches - 1)\n",
    "        ) + sequence_length  # scalar\n",
    "\n",
    "        batch_indices, non_image_indices = torch.where(\n",
    "            input_ids != self.image_token_index\n",
    "        )  # [N], [N]\n",
    "\n",
    "        # Step 2: Compute new token positions\n",
    "        new_token_positions = (\n",
    "            torch.cumsum(special_image_token_mask * (num_image_patches - 1) + 1, dim=-1)\n",
    "            - 1\n",
    "        )  # [B, S]\n",
    "\n",
    "        nb_image_pad = max_embed_dim - 1 - new_token_positions[:, -1]  # [B]\n",
    "\n",
    "        # left_padding_flag = (input_ids[:, -1] != self.pad_token_id).to(nb_image_pad.dtype)  # original\n",
    "        left_padding_flag = (\n",
    "            input_ids[:, -1] != self.pad_token_id\n",
    "        ).long()  # more idiomatic torch\n",
    "        # new_token_positions = new_token_positions + (left_padding_flag[:, None] * nb_image_pad[:, None])  # original\n",
    "        new_token_positions += (\n",
    "            left_padding_flag[:, None] * nb_image_pad[:, None]\n",
    "        )  # updated\n",
    "\n",
    "        text_to_overwrite = new_token_positions[batch_indices, non_image_indices]  # [N]\n",
    "\n",
    "        # Step 3: Init final tensors\n",
    "        final_embedding = torch.zeros(\n",
    "            batch_size,\n",
    "            max_embed_dim,\n",
    "            embed_dim,\n",
    "            dtype=inputs_embeds.dtype,\n",
    "            device=inputs_embeds.device,\n",
    "        )\n",
    "        final_attention_mask = torch.zeros(\n",
    "            batch_size,\n",
    "            max_embed_dim,\n",
    "            dtype=attention_mask.dtype,\n",
    "            device=inputs_embeds.device,\n",
    "        )\n",
    "\n",
    "        # final_embedding[batch_indices, text_to_overwrite] = inputs_embeds[batch_indices, non_image_indices]  # original\n",
    "        final_embedding.index_put_(\n",
    "            (batch_indices, text_to_overwrite),\n",
    "            inputs_embeds[batch_indices, non_image_indices],\n",
    "        )  # torch native\n",
    "\n",
    "        # final_attention_mask[batch_indices, text_to_overwrite] = attention_mask[batch_indices, non_image_indices]  # original\n",
    "        final_attention_mask.index_put_(\n",
    "            (batch_indices, text_to_overwrite),\n",
    "            attention_mask[batch_indices, non_image_indices],\n",
    "        )  # torch native\n",
    "\n",
    "        # Step 5: fill in image features\n",
    "        image_to_overwrite = (final_embedding == 0).all(dim=-1)  # [B, L]\n",
    "        image_to_overwrite &= (image_to_overwrite.cumsum(-1) - 1) >= nb_image_pad[\n",
    "            :, None\n",
    "        ]  # apply pad cutoff\n",
    "\n",
    "        flat_image_features = image_features.reshape(-1, embed_dim).to(\n",
    "            inputs_embeds.device\n",
    "        )  # [N_img, D]\n",
    "\n",
    "        # final_embedding[image_to_overwrite] = flat_image_features  # original\n",
    "        final_embedding[image_to_overwrite] = flat_image_features[\n",
    "            : image_to_overwrite.sum()\n",
    "        ]  # safe assignment\n",
    "\n",
    "        final_attention_mask |= image_to_overwrite  # logical or with existing mask\n",
    "\n",
    "        position_ids = final_attention_mask.cumsum(-1) - 1\n",
    "        position_ids = position_ids.masked_fill(final_attention_mask == 0, 1)\n",
    "\n",
    "        # Step 6: remove pad token embeddings\n",
    "        batch_pad_indices, pad_token_positions = torch.where(\n",
    "            input_ids == self.pad_token_id\n",
    "        )  # [N_pad]\n",
    "        indices_to_mask = new_token_positions[\n",
    "            batch_pad_indices, pad_token_positions\n",
    "        ]  # [N_pad]\n",
    "\n",
    "        # final_embedding[batch_pad_indices, indices_to_mask] = 0  # original\n",
    "        final_embedding.index_put_(\n",
    "            (batch_pad_indices, indices_to_mask),\n",
    "            torch.zeros_like(final_embedding[batch_pad_indices, indices_to_mask]),\n",
    "        )  # updated\n",
    "\n",
    "        return {\n",
    "            \"final_embedding\": final_embedding,\n",
    "            \"final_attention_mask\": final_attention_mask,\n",
    "            \"position_ids\": position_ids,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb25757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the models\n",
    "language_model = core.read_model(LANGUAGE_MODEL_PATH)\n",
    "compiled_language_model = core.compile_model(language_model, \"AUTO\")\n",
    "\n",
    "image_embed_model = core.compile_model(IMAGE_ENCODER_PATH, device)\n",
    "text_embeddings_model = core.compile_model(INPUT_EMBEDDING_PATH, device)\n",
    "\n",
    "if IMAGE_PACKER_PATH.exists():\n",
    "    image_packer_model = core.compile_model(IMAGE_PACKER_PATH, device)\n",
    "else:\n",
    "    image_packer_model = None\n",
    "if MULTIMODAL_MERGER_PATH.exists()\n",
    "    multimodal_merger_model = core.compile_model(MULTIMODAL_MERGER_PATH, device)\n",
    "else:\n",
    "    multimodal_merger_model = None\n",
    "\n",
    "# multimodal_merger_model = core.compile_model(MODEL_MERGER_PATH, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d5643ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/prabod/anaconda3/envs/e5v/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: (360, 282), Mode: RGB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "\n",
    "llama3_template = \"<|start_header_id|>user<|end_header_id|>\\n\\n{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n \\n\"\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(\"royokong/e5-v\")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"royokong/e5-v\")\n",
    "img_prompt = llama3_template.format(\"<image>\\nSummary above image in one word: \")\n",
    "text_prompt = llama3_template.format(\"<sent>\\nSummary above sentence in one word: \")\n",
    "\n",
    "images = [Image.open(\"dog.jpg\").convert(\"RGB\")]\n",
    "\n",
    "for image in images:\n",
    "    print(f\"Image size: {image.size}, Mode: {image.mode}\")\n",
    "\n",
    "texts = [\"A dog sitting in the grass.\"]\n",
    "\n",
    "text_inputs = processor(\n",
    "    [text_prompt.replace(\"<sent>\", text) for text in texts],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")\n",
    "img_inputs = processor(\n",
    "    [img_prompt] * len(images), images, return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad1b402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input_ids = img_inputs[\"input_ids\"]\n",
    "img_attention_mask = img_inputs[\"attention_mask\"]\n",
    "image_sizes = img_inputs[\"image_sizes\"]\n",
    "pixel_values = img_inputs[\"pixel_values\"]\n",
    "\n",
    "text_input_ids = text_inputs[\"input_ids\"]\n",
    "text_attention_mask = text_inputs[\"attention_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2649d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = torch.from_numpy(image_embed_model(pixel_values)[0])\n",
    "image_inputs_embeds = torch.from_numpy(text_embeddings_model(img_input_ids)[0])\n",
    "text_inputs_embeds = torch.from_numpy(text_embeddings_model(text_input_ids)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "844968c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_packer = PackImageFeatures(config)\n",
    "input_merger = MergeInputWithImageFeatures(\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    image_token_index=config.image_token_index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "190da649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union, List, Tuple\n",
    "import torch\n",
    "\n",
    "\n",
    "def select_best_resolution(original_size: tuple, possible_resolutions: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Selects the best resolution from a list of possible resolutions based on the original size.\n",
    "\n",
    "    This is done by calculating the effective and wasted resolution for each possible resolution.\n",
    "\n",
    "    The best fit resolution is the one that maximizes the effective resolution and minimizes the wasted resolution.\n",
    "\n",
    "    Args:\n",
    "        original_size (tuple):\n",
    "            The original size of the image in the format (height, width).\n",
    "        possible_resolutions (list):\n",
    "            A list of possible resolutions in the format [(height1, width1), (height2, width2), ...].\n",
    "\n",
    "    Returns:\n",
    "        tuple: The best fit resolution in the format (height, width).\n",
    "    \"\"\"\n",
    "    original_height, original_width = original_size\n",
    "    best_fit = None\n",
    "    max_effective_resolution = 0\n",
    "    min_wasted_resolution = float(\"inf\")\n",
    "\n",
    "    for height, width in possible_resolutions:\n",
    "        scale = min(width / original_width, height / original_height)\n",
    "        downscaled_width, downscaled_height = (\n",
    "            int(original_width * scale),\n",
    "            int(original_height * scale),\n",
    "        )\n",
    "        effective_resolution = min(\n",
    "            downscaled_width * downscaled_height, original_width * original_height\n",
    "        )\n",
    "        wasted_resolution = (width * height) - effective_resolution\n",
    "\n",
    "        if effective_resolution > max_effective_resolution or (\n",
    "            effective_resolution == max_effective_resolution\n",
    "            and wasted_resolution < min_wasted_resolution\n",
    "        ):\n",
    "            max_effective_resolution = effective_resolution\n",
    "            min_wasted_resolution = wasted_resolution\n",
    "            best_fit = (height, width)\n",
    "\n",
    "    return best_fit\n",
    "\n",
    "\n",
    "def image_size_to_num_patches(image_size, grid_pinpoints, patch_size: int):\n",
    "    \"\"\"\n",
    "    Calculate the number of patches after the preprocessing for images of any resolution.\n",
    "\n",
    "    Args:\n",
    "        image_size (`Union[torch.LongTensor, np.ndarray, Tuple[int, int]):\n",
    "            The size of the input image in the format (height, width). ?\n",
    "        grid_pinpoints (`List`):\n",
    "            A list containing possible resolutions. Each item in the list should be a tuple or list\n",
    "            of the form `(height, width)`.\n",
    "        patch_size (`int`):\n",
    "            The size of each image patch.\n",
    "\n",
    "    Returns:\n",
    "        int: the number of patches\n",
    "    \"\"\"\n",
    "    if not isinstance(grid_pinpoints, list):\n",
    "        raise ValueError(\"grid_pinpoints should be a list of tuples or lists\")\n",
    "\n",
    "    # ! VERY IMPORTANT if image_size is tensor, must convert to into tuple, otherwise it will cause wrong calculate\n",
    "    if not isinstance(image_size, (list, tuple)):\n",
    "        if not isinstance(image_size, (torch.Tensor, np.ndarray)):\n",
    "            raise ValueError(\n",
    "                f\"image_size invalid type {type(image_size)} with value {image_size}\"\n",
    "            )\n",
    "        image_size = image_size.tolist()\n",
    "\n",
    "    best_resolution = select_best_resolution(image_size, grid_pinpoints)\n",
    "    height, width = best_resolution\n",
    "    num_patches = 0\n",
    "    # consider change to ceil(height/patch_size)*ceil(width/patch_size) + 1\n",
    "    for i in range(0, height, patch_size):\n",
    "        for j in range(0, width, patch_size):\n",
    "            num_patches += 1\n",
    "    # add the base patch\n",
    "    num_patches += 1\n",
    "    return num_patches\n",
    "\n",
    "\n",
    "def get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size):\n",
    "    \"\"\"\n",
    "    Calculate the shape of the image patch grid after the preprocessing for images of any resolution.\n",
    "\n",
    "    Args:\n",
    "        image_size (`tuple`):\n",
    "            The size of the input image in the format (width, height).\n",
    "        grid_pinpoints (`List`):\n",
    "            A list containing possible resolutions. Each item in the list should be a tuple or list\n",
    "            of the form `(height, width)`.\n",
    "        patch_size (`int`):\n",
    "            The size of each image patch.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The shape of the image patch grid in the format (width, height).\n",
    "    \"\"\"\n",
    "    if not isinstance(grid_pinpoints, list):\n",
    "        raise ValueError(\"grid_pinpoints should be a list of tuples or lists\")\n",
    "\n",
    "    # ! VERY IMPORTANT if image_size is tensor, must convert to into tuple, otherwise it will cause wrong calculate\n",
    "    if not isinstance(image_size, (list, tuple)):\n",
    "        if not isinstance(image_size, (torch.Tensor, np.ndarray)):\n",
    "            raise ValueError(\n",
    "                f\"image_size invalid type: {type(image_size)} not valid, should be either list, tuple, np.ndarray or tensor\"\n",
    "            )\n",
    "        image_size = image_size.tolist()\n",
    "\n",
    "    height, width = select_best_resolution(image_size, grid_pinpoints)\n",
    "    return height // patch_size, width // patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcbec245",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patch_width, num_patch_height = get_anyres_image_grid_shape(\n",
    "    image_sizes[0],\n",
    "    config.image_grid_pinpoints,\n",
    "    config.vision_config.image_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40620525",
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_image_features = image_packer(\n",
    "    image_features,\n",
    "    image_sizes,\n",
    "    num_patch_height=num_patch_height,\n",
    "    num_patch_width=num_patch_width\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0eb947f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if IMAGE_PACKER_PATH.exists():\n",
    "    IMAGE_PACKER_PATH.unlink()\n",
    "\n",
    "ov_image_packer = ov.convert_model(\n",
    "    image_packer,\n",
    "    example_input={\n",
    "        \"image_feature\": image_features,\n",
    "        \"image_sizes\": image_sizes,\n",
    "        \"num_patch_height\": torch.tensor(num_patch_height, dtype=torch.int64),\n",
    "        \"num_patch_width\": torch.tensor(num_patch_width, dtype=torch.int64)\n",
    "    }\n",
    ")\n",
    "ov.save_model(ov_image_packer, IMAGE_PACKER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2fae423",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MULTIMODAL_MERGER_PATH.exists():\n",
    "    MULTIMODAL_MERGER_PATH.unlink()\n",
    "ov_multimodal_merger = ov.convert_model(\n",
    "    input_merger,\n",
    "    example_input={\n",
    "        \"image_features\": packed_image_features,\n",
    "        \"inputs_embeds\": image_inputs_embeds,\n",
    "        \"input_ids\": img_input_ids,\n",
    "        \"attention_mask\": img_attention_mask\n",
    "    }\n",
    ")\n",
    "ov.save_model(ov_multimodal_merger, MULTIMODAL_MERGER_PATH)\n",
    "cleanup_torchscript_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0599dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "if not os.path.exists(f\"{output_dir}/assets\"):\n",
    "    output_dir = Path(output_dir)\n",
    "    assets_dir = output_dir/\"assets\"\n",
    "    assets_dir.mkdir(exist_ok=True)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    # copy all the assets to the assets directory (json files, vocab files, etc.)\n",
    "    for file in output_dir.glob(\"*.json\"):\n",
    "        shutil.copy(file, assets_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27e894ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the f32 language model\n",
    "if LANGUAGE_MODEL_PATH.exists():\n",
    "    LANGUAGE_MODEL_PATH.unlink()\n",
    "\n",
    "# delete the f32 language model bin file if exists\n",
    "if LANGUAGE_MODEL_PATH.with_suffix(\".bin\").exists():\n",
    "    LANGUAGE_MODEL_PATH.with_suffix(\".bin\").unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ecebb",
   "metadata": {},
   "source": [
    "## 2. Test the Exported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17eaa581",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_ENCODER_PATH = output_dir / \"openvino_vision_embeddings_model.xml\"\n",
    "LANGUAGE_MODEL_PATH = output_dir / \"openvino_language_model-int4.xml\"\n",
    "INPUT_EMBEDDING_PATH = output_dir / \"openvino_text_embeddings_model.xml\"\n",
    "\n",
    "IMAGE_PACKER_PATH = output_dir / \"openvino_image_packer.xml\"\n",
    "MULTIMODAL_MERGER_PATH = output_dir / \"openvino_multimodal_merger.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0782a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the models\n",
    "language_model = core.read_model(LANGUAGE_MODEL_PATH)\n",
    "compiled_language_model = core.compile_model(language_model, \"AUTO\")\n",
    "\n",
    "image_embed_model = core.compile_model(IMAGE_ENCODER_PATH, device)\n",
    "text_embeddings_model = core.compile_model(INPUT_EMBEDDING_PATH, device)\n",
    "\n",
    "if IMAGE_PACKER_PATH.exists():\n",
    "    image_packer_model = core.compile_model(IMAGE_PACKER_PATH, device)\n",
    "else:\n",
    "    image_packer_model = None\n",
    "if MULTIMODAL_MERGER_PATH.exists():\n",
    "    multimodal_merger_model = core.compile_model(MULTIMODAL_MERGER_PATH, device)\n",
    "else:\n",
    "    multimodal_merger_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b88a40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use openvino model to pack the image features\n",
    "packed_image_features = image_packer_model({\n",
    "    'image_feature': image_features,\n",
    "    'image_sizes': image_sizes,\n",
    "    'num_patch_height': torch.tensor(num_patch_height, dtype=torch.int64),\n",
    "    'num_patch_width': torch.tensor(num_patch_width, dtype=torch.int64)\n",
    "})[0]\n",
    "packed_image_features = torch.from_numpy(packed_image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a69b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use openvino model to merge the image features with text features\n",
    "merger_out = multimodal_merger_model({\n",
    "        \"image_features\": packed_image_features,\n",
    "        \"inputs_embeds\": image_inputs_embeds,\n",
    "        \"input_ids\": img_input_ids,\n",
    "        \"attention_mask\": img_attention_mask\n",
    "    }\n",
    ")\n",
    "image_final_embeds = torch.from_numpy(merger_out['final_embedding'])\n",
    "image_final_attention_mask = torch.from_numpy(merger_out['final_attention_mask'])\n",
    "image_position_ids = torch.from_numpy(merger_out['position_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "131763dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = compiled_language_model.create_infer_request()\n",
    "img_input_lm = {\n",
    "    \"inputs_embeds\": image_final_embeds.detach().numpy(),\n",
    "    \"attention_mask\": image_final_attention_mask.detach().numpy(),\n",
    "    \"position_ids\": image_position_ids.detach().numpy(),\n",
    "}\n",
    "request.start_async(img_input_lm, share_inputs=True)\n",
    "request.wait()\n",
    "img_lm_output = torch.from_numpy(request.get_tensor(\"last_hidden_state\").data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68787196",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_request = compiled_language_model.create_infer_request()\n",
    "text_position_ids = text_attention_mask.long().cumsum(-1) - 1\n",
    "text_position_ids.masked_fill_(text_attention_mask == 0, 1)\n",
    "text_input_lm = {\n",
    "    \"inputs_embeds\": text_inputs_embeds.detach().numpy(),\n",
    "    \"attention_mask\": text_attention_mask.detach().numpy(),\n",
    "    \"position_ids\": text_position_ids.detach().numpy(),\n",
    "}\n",
    "text_request.start_async(text_input_lm, share_inputs=True)\n",
    "text_request.wait()\n",
    "text_lm_output = torch.from_numpy(text_request.get_tensor(\"last_hidden_state\").data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df6a5ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7158]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "txt_embed = F.normalize(text_lm_output, dim=-1)\n",
    "img_embed = F.normalize(img_lm_output, dim=-1)\n",
    "\n",
    "print(txt_embed @ img_embed.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3764af1b",
   "metadata": {},
   "source": [
    "## 3 Import and Save E5V in Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ecf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285bb60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "\n",
    "# let's start Spark with Spark NLP\n",
    "spark = sparknlp.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18611787",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"royokong/e5-v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ca2060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/10 03:45:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/06/10 03:45:41 WARN NativeLibrary: Failed to load library null: java.lang.UnsatisfiedLinkError: Can't load library: /tmp/openvino-native4021672575912693842/libtbb.so.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/prabod/spark/jars/spark-core_2.12-3.3.2.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "e5v_embeddings_sn = E5VEmbeddings \\\n",
    "            .loadSavedModel(str(output_dir),spark) \\\n",
    "            .setInputCols(\"image_assembler\") \\\n",
    "            .setOutputCol(\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5b60572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "e5v_embeddings_sn.write().overwrite().save(f\"file:///tmp/{model_id}_spark_nlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9cf656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.util import EmbeddingsDataFrameUtils\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# download two images to test into ./images folder\n",
    "\n",
    "url1 = \"https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/d5fbbd1a-d484-415c-88cb-9986625b7b11\"\n",
    "\n",
    "Path(\"images\").mkdir(exist_ok=True)\n",
    "\n",
    "!wget -q -O images/image1.jpg {url1}\n",
    "\n",
    "\n",
    "\n",
    "images_path = \"file://\" + os.getcwd() + \"/images/\"\n",
    "image_df = spark.read.format(\"image\").load(\n",
    "    path=images_path\n",
    ")\n",
    "\n",
    "imagePrompt = \"<|start_header_id|>user<|end_header_id|>\\n\\n<image>\\\\nSummary above image in one word: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n \\n\"\n",
    "image_df = spark.read.format(\"image\").option(\"dropInvalid\", True).load(images_path)\n",
    "test_df = image_df.withColumn(\"text\", lit(imagePrompt))\n",
    "\n",
    "textPrompt = \"<|start_header_id|>user<|end_header_id|>\\n\\n<sent>\\\\nSummary above sentence in one word: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n \\n\"\n",
    "textDesc = \"A cat sitting in a box.\"\n",
    "nullImageDF = spark.createDataFrame(\n",
    "    [EmbeddingsDataFrameUtils.emptyImageRow], schema=\n",
    "    EmbeddingsDataFrameUtils.imageSchema)\n",
    "textDF = nullImageDF.withColumn(\"text\", lit(textPrompt.replace(\"<sent>\", textDesc)))\n",
    "\n",
    "test_df = test_df.union(textDF)\n",
    "\n",
    "imageAssembler = ImageAssembler() \\\n",
    "            .setInputCol(\"image\") \\\n",
    "            .setOutputCol(\"image_assembler\")\n",
    "e5v = E5VEmbeddings.load(f\"file:///tmp/{model_id}_spark_nlp\") \\\n",
    "    .setInputCols([\"image_assembler\"]) \\\n",
    "    .setOutputCol(\"e5v\")\n",
    "pipeline = Pipeline().setStages([imageAssembler, e5v])\n",
    "results = pipeline.fit(test_df).transform(test_df)\n",
    "results.select(\"e5v.embeddings\").show(truncate=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e5v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
