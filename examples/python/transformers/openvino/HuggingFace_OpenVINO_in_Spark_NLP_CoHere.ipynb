{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvX_yCcI4W7D"
   },
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/openvino/HuggingFace_OpenVINO_in_Spark_NLP_CoHere.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8J48sFcb4W7G"
   },
   "source": [
    "# Import OpenVINO CoHere models from HuggingFace ðŸ¤— into Spark NLP ðŸš€\n",
    "\n",
    "This notebook provides a detailed walkthrough on optimizing and importing CoHere models from HuggingFace  for use in Spark NLP, with [Intel OpenVINO toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html). The focus is on converting the model to the OpenVINO format and applying precision optimizations (INT8 and INT4), to enhance the performance and efficiency on CPU platforms using [Optimum Intel](https://huggingface.co/docs/optimum/main/en/intel/inference).\n",
    "\n",
    "Let's keep in mind a few things before we start ðŸ˜Š\n",
    "\n",
    "- OpenVINO support was introduced in  `Spark NLP 5.4.0`, enabling high performance CPU inference for models. So please make sure you have upgraded to the latest Spark NLP release.\n",
    "- Model quantization is a computationally expensive process, so it is recommended to use a runtime with more than 32GB memory for exporting the quantized model from HuggingFace.\n",
    "- You can import CoHere models via `CoHereModel`. These models are usually under `Text Generation` category and have `CoHere` in their labels.\n",
    "- Reference: [CoHereModel](https://huggingface.co/docs/transformers/model_doc/CoHereTransformer#transformers.CoHereModel)\n",
    "- Some [example models](https://huggingface.co/models?search=CoHere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ko24PkTd4W7H"
   },
   "source": [
    "## 1. Export and Save the HuggingFace model\n",
    "\n",
    "- Let's install `transformers` and `openvino` packages with other dependencies. You don't need `openvino` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
    "- We lock `transformers` on version `4.41.2`. This doesn't mean it won't work with the future release, but we wanted you to know which versions have been tested successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rOdslOi4W7H",
    "outputId": "0fe0d124-f09d-4fc0-b822-655d7b616125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"nncf>=2.14.0\" \"torch>=2.3\" \"transformers>=4.39.1\" \"accelerate\" \"pillow\" \"gradio>=4.26\" \"datasets>=2.14.6\" \"tqdm\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q -U \"openvino>=2024.5.0\" \"openvino-tokenizers>=2024.5.0\" \"openvino-genai>=2024.5\"\n",
    "%pip install -q \"git+https://github.com/huggingface/optimum-intel.git\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "8420c288f5e44084af6589d767899664",
      "a03258e8bcb241b2be89ac5c03fba9fe",
      "0540ea7b02994fa1a8318a7d2f38c12c",
      "4f57921b6c234eabae3f424afe3c04b5",
      "97bca2fe9b06436ab7174a8e0b921fcf",
      "529731f33fb242d9a1d283931beaa70f",
      "1b76dafe2da64c1fa55e52a5f83715c9",
      "8087b4ffd55b450ca453fd4c5ffd21f9",
      "ee6313eca4be4f6b9d386b2c27624452",
      "e23e8b6170294d4999b90a293da45b19",
      "452dbb332660410ca9b94d11017075c0",
      "f15d2dd70cee40899a34443cd1589e21",
      "b20f5c394c9b4c7e9a7d68c1c1dd89ba",
      "374c8537fa7443d4aa6f6b8047fc090b",
      "8cecf94197a040e88791faddd5df7698",
      "d52ee940ddd64d44aa8d08ad032f4225",
      "65686043fcb4475baa17734312cc7f7d",
      "6d4a762cf1f847a59c5e2acf27d3780b",
      "cb0cf954d70d4a20b45b6a7a5508d05d",
      "174693aa52194cae9bde419572ac117e",
      "ec830e5068ef40a7b596fef9908e9c0b",
      "9f994a6df3b94907a6da46c63209dac2",
      "1ca22e25121b4d36a7a8bd88c6d39efe",
      "3154cd7ba0b841bf909030a40dba671a",
      "68b4590ad1bf4eebb05be97c3445bf11",
      "90ac8ccbb2c447b79064050316b4fa1e",
      "446c4a71c2574673b4f54d06ff24a4ba",
      "12e23151dcc74313be8c7e02b0f4ea05",
      "613ffc0f9ac74c0fab8f3cb05f9deb43",
      "8cf69353a540492a8f81795d635e9069",
      "9802c5078cb245a793c8ab8a97e370ca",
      "4fed2ab467c94954b8b463b96c751715"
     ]
    },
    "id": "bYxXi0Gr4W7J",
    "outputId": "a421b770-6287-439a-c892-816448fc23f5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6237118d2c1e42a687289d6dc49e3389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix0eFHLu4W7J"
   },
   "source": [
    "[Optimum Intel](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#openvino) is the interface between the Transformers library and the various model optimization and acceleration tools provided by Intel. HuggingFace models loaded with optimum-intel are automatically optimized for OpenVINO, while being compatible with the Transformers API. It also offers the ability to perform weight compression during export.\n",
    "- To load a HuggingFace model directly for inference/export, just replace the `AutoModelForXxx` class with the corresponding `OVModelForXxx` class. We can use this to import and export OpenVINO models with `from_pretrained` and `save_pretrained`.\n",
    "- By setting `export=True`, the source model is converted to OpenVINO IR format on the fly.\n",
    "- We'll use [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) model from HuggingFace as an example.\n",
    "- In addition to `CoHereModel` we also need to save the tokenizer. This is the same for every model, these are assets needed for tokenization inside Spark NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nuLKzS8F1vt"
   },
   "source": [
    "### Exporting to OpenVINO IR in INT4 Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "80a1163781ca4b76952de9b2dc3b6fb1",
      "36adb757251e475b9d854456b6a59a60",
      "cb7635efbf78425e82caafc51e05588d",
      "fadeed4224c44883942b67fee7691241",
      "a959a396bdeb4d51b5819ba8ee12be03",
      "8291ca2579ee4c3bbaf3bf34614e865f",
      "5ae825fa761a4cdab40831ec71624dfa",
      "48199a26cd8047acbe897c6600919b67",
      "c354400f56d84c19ab16fd9533bc4abf",
      "6e7be2d51b3b4bd4967a0d0193078629",
      "c0c8f56586684c95a71f5926b1ecc4fb"
     ]
    },
    "id": "HekcHRGGGBUE",
    "outputId": "d3fec17a-03f3-4672-b551-eb88b871dafb"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "utility_files = [\"notebook_utils.py\", \"cmd_helper.py\"]\n",
    "\n",
    "for utility in utility_files:\n",
    "    local_path = Path(utility)\n",
    "    if not local_path.exists():\n",
    "        r = requests.get(\n",
    "            url=f\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/{local_path.name}\",\n",
    "        )\n",
    "        with local_path.open(\"w\") as f:\n",
    "            f.write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Export command:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`optimum-cli export openvino --model CohereForAI/c4ai-command-r-v01 /mnt/research/c4ai-command-r-v01/INT4 --weight-format int4 --task text-generation-with-past --group-size 128 --ratio 1 --all-layers`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prabod/anaconda3/envs/cohere/lib/python3.9/importlib/util.py:245: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  self.__spec__.loader.exec_module(self)\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:03<00:00,  4.13it/s]\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "/home/prabod/anaconda3/envs/cohere/lib/python3.9/site-packages/transformers/cache_utils.py:460: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n",
      "/home/prabod/anaconda3/envs/cohere/lib/python3.9/site-packages/optimum/exporters/openvino/model_patcher.py:515: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "/home/prabod/anaconda3/envs/cohere/lib/python3.9/site-packages/transformers/cache_utils.py:444: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  len(self.key_cache[layer_idx]) == 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”‘\n",
      "â”‚ Weight compression mode   â”‚ % all parameters (layers)   â”‚ % ratio-defining parameters (layers)   â”‚\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¿â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¿â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¥\n",
      "â”‚ int4_asym                 â”‚ 100% (281 / 281)            â”‚ 100% (281 / 281)                       â”‚\n",
      "â”•â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”™\n",
      "\u001b[2KApplying Weight Compression \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m â€¢ \u001b[36m0:04:08\u001b[0m â€¢ \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:08\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "from cmd_helper import optimum_cli\n",
    "\n",
    "model_id = \"CohereForAI/c4ai-command-r-v01\"\n",
    "model_path = Path(model_id.split(\"/\")[-1]) / \"INT4\"\n",
    "\n",
    "model_path = \"/mnt/research\" / model_path\n",
    "if not model_path.exists():\n",
    "    optimum_cli(\n",
    "        model_id,\n",
    "        model_path,\n",
    "        additional_args={\"weight-format\": \"int4\", \"task\": \"text-generation-with-past\",\"group-size\": \"128\", \"ratio\": \"1\", \"all-layers\": \"\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4_STbc7kJji"
   },
   "source": [
    "Once the model export and quantization is complete, move the model assets needed for tokenization in Spark NLP to the `assets` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PP6xDXDC4W7K"
   },
   "source": [
    "Let's have a look inside these two directories and see what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_PATH = model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOLmL1S14W7K",
    "outputId": "32f9bf09-3b78-43b8-e250-9bc24aa4d4ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 17G\n",
      "drwxrwxr-x 3 prabod prabod 4.0K Feb 13 09:13 .\n",
      "drwxrwxr-x 3 prabod prabod 4.0K Feb 13 09:02 ..\n",
      "drwxrwxr-x 2 prabod prabod 4.0K Feb 13 09:13 assets\n",
      "-rw-rw-r-- 1 prabod prabod  810 Feb 13 09:02 config.json\n",
      "-rw-rw-r-- 1 prabod prabod  137 Feb 13 09:02 generation_config.json\n",
      "-rw-rw-r-- 1 prabod prabod 2.8M Feb 13 09:06 openvino_detokenizer.bin\n",
      "-rw-rw-r-- 1 prabod prabod  23K Feb 13 09:06 openvino_detokenizer.xml\n",
      "-rw-rw-r-- 1 prabod prabod  17G Feb 13 09:11 openvino_model.bin\n",
      "-rw-rw-r-- 1 prabod prabod 3.4M Feb 13 09:11 openvino_model.xml\n",
      "-rw-rw-r-- 1 prabod prabod 6.6M Feb 13 09:06 openvino_tokenizer.bin\n",
      "-rw-rw-r-- 1 prabod prabod  40K Feb 13 09:06 openvino_tokenizer.xml\n",
      "-rw-rw-r-- 1 prabod prabod  439 Feb 13 09:02 special_tokens_map.json\n",
      "-rw-rw-r-- 1 prabod prabod  21K Feb 13 09:02 tokenizer_config.json\n",
      "-rw-rw-r-- 1 prabod prabod  20M Feb 13 09:02 tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "!ls -lah {EXPORT_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_dir = EXPORT_PATH / \"assets\"\n",
    "assets_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# copy all the assets to the assets directory (json files, vocab files, etc.)\n",
    "\n",
    "import shutil\n",
    "\n",
    "# copy all json files\n",
    "\n",
    "for file in EXPORT_PATH.glob(\"*.json\"):\n",
    "    shutil.copy(file, assets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQ1SbNAc4W7K",
    "outputId": "bbb93961-3dbf-459f-d3c0-bdca7965bf53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 19692\n",
      "-rw-rw-r-- 1 prabod prabod      810 Feb 13 09:13 config.json\n",
      "-rw-rw-r-- 1 prabod prabod      137 Feb 13 09:13 generation_config.json\n",
      "-rw-rw-r-- 1 prabod prabod      439 Feb 13 09:13 special_tokens_map.json\n",
      "-rw-rw-r-- 1 prabod prabod    20749 Feb 13 09:13 tokenizer_config.json\n",
      "-rw-rw-r-- 1 prabod prabod 20124090 Feb 13 09:13 tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "!ls -l {EXPORT_PATH}/assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svbT3OG24W7L"
   },
   "source": [
    "## 2. Import and Save CoHere in Spark NLP\n",
    "\n",
    "- Let's install and setup Spark NLP in Google Colab\n",
    "- This part is pretty easy via our simple script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6TWf2r14W7L"
   },
   "outputs": [],
   "source": [
    "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYI03iqp4W7L"
   },
   "source": [
    "Let's start Spark with Spark NLP included via our simple `start()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_Oy0zMi4W7L"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "\n",
    "# let's start Spark with Spark NLP\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXCJqb9i4W7M"
   },
   "source": [
    "- Let's use `loadSavedModel` functon in `CoHereTransformer` which allows us to load the OpenVINO model.\n",
    "- Most params will be set automatically. They can also be set later after loading the model in `CoHereTransformer` during runtime, so don't worry about setting them now.\n",
    "- `loadSavedModel` accepts two params, first is the path to the exported model. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
    "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "T3591W9R4W7M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/13 09:19:52 WARN NativeLibrary: Failed to load library null: java.lang.UnsatisfiedLinkError: Can't load library: /tmp/openvino-native14220754060683836653/libtbb.so.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/prabod/spark/jars/spark-core_2.12-3.3.2.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import *\n",
    "\n",
    "CoHere = CoHereTransformer \\\n",
    "    .loadSavedModel(str(EXPORT_PATH), spark) \\\n",
    "    .setMaxOutputLength(50) \\\n",
    "    .setDoSample(False) \\\n",
    "    .setInputCols([\"documents\"]) \\\n",
    "    .setOutputCol(\"generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9X3RphM-4W7M"
   },
   "source": [
    "Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"CohereForAI/c4ai-command-r-v01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6GaugQa4W7M"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "CoHere.write().overwrite().save(f\"{MODEL_NAME}_spark_nlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0kroa6u4W7M"
   },
   "source": [
    "Let's clean up stuff we don't need anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHvWriCn4W7M"
   },
   "outputs": [],
   "source": [
    "!rm -rf {EXPORT_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gz4cU4Q54W7N"
   },
   "source": [
    "Awesome  ðŸ˜Ž !\n",
    "\n",
    "This is your OpenVINO CoHere model from HuggingFace ðŸ¤—  loaded and saved by Spark NLP ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17klLp1M4W7N",
    "outputId": "eccfaaba-5b98-4914-dcfc-aedb8de3d285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 17754944\n",
      "-rw-r--r-- 1 prabod prabod 18181049933 Feb 13 09:34 CoHere_openvino\n",
      "drwxr-xr-x 6 prabod prabod        4096 Feb 13 09:32 fields\n",
      "drwxr-xr-x 2 prabod prabod        4096 Feb 13 09:32 metadata\n"
     ]
    }
   ],
   "source": [
    "! ls -l {MODEL_NAME}_spark_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3R_rS8Fj4W7N"
   },
   "source": [
    "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny CoHere model ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxSo5-b24W7N",
    "outputId": "c4c91a3a-de46-41d7-98c7-e301fbe9419a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:======================================================> (30 + 1) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|result                                                                                                                                                  |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[ Hello, how are you?Hello! I'm doing well, thank you for asking! I'm excited to help you with whatever questions you have today. How can I assist you?]|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "test_data = spark.createDataFrame([\n",
    "            (\n",
    "                1,\n",
    "                \"<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\"\n",
    "            )\n",
    "        ]).toDF(\"id\", \"text\")\n",
    "\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "            .setInputCol(\"text\") \\\n",
    "            .setOutputCol(\"documents\")\n",
    "\n",
    "CoHere = CoHereTransformer \\\n",
    "            .load(f\"{MODEL_NAME}_spark_nlp\") \\\n",
    "            .setMaxOutputLength(50) \\\n",
    "            .setDoSample(False) \\\n",
    "            .setBeamSize(1) \\\n",
    "            .setInputCols([\"documents\"]) \\\n",
    "            .setOutputCol(\"generation\")\n",
    "\n",
    "pipeline = Pipeline().setStages([document_assembler, CoHere])\n",
    "results = pipeline.fit(test_data).transform(test_data)\n",
    "\n",
    "results.select(\"generation.result\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdvQAAfo4W7N"
   },
   "source": [
    "That's it! You can now go wild and use hundreds of CoHere models from HuggingFace ðŸ¤— in Spark NLP ðŸš€\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cohere",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
