{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V5XcDCnVgSi"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/openvino/HuggingFace_OpenVINO_in_Spark_NLP_UAE.ipynb)\n",
        "\n",
        "# Import OpenVINO XlmRoBertaSentenceEmbeddings  models from HuggingFace ü§ó into Spark NLP üöÄ\n",
        "\n",
        "This notebook provides a detailed walkthrough on optimizing and exporting BGE models from HuggingFace for use in Spark NLP, leveraging the various tools provided in the [Intel OpenVINO toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html) ecosystem.\n",
        "\n",
        "Let's keep in mind a few things before we start üòä\n",
        "\n",
        "- OpenVINO support was introduced in  `Spark NLP 5.4.0`, enabling high performance inference for models. Please make sure you have upgraded to the latest Spark NLP release.\n",
        "- You can import models for XlmRoBertaSentenceEmbeddings  from XlmRoBertaSentenceEmbeddings  and they have to be in `Fill Mask` category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aghasVppVgSk"
      },
      "source": [
        "## 1. Export and Save the HuggingFace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be4HsTDMVgSk"
      },
      "source": [
        "- Let's install `transformers` and `openvino` packages with other dependencies. You don't need `openvino` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.48.3`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7L-2ZWUVgSl",
        "outputId": "8d34f1f0-b30d-44b2-c15a-05055d3e2ffb"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade transformers==4.48.3 optimum[openvino]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI7uz_6hVgSl"
      },
      "source": [
        "[Optimum Intel](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#openvino) is the interface between the Transformers library and the various model optimization and acceleration tools provided by Intel. HuggingFace models loaded with optimum-intel are automatically optimized for OpenVINO, while being compatible with the Transformers API.\n",
        "- To load a HuggingFace model directly for inference/export, just replace the `AutoModelForXxx` class with the corresponding `OVModelForXxx` class. We can use this to import and export OpenVINO models with `from_pretrained` and `save_pretrained`.\n",
        "- By setting `export=True`, the source model is converted to OpenVINO IR format on the fly.\n",
        "- We'll use [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) model from HuggingFace as an example and load it as a `OVModelForFeatureExtraction`, representing an OpenVINO model.\n",
        "- In addition to the OVModelForFeatureExtraction model, we also need to save the `AutoTokenizer`. This is the same for every model, these are assets (saved in `/assets`) needed for tokenization inside Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF5Pp3DuVgSm",
        "outputId": "5f02c5e8-dc62-4ab4-a80d-4dd7fa18486c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('ov_models/xlm-roberta-base/tokenizer_config.json',\n",
              " 'ov_models/xlm-roberta-base/special_tokens_map.json',\n",
              " 'ov_models/xlm-roberta-base/sentencepiece.bpe.model',\n",
              " 'ov_models/xlm-roberta-base/added_tokens.json',\n",
              " 'ov_models/xlm-roberta-base/tokenizer.json')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from optimum.intel import OVModelForFeatureExtraction\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"xlm-roberta-base\"\n",
        "EXPORT_PATH = f\"ov_models/{MODEL_NAME}\"\n",
        "\n",
        "ov_model = OVModelForFeatureExtraction.from_pretrained(MODEL_NAME, export=True, trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "ov_model.save_pretrained(EXPORT_PATH)\n",
        "tokenizer.save_pretrained(EXPORT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBnlNnzqboAH"
      },
      "source": [
        "- We need to move `sentencepiece.bpe.model` from the tokenizer to `assets` folder which Spark NLP will look for\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hRD1xsRpeLJN"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {EXPORT_PATH}/assets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JjuxeO8sC7ry"
      },
      "outputs": [],
      "source": [
        "!mv {EXPORT_PATH}/sentencepiece.bpe.model {EXPORT_PATH}/assets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caWsKjX3br_U",
        "outputId": "7ce7d63c-2913-498a-8e82-05276d08633c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 4952\n",
            "-rw-r--r-- 1 root root 5069051 Jul  3 08:49 sentencepiece.bpe.model\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}/assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-5CakmEk2J-"
      },
      "source": [
        "## Import and Save XLM-RoBERTa in Spark NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRYjGMvIbvU3"
      },
      "source": [
        "- Install and set up Spark NLP in Google Colab\n",
        "- This example uses specific versions of `pyspark` and `spark-nlp` that have been tested with the transformer model to ensure everything runs smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI6D8fp9k2J-",
        "outputId": "df189240-23e4-43d1-beb6-895f9a357e77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark==3.5.4 spark-nlp==5.5.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xeiWZPWk2J-"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2npUDyIk2J-",
        "outputId": "7fdc1a54-453b-4eda-e64b-8bc2880d70ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark NLP version:  5.5.3\n",
            "Apache Spark version:  3.5.4\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSUj7FtKk2J-"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `XlmRoBertaSentenceEmbeddings` which allows us to load the ONNX model\n",
        "- Most params will be set automatically. They can also be set later after loading the model in `XlmRoBertaSentenceEmbeddings` during runtime, so don't worry about setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the exported model. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- `setStorageRef` is very important. When you are training a task like NER or any Text Classification, we use this reference to bound the trained model to this specific embeddings so you won't load a different embeddings by mistake and see terrible results üòä\n",
        "- It's up to you what you put in `setStorageRef` but it cannot be changed later on. We usually use the name of the model to be clear, but you can get creative if you want!\n",
        "- The `dimension` param is is purely cosmetic and won't change anything. It's mostly for you to know later via `.getDimension` what is the dimension of your model. So set this accordingly.\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9zwF8BR-k2J-"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import XlmRoBertaSentenceEmbeddings\n",
        "\n",
        "xlm_roberta = XlmRoBertaSentenceEmbeddings.loadSavedModel(f\"{EXPORT_PATH}\", spark)\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"xlm_roberta\")\\\n",
        "    .setCaseSensitive(True)\\\n",
        "    .setDimension(768)\\\n",
        "    .setStorageRef('xlm_roberta_base')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv-tk-Yyk2J_"
      },
      "source": [
        "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Q3VhjuF1k2J_"
      },
      "outputs": [],
      "source": [
        "xlm_roberta.write().overwrite().save(f\"{MODEL_NAME}_spark_nlp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeIh82LNk2J_"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "y1c7D7aak2J_"
      },
      "outputs": [],
      "source": [
        "!rm -rf {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xYRp9OTk2J_"
      },
      "source": [
        "Awesome  üòé !\n",
        "\n",
        "This is your ONNX XLM-RoBERTa model from HuggingFace ü§ó  loaded and saved by Spark NLP üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW_pPdenk2J_",
        "outputId": "eafadb8a-01e9-40fb-c22b-2d6c438d1e24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 1089292\n",
            "drwxr-xr-x 2 root root       4096 Jul  3 09:01 metadata\n",
            "-rw-r--r-- 1 root root 1110354169 Jul  3 09:01 xlmroberta_openvino\n",
            "-rw-r--r-- 1 root root    5069051 Jul  3 09:01 xlmroberta_spp\n"
          ]
        }
      ],
      "source": [
        "! ls -l {MODEL_NAME}_spark_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWNCKtK8k2J_"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny XLM-RoBERTa model üòä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CewIy2tnk2J_",
        "outputId": "1a66baa8-88bd-47f1-eacc-fd0b5d4844d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|          embeddings|\n",
            "+--------------------+\n",
            "|[4.0073096E-4, 0....|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import SentenceDetector, XlmRoBertaSentenceEmbeddings\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentence_detector = SentenceDetector() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentence\")\n",
        "\n",
        "xlm_roberta_loaded = XlmRoBertaSentenceEmbeddings.load(f\"{MODEL_NAME}_spark_nlp\") \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"xlm_roberta\")\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    sentence_detector,\n",
        "    xlm_roberta_loaded\n",
        "])\n",
        "\n",
        "data = spark.createDataFrame([\n",
        "    [\"William Henry Gates III (born October 28, 1955) is an American business magnate, software developer, investor, and philanthropist.\"]\n",
        "]).toDF(\"text\")\n",
        "\n",
        "model = pipeline.fit(data)\n",
        "result = model.transform(data)\n",
        "\n",
        "result.selectExpr(\"explode(xlm_roberta.embeddings) as embeddings\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nQ-9GAPk2J_"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of XLM-RoBERTa models from HuggingFace ü§ó in Spark NLP üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
