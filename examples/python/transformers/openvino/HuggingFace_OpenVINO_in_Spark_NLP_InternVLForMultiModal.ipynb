{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/openvino/HuggingFace_OpenVINO_in_Spark_NLP_InternVLForMultiModal.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import OpenVINO InternVL models from HuggingFace ðŸ¤— into Spark NLP ðŸš€\n",
    "\n",
    "This notebook provides a detailed walkthrough on optimizing and importing InternVL models from HuggingFace for use in Spark NLP, with [Intel OpenVINO toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html). The focus is on converting the model to the OpenVINO format and applying precision optimizations (INT8 and INT4), to enhance the performance and efficiency on CPU platforms using [Optimum Intel](https://huggingface.co/docs/optimum/main/en/intel/inference).\n",
    "\n",
    "Let's keep in mind a few things before we start ðŸ˜Š\n",
    "\n",
    "- OpenVINO support was introduced in `Spark NLP 5.4.0`, enabling high performance CPU inference for models. So please make sure you have upgraded to the latest Spark NLP release.\n",
    "- Model quantization is a computationally expensive process, so it is recommended to use a runtime with more than 32GB memory for exporting the quantized model from HuggingFace.\n",
    "- You can import InternVL models via `InternVL`. These models are usually under the `Text Generation` category and have `InternVL` in their labels.\n",
    "- Reference: [InternVL](https://huggingface.co/docs/transformers/model_doc/llama#transformers.InternVL)\n",
    "- Some [example models](https://huggingface.co/models?search=InternVL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Installation](#setup-and-installation)\n",
    "2. [Model Configuration](#model-configuration)\n",
    "3. [Model Loading and Preparation](#model-loading-and-preparation)\n",
    "4. [Model Conversion to OpenVINO](#model-conversion-to-openvino)\n",
    "5. [Model Quantization](#model-quantization)\n",
    "6. [Model Merger Implementation](#model-merger-implementation)\n",
    "7. [Testing OpenVINO Model](#7-testing-openvino-model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install all the required dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33m  DEPRECATION: Building 'jstyleson' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'jstyleson'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Building 'grapheme' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'grapheme'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install OpenVINO and NNCF for model optimization\n",
    "import platform\n",
    "\n",
    "%pip install -q \"transformers>4.36\" \"torch>=2.1\" \"torchvision\" \"einops\" \"timm\" \"Pillow\" \"gradio>=4.36\"  --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q \"nncf>=2.14.0\" \"datasets\"\n",
    "%pip install -q \"git+https://github.com/huggingface/optimum-intel.git\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q -U --pre \"openvino>=2025.0\" \"openvino-tokenizers>=2025.0\" \"openvino-genai>=2025.0\" --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    %pip install -q \"numpy<2.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Configuration\n",
    "\n",
    "Configure the environment to disable tokenizer parallelism for better compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable tokenizer parallelism to avoid potential issues\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prabod/anaconda3/envs/intern/lib/python3.11/site-packages/openvino/runtime/__init__.py:10: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import types\n",
    "from typing import Optional, List\n",
    "import gc\n",
    "import openvino as ov\n",
    "from openvino.runtime import opset13\n",
    "import nncf\n",
    "import numpy as np\n",
    "import torch\n",
    "from openvino.frontend.pytorch.patch_model import __make_16bit_traceable\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Configuration\n",
    "\n",
    "Set up the model ID and quantization parameters for the conversion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = [\n",
    "    \"OpenGVLab/InternVL3-1B\",\n",
    "    # \"OpenGVLab/InternVL3-2B\",\n",
    "    # \"OpenGVLab/InternVL3-8B\",\n",
    "    # \"OpenGVLab/InternVL3-9B\",\n",
    "    # \"OpenGVLab/InternVL3-14B\",\n",
    "    # \"OpenGVLab/InternVL2_5-1B\",\n",
    "    # \"OpenGVLab/InternVL2_5-2B\",\n",
    "    # \"OpenGVLab/InternVL2_5-4B\",\n",
    "    # \"OpenGVLab/InternVL2_5-8B\",\n",
    "    # \"OpenGVLab/InternVL2-1B\",\n",
    "    # \"OpenGVLab/InternVL2-2B\",\n",
    "    # \"OpenGVLab/InternVL2-4B\",\n",
    "    # \"OpenGVLab/InternVL2-8B\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading and Preparation\n",
    "\n",
    "Load the model processor, configuration, and prepare the model for conversion to OpenVINO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "for model_id in model_ids:\n",
    "    output_dir = f\"./models/int4/{model_id}\"\n",
    "    # check if the model is already optimized\n",
    "    if not os.path.exists(f\"{output_dir}/openvino_language_model.xml\") and not os.path.exists(f\"{output_dir}/openvino_language_model.bin\"):\n",
    "        !optimum-cli export openvino --model {model_id} --weight-format int4 {output_dir} --trust-remote-code --dataset contextual --awq --num-samples 32\n",
    "    else:\n",
    "        print(f\"Model {model_id} already optimized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Conversion to OpenVINO\n",
    "\n",
    "Define paths for the converted model components and implement conversion utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating assets directory at models/int4/OpenGVLab/InternVL3-1B/assets\n",
      "Creating assets directory at models/int4/OpenGVLab/InternVL3-2B/assets\n",
      "Creating assets directory at models/int4/OpenGVLab/InternVL3-8B/assets\n",
      "Creating assets directory at models/int4/OpenGVLab/InternVL3-14B/assets\n",
      "Creating assets directory at models/int4/OpenGVLab/InternVL2_5-1B/assets\n",
      "Creating assets directory at models/int4/OpenGVLab/InternVL2_5-4B/assets\n"
     ]
    }
   ],
   "source": [
    "for model_id in model_ids:\n",
    "    # change vision embed avg pool to opset1\n",
    "    # this is a workaround for the issue with the InternVL model\n",
    "    output_dir = f\"./models/int4/{model_id}\"\n",
    "    if os.path.exists(output_dir):\n",
    "        if not os.path.exists(f\"{output_dir}/assets\"):\n",
    "            output_dir = Path(output_dir)\n",
    "            assets_dir = output_dir/\"assets\"\n",
    "            assets_dir.mkdir(exist_ok=True)\n",
    "            print(f\"Creating assets directory at {assets_dir}\")\n",
    "\n",
    "            # copy all the assets to the assets directory (json files, vocab files, etc.)\n",
    "            for file in output_dir.glob(\"*.json\"):\n",
    "                shutil.copy(file, assets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_torchscript_cache():\n",
    "    \"\"\"\n",
    "    Helper function for removing cached model representation to prevent memory leaks\n",
    "    during model conversion.\n",
    "    \"\"\"\n",
    "    torch._C._jit_clear_class_registry()\n",
    "    torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "    torch.jit._state._clear_class_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Merger Implementation\n",
    "\n",
    "Implement the model merger to combine text and image components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoProcessor\n",
    "import openvino as ov\n",
    "import gc\n",
    "\n",
    "class MergeMultiModalInputs(torch.nn.Module):\n",
    "    def __init__(self,image_token_index=151648):\n",
    "        \"\"\"\n",
    "        Merge multimodal inputs with the image token index.\n",
    "        Args:\n",
    "            image_token_index (int): The token index for the image token.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_token_index = image_token_index\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        vision_embeds,\n",
    "        inputs_embeds,\n",
    "        input_ids,\n",
    "    ):\n",
    "        image_features = vision_embeds\n",
    "        inputs_embeds = inputs_embeds\n",
    "        special_image_mask = (input_ids == self.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n",
    "        # image_features = image_features.to(inputs_embeds.dtype)\n",
    "        final_embedding = inputs_embeds.masked_scatter(special_image_mask, image_features)\n",
    "\n",
    "        return {\n",
    "            \"final_embedding\": final_embedding\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting model OpenGVLab/InternVL3-1B merger to OpenVINO format...\n",
      "WARNING:nncf:NNCF provides best results with torch==2.6.*, while current torch version is 2.7.0+cpu. If you encounter issues, consider switching to torch==2.6.*\n",
      "Converting model OpenGVLab/InternVL3-2B merger to OpenVINO format...\n",
      "Converting model OpenGVLab/InternVL3-8B merger to OpenVINO format...\n",
      "Converting model OpenGVLab/InternVL3-14B merger to OpenVINO format...\n",
      "Converting model OpenGVLab/InternVL2_5-1B merger to OpenVINO format...\n",
      "Converting model OpenGVLab/InternVL2_5-4B merger to OpenVINO format...\n",
      "Converting model OpenGVLab/InternVL2-1B merger to OpenVINO format...\n"
     ]
    }
   ],
   "source": [
    "for model_id in model_ids:\n",
    "    if os.path.exists(f\"./models/int4/{model_id}/openvino_language_model.xml\"):\n",
    "        print(f\"Converting model {model_id} merger to OpenVINO format...\")\n",
    "        core = ov.Core()\n",
    "        output_dir = f\"./models/int4/{model_id}\"\n",
    "        model_merger_path = f\"{output_dir}/openvino_merger_model.xml\"\n",
    "        config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "        image_size = config.force_image_size or config.vision_config.image_size\n",
    "        patch_size = config.vision_config.patch_size\n",
    "        patch_size = patch_size\n",
    "        select_layer = config.select_layer\n",
    "        num_image_token = int((image_size // patch_size) ** 2 * (config.downsample_ratio ** 2))\n",
    "        processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "        IMG_CONTEXT_TOKEN='<IMG_CONTEXT>'\n",
    "        img_context_token_id = processor.convert_tokens_to_ids(IMG_CONTEXT_TOKEN)\n",
    "\n",
    "        multimodal_merger = MergeMultiModalInputs(img_context_token_id)\n",
    "        with torch.no_grad():\n",
    "            ov_model = ov.convert_model(\n",
    "                multimodal_merger,\n",
    "                example_input= {\n",
    "                    \"input_ids\": torch.ones([2, 1198], dtype=torch.int64),\n",
    "                    \"inputs_embeds\": torch.ones([2, 1198, config.llm_config.hidden_size], dtype=torch.float32),\n",
    "                    \"vision_embeds\": torch.ones([2, num_image_token, config.llm_config.hidden_size], dtype=torch.float32),\n",
    "                }\n",
    "            )\n",
    "            ov.save_model(ov_model, model_merger_path)\n",
    "            del ov_model\n",
    "            cleanup_torchscript_cache()\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Testing OpenVINO Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "device = \"CPU\"\n",
    "\n",
    "# lets pick the first model\n",
    "model_id = model_ids[0]\n",
    "output_dir = f\"./models/int4/{model_id}\"\n",
    "output_dir = Path(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths for the exported models\n",
    "image_embed_path = output_dir / \"openvino_vision_embeddings_model.xml\"\n",
    "language_model_path = output_dir / \"openvino_language_model.xml\"\n",
    "text_embeddings_path = output_dir / \"openvino_text_embeddings_model.xml\"\n",
    "model_merger_path = output_dir / \"openvino_merger_model.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the models\n",
    "language_model = core.read_model(language_model_path)\n",
    "compiled_language_model = core.compile_model(language_model, \"AUTO\")\n",
    "\n",
    "image_embed_model = core.compile_model(image_embed_path, device)\n",
    "text_embeddings_model = core.compile_model(text_embeddings_path, device)\n",
    "multimodal_merger_model = core.compile_model(model_merger_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "# from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    print(f\"Image size: {image.size}\")\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â€˜imagesâ€™: File exists\n",
      "--2025-05-12 07:04:32--  https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\n",
      "Resolving cdn.britannica.com (cdn.britannica.com)... 18.64.50.12, 18.64.50.124, 18.64.50.34, ...\n",
      "Connecting to cdn.britannica.com (cdn.britannica.com)|18.64.50.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 326629 (319K) [image/jpeg]\n",
      "Saving to: â€˜images/image1.jpgâ€™\n",
      "\n",
      "images/image1.jpg   100%[===================>] 318.97K   528KB/s    in 0.6s    \n",
      "\n",
      "2025-05-12 07:04:33 (528 KB/s) - â€˜images/image1.jpgâ€™ saved [326629/326629]\n",
      "\n",
      "--2025-05-12 07:04:34--  https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg\n",
      "Resolving huggingface.co (huggingface.co)... 65.8.134.119, 65.8.134.40, 65.8.134.116, ...\n",
      "Connecting to huggingface.co (huggingface.co)|65.8.134.119|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.hf.co/repos/6d/5b/6d5bc8ab63260b95af97fe910b8fb660b88a9b19e97bfada63102f0f1ee9110c/8b21ba78250f852ca5990063866b1ace6432521d0251bde7f8de783b22c99a6d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27bee.jpg%3B+filename%3D%22bee.jpg%22%3B&response-content-type=image%2Fjpeg&Expires=1747037074&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NzAzNzA3NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzZkLzViLzZkNWJjOGFiNjMyNjBiOTVhZjk3ZmU5MTBiOGZiNjYwYjg4YTliMTllOTdiZmFkYTYzMTAyZjBmMWVlOTExMGMvOGIyMWJhNzgyNTBmODUyY2E1OTkwMDYzODY2YjFhY2U2NDMyNTIxZDAyNTFiZGU3ZjhkZTc4M2IyMmM5OWE2ZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=YTgr%7EoXbXlGctRz4il4J995Jc3JI6byOjzvDVuSWUIAuF%7EXDg1jSePwIgAp44bx%7Eg0IYPNCM2sVxrhBp%7ElBgu4nSMX7U8mj7B33x7ITjieWw-AVnxf7ARZyt5SFq9csgcIK9WpBbF7YKWqO0XaA45Nm%7E9C8BQ1KtUFL6qTJ6qTFxFob%7EXWeu774RdtFMIGAWPar8aAwLHEMBhWHi%7Ea85V%7Elmg4B5x8xTizdZZKlRnX2f5RCT%7EMzwkERS-GpPjv-1S0oJ%7EHPPO8q5ZCK003siFKThgIx5MiSdxTs2zbafj-0gHKpcLEh2olPph%7EVEA1tDdm2qxjuKD5-vUgwnT5uSUA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
      "--2025-05-12 07:04:34--  https://cdn-lfs-us-1.hf.co/repos/6d/5b/6d5bc8ab63260b95af97fe910b8fb660b88a9b19e97bfada63102f0f1ee9110c/8b21ba78250f852ca5990063866b1ace6432521d0251bde7f8de783b22c99a6d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27bee.jpg%3B+filename%3D%22bee.jpg%22%3B&response-content-type=image%2Fjpeg&Expires=1747037074&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NzAzNzA3NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzZkLzViLzZkNWJjOGFiNjMyNjBiOTVhZjk3ZmU5MTBiOGZiNjYwYjg4YTliMTllOTdiZmFkYTYzMTAyZjBmMWVlOTExMGMvOGIyMWJhNzgyNTBmODUyY2E1OTkwMDYzODY2YjFhY2U2NDMyNTIxZDAyNTFiZGU3ZjhkZTc4M2IyMmM5OWE2ZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=YTgr%7EoXbXlGctRz4il4J995Jc3JI6byOjzvDVuSWUIAuF%7EXDg1jSePwIgAp44bx%7Eg0IYPNCM2sVxrhBp%7ElBgu4nSMX7U8mj7B33x7ITjieWw-AVnxf7ARZyt5SFq9csgcIK9WpBbF7YKWqO0XaA45Nm%7E9C8BQ1KtUFL6qTJ6qTFxFob%7EXWeu774RdtFMIGAWPar8aAwLHEMBhWHi%7Ea85V%7Elmg4B5x8xTizdZZKlRnX2f5RCT%7EMzwkERS-GpPjv-1S0oJ%7EHPPO8q5ZCK003siFKThgIx5MiSdxTs2zbafj-0gHKpcLEh2olPph%7EVEA1tDdm2qxjuKD5-vUgwnT5uSUA__&Key-Pair-Id=K24J24Z295AEI9\n",
      "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.155.88.100, 18.155.88.66, 18.155.88.18, ...\n",
      "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.155.88.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5373297 (5.1M) [image/jpeg]\n",
      "Saving to: â€˜images/image2.jpgâ€™\n",
      "\n",
      "images/image2.jpg   100%[===================>]   5.12M  12.0MB/s    in 0.4s    \n",
      "\n",
      "2025-05-12 07:04:35 (12.0 MB/s) - â€˜images/image2.jpgâ€™ saved [5373297/5373297]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir images\n",
    "!wget -O images/image1.jpg https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\n",
    "!wget -O images/image2.jpg https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: (1600, 1067)\n",
      "This image showcases the iconic Statue of Liberty on Liberty Island, a symbol of freedom and welcome to immigrants around the world. The statue is prominently featured in the foreground, standing on a stone pedestal surrounded by water. The background reveals a bustling city skyline with numerous tall buildings, including the Empire State Building, suggesting a major metropolitan area. The sky is clear, and the lighting suggests it might be early morning or late afternoon, casting a warm glow over the scene. The water is calm, and there are a few small boats visible, adding to the serene atmosphere. The island is bordered by a grassy area with trees, and the overall setting is picturesque and inviting.\n"
     ]
    }
   ],
   "source": [
    "from transformers.image_utils import load_image as load_image_transformers\n",
    "from transformers import AutoProcessor, TextStreamer\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "IMG_START_TOKEN='<img>'\n",
    "IMG_END_TOKEN='</img>'\n",
    "IMG_CONTEXT_TOKEN='<IMG_CONTEXT>'\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "question = \"<|im_start|><image>\\nDescribe this image in detail. reply in dot points <|im_end|><|im_start|>assistant\\n\"\n",
    "\n",
    "pixel_values = load_image(\"images/image1.jpg\", max_num=12)\n",
    "num_patches = pixel_values.shape[0]\n",
    "num_patches_list = [pixel_values.shape[0]] if pixel_values is not None else []\n",
    "image_size = config.force_image_size or config.vision_config.image_size\n",
    "patch_size = config.vision_config.patch_size\n",
    "patch_size = patch_size\n",
    "select_layer = config.select_layer\n",
    "num_image_token = int((image_size // patch_size) ** 2 * (config.downsample_ratio ** 2))\n",
    "img_context_token_id = processor.convert_tokens_to_ids(IMG_CONTEXT_TOKEN)\n",
    "image_tokens = IMG_START_TOKEN + IMG_CONTEXT_TOKEN * num_image_token * num_patches + IMG_END_TOKEN\n",
    "query = question.replace('<image>', image_tokens, 1)\n",
    "\n",
    "inputs_new = processor(query,return_tensors=\"pt\")\n",
    "inputs_new[\"pixel_values\"] = pixel_values\n",
    "\n",
    "request = compiled_language_model.create_infer_request()\n",
    "merge_model_request = multimodal_merger_model.create_infer_request()\n",
    "# Set the input names\n",
    "input_names = {key.get_any_name(): idx for idx, key in enumerate(language_model.inputs)}\n",
    "inputs = {}\n",
    "# Set the initial input_ids\n",
    "current_input_ids = inputs_new[\"input_ids\"]\n",
    "attention_mask = inputs_new[\"attention_mask\"]\n",
    "position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "pixel_values = inputs_new[\"pixel_values\"]\n",
    "generated_tokens = []\n",
    "\n",
    "for i in range(200):\n",
    "    # Generate input embeds each time\n",
    "    text_embeds = torch.from_numpy(\n",
    "            text_embeddings_model(current_input_ids\n",
    "            )[0]\n",
    "        )\n",
    "    if current_input_ids.shape[-1] > 1:\n",
    "        vision_embeds = torch.from_numpy(\n",
    "            image_embed_model(\n",
    "                {\n",
    "                    \"pixel_values\": pixel_values,\n",
    "                }\n",
    "            )[0]\n",
    "        )\n",
    "        vision_embeds = vision_embeds.reshape(1, -1, config.llm_config.hidden_size)\n",
    "        merge_model_request.start_async({\n",
    "            \"vision_embeds\": vision_embeds,\n",
    "            \"inputs_embeds\": text_embeds,\n",
    "            \"input_ids\": current_input_ids,\n",
    "        }, share_inputs=True)\n",
    "        merge_model_request.wait()\n",
    "        final_embedding = torch.from_numpy(merge_model_request.get_tensor(\"final_embedding\").data)\n",
    "    else:\n",
    "        final_embedding = text_embeds\n",
    "\n",
    "    \n",
    "    if i>0:\n",
    "        inputs = {}\n",
    "    # Prepare inputs for the model\n",
    "    inputs[\"inputs_embeds\"] = final_embedding\n",
    "    inputs[\"attention_mask\"] = attention_mask\n",
    "    inputs[\"position_ids\"] = position_ids\n",
    "    # inputs[\"token_type_ids\"] = token_type_ids\n",
    "    if \"beam_idx\" in input_names:\n",
    "        inputs[\"beam_idx\"] = np.arange(attention_mask.shape[0], dtype=int)\n",
    "    \n",
    "    # Start inference\n",
    "    request.start_async(inputs, share_inputs=True)\n",
    "    request.wait()\n",
    "    \n",
    "    # Get the logits and find the next token\n",
    "    logits = torch.from_numpy(request.get_tensor(\"logits\").data)\n",
    "    next_token = logits.argmax(-1)[0][-1]\n",
    "    \n",
    "    # Append the generated token\n",
    "    generated_tokens.append(next_token)\n",
    "    \n",
    "    # Update input_ids with the new token\n",
    "    current_input_ids = torch.cat([next_token.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "    \n",
    "    # update the attention mask\n",
    "    attention_mask = torch.cat([attention_mask, torch.ones_like(attention_mask[:, :1])], dim=-1)\n",
    "\n",
    "    # Update inputs for the next iteration\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "    position_ids = position_ids[:, -current_input_ids.shape[1] :]\n",
    "    inputs[\"position_ids\"] = position_ids\n",
    "    token_type_ids = torch.zeros_like(current_input_ids)\n",
    "\n",
    "generated_text = processor.decode(generated_tokens, skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Import and Save in Spark NLP\n",
    "- Let's install and setup Spark NLP in Google Colab\n",
    "- This part is pretty easy via our simple script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start Spark with Spark NLP included via our simple `start()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "\n",
    "# let's start Spark with Spark NLP\n",
    "spark = sparknlp.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "model_id = model_ids[0]\n",
    "output_dir = f\"./models/int4/{model_id}\"\n",
    "output_dir = Path(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/12 07:55:26 WARN NativeLibrary: Failed to load library null: java.lang.UnsatisfiedLinkError: Can't load library: /tmp/openvino-native17051517669691827340/libtbb.so.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/prabod/spark/jars/spark-core_2.12-3.3.2.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "imageClassifier = InternVLForMultiModal \\\n",
    "            .loadSavedModel(str(output_dir),spark) \\\n",
    "            .setInputCols(\"image_assembler\") \\\n",
    "            .setOutputCol(\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "imageClassifier.write().overwrite().save(f\"file:///tmp/{model_id}_spark_nlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml import Pipeline\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# download two images to test into ./images folder\n",
    "\n",
    "url1 = \"https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/d5fbbd1a-d484-415c-88cb-9986625b7b11\"\n",
    "url2 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "\n",
    "Path(\"images\").mkdir(exist_ok=True)\n",
    "\n",
    "!wget -q -O images/image1.jpg {url1}\n",
    "!wget -q -O images/image2.jpg {url2}\n",
    "\n",
    "\n",
    "\n",
    "images_path = \"file://\" + os.getcwd() + \"/images/\"\n",
    "image_df = spark.read.format(\"image\").load(\n",
    "    path=images_path\n",
    ")\n",
    "\n",
    "test_df = image_df.withColumn(\"text\", lit(\"<|im_start|><image>\\nDescribe this image in detail.<|im_end|><|im_start|>assistant\\n\")) \\\n",
    "\n",
    "image_assembler = ImageAssembler().setInputCol(\"image\").setOutputCol(\"image_assembler\")\n",
    "\n",
    "imageClassifier = InternVLForMultiModal.load(f\"file:///tmp/{model_id}_spark_nlp\")\\\n",
    "            .setMaxOutputLength(50) \\\n",
    "            .setInputCols(\"image_assembler\") \\\n",
    "            .setOutputCol(\"answer\")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "            stages=[\n",
    "                image_assembler,\n",
    "                imageClassifier,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "model = pipeline.fit(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path: /mnt/research/Projects/ModelZoo/internVL/images/image1.jpg\n",
      "[Annotation(document, 0, 227, The image features a gray tabby cat with fluffy fur, lying on its back inside an open cardboard box. The cat appears to be relaxed and content, with its eyes closed and ears perked up. The box is placed on a light-colored carpet, Map(), [])]\n"
     ]
    }
   ],
   "source": [
    "light_pipeline = LightPipeline(model)\n",
    "image_path = os.getcwd() + \"/images/\" + \"image1.jpg\"\n",
    "print(\"image_path: \" + image_path)\n",
    "annotations_result = light_pipeline.fullAnnotateImage(\n",
    "    image_path,\n",
    "    \"<|im_start|><image>\\nDescribe this image in detail.<|im_end|><|im_start|>assistant\\n\",\n",
    ")\n",
    "\n",
    "for result in annotations_result:\n",
    "    print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id in model_ids:\n",
    "    ZIP_NAME = f\"{model_id.split('/')[-1].replace(' ','_').replace('-','_').lower()}_int4_sn\"\n",
    "    !aws s3 cp /tmp/{model_id}_spark_nlp/{ZIP_NAME}.zip s3://dev.johnsnowlabs.com/prabod/models/{ZIP_NAME}.zip --acl public-read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
