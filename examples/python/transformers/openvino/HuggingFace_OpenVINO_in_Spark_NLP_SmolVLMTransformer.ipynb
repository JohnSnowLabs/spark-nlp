{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/openvino/HuggingFace_OpenVINO_in_Spark_NLP_SmolVLMTransformer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import OpenVINO SmolVLM models from HuggingFace ğŸ¤— into Spark NLP ğŸš€\n",
    "\n",
    "This notebook provides a detailed walkthrough on optimizing and importing SmolVLM models from HuggingFace for use in Spark NLP, with [Intel OpenVINO toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html). The focus is on converting the model to the OpenVINO format and applying precision optimizations (INT8 and INT4), to enhance the performance and efficiency on CPU platforms using [Optimum Intel](https://huggingface.co/docs/optimum/main/en/intel/inference).\n",
    "\n",
    "Let's keep in mind a few things before we start ğŸ˜Š\n",
    "\n",
    "- OpenVINO support was introduced in `Spark NLP 5.4.0`, enabling high performance CPU inference for models. So please make sure you have upgraded to the latest Spark NLP release.\n",
    "- Model quantization is a computationally expensive process, so it is recommended to use a runtime with more than 32GB memory for exporting the quantized model from HuggingFace.\n",
    "- You can import SmolVLM models via `SmolVLM`. These models are usually under the `Text Generation` category and have `SmolVLM` in their labels.\n",
    "- Reference: [SmolVLM](https://huggingface.co/docs/transformers/model_doc/llama#transformers.SmolVLM)\n",
    "- Some [example models](https://huggingface.co/models?search=SmolVLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Installation](#setup-and-installation)\n",
    "2. [Model Configuration](#model-configuration)\n",
    "3. [Model Loading and Preparation](#model-loading-and-preparation)\n",
    "4. [Model Conversion to OpenVINO](#model-conversion-to-openvino)\n",
    "5. [Model Quantization](#model-quantization)\n",
    "6. [Model Merger Implementation](#model-merger-implementation)\n",
    "7. [Testing OpenVINO Model](#7-testing-openvino-model)\n",
    "8. [Import and Save in Spark NLP](#8-import-and-save-in-spark-nlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install all the required dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenVINO and NNCF for model optimization\n",
    "%pip install -qU \"openvino>=2024.4.0\" \"nncf>=2.13.0\"\n",
    "\n",
    "# Install NLP and tokenization libraries\n",
    "%pip install -q  \"sentencepiece\" \"tokenizers>=0.12.1\" \"transformers>=4.46.0\" \"gradio>=4.36\"\n",
    "\n",
    "# Install OpenVINO nightly builds for latest features\n",
    "%pip install -q -U --pre --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly openvino-tokenizers openvino openvino-genai\n",
    "\n",
    "# Install HuggingFace Hub and PyTorch\n",
    "%pip install -q --upgrade huggingface_hub\n",
    "%pip install -q --upgrade torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Configuration\n",
    "\n",
    "Configure the environment to disable tokenizer parallelism for better compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable tokenizer parallelism to avoid potential issues\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prabod/anaconda3/envs/smolvlm/lib/python3.9/site-packages/openvino/runtime/__init__.py:10: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  warnings.warn(\n",
      "/home/prabod/anaconda3/envs/smolvlm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import types\n",
    "from typing import Optional, List\n",
    "import gc\n",
    "import openvino as ov\n",
    "from openvino.runtime import opset13\n",
    "import nncf\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoConfig,AutoModelForVision2Seq\n",
    "from openvino.frontend.pytorch.patch_model import __make_16bit_traceable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Configuration\n",
    "\n",
    "Set up the model ID and quantization parameters for the conversion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceTB/SmolVLM-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_method = \"int4\"\n",
    "output_dir = Path(f\"models/{model_id}/{quantization_method}\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading and Preparation\n",
    "\n",
    "Load the model processor, configuration, and prepare the model for conversion to OpenVINO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True, )\n",
    "\n",
    "# load the config\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# Change the model to use SDPA attention\n",
    "# This is a workaround for the model to be compatible with OpenVINO\n",
    "config.text_config._attn_implementation = \"sdpa\"\n",
    "\n",
    "\n",
    "# export the processor and config to output_dir/assets\n",
    "processor.save_pretrained(output_dir/\"assets\")\n",
    "config.save_pretrained(output_dir/\"assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_id, trust_remote_code=True, config=config)\n",
    "model.eval()\n",
    "model.model.eval()\n",
    "__make_16bit_traceable(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Conversion to OpenVINO\n",
    "\n",
    "Define paths for the converted model components and implement conversion utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "image_embed_path = output_dir / \"image_embed.xml\"\n",
    "image_encoder_path = output_dir / \"image_encoder.xml\"\n",
    "image_post_layernorm_path = output_dir / \"image_post_layernorm.xml\"\n",
    "image_connector_path = output_dir / \"image_connector.xml\"\n",
    "language_model_path = output_dir / \"language_model.xml\"\n",
    "model_merger_path = output_dir / \"model_merger.xml\"\n",
    "text_embeddings_path = output_dir / \"text_embeddings.xml\"\n",
    "lm_head_path = output_dir / \"lm_head.xml\"\n",
    "image_model_path = output_dir / \"image_model.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_torchscript_cache():\n",
    "    \"\"\"\n",
    "    Helper function for removing cached model representation to prevent memory leaks\n",
    "    during model conversion.\n",
    "    \"\"\"\n",
    "    torch._C._jit_clear_class_registry()\n",
    "    torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "    torch.jit._state._clear_class_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Text Embeddings Component\n",
    "\n",
    "Convert the text embeddings component of the model to OpenVINO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:nncf:NNCF provides best results with torch==2.5.*, while current torch version is 2.6.0+cu124. If you encounter issues, consider switching to torch==2.5.*\n"
     ]
    }
   ],
   "source": [
    "# save text embeddings\n",
    "with torch.no_grad():\n",
    "    ov_model = ov.convert_model(\n",
    "        model.model.text_model.embed_tokens,\n",
    "        example_input=torch.ones([2, 2], dtype=torch.int64),\n",
    "    )\n",
    "    ov.save_model(ov_model, text_embeddings_path)\n",
    "    del ov_model\n",
    "    cleanup_torchscript_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Vision Model Components\n",
    "\n",
    "Convert the vision model components to OpenVINO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_wrap(self, pixel_values: torch.FloatTensor, patch_attention_mask: torch.FloatTensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass optimized for ONNX/OpenVINO export.\n",
    "\n",
    "        Args:\n",
    "            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, max_im_h, max_im_w)`):\n",
    "                Pixel values corresponding to the images. `max_im_h` and `max_im_w` are the\n",
    "                maximum height and width in the batch after padding.\n",
    "            patch_attention_mask (`torch.BoolTensor` of shape `(batch_size, max_nb_patches_h, max_nb_patches_w)`):\n",
    "                Mask indicating which patches are actual image patches vs padding.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor` of shape `(batch_size, sequence_length, hidden_size)`: Patch embeddings\n",
    "            with added positional embeddings. sequence_length is `max_nb_patches_h * max_nb_patches_w`.\n",
    "        \"\"\"\n",
    "        patch_attention_mask = (patch_attention_mask > 0).to(torch.bool) # Convert to boolean mask\n",
    "\n",
    "        batch_size, _, max_im_h, max_im_w = pixel_values.shape\n",
    "        device = pixel_values.device\n",
    "\n",
    "        # 1. Calculate Patch Embeddings (ONNX-friendly)\n",
    "        # Shape: (batch_size, embed_dim, max_nb_patches_h, max_nb_patches_w)\n",
    "        patch_embeds = self.patch_embedding(pixel_values)\n",
    "        max_nb_patches_h, max_nb_patches_w = patch_embeds.shape[-2:]\n",
    "\n",
    "        # Shape: (batch_size, max_nb_patches_h * max_nb_patches_w, embed_dim)\n",
    "        embeddings = patch_embeds.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # 2. Calculate Positional Embeddings (Vectorized & ONNX-friendly)\n",
    "\n",
    "        # Determine actual number of patches per image in the batch\n",
    "        # Add float() and clamp(min=1) for safe division later. Add 1e-6 to avoid edge cases at 1.0.\n",
    "        # Shape: (batch_size,)\n",
    "        nb_patches_h = patch_attention_mask.any(dim=2).sum(dim=1).float().clamp(min=1.0)\n",
    "        nb_patches_w = patch_attention_mask.any(dim=1).sum(dim=1).float().clamp(min=1.0)\n",
    "\n",
    "        # Create coordinate grids for the *maximum* patch layout\n",
    "        # Shape: (max_nb_patches_h,) and (max_nb_patches_w,)\n",
    "        h_indices = torch.arange(max_nb_patches_h, device=device, dtype=torch.float32)\n",
    "        w_indices = torch.arange(max_nb_patches_w, device=device, dtype=torch.float32)\n",
    "\n",
    "        # Calculate fractional coordinates relative to the *actual* image dimensions\n",
    "        # We scale the grid indices by the ratio of max patches to actual patches\n",
    "        # to get coordinates in the [0, 1) range representative of the original image proportions.\n",
    "        # Use broadcasting: h_indices (H) / nb_patches_h (B, 1) -> (B, H)\n",
    "        #                   w_indices (W) / nb_patches_w (B, 1) -> (B, W)\n",
    "        # Add epsilon to denominator to avoid division by zero potential, although clamp(min=1) helps.\n",
    "        # The range should be [0, ~1) for bucketize.\n",
    "        frac_coords_h = h_indices.unsqueeze(0) / (nb_patches_h.unsqueeze(1)) # Shape: (batch_size, max_nb_patches_h)\n",
    "        frac_coords_w = w_indices.unsqueeze(0) / (nb_patches_w.unsqueeze(1)) # Shape: (batch_size, max_nb_patches_w)\n",
    "\n",
    "        # Bucketize the fractional coordinates using the pre-defined boundaries\n",
    "        # These map the fractional coordinates to the discrete grid cells of the *reference* embedding table\n",
    "        # Shape: (batch_size, max_nb_patches_h)\n",
    "        bucket_coords_h = torch.bucketize(frac_coords_h, self.boundaries, right=True)\n",
    "        # Shape: (batch_size, max_nb_patches_w)\n",
    "        bucket_coords_w = torch.bucketize(frac_coords_w, self.boundaries, right=True)\n",
    "\n",
    "        # Combine bucket coordinates to get position IDs for the reference grid\n",
    "        # Expand dims for broadcasting:\n",
    "        # bucket_coords_h (B, H) -> (B, H, 1)\n",
    "        # bucket_coords_w (B, W) -> (B, 1, W)\n",
    "        # Result shape: (batch_size, max_nb_patches_h, max_nb_patches_w)\n",
    "        position_ids_full = (\n",
    "            bucket_coords_h.unsqueeze(2) * self.num_patches_per_side + bucket_coords_w.unsqueeze(1)\n",
    "        )\n",
    "\n",
    "        # Flatten the position IDs and the attention mask\n",
    "        # Shape: (batch_size, max_nb_patches_h * max_nb_patches_w)\n",
    "        position_ids_flat = position_ids_full.flatten(1)\n",
    "        patch_attention_mask_flat = patch_attention_mask.flatten(1) # Shape: (batch_size, max_nb_patches_h * max_nb_patches_w)\n",
    "\n",
    "        # Use the attention mask to select the calculated position IDs for actual patches\n",
    "        # and use a default ID (e.g., 0) for padding patches. This mimics the original loop's behavior\n",
    "        # where IDs were only calculated and assigned for active patches.\n",
    "        # Using torch.zeros_like ensures the default ID tensor is on the correct device.\n",
    "        final_position_ids = torch.where(\n",
    "            patch_attention_mask_flat,\n",
    "            position_ids_flat,\n",
    "            torch.zeros_like(position_ids_flat) # Pad with position ID 0\n",
    "        )\n",
    "\n",
    "        # 3. Add Positional Embeddings\n",
    "        # Shape: (batch_size, max_nb_patches_h * max_nb_patches_w, embed_dim)\n",
    "        pos_embeds = self.position_embedding(final_position_ids)\n",
    "\n",
    "        # Add to patch embeddings\n",
    "        embeddings = embeddings + pos_embeds\n",
    "\n",
    "        return embeddings\n",
    "# boundaries = torch.arange(1 / self.num_patches_per_side, 1.0, 1 / self.num_patches_per_side)\n",
    "# self.register_buffer(\"boundaries\", boundaries, persistent=False)\n",
    "\n",
    "model.model.vision_model.embeddings._boundaries = torch.arange(1 / model.model.vision_model.embeddings.num_patches_per_side, 1.0, 1 / model.model.vision_model.embeddings.num_patches_per_side)\n",
    "# Register the boundaries buffer\n",
    "model.model.vision_model.embeddings.register_buffer(\"boundaries\", model.model.vision_model.embeddings._boundaries, persistent=False)\n",
    "\n",
    "model.model.vision_model.embeddings._orig_forward = model.model.vision_model.embeddings.forward\n",
    "# Wrap the forward method\n",
    "model.model.vision_model.embeddings.forward = types.MethodType(forward_wrap, model.model.vision_model.embeddings)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ov_model = ov.convert_model(\n",
    "        model.model.vision_model.embeddings,\n",
    "        example_input={\n",
    "            \"pixel_values\": torch.ones([13, 3, 384, 384], dtype=torch.float32),\n",
    "            \"patch_attention_mask\": torch.ones([13, 27, 27], dtype=torch.int64),\n",
    "        }\n",
    "    )\n",
    "    ov.save_model(ov_model, image_embed_path)\n",
    "    del ov_model\n",
    "    cleanup_torchscript_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "/home/prabod/anaconda3/envs/smolvlm/lib/python3.9/site-packages/transformers/models/idefics3/modeling_idefics3.py:236: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (batch_size, self.num_heads, q_len, k_v_seq_len):\n",
      "/home/prabod/anaconda3/envs/smolvlm/lib/python3.9/site-packages/transformers/models/idefics3/modeling_idefics3.py:254: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (batch_size, self.num_heads, q_len, self.head_dim):\n"
     ]
    }
   ],
   "source": [
    "def forward_wrap(\n",
    "        self,\n",
    "        inputs_embeds,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "):\n",
    "    result = self.encoder(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attention_mask,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    last_hidden_state = result[0]\n",
    "    return self.post_layernorm(last_hidden_state)\n",
    "\n",
    "\n",
    "model.model.vision_model._orig_forward = model.forward\n",
    "model.model.vision_model.forward = types.MethodType(forward_wrap, model.model.vision_model)\n",
    "with torch.no_grad():\n",
    "    ov_model = ov.convert_model(\n",
    "        model.model.vision_model,\n",
    "        example_input={\n",
    "            \"inputs_embeds\": torch.ones([13, 729, 1152], dtype=torch.float32),\n",
    "        }\n",
    "    )\n",
    "    ov.save_model(ov_model, image_encoder_path)\n",
    "    del ov_model\n",
    "    cleanup_torchscript_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prabod/anaconda3/envs/smolvlm/lib/python3.9/site-packages/transformers/models/idefics3/modeling_idefics3.py:580: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  height = width = int(seq**0.5)\n",
      "/home/prabod/anaconda3/envs/smolvlm/lib/python3.9/site-packages/transformers/models/idefics3/modeling_idefics3.py:586: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = x.reshape(bsz, int(seq / (scale_factor**2)), embed_dim * (scale_factor**2))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ov_model = ov.convert_model(\n",
    "        model.model.connector,\n",
    "        example_input=torch.ones([13, 729, 1152], dtype=torch.float32)\n",
    "    )\n",
    "    ov.save_model(ov_model, image_connector_path)\n",
    "    del ov_model\n",
    "    cleanup_torchscript_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Language Model Head\n",
    "\n",
    "Convert the language model head to OpenVINO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ov_model = ov.convert_model(\n",
    "        model.lm_head,\n",
    "        example_input=torch.ones([2, 2, 2048], dtype=torch.float32),\n",
    "    )\n",
    "    ov.save_model(ov_model, lm_head_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Quantization\n",
    "\n",
    "Implement functions for model state management and quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_has_state(ov_model: ov.Model):\n",
    "    return len(ov_model.get_sinks()) > 0\n",
    "\n",
    "\n",
    "def model_has_input_output_name(ov_model: ov.Model, name: str):\n",
    "    \"\"\"\n",
    "    Helper function for checking that model has specified input or output name\n",
    "\n",
    "    Parameters:\n",
    "      ov_model (ov.Model):\n",
    "      name (str):\n",
    "          name of input or output\n",
    "\n",
    "    Returns:\n",
    "      True if input or output with requested name exists else False\n",
    "    \"\"\"\n",
    "    return name in sum([list(t.get_names()) for t in ov_model.inputs + ov_model.outputs], [])\n",
    "\n",
    "\n",
    "def fuse_cache_reorder(\n",
    "    ov_model: ov.Model,\n",
    "    not_kv_inputs: List[str],\n",
    "    key_value_input_names: List[str],\n",
    "    gather_dim: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fuses reored_cache during generate cycle into ov.Model. Used with stateful models, because we can not modify model state directly.\n",
    "\n",
    "    Adds a new beam_idx parameter and Gather op per each kv-cache input in a given model.\n",
    "    Should be run before make_stateful. Implements optimumum's _reorder_cache\n",
    "    inside the model in the beginning of each iteration.\n",
    "    Gather works along given gather_dim dimension that may vary from model to model.\n",
    "    KV-cache inputs are identified based on names in key_value_input_names.\n",
    "    Append the new beam_idx parameter to not_kv_inputs.\n",
    "\n",
    "    Parameters:\n",
    "      ov_model (`ov.Model`):\n",
    "          openvino model for processing\n",
    "      not_kv_inputs (`List[str]`):\n",
    "          list of input nodes in model that not related to past key values\n",
    "      key_value_input_names (`List[str]`):\n",
    "          list of names for key value input layers\n",
    "      gather_dim (int):\n",
    "          dimension for gathering cache during reorder pass\n",
    "    \"\"\"\n",
    "\n",
    "    if model_has_input_output_name(ov_model, \"beam_idx\"):\n",
    "        raise ValueError(\"Model already has fused cache\")\n",
    "    input_batch = ov_model.input(\"inputs_embeds\").get_partial_shape()[0]\n",
    "    beam_idx = opset13.parameter(name=\"beam_idx\", dtype=ov.Type.i32, shape=ov.PartialShape([input_batch]))\n",
    "    beam_idx.output(0).get_tensor().add_names({\"beam_idx\"})  # why list is not accepted?\n",
    "    ov_model.add_parameters([beam_idx])\n",
    "    not_kv_inputs.append(ov_model.inputs[-1])\n",
    "    # Go over all cache parameters and fuse _reorder_cache with indices provided by the new parameter beam_idx\n",
    "    for input_name in key_value_input_names:\n",
    "        parameter_output_port = ov_model.input(input_name)\n",
    "        consumers = parameter_output_port.get_target_inputs()\n",
    "        gather = opset13.gather(parameter_output_port, beam_idx, opset13.constant(gather_dim))\n",
    "        for consumer in consumers:\n",
    "            consumer.replace_source_output(gather.output(0))\n",
    "    ov_model.validate_nodes_and_infer_types()\n",
    "\n",
    "\n",
    "def build_state_initializer(ov_model: ov.Model, batch_dim: int):\n",
    "    \"\"\"\n",
    "    Build initialization ShapeOf Expression for all ReadValue ops\n",
    "\n",
    "    Parameters:\n",
    "      ov_model (ov.Model):\n",
    "          openvino model\n",
    "      batch_dim (int):\n",
    "          index of dimension corresponding to batch size\n",
    "    \"\"\"\n",
    "    input_ids = ov_model.input(\"inputs_embeds\")\n",
    "    batch = opset13.gather(\n",
    "        opset13.shape_of(input_ids, output_type=\"i64\"),\n",
    "        opset13.constant([0]),\n",
    "        opset13.constant(0),\n",
    "    )\n",
    "    for op in ov_model.get_ops():\n",
    "        if op.get_type_name() == \"ReadValue\":\n",
    "            dims = [dim.min_length for dim in list(op.get_output_partial_shape(0))]\n",
    "            dims[batch_dim] = batch\n",
    "            dims = [(opset13.constant(np.array([dim], dtype=np.int64)) if isinstance(dim, int) else dim) for dim in dims]\n",
    "            shape = opset13.concat(dims, axis=0)\n",
    "            broadcast = opset13.broadcast(opset13.constant(0.0, dtype=op.get_output_element_type(0)), shape)\n",
    "            op.set_arguments([broadcast])\n",
    "    ov_model.validate_nodes_and_infer_types()\n",
    "\n",
    "\n",
    "def make_stateful(\n",
    "    ov_model: ov.Model,\n",
    "    not_kv_inputs: List[str],\n",
    "    key_value_input_names: List[str],\n",
    "    key_value_output_names: List[str],\n",
    "    batch_dim: int,\n",
    "    num_attention_heads: int,\n",
    "    num_beams_and_batch: int = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Hides kv-cache inputs and outputs inside the model as variables.\n",
    "\n",
    "    Parameters:\n",
    "        ov_model (ov.Model):\n",
    "            openvino model\n",
    "        not_kv_inputs (`List[str]`):\n",
    "            list of input nodes in model that not related to past key values\n",
    "        key_value_input_names (`List[str]`):\n",
    "            list of names for key value input layers\n",
    "        key_value_output_names (`List[str]`):\n",
    "            list of names for key value input layers\n",
    "        batch_dim (int):\n",
    "            index of batch dimension in key value layers\n",
    "        num_attention_heads (int):\n",
    "            number of attention heads for batch dimension initialization\n",
    "        num_beams_an_batch (int):\n",
    "            precalculated number of beams and batch for shapes initialization\n",
    "    \"\"\"\n",
    "    from openvino._offline_transformations import apply_make_stateful_transformation\n",
    "\n",
    "    input_output_map = {}\n",
    "\n",
    "    if num_beams_and_batch is not None:\n",
    "        # Set batch size for input_ids and attention mask to avoid dynamic dimension got propagated from the end of the model back to ReadValue\n",
    "        for input in not_kv_inputs:\n",
    "            shape = input.get_partial_shape()\n",
    "            if shape.rank.get_length() <= 2:  # == 1 for beam_index\n",
    "                shape[0] = num_beams_and_batch\n",
    "                input.get_node().set_partial_shape(shape)\n",
    "    for kv_name_pair in zip(key_value_input_names, key_value_output_names):\n",
    "        input_output_map[kv_name_pair[0]] = kv_name_pair[1]\n",
    "        if num_beams_and_batch is not None:\n",
    "            input = ov_model.input(kv_name_pair[0])\n",
    "            shape = input.get_partial_shape()\n",
    "            shape[batch_dim] = num_beams_and_batch * num_attention_heads\n",
    "            input.get_node().set_partial_shape(shape)\n",
    "\n",
    "    if num_beams_and_batch is not None:\n",
    "        # Re-validation model if shapes are altered above\n",
    "        ov_model.validate_nodes_and_infer_types()\n",
    "\n",
    "    apply_make_stateful_transformation(ov_model, input_output_map)\n",
    "    if num_beams_and_batch is None:\n",
    "        build_state_initializer(ov_model, batch_dim)\n",
    "\n",
    "\n",
    "def patch_stateful(ov_model):\n",
    "    key_value_input_names = [key.get_any_name() for key in ov_model.inputs[2:-1]]\n",
    "    key_value_output_names = [key.get_any_name() for key in ov_model.outputs[1:]]\n",
    "    not_kv_inputs = [input for input in ov_model.inputs if not any(name in key_value_input_names for name in input.get_names())]\n",
    "    if not key_value_input_names or not key_value_output_names:\n",
    "        return\n",
    "    batch_dim = 0\n",
    "    num_attention_heads = 1\n",
    "\n",
    "    fuse_cache_reorder(ov_model, not_kv_inputs, key_value_input_names, batch_dim)\n",
    "    make_stateful(\n",
    "        ov_model,\n",
    "        not_kv_inputs,\n",
    "        key_value_input_names,\n",
    "        key_value_output_names,\n",
    "        batch_dim,\n",
    "        num_attention_heads,\n",
    "        None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Quantization Process\n",
    "\n",
    "The quantization process reduces the precision of the model weights from floating-point to integer format, which can significantly reduce model size and improve inference speed. We're using INT4 quantization in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prabod/anaconda3/envs/smolvlm/lib/python3.9/site-packages/transformers/cache_utils.py:457: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n",
      "/home/prabod/anaconda3/envs/smolvlm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:743: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "/home/prabod/anaconda3/envs/smolvlm/lib/python3.9/site-packages/transformers/cache_utils.py:441: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  len(self.key_cache[layer_idx]) == 0\n",
      "/home/prabod/anaconda3/envs/smolvlm/lib/python3.9/site-packages/transformers/integrations/sdpa_attention.py:47: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  is_causal = query.shape[2] > 1 and causal_mask is None\n",
      "/home/prabod/anaconda3/envs/smolvlm/lib/python3.9/site-packages/torch/jit/_trace.py:165: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  if a.grad is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Language model successfully converted\n",
      "âŒ› Weights compression with int4_asym mode started\n",
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”‘\n",
      "â”‚ Weight compression mode   â”‚ % all parameters (layers)   â”‚ % ratio-defining parameters (layers)   â”‚\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¿â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¿â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¥\n",
      "â”‚ int8_asym                 â”‚ 1% (1 / 168)                â”‚ 0% (0 / 167)                           â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ int4_asym                 â”‚ 99% (167 / 168)             â”‚ 100% (167 / 167)                       â”‚\n",
      "â”•â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”™\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/prabod/anaconda3/envs/smolvlm/lib/python3.9/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\"\n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/prabod/anaconda3/envs/smolvlm/lib/python3.9/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\"\n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Weights compression finished\n"
     ]
    }
   ],
   "source": [
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "quantization_config = {\n",
    "    \"mode\": nncf.CompressWeightsMode.INT4_ASYM,\n",
    "    \"group_size\": 128,\n",
    "    \"ratio\": 1.0,\n",
    "}\n",
    "lang_model = model.model.text_model\n",
    "def forward_wrap(\n",
    "        self,\n",
    "        attention_mask,\n",
    "        position_ids=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "    ):\n",
    "        if past_key_values is not None:\n",
    "            new_past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n",
    "        else:\n",
    "            new_past_key_values = None\n",
    "        result = self._orig_forward(\n",
    "            input_ids=None,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=new_past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        if past_key_values is not None:\n",
    "            result[\"past_key_values\"] = result[\"past_key_values\"].to_legacy_cache()\n",
    "        return tuple(result.values())\n",
    "\n",
    "lang_model._orig_forward = lang_model.forward\n",
    "lang_model.forward = types.MethodType(forward_wrap, lang_model)\n",
    "hidden_size = lang_model.config.hidden_size\n",
    "llm_input = torch.zeros([2, 2, hidden_size])\n",
    "pkv = lang_model._orig_forward(\n",
    "    inputs_embeds=llm_input,\n",
    "    attention_mask=torch.ones((2, 2), dtype=torch.int64),\n",
    ")[1].to_legacy_cache()\n",
    "model_inputs = [\"attention_mask\", \"position_ids\"]\n",
    "model_outputs = [\"last_hidden_state\"]\n",
    "for idx in range(len(pkv)):\n",
    "    model_inputs.extend([f\"past_key_values.{idx}.key\", f\"past_key_values.{idx}.value\"])\n",
    "    model_outputs.extend([f\"present.{idx}.key\", f\"present.{idx}.value\"])\n",
    "model_inputs.append(\"inputs_embeds\")\n",
    "position_ids = torch.tensor([[2, 3], [2, 3]])\n",
    "with torch.no_grad():\n",
    "    ov_model = ov.convert_model(\n",
    "        lang_model,\n",
    "        example_input={\n",
    "            \"inputs_embeds\": llm_input,\n",
    "            \"attention_mask\": torch.ones([2, 4], dtype=torch.int64),\n",
    "            \"past_key_values\": pkv,\n",
    "            \"position_ids\": position_ids,\n",
    "        },\n",
    "    )\n",
    "for input, input_name in zip(ov_model.inputs, model_inputs):\n",
    "            input.get_tensor().set_names({input_name})\n",
    "\n",
    "for output, output_name in zip(ov_model.outputs, model_outputs):\n",
    "    output.get_tensor().set_names({output_name})\n",
    "patch_stateful(ov_model)\n",
    "print(\"âœ… Language model successfully converted\")\n",
    "fp_lang_model_path = language_model_path if quantization_config is None else language_model_path.parent / (\"fp_\" + language_model_path.name)\n",
    "ov.save_model(ov_model, fp_lang_model_path)\n",
    "del ov_model\n",
    "cleanup_torchscript_cache()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "ov_model = core.read_model(fp_lang_model_path)\n",
    "print(f\"âŒ› Weights compression with {quantization_config['mode']} mode started\")\n",
    "c_ov_model = nncf.compress_weights(ov_model, **quantization_config)\n",
    "print(\"âœ… Weights compression finished\")\n",
    "ov.save_model(c_ov_model, language_model_path)\n",
    "del c_ov_model\n",
    "del ov_model\n",
    "gc.collect()\n",
    "\n",
    "# delete fp_lang_model_path\n",
    "fp_lang_model_path.unlink(missing_ok=True)\n",
    "fp_lang_model_path.with_suffix(\".bin\").unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Merger Implementation\n",
    "\n",
    "Implement the model merger to combine text and image components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModelMerger(nn.Module):\n",
    "    def __init__(self, image_token_id: int, patch_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.image_token_id = image_token_id\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor, inputs_embeds: torch.Tensor, image_hidden_states: torch.Tensor):\n",
    "        # input_ids: (B, L)\n",
    "        # inputs_embeds: (B, L, D)\n",
    "        # image_hidden_states: (B * num_blocks, patch_size, D)\n",
    "\n",
    "        B, L = input_ids.shape\n",
    "        _, _, D = inputs_embeds.shape\n",
    "\n",
    "        image_mask = (input_ids == self.image_token_id)  # (B, L)\n",
    "\n",
    "        # Flatten inputs\n",
    "        image_mask_flat = image_mask.view(-1)  # (B*L)\n",
    "        input_ids_flat = input_ids.view(-1)\n",
    "\n",
    "        # Compute indices for image_hidden_states\n",
    "        token_indices = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, L)\n",
    "        image_token_counts = torch.cumsum(image_mask.to(torch.int32), dim=1) - 1\n",
    "        image_token_counts = torch.where(image_mask, image_token_counts, torch.zeros_like(image_token_counts))\n",
    "\n",
    "        block_index = image_token_counts // self.patch_size\n",
    "        local_index = image_token_counts % self.patch_size\n",
    "\n",
    "        flat_index = block_index * self.patch_size + local_index  # Index into flattened image_hidden_states\n",
    "\n",
    "        # Flatten image_hidden_states to (B * num_blocks * patch_size, D)\n",
    "        image_hidden_states_flat = image_hidden_states.view(-1, D)\n",
    "\n",
    "        # Gather image embeddings for image tokens\n",
    "        image_token_embeddings = torch.zeros_like(inputs_embeds)\n",
    "        flat_index_expanded = flat_index.unsqueeze(-1).expand(-1, -1, D)\n",
    "        image_token_embeddings = torch.where(\n",
    "            image_mask.unsqueeze(-1),\n",
    "            torch.gather(image_hidden_states_flat, 0, flat_index_expanded.view(-1, D)).view(B, L, D),\n",
    "            torch.zeros_like(inputs_embeds)\n",
    "        )\n",
    "\n",
    "        # Merge\n",
    "        merged_embeds = torch.where(image_mask.unsqueeze(-1), image_token_embeddings, inputs_embeds)\n",
    "\n",
    "        return merged_embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_merger = ModelMerger(image_token_id=model.image_token_id, patch_size=model.config.image_seq_len)\n",
    "\n",
    "ov_model = ov.convert_model(\n",
    "    model_merger,\n",
    "    example_input={\n",
    "        \"input_ids\": torch.ones([1, 1198], dtype=torch.int64),\n",
    "        \"inputs_embeds\": torch.ones([1, 1198, 2048], dtype=torch.float32),\n",
    "        \"image_hidden_states\": torch.ones([13, 81, 2048], dtype=torch.float32),\n",
    "    },\n",
    ")\n",
    "\n",
    "ov.save_model(ov_model, model_merger_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Testing OpenVINO Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "device = \"CPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths for the exported models\n",
    "image_embed_path = output_dir / \"image_embed.xml\"\n",
    "image_encoder_path = output_dir / \"image_encoder.xml\"\n",
    "image_connector_path = output_dir / \"image_connector.xml\"\n",
    "language_model_path = output_dir / \"language_model.xml\"\n",
    "model_merger_path = output_dir / \"model_merger.xml\"\n",
    "text_embeddings_path = output_dir / \"text_embeddings.xml\"\n",
    "lm_head_path = output_dir / \"lm_head.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the models\n",
    "language_model = core.read_model(language_model_path)\n",
    "compiled_language_model = core.compile_model(language_model, \"AUTO\")\n",
    "\n",
    "image_embed = core.compile_model(image_embed_path, device)\n",
    "image_encoder = core.compile_model(image_encoder_path, device)\n",
    "image_connector = core.compile_model(image_connector_path, device)\n",
    "model_merger = core.compile_model(model_merger_path, device)\n",
    "text_embeddings = core.compile_model(text_embeddings_path, device)\n",
    "lm_head = core.compile_model(lm_head_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The image features a prominent green statue of liberty in the foreground, standing on a small island in the middle of a body of water. The statue is holding a torch in its right hand. The water is calm and blue, with a few small waves visible. In the background, there is a large cityscape with numerous skyscrapers and buildings, including the Empire State Building and the One World Trade Center. The sky is clear and blue, with a hint of sunlight reflecting off the water. The cityscape is densely packed with high-rise buildings, with some of the buildings having distinctive architectural features such as domes or spires. The image is clear and well-lit, with a focus on the statue and the cityscape.\n"
     ]
    }
   ],
   "source": [
    "from transformers.image_utils import load_image\n",
    "from transformers import AutoProcessor, TextStreamer\n",
    "from transformers import AutoProcessor, AutoConfig,AutoModelForVision2Seq\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# Load images\n",
    "image1 = load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\")\n",
    "image2 = load_image(\"https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "# Create input messages\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Can you describe the image?\"}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "\n",
    "# Prepare inputs\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs_new = processor(text=prompt, images=[image1], return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "request = compiled_language_model.create_infer_request()\n",
    "input_names = {key.get_any_name(): idx for idx, key in enumerate(language_model.inputs)}\n",
    "inputs = {}\n",
    "# Set the initial input_ids\n",
    "current_input_ids = inputs_new[\"input_ids\"]\n",
    "attention_mask = inputs_new[\"attention_mask\"]\n",
    "position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "pixel_values = inputs_new[\"pixel_values\"]\n",
    "pixel_attention_mask = inputs_new[\"pixel_attention_mask\"]\n",
    "\n",
    "patch_size = config.vision_config.patch_size\n",
    "\n",
    "generation_args = {\"max_new_tokens\": 200, \"do_sample\": False, \"streamer\": TextStreamer(processor.tokenizer, skip_prompt=True, skip_special_tokens=True)}\n",
    "generated_tokens = []\n",
    "\n",
    "for i in range(generation_args[\"max_new_tokens\"]):\n",
    "    # Generate input embeds each time\n",
    "    if current_input_ids.shape[-1] > 1:\n",
    "        batch_size, num_images, num_channels, height, width = pixel_values.shape\n",
    "        pixel_values = pixel_values\n",
    "        pixel_values = pixel_values.view(batch_size * num_images, *pixel_values.shape[2:])\n",
    "\n",
    "        # Remove padding images - padding images are full 0.\n",
    "        nb_values_per_image = pixel_values.shape[1:].numel()\n",
    "        real_images_inds = (pixel_values == 0.0).sum(dim=(-1, -2, -3)) != nb_values_per_image\n",
    "\n",
    "        if not any(real_images_inds):\n",
    "            # no images, leave one empty image.\n",
    "            real_images_inds[0] = True\n",
    "\n",
    "        pixel_values = pixel_values[real_images_inds].contiguous()\n",
    "\n",
    "        # Handle the vision attention mask\n",
    "        if pixel_attention_mask is None:\n",
    "            pixel_attention_mask = torch.ones(\n",
    "                size=[pixel_values.shape[i] for i in (0, 2, 3)],\n",
    "                dtype=torch.bool,\n",
    "                device=pixel_values.device,\n",
    "            )\n",
    "        else:\n",
    "            # Remove padding images from the mask\n",
    "            pixel_attention_mask = pixel_attention_mask.view(\n",
    "                batch_size * num_images, *pixel_attention_mask.shape[2:]\n",
    "            )\n",
    "            pixel_attention_mask = pixel_attention_mask[real_images_inds].contiguous()\n",
    "\n",
    "        patches_subgrid = pixel_attention_mask.unfold(dimension=1, size=patch_size, step=patch_size)\n",
    "        patches_subgrid = patches_subgrid.unfold(dimension=2, size=patch_size, step=patch_size)\n",
    "        patch_attention_mask = (patches_subgrid.sum(dim=(-1, -2)) > 0).bool()\n",
    "\n",
    "        hidden_states =  torch.from_numpy(\n",
    "            image_embed({\n",
    "                \"pixel_values\": pixel_values,\n",
    "                \"patch_attention_mask\": patch_attention_mask,\n",
    "            })[0]\n",
    "        )\n",
    "\n",
    "        patch_attention_mask = patch_attention_mask.view(batch_size, -1)\n",
    "\n",
    "        image_hidden_states_before = torch.from_numpy(\n",
    "            image_encoder({\n",
    "                \"inputs_embeds\": hidden_states\n",
    "            })[0]\n",
    "        )\n",
    "\n",
    "        image_hidden_states = torch.from_numpy(\n",
    "            image_connector({\n",
    "                \"image_hidden_states\": image_hidden_states_before,\n",
    "            })[0]\n",
    "        )\n",
    "\n",
    "        text_input_embeds = torch.from_numpy(\n",
    "            text_embeddings(current_input_ids)[0]\n",
    "        )\n",
    "\n",
    "        inputs_embeds = torch.from_numpy(\n",
    "            model_merger({\n",
    "                \"input_ids\": current_input_ids,\n",
    "                \"inputs_embeds\": text_input_embeds,\n",
    "                \"image_hidden_states\": image_hidden_states,\n",
    "            })[0]\n",
    "        )\n",
    "    else:\n",
    "        text_input_embeds = torch.from_numpy(\n",
    "            text_embeddings(current_input_ids)[0]\n",
    "        )\n",
    "        inputs_embeds = torch.from_numpy(\n",
    "            model_merger({\n",
    "                \"input_ids\": current_input_ids,\n",
    "                \"inputs_embeds\": text_input_embeds,\n",
    "                \"image_hidden_states\": image_hidden_states,\n",
    "            })[0]\n",
    "        )\n",
    "    \n",
    "    inputs[\"inputs_embeds\"] = inputs_embeds\n",
    "    inputs[\"attention_mask\"] = attention_mask\n",
    "    inputs[\"position_ids\"] = position_ids\n",
    "    if \"beam_idx\" in input_names:\n",
    "        inputs[\"beam_idx\"] = np.arange(inputs_embeds.shape[0], dtype=int)\n",
    "    \n",
    "    # Start inference\n",
    "    request.start_async(inputs, share_inputs=True)\n",
    "    request.wait()\n",
    "    \n",
    "    # Get the logits and find the next token\n",
    "    last_hidden_state = torch.from_numpy(request.get_tensor(\"last_hidden_state\").data)\n",
    "    logits = torch.from_numpy(lm_head(last_hidden_state)[0])\n",
    "\n",
    "    next_token = logits.argmax(-1)[0][-1]\n",
    "\n",
    "    # Append the generated token\n",
    "    generated_tokens.append(next_token)\n",
    "    \n",
    "    # Update input_ids with the new token\n",
    "    current_input_ids = torch.cat([next_token.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "    \n",
    "    # update the attention mask\n",
    "    attention_mask = torch.cat([attention_mask, torch.ones_like(attention_mask[:, :1])], dim=-1)\n",
    "\n",
    "    # Update inputs for the next iteration\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "    position_ids = position_ids[:, -current_input_ids.shape[1] :]\n",
    "    inputs[\"position_ids\"] = position_ids\n",
    "\n",
    "    if next_token == processor.tokenizer.eos_token_id:\n",
    "        break\n",
    "\n",
    "generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True, eos_token_id=processor.tokenizer.eos_token_id)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Import and Save in Spark NLP\n",
    "- Let's install and setup Spark NLP in Google Colab\n",
    "- This part is pretty easy via our simple script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start Spark with Spark NLP included via our simple `start()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/07 09:56:55 WARN Utils: Your hostname, minotaur resolves to a loopback address: 127.0.1.1; using 192.168.1.4 instead (on interface eno1)\n",
      "24/11/07 09:56:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/11/07 09:56:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "# let's start Spark with Spark NLP\n",
    "spark = sparknlp.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/11 05:51:22 WARN NativeLibrary: Failed to load library null: java.lang.UnsatisfiedLinkError: Can't load library: /tmp/openvino-native17302445033292358869/libtbb.so.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/prabod/spark/jars/spark-core_2.12-3.3.2.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "imageClassifier = SmolVLMTransformer \\\n",
    "            .loadSavedModel(str(output_dir),spark) \\\n",
    "            .setInputCols(\"image_assembler\") \\\n",
    "            .setOutputCol(\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "imageClassifier.write().overwrite().save(\"file:///tmp/SmolVLM_spark_nlp_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml import Pipeline\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# download two images to test into ./images folder\n",
    "\n",
    "url1 = \"https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/d5fbbd1a-d484-415c-88cb-9986625b7b11\"\n",
    "url2 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "\n",
    "Path(\"images\").mkdir(exist_ok=True)\n",
    "\n",
    "!wget -q -O images/image1.jpg {url1}\n",
    "!wget -q -O images/image2.jpg {url2}\n",
    "\n",
    "\n",
    "\n",
    "images_path = \"file://\" + os.getcwd() + \"/images/\"\n",
    "image_df = spark.read.format(\"image\").load(\n",
    "    path=images_path\n",
    ")\n",
    "\n",
    "test_df = image_df.withColumn(\"text\", lit(\"<|im_start|>User:<image>Can you describe the image?<end_of_utterance>\\nAssistant:\"))\n",
    "\n",
    "image_assembler = ImageAssembler().setInputCol(\"image\").setOutputCol(\"image_assembler\")\n",
    "\n",
    "imageClassifier = SmolVLMTransformer.load(\"file:///tmp/SmolVLM_spark_nlp_2\")\\\n",
    "            .setMaxOutputLength(50) \\\n",
    "            .setInputCols(\"image_assembler\") \\\n",
    "            .setOutputCol(\"answer\")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "            stages=[\n",
    "                image_assembler,\n",
    "                imageClassifier,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "model = pipeline.fit(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path: /mnt/research/Projects/ModelZoo/SmolVLM/images/image1.jpg\n",
      "[Annotation(document, 0, 224,  The image features a gray tabby cat lying in a cardboard box. The cat has its eyes closed, suggesting it is relaxed. Its fur is light gray with darker patches, and its paws are visible, including one with a pink toe. The cat, Map(), [])]\n"
     ]
    }
   ],
   "source": [
    "light_pipeline = LightPipeline(model)\n",
    "image_path = os.getcwd() + \"/images/\" + \"image1.jpg\"\n",
    "print(\"image_path: \" + image_path)\n",
    "annotations_result = light_pipeline.fullAnnotateImage(\n",
    "    image_path,\n",
    "    \"<|im_start|>User:<image>Can you describe the image?<end_of_utterance>\\nAssistant:\"\n",
    ")\n",
    "\n",
    "for result in annotations_result:\n",
    "    print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: model_merger.xml (deflated 90%)\n",
      "  adding: .model_merger.xml.crc (stored 0%)\n",
      "  adding: image_connector.xml (deflated 23%)\n",
      "  adding: .image_encoder.xml.crc (deflated 0%)\n",
      "  adding: .language_model.xml.crc (deflated 0%)\n",
      "  adding: fields/ (stored 0%)\n",
      "  adding: fields/merges/ (stored 0%)\n",
      "  adding: fields/merges/.part-00017.crc (stored 0%)\n",
      "  adding: fields/merges/part-00022 (deflated 77%)\n",
      "  adding: fields/merges/part-00005 (deflated 77%)\n",
      "  adding: fields/merges/.part-00054.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00037.crc (stored 0%)\n",
      "  adding: fields/merges/part-00037 (deflated 77%)\n",
      "  adding: fields/merges/part-00017 (deflated 78%)\n",
      "  adding: fields/merges/part-00009 (deflated 78%)\n",
      "  adding: fields/merges/part-00010 (deflated 78%)\n",
      "  adding: fields/merges/.part-00039.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00012.crc (stored 0%)\n",
      "  adding: fields/merges/part-00040 (deflated 77%)\n",
      "  adding: fields/merges/.part-00005.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00019.crc (stored 0%)\n",
      "  adding: fields/merges/part-00021 (deflated 78%)\n",
      "  adding: fields/merges/part-00004 (deflated 78%)\n",
      "  adding: fields/merges/.part-00035.crc (stored 0%)\n",
      "  adding: fields/merges/part-00018 (deflated 77%)\n",
      "  adding: fields/merges/part-00003 (deflated 77%)\n",
      "  adding: fields/merges/part-00053 (deflated 78%)\n",
      "  adding: fields/merges/part-00000 (deflated 78%)\n",
      "  adding: fields/merges/.part-00042.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00029.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00038.crc (stored 0%)\n",
      "  adding: fields/merges/part-00044 (deflated 78%)\n",
      "  adding: fields/merges/part-00050 (deflated 77%)\n",
      "  adding: fields/merges/part-00039 (deflated 77%)\n",
      "  adding: fields/merges/.part-00001.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00004.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00010.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00041.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00044.crc (stored 0%)\n",
      "  adding: fields/merges/part-00013 (deflated 78%)\n",
      "  adding: fields/merges/part-00015 (deflated 78%)\n",
      "  adding: fields/merges/part-00025 (deflated 78%)\n",
      "  adding: fields/merges/part-00024 (deflated 78%)\n",
      "  adding: fields/merges/part-00002 (deflated 78%)\n",
      "  adding: fields/merges/part-00028 (deflated 78%)\n",
      "  adding: fields/merges/part-00008 (deflated 77%)\n",
      "  adding: fields/merges/.part-00031.crc (stored 0%)\n",
      "  adding: fields/merges/part-00036 (deflated 78%)\n",
      "  adding: fields/merges/.part-00030.crc (stored 0%)\n",
      "  adding: fields/merges/part-00014 (deflated 78%)\n",
      "  adding: fields/merges/.part-00025.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00034.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00052.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00002.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00032.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00033.crc (stored 0%)\n",
      "  adding: fields/merges/part-00016 (deflated 78%)\n",
      "  adding: fields/merges/part-00052 (deflated 78%)\n",
      "  adding: fields/merges/.part-00048.crc (stored 0%)\n",
      "  adding: fields/merges/part-00023 (deflated 78%)\n",
      "  adding: fields/merges/part-00034 (deflated 78%)\n",
      "  adding: fields/merges/part-00007 (deflated 78%)\n",
      "  adding: fields/merges/part-00049 (deflated 78%)\n",
      "  adding: fields/merges/.part-00016.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00050.crc (stored 0%)\n",
      "  adding: fields/merges/part-00041 (deflated 77%)\n",
      "  adding: fields/merges/.part-00024.crc (stored 0%)\n",
      "  adding: fields/merges/part-00046 (deflated 77%)\n",
      "  adding: fields/merges/part-00011 (deflated 78%)\n",
      "  adding: fields/merges/part-00047 (deflated 77%)\n",
      "  adding: fields/merges/.part-00040.crc (stored 0%)\n",
      "  adding: fields/merges/part-00043 (deflated 77%)\n",
      "  adding: fields/merges/part-00012 (deflated 78%)\n",
      "  adding: fields/merges/.part-00051.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00020.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00000.crc (stored 0%)\n",
      "  adding: fields/merges/part-00042 (deflated 78%)\n",
      "  adding: fields/merges/.part-00008.crc (stored 0%)\n",
      "  adding: fields/merges/part-00006 (deflated 78%)\n",
      "  adding: fields/merges/.part-00018.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00013.crc (stored 0%)\n",
      "  adding: fields/merges/part-00055 (deflated 77%)\n",
      "  adding: fields/merges/.part-00047.crc (stored 0%)\n",
      "  adding: fields/merges/part-00001 (deflated 77%)\n",
      "  adding: fields/merges/part-00019 (deflated 78%)\n",
      "  adding: fields/merges/part-00032 (deflated 78%)\n",
      "  adding: fields/merges/.part-00055.crc (stored 0%)\n",
      "  adding: fields/merges/part-00051 (deflated 77%)\n",
      "  adding: fields/merges/.part-00009.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00015.crc (stored 0%)\n",
      "  adding: fields/merges/part-00020 (deflated 77%)\n",
      "  adding: fields/merges/part-00038 (deflated 77%)\n",
      "  adding: fields/merges/part-00027 (deflated 78%)\n",
      "  adding: fields/merges/.part-00045.crc (stored 0%)\n",
      "  adding: fields/merges/part-00031 (deflated 78%)\n",
      "  adding: fields/merges/.part-00049.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00021.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00053.crc (stored 0%)\n",
      "  adding: fields/merges/part-00035 (deflated 77%)\n",
      "  adding: fields/merges/.part-00026.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00043.crc (stored 0%)\n",
      "  adding: fields/merges/._SUCCESS.crc (stored 0%)\n",
      "  adding: fields/merges/part-00029 (deflated 77%)\n",
      "  adding: fields/merges/.part-00023.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00011.crc (stored 0%)\n",
      "  adding: fields/merges/part-00054 (deflated 78%)\n",
      "  adding: fields/merges/.part-00028.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00014.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00003.crc (stored 0%)\n",
      "  adding: fields/merges/.part-00046.crc (stored 0%)\n",
      "  adding: fields/merges/part-00033 (deflated 77%)\n",
      "  adding: fields/merges/.part-00007.crc (stored 0%)\n",
      "  adding: fields/merges/part-00026 (deflated 77%)\n",
      "  adding: fields/merges/.part-00006.crc (stored 0%)\n",
      "  adding: fields/merges/part-00048 (deflated 77%)\n",
      "  adding: fields/merges/.part-00036.crc (stored 0%)\n",
      "  adding: fields/merges/_SUCCESS (stored 0%)\n",
      "  adding: fields/merges/.part-00027.crc (stored 0%)\n",
      "  adding: fields/merges/part-00030 (deflated 78%)\n",
      "  adding: fields/merges/.part-00022.crc (stored 0%)\n",
      "  adding: fields/merges/part-00045 (deflated 78%)\n",
      "  adding: fields/generationConfig/ (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00017.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00022 (deflated 27%)\n",
      "  adding: fields/generationConfig/part-00005 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00054.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00037.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00037 (deflated 26%)\n",
      "  adding: fields/generationConfig/part-00017 (deflated 26%)\n",
      "  adding: fields/generationConfig/part-00009 (deflated 27%)\n",
      "  adding: fields/generationConfig/part-00010 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00039.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00012.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00040 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00005.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00019.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00021 (deflated 27%)\n",
      "  adding: fields/generationConfig/part-00004 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00035.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00018 (deflated 27%)\n",
      "  adding: fields/generationConfig/part-00003 (deflated 26%)\n",
      "  adding: fields/generationConfig/part-00053 (deflated 27%)\n",
      "  adding: fields/generationConfig/part-00000 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00042.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00029.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00038.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00044 (deflated 27%)\n",
      "  adding: fields/generationConfig/part-00050 (deflated 27%)\n",
      "  adding: fields/generationConfig/part-00039 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00001.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00004.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00010.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00041.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00044.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00013 (deflated 27%)\n",
      "  adding: fields/generationConfig/part-00015 (deflated 27%)\n",
      "  adding: fields/generationConfig/part-00025 (deflated 27%)\n",
      "  adding: fields/generationConfig/part-00024 (deflated 26%)\n",
      "  adding: fields/generationConfig/part-00002 (deflated 26%)\n",
      "  adding: fields/generationConfig/part-00028 (deflated 27%)\n",
      "  adding: fields/generationConfig/part-00008 (deflated 26%)\n",
      "  adding: fields/generationConfig/.part-00031.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00036 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00030.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00014 (deflated 26%)\n",
      "  adding: fields/generationConfig/.part-00025.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00034.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00052.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00002.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00032.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00033.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00016 (deflated 27%)\n",
      "  adding: fields/generationConfig/part-00052 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00048.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00023 (deflated 27%)\n",
      "  adding: fields/generationConfig/part-00034 (deflated 27%)\n",
      "  adding: fields/generationConfig/part-00007 (deflated 26%)\n",
      "  adding: fields/generationConfig/part-00049 (deflated 26%)\n",
      "  adding: fields/generationConfig/.part-00016.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00050.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00041 (deflated 26%)\n",
      "  adding: fields/generationConfig/.part-00024.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00046 (deflated 26%)\n",
      "  adding: fields/generationConfig/part-00011 (deflated 26%)\n",
      "  adding: fields/generationConfig/part-00047 (deflated 26%)\n",
      "  adding: fields/generationConfig/.part-00040.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00043 (deflated 27%)\n",
      "  adding: fields/generationConfig/part-00012 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00051.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00020.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00000.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00042 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00008.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00006 (deflated 26%)\n",
      "  adding: fields/generationConfig/.part-00018.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00013.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00055 (deflated 32%)\n",
      "  adding: fields/generationConfig/.part-00047.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00001 (deflated 26%)\n",
      "  adding: fields/generationConfig/part-00019 (deflated 27%)\n",
      "  adding: fields/generationConfig/part-00032 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00055.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00051 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00009.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00015.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00020 (deflated 26%)\n",
      "  adding: fields/generationConfig/part-00038 (deflated 27%)\n",
      "  adding: fields/generationConfig/part-00027 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00045.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00031 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00049.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00021.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00053.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00035 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00026.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00043.crc (stored 0%)\n",
      "  adding: fields/generationConfig/._SUCCESS.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00029 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00023.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00011.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00054 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00028.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00014.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00003.crc (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00046.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00033 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00007.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00026 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00006.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00048 (deflated 27%)\n",
      "  adding: fields/generationConfig/.part-00036.crc (stored 0%)\n",
      "  adding: fields/generationConfig/_SUCCESS (stored 0%)\n",
      "  adding: fields/generationConfig/.part-00027.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00030 (deflated 26%)\n",
      "  adding: fields/generationConfig/.part-00022.crc (stored 0%)\n",
      "  adding: fields/generationConfig/part-00045 (deflated 26%)\n",
      "  adding: fields/vocabulary/ (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00017.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00022 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00005 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00054.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00037.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00037 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00017 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00009 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00010 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00039.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00012.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00040 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00005.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00019.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00021 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00004 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00035.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00018 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00003 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00053 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00000 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00042.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00029.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00038.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00044 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00050 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00039 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00001.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00004.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00010.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00041.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00044.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00013 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00015 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00025 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00024 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00002 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00028 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00008 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00031.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00036 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00030.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00014 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00025.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00034.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00052.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00002.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00032.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00033.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00016 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00052 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00048.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00023 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00034 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00007 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00049 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00016.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00050.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00041 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00024.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00046 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00011 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00047 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00040.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00043 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00012 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00051.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00020.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00000.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00042 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00008.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00006 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00018.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00013.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00055 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00047.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00001 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00019 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00032 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00055.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00051 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00009.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00015.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00020 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00038 (deflated 76%)\n",
      "  adding: fields/vocabulary/part-00027 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00045.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00031 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00049.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00021.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00053.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00035 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00026.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00043.crc (stored 0%)\n",
      "  adding: fields/vocabulary/._SUCCESS.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00029 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00023.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00011.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00054 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00028.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00014.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00003.crc (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00046.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00033 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00007.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00026 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00006.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00048 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00036.crc (stored 0%)\n",
      "  adding: fields/vocabulary/_SUCCESS (stored 0%)\n",
      "  adding: fields/vocabulary/.part-00027.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00030 (deflated 76%)\n",
      "  adding: fields/vocabulary/.part-00022.crc (stored 0%)\n",
      "  adding: fields/vocabulary/part-00045 (deflated 76%)\n",
      "  adding: fields/addedTokens/ (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00017.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00022 (deflated 20%)\n",
      "  adding: fields/addedTokens/part-00005 (deflated 20%)\n",
      "  adding: fields/addedTokens/.part-00054.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00037.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00037 (deflated 27%)\n",
      "  adding: fields/addedTokens/part-00017 (deflated 27%)\n",
      "  adding: fields/addedTokens/part-00009 (deflated 26%)\n",
      "  adding: fields/addedTokens/part-00010 (deflated 27%)\n",
      "  adding: fields/addedTokens/.part-00039.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00012.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00040 (deflated 26%)\n",
      "  adding: fields/addedTokens/.part-00005.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00019.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00021 (deflated 27%)\n",
      "  adding: fields/addedTokens/part-00004 (deflated 27%)\n",
      "  adding: fields/addedTokens/.part-00035.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00018 (deflated 27%)\n",
      "  adding: fields/addedTokens/part-00003 (deflated 27%)\n",
      "  adding: fields/addedTokens/part-00053 (deflated 20%)\n",
      "  adding: fields/addedTokens/part-00000 (deflated 26%)\n",
      "  adding: fields/addedTokens/.part-00042.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00029.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00038.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00044 (deflated 20%)\n",
      "  adding: fields/addedTokens/part-00050 (deflated 20%)\n",
      "  adding: fields/addedTokens/part-00039 (deflated 20%)\n",
      "  adding: fields/addedTokens/.part-00001.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00004.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00010.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00041.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00044.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00013 (deflated 20%)\n",
      "  adding: fields/addedTokens/part-00015 (deflated 26%)\n",
      "  adding: fields/addedTokens/part-00025 (deflated 19%)\n",
      "  adding: fields/addedTokens/part-00024 (deflated 27%)\n",
      "  adding: fields/addedTokens/part-00002 (deflated 19%)\n",
      "  adding: fields/addedTokens/part-00028 (deflated 27%)\n",
      "  adding: fields/addedTokens/part-00008 (deflated 20%)\n",
      "  adding: fields/addedTokens/.part-00031.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00036 (deflated 20%)\n",
      "  adding: fields/addedTokens/.part-00030.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00014 (deflated 27%)\n",
      "  adding: fields/addedTokens/.part-00025.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00034.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00052.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00002.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00032.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00033.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00016 (deflated 20%)\n",
      "  adding: fields/addedTokens/part-00052 (deflated 26%)\n",
      "  adding: fields/addedTokens/.part-00048.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00023 (deflated 26%)\n",
      "  adding: fields/addedTokens/part-00034 (deflated 27%)\n",
      "  adding: fields/addedTokens/part-00007 (deflated 27%)\n",
      "  adding: fields/addedTokens/part-00049 (deflated 27%)\n",
      "  adding: fields/addedTokens/.part-00016.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00050.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00041 (deflated 20%)\n",
      "  adding: fields/addedTokens/.part-00024.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00046 (deflated 27%)\n",
      "  adding: fields/addedTokens/part-00011 (deflated 20%)\n",
      "  adding: fields/addedTokens/part-00047 (deflated 20%)\n",
      "  adding: fields/addedTokens/.part-00040.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00043 (deflated 26%)\n",
      "  adding: fields/addedTokens/part-00012 (deflated 28%)\n",
      "  adding: fields/addedTokens/.part-00051.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00020.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00000.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00042 (deflated 27%)\n",
      "  adding: fields/addedTokens/.part-00008.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00006 (deflated 27%)\n",
      "  adding: fields/addedTokens/.part-00018.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00013.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00055 (deflated 20%)\n",
      "  adding: fields/addedTokens/.part-00047.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00001 (deflated 27%)\n",
      "  adding: fields/addedTokens/part-00019 (deflated 20%)\n",
      "  adding: fields/addedTokens/part-00032 (deflated 27%)\n",
      "  adding: fields/addedTokens/.part-00055.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00051 (deflated 26%)\n",
      "  adding: fields/addedTokens/.part-00009.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00015.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00020 (deflated 27%)\n",
      "  adding: fields/addedTokens/part-00038 (deflated 27%)\n",
      "  adding: fields/addedTokens/part-00027 (deflated 20%)\n",
      "  adding: fields/addedTokens/.part-00045.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00031 (deflated 27%)\n",
      "  adding: fields/addedTokens/.part-00049.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00021.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00053.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00035 (deflated 27%)\n",
      "  adding: fields/addedTokens/.part-00026.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00043.crc (stored 0%)\n",
      "  adding: fields/addedTokens/._SUCCESS.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00029 (deflated 27%)\n",
      "  adding: fields/addedTokens/.part-00023.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00011.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00054 (deflated 27%)\n",
      "  adding: fields/addedTokens/.part-00028.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00014.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00003.crc (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00046.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00033 (deflated 20%)\n",
      "  adding: fields/addedTokens/.part-00007.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00026 (deflated 27%)\n",
      "  adding: fields/addedTokens/.part-00006.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00048 (deflated 26%)\n",
      "  adding: fields/addedTokens/.part-00036.crc (stored 0%)\n",
      "  adding: fields/addedTokens/_SUCCESS (stored 0%)\n",
      "  adding: fields/addedTokens/.part-00027.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00030 (deflated 20%)\n",
      "  adding: fields/addedTokens/.part-00022.crc (stored 0%)\n",
      "  adding: fields/addedTokens/part-00045 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/ (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00017.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00022 (deflated 26%)\n",
      "  adding: fields/smolVLMConfig/part-00005 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00054.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00037.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00037 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00017 (deflated 26%)\n",
      "  adding: fields/smolVLMConfig/part-00009 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00010 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00039.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00012.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00040 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00005.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00019.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00021 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00004 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00035.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00018 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00003 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00053 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00000 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00042.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00029.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00038.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00044 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00050 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00039 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00001.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00004.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00010.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00041.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00044.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00013 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00015 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00025 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00024 (deflated 26%)\n",
      "  adding: fields/smolVLMConfig/part-00002 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00028 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00008 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00031.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00036 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00030.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00014 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00025.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00034.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00052.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00002.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00032.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00033.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00016 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00052 (deflated 26%)\n",
      "  adding: fields/smolVLMConfig/.part-00048.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00023 (deflated 26%)\n",
      "  adding: fields/smolVLMConfig/part-00034 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00007 (deflated 26%)\n",
      "  adding: fields/smolVLMConfig/part-00049 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00016.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00050.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00041 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00024.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00046 (deflated 26%)\n",
      "  adding: fields/smolVLMConfig/part-00011 (deflated 26%)\n",
      "  adding: fields/smolVLMConfig/part-00047 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00040.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00043 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00012 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00051.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00020.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00000.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00042 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00008.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00006 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00018.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00013.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00055 (deflated 35%)\n",
      "  adding: fields/smolVLMConfig/.part-00047.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00001 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00019 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00032 (deflated 26%)\n",
      "  adding: fields/smolVLMConfig/.part-00055.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00051 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00009.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00015.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00020 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00038 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/part-00027 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00045.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00031 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00049.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00021.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00053.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00035 (deflated 26%)\n",
      "  adding: fields/smolVLMConfig/.part-00026.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00043.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/._SUCCESS.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00029 (deflated 26%)\n",
      "  adding: fields/smolVLMConfig/.part-00023.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00011.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00054 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00028.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00014.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00003.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00046.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00033 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00007.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00026 (deflated 26%)\n",
      "  adding: fields/smolVLMConfig/.part-00006.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00048 (deflated 26%)\n",
      "  adding: fields/smolVLMConfig/.part-00036.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/_SUCCESS (stored 0%)\n",
      "  adding: fields/smolVLMConfig/.part-00027.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00030 (deflated 27%)\n",
      "  adding: fields/smolVLMConfig/.part-00022.crc (stored 0%)\n",
      "  adding: fields/smolVLMConfig/part-00045 (deflated 27%)\n",
      "  adding: language_model.xml (deflated 10%)\n",
      "  adding: .lm_head.xml.crc (deflated 0%)\n",
      "  adding: metadata/ (stored 0%)\n",
      "  adding: metadata/part-00000 (deflated 49%)\n",
      "  adding: metadata/.part-00000.crc (stored 0%)\n",
      "  adding: metadata/._SUCCESS.crc (stored 0%)\n",
      "  adding: metadata/_SUCCESS (stored 0%)\n",
      "  adding: .image_connector.xml.crc (deflated 0%)\n",
      "  adding: text_embeddings.xml (deflated 23%)\n",
      "  adding: .image_embed.xml.crc (stored 0%)\n",
      "  adding: image_embed.xml (deflated 22%)\n",
      "  adding: lm_head.xml (deflated 23%)\n",
      "  adding: image_encoder.xml (deflated 23%)\n",
      "  adding: .text_embeddings.xml.crc (deflated 0%)\n"
     ]
    }
   ],
   "source": [
    "ZIP_NAME = f\"smolvlm_instruct_int4_sn\"\n",
    "!cd /tmp/SmolVLM_spark_nlp_2 && zip -r {ZIP_NAME}.zip ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smolvlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
