{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfvgqqN_GQ0s"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/openvino/HuggingFace_OpenVINO_in_Spark_NLP_LLAVA.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYD1juKTGQ0t"
      },
      "source": [
        "# Import OpenVINO LLAVA models from HuggingFace ðŸ¤— into Spark NLP ðŸš€\n",
        "\n",
        "This notebook provides a detailed walkthrough on optimizing and importing LLAVA models from HuggingFace  for use in Spark NLP, with [Intel OpenVINO toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html). The focus is on converting the model to the OpenVINO format and applying precision optimizations (INT8 and INT4), to enhance the performance and efficiency on CPU platforms using [Optimum Intel](https://huggingface.co/docs/optimum/main/en/intel/inference).\n",
        "\n",
        "Let's keep in mind a few things before we start ðŸ˜Š\n",
        "\n",
        "- OpenVINO support was introduced in  `Spark NLP 5.4.0`, enabling high performance CPU inference for models. So please make sure you have upgraded to the latest Spark NLP release.\n",
        "- Model quantization is a computationally expensive process, so it is recommended to use a runtime with more than 32GB memory for exporting the quantized model from HuggingFace.\n",
        "- You can import LLAVA models via `LLAVA`. These models are usually under `Text Generation` category and have `LLAVA` in their labels.\n",
        "- Reference: [LLAVA](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LLAVA)\n",
        "- Some [example models](https://huggingface.co/models?search=LLAVA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27xoGQj7GQ0t"
      },
      "source": [
        "## 1. Export and Save the HuggingFace model\n",
        "\n",
        "- Let's install `transformers` and `openvino` packages with other dependencies. You don't need `openvino` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.41.2`. This doesn't mean it won't work with the future release, but we wanted you to know which versions have been tested successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBYupsqBGQ0u",
        "outputId": "7e90709a-deea-440d-baee-af12f069d945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%pip install -q \"nncf>=2.14.0\" \"torch>=2.1\" \"transformers>=4.39.1\" \"accelerate\" \"pillow\" \"gradio>=4.26\" \"datasets>=2.14.6\" \"tqdm\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
        "%pip install -q -U \"openvino>=2024.5.0\" \"openvino-tokenizers>=2024.5.0\" \"openvino-genai>=2024.5\"\n",
        "%pip install -q \"git+https://github.com/huggingface/optimum-intel.git\" --extra-index-url https://download.pytorch.org/whl/cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHvnDR7KGQ0u"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "utility_files = [\"notebook_utils.py\", \"cmd_helper.py\"]\n",
        "\n",
        "for utility in utility_files:\n",
        "    local_path = Path(utility)\n",
        "    if not local_path.exists():\n",
        "        r = requests.get(\n",
        "            url=f\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/{local_path.name}\",\n",
        "        )\n",
        "    with local_path.open(\"w\") as f:\n",
        "        f.write(r.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVKQS_uQGQ0v"
      },
      "source": [
        "### 1.1 Convert the model to OpenVino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SP83FkK8GQ0v",
        "outputId": "aec9bc5b-17e5-4802-914f-693541de6b3a"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Export command:**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "`optimum-cli export openvino --model llava-hf/llava-1.5-7b-hf llava-1.5-7b-hf/FP16 --weight-format fp16`"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/prabod/anaconda3/envs/llava/lib/python3.9/importlib/util.py:245: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
            "  self.__spec__.loader.exec_module(self)\n",
            "Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.84it/s]\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.90s/it]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
            "/home/prabod/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/cache_utils.py:460: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
            "  or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n",
            "/home/prabod/anaconda3/envs/llava/lib/python3.9/site-packages/optimum/exporters/openvino/model_patcher.py:515: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if sequence_length != 1:\n",
            "/home/prabod/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/cache_utils.py:444: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
            "  len(self.key_cache[layer_idx]) == 0\n",
            "/home/prabod/anaconda3/envs/llava/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:243: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n"
          ]
        }
      ],
      "source": [
        "from cmd_helper import optimum_cli\n",
        "\n",
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "model_path = Path(model_id.split(\"/\")[-1]) / \"FP16\"\n",
        "\n",
        "if not model_path.exists():\n",
        "    optimum_cli(model_id, model_path, additional_args={\"weight-format\": \"fp16\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyrwcPDTGQ0v",
        "outputId": "16f58ff9-1ecc-473d-d669-a156697ba120"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:nncf:Statistics of the bitwidth distribution:\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”‘\n",
            "â”‚ Weight compression mode   â”‚ % all parameters (layers)   â”‚ % ratio-defining parameters (layers)   â”‚\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¿â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¿â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¥\n",
            "â”‚ int4_asym                 â”‚ 100% (225 / 225)            â”‚ 100% (225 / 225)                       â”‚\n",
            "â”•â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”™\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import shutil\n",
        "import nncf\n",
        "import openvino as ov\n",
        "import gc\n",
        "\n",
        "\n",
        "compression_mode = \"INT4\"\n",
        "\n",
        "core = ov.Core()\n",
        "\n",
        "\n",
        "def compress_model_weights(precision):\n",
        "    int4_compression_config = {\"mode\": nncf.CompressWeightsMode.INT4_ASYM, \"group_size\": 128, \"ratio\": 1, \"all_layers\": True}\n",
        "    int8_compression_config = {\"mode\": nncf.CompressWeightsMode.INT8_ASYM}\n",
        "\n",
        "    compressed_model_path = model_path.parent / precision\n",
        "\n",
        "    if not compressed_model_path.exists():\n",
        "        ov_model = core.read_model(model_path / \"openvino_language_model.xml\")\n",
        "        compression_config = int4_compression_config if precision == \"INT4\" else int8_compression_config\n",
        "        compressed_ov_model = nncf.compress_weights(ov_model, **compression_config)\n",
        "        ov.save_model(compressed_ov_model, compressed_model_path / \"openvino_language_model.xml\")\n",
        "        del compressed_ov_model\n",
        "        del ov_model\n",
        "        gc.collect()\n",
        "        for file_name in model_path.glob(\"*\"):\n",
        "            if file_name.name in [\"openvino_language_model.xml\", \"openvino_language_model.bin\"]:\n",
        "                continue\n",
        "            shutil.copy(file_name, compressed_model_path)\n",
        "\n",
        "\n",
        "compress_model_weights(compression_mode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUUCQrXJGQ0v"
      },
      "source": [
        "### 1.2 Load openvino models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAUn3UeeGQ0v"
      },
      "outputs": [],
      "source": [
        "model_dir = model_path.parent / compression_mode\n",
        "language_model = core.read_model(model_dir / \"openvino_language_model.xml\")\n",
        "vision_embedding = core.compile_model(model_dir / \"openvino_vision_embeddings_model.xml\", \"AUTO\")\n",
        "text_embedding = core.compile_model(model_dir / \"openvino_text_embeddings_model.xml\", \"AUTO\")\n",
        "compiled_language_model = core.compile_model(language_model, \"AUTO\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loVNwwFwGQ0v",
        "outputId": "1d71e95e-f426-46e8-80ed-11018e483b9f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/prabod/anaconda3/envs/llava/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from transformers import AutoProcessor, AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_path)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    model_path, patch_size=config.vision_config.patch_size, vision_feature_select_strategy=config.vision_feature_select_strategy\n",
        ")\n",
        "\n",
        "\n",
        "def load_image(image_file):\n",
        "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
        "        response = requests.get(image_file)\n",
        "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    else:\n",
        "        image = Image.open(image_file).convert(\"RGB\")\n",
        "    return image\n",
        "\n",
        "\n",
        "image_file = \"https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/d5fbbd1a-d484-415c-88cb-9986625b7b11\"\n",
        "text_message = \"What is unusual on this image?\"\n",
        "\n",
        "image = load_image(image_file)\n",
        "\n",
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": text_message},\n",
        "            {\"type\": \"image\"},\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "\n",
        "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
        "\n",
        "inputs_new = processor(images=image, text=prompt, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lbwq3lebGQ0v"
      },
      "outputs": [],
      "source": [
        "\n",
        "request = compiled_language_model.create_infer_request()\n",
        "input_names = {key.get_any_name(): idx for idx, key in enumerate(language_model.inputs)}\n",
        "inputs = {}\n",
        "# Set the initial input_ids\n",
        "current_input_ids = inputs_new[\"input_ids\"]\n",
        "attention_mask = inputs_new[\"attention_mask\"]\n",
        "position_ids = attention_mask.long().cumsum(-1) - 1\n",
        "position_ids.masked_fill_(attention_mask == 0, 1)\n",
        "pixel_values = inputs_new[\"pixel_values\"]\n",
        "\n",
        "# Set the initial input_ids\n",
        "text_out = text_embedding(inputs_new[\"input_ids\"])[0]\n",
        "vision_out = vision_embedding(pixel_values)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K79AlNPGQ0v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class MergeMultiModalInputs(torch.nn.Module):\n",
        "    def __init__(self,image_seq_length=576,image_token_index=32000):\n",
        "        super().__init__()\n",
        "        self.image_seq_length = image_seq_length\n",
        "        self.image_token_index = image_token_index\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        vision_embeds,\n",
        "        inputs_embeds,\n",
        "        input_ids,\n",
        "    ):\n",
        "        image_features = vision_embeds\n",
        "        inputs_embeds = inputs_embeds\n",
        "        special_image_mask = (input_ids == self.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n",
        "        # image_features = image_features.to(inputs_embeds.dtype)\n",
        "        final_embedding = inputs_embeds.masked_scatter(special_image_mask, image_features)\n",
        "\n",
        "        return {\n",
        "            \"final_embedding\": final_embedding\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ajKrh58GQ0w"
      },
      "outputs": [],
      "source": [
        "torch_model_merge = MergeMultiModalInputs(\n",
        "    image_seq_length=config.image_seq_length,\n",
        "    image_token_index=config.image_token_index\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67sZ1XT9GQ0w"
      },
      "outputs": [],
      "source": [
        "# test the model\n",
        "inputs_embeds = torch.from_numpy(text_out)\n",
        "input_ids = inputs_new[\"input_ids\"]\n",
        "vision_embeds = torch.from_numpy(vision_out)\n",
        "\n",
        "final_embedding = torch_model_merge(vision_embeds, inputs_embeds, input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sddgcN4GQ0w",
        "outputId": "9863b96f-6f81-4f18-c149-b56946ed05b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:nncf:NNCF provides best results with torch==2.5.*, while current torch version is 2.6.0+cpu. If you encounter issues, consider switching to torch==2.5.*\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "import openvino as ov\n",
        "\n",
        "# convert MergeMultiModalInputs to OpenVINO IR\n",
        "ov_model_merge = ov.convert_model(\n",
        "    torch_model_merge,\n",
        "    example_input={\n",
        "        \"vision_embeds\": torch.from_numpy(vision_out),\n",
        "        \"inputs_embeds\": torch.from_numpy(text_out),\n",
        "        \"input_ids\": inputs_new[\"input_ids\"],\n",
        "    }\n",
        ")\n",
        "ov.save_model(ov_model_merge, model_dir/\"openvino_merge_model.xml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G1-N7EIGQ0w",
        "outputId": "c7611454-24a7-4a13-d7dc-3dfe88f01153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ› Check if all models are converted\n",
            "âœ… All models are converted. You can find results in llava-1.5-7b-hf/INT4\n"
          ]
        }
      ],
      "source": [
        "# check if all the models are converted\n",
        "\n",
        "print(\"âŒ› Check if all models are converted\")\n",
        "lang_model_path = model_dir / \"openvino_language_model.xml\"\n",
        "image_embed_path = model_dir / \"openvino_vision_embeddings_model.xml\"\n",
        "img_projection_path = model_dir / \"openvino_text_embeddings_model.xml\"\n",
        "merge_model_path = model_dir / \"openvino_merge_model.xml\"\n",
        "\n",
        "\n",
        "\n",
        "if all(\n",
        "    [\n",
        "        lang_model_path.exists(),\n",
        "        image_embed_path.exists(),\n",
        "        img_projection_path.exists(),\n",
        "        merge_model_path.exists(),\n",
        "    ]\n",
        "):\n",
        "    print(f\"âœ… All models are converted. You can find results in {model_dir}\")\n",
        "else:\n",
        "    print(\"âŒ Not all models are converted. Please check the conversion process\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4f8J6NoGQ0w"
      },
      "source": [
        "### 1.2 Copy assets to the assets folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hd9ff9MGQ0w",
        "outputId": "b9808e1e-74e7-4e91-9265-075d470c847d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/prabod/anaconda3/envs/llava/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "assets_dir = model_dir / \"assets\"\n",
        "assets_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# copy all the assets to the assets directory (json files, vocab files, etc.)\n",
        "\n",
        "import shutil\n",
        "\n",
        "# copy all json files\n",
        "\n",
        "for file in model_dir.glob(\"*.json\"):\n",
        "    shutil.copy(file, assets_dir)\n",
        "\n",
        "from transformers import AutoConfig\n",
        "\n",
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_id)\n",
        "config.save_pretrained(assets_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3AJih7LGQ0w",
        "outputId": "b0ce3d2b-51b6-41a7-95dc-5df444d622f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 4.1G\n",
            "-rw-rw-r-- 1 prabod prabod   41 Feb 13 05:09 added_tokens.json\n",
            "drwxrwxr-x 2 prabod prabod 4.0K Feb 13 05:10 assets\n",
            "-rw-rw-r-- 1 prabod prabod  701 Feb 13 05:09 chat_template.json\n",
            "-rw-rw-r-- 1 prabod prabod 1.1K Feb 13 05:09 config.json\n",
            "-rw-rw-r-- 1 prabod prabod  136 Feb 13 05:09 generation_config.json\n",
            "-rw-rw-r-- 1 prabod prabod 332K Feb 13 05:09 openvino_detokenizer.bin\n",
            "-rw-rw-r-- 1 prabod prabod  12K Feb 13 05:09 openvino_detokenizer.xml\n",
            "-rw-rw-r-- 1 prabod prabod 3.2G Feb 13 05:09 openvino_language_model.bin\n",
            "-rw-rw-r-- 1 prabod prabod 2.9M Feb 13 05:09 openvino_language_model.xml\n",
            "-rw-rw-r-- 1 prabod prabod   40 Feb 13 05:10 openvino_merge_model.bin\n",
            "-rw-rw-r-- 1 prabod prabod 9.9K Feb 13 05:10 openvino_merge_model.xml\n",
            "-rw-rw-r-- 1 prabod prabod 251M Feb 13 05:09 openvino_text_embeddings_model.bin\n",
            "-rw-rw-r-- 1 prabod prabod 3.1K Feb 13 05:09 openvino_text_embeddings_model.xml\n",
            "-rw-rw-r-- 1 prabod prabod 1.2M Feb 13 05:09 openvino_tokenizer.bin\n",
            "-rw-rw-r-- 1 prabod prabod  25K Feb 13 05:09 openvino_tokenizer.xml\n",
            "-rw-rw-r-- 1 prabod prabod 595M Feb 13 05:09 openvino_vision_embeddings_model.bin\n",
            "-rw-rw-r-- 1 prabod prabod 928K Feb 13 05:09 openvino_vision_embeddings_model.xml\n",
            "-rw-rw-r-- 1 prabod prabod  505 Feb 13 05:09 preprocessor_config.json\n",
            "-rw-rw-r-- 1 prabod prabod  173 Feb 13 05:09 processor_config.json\n",
            "-rw-rw-r-- 1 prabod prabod  580 Feb 13 05:09 special_tokens_map.json\n",
            "-rw-rw-r-- 1 prabod prabod 1.5K Feb 13 05:09 tokenizer_config.json\n",
            "-rw-rw-r-- 1 prabod prabod 3.5M Feb 13 05:09 tokenizer.json\n",
            "-rw-rw-r-- 1 prabod prabod 489K Feb 13 05:09 tokenizer.model\n"
          ]
        }
      ],
      "source": [
        "!ls -lh {model_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61MA7Y2_GQ0w",
        "outputId": "e5483fa1-d7af-47d3-f55a-485dc3b95ce8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 3.5M\n",
            "-rw-rw-r-- 1 prabod prabod   41 Feb 13 05:10 added_tokens.json\n",
            "-rw-rw-r-- 1 prabod prabod  701 Feb 13 05:10 chat_template.json\n",
            "-rw-rw-r-- 1 prabod prabod 1.1K Feb 13 05:10 config.json\n",
            "-rw-rw-r-- 1 prabod prabod  136 Feb 13 05:10 generation_config.json\n",
            "-rw-rw-r-- 1 prabod prabod  505 Feb 13 05:10 preprocessor_config.json\n",
            "-rw-rw-r-- 1 prabod prabod  173 Feb 13 05:10 processor_config.json\n",
            "-rw-rw-r-- 1 prabod prabod  580 Feb 13 05:10 special_tokens_map.json\n",
            "-rw-rw-r-- 1 prabod prabod 1.5K Feb 13 05:10 tokenizer_config.json\n",
            "-rw-rw-r-- 1 prabod prabod 3.5M Feb 13 05:10 tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "!ls -lh {assets_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXiKJDdvGQ0w"
      },
      "source": [
        "### 1.3 Test the openvino model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLY_fj1gGQ0w"
      },
      "outputs": [],
      "source": [
        "import openvino as ov\n",
        "import torch\n",
        "\n",
        "core = ov.Core()\n",
        "device = \"CPU\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23swRljDGQ0x"
      },
      "outputs": [],
      "source": [
        "language_model = core.read_model(model_dir / \"openvino_language_model.xml\")\n",
        "language_model = core.read_model(model_dir / \"openvino_language_model.xml\")\n",
        "vision_embedding = core.compile_model(model_dir / \"openvino_vision_embeddings_model.xml\", \"AUTO\")\n",
        "text_embedding = core.compile_model(model_dir / \"openvino_text_embeddings_model.xml\", \"AUTO\")\n",
        "compiled_language_model = core.compile_model(language_model, \"AUTO\")\n",
        "merge_multi_modal = core.compile_model(model_dir / \"openvino_merge_model.xml\", \"AUTO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4WplxIVGQ0x"
      },
      "outputs": [],
      "source": [
        "generated_tokens = []\n",
        "\n",
        "from transformers import AutoProcessor, TextStreamer\n",
        "\n",
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": \"What is unusual on this image?\"},\n",
        "            {\"type\": \"image\"},\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "\n",
        "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
        "\n",
        "inputs_new = processor(images=image, text=prompt, return_tensors=\"pt\")\n",
        "\n",
        "# inputs_new = processor(prompt, [image], return_tensors=\"pt\")\n",
        "\n",
        "generation_args = {\"max_new_tokens\": 50, \"do_sample\": False, \"streamer\": TextStreamer(processor.tokenizer, skip_prompt=True, skip_special_tokens=True)}\n",
        "\n",
        "\n",
        "request = compiled_language_model.create_infer_request()\n",
        "merge_model_request = merge_multi_modal.create_infer_request()\n",
        "input_names = {key.get_any_name(): idx for idx, key in enumerate(language_model.inputs)}\n",
        "inputs = {}\n",
        "# Set the initial input_ids\n",
        "current_input_ids = inputs_new[\"input_ids\"]\n",
        "attention_mask = inputs_new[\"attention_mask\"]\n",
        "position_ids = attention_mask.long().cumsum(-1) - 1\n",
        "position_ids.masked_fill_(attention_mask == 0, 1)\n",
        "pixel_values = inputs_new[\"pixel_values\"]\n",
        "\n",
        "for i in range(generation_args[\"max_new_tokens\"]):\n",
        "    # Generate input embeds each time\n",
        "    if current_input_ids.shape[-1] > 1:\n",
        "        vision_embeds = torch.from_numpy(vision_embedding({\n",
        "            \"pixel_values\": pixel_values,\n",
        "        })[0])\n",
        "\n",
        "    text_embeds = torch.from_numpy(text_embedding(current_input_ids)[0])\n",
        "\n",
        "    if i == 0:\n",
        "        merge_model_request.start_async({\n",
        "            \"vision_embeds\": vision_embeds,\n",
        "            \"inputs_embeds\": text_embeds,\n",
        "            \"input_ids\": current_input_ids,\n",
        "        }, share_inputs=True)\n",
        "        merge_model_request.wait()\n",
        "        final_embedding = torch.from_numpy(merge_model_request.get_tensor(\"final_embedding\").data)\n",
        "    else:\n",
        "        final_embedding = text_embeds\n",
        "    if i>0:\n",
        "        inputs = {}\n",
        "    # Prepare inputs for the model\n",
        "    inputs[\"inputs_embeds\"] = final_embedding\n",
        "    inputs[\"attention_mask\"] = attention_mask\n",
        "    inputs[\"position_ids\"] = position_ids\n",
        "    if \"beam_idx\" in input_names:\n",
        "        inputs[\"beam_idx\"] = np.arange(attention_mask.shape[0], dtype=int)\n",
        "\n",
        "    # Start inference\n",
        "    request.start_async(inputs, share_inputs=True)\n",
        "    request.wait()\n",
        "\n",
        "    # Get the logits and find the next token\n",
        "    logits = torch.from_numpy(request.get_tensor(\"logits\").data)\n",
        "    next_token = logits.argmax(-1)[0][-1]\n",
        "\n",
        "    # Append the generated token\n",
        "    generated_tokens.append(next_token)\n",
        "\n",
        "    # Update input_ids with the new token\n",
        "    current_input_ids = torch.cat([next_token.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
        "\n",
        "    # update the attention mask\n",
        "    attention_mask = torch.cat([attention_mask, torch.ones_like(attention_mask[:, :1])], dim=-1)\n",
        "\n",
        "    # Update inputs for the next iteration\n",
        "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
        "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
        "    position_ids = position_ids[:, -current_input_ids.shape[1] :]\n",
        "    inputs[\"position_ids\"] = position_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NyJ173oGQ0x",
        "outputId": "5e6aa177-5cb0-4e0e-9fbf-785a0f08c855"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            " What is unusual on this picture?\n",
            "Answer:\n",
            "The unusual aspect of this image is that a cat is lying inside a cardboard box, which is not a typical place for a cat to rest. Cats are known for their curiosity and love for small, enclosed spaces, but in this case\n"
          ]
        }
      ],
      "source": [
        "generated_text = processor.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "image\n",
        "print(\"Question:\\n What is unusual on this picture?\")\n",
        "print(\"Answer:\")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aglH8YDTGQ0x"
      },
      "source": [
        "## 2. Import and Save LLAVA in Spark NLP\n",
        "\n",
        "- Let's install and setup Spark NLP in Google Colab\n",
        "- This part is pretty easy via our simple script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B_SNRvRGQ0x"
      },
      "outputs": [],
      "source": [
        "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fLNdXyuGQ0x"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giUfY5SSGQ0x",
        "outputId": "c26dc4c5-38dc-4ae4-e2b8-74b256a73908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/11/07 09:56:55 WARN Utils: Your hostname, minotaur resolves to a loopback address: 127.0.1.1; using 192.168.1.4 instead (on interface eno1)\n",
            "24/11/07 09:56:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "24/11/07 09:56:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "# let's start Spark with Spark NLP\n",
        "spark = sparknlp.start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7jmghQEGQ0x",
        "outputId": "63cd90e3-16db-4aaf-bf54-6dd586a03b9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/02/13 06:30:15 WARN NativeLibrary: Failed to load library null: java.lang.UnsatisfiedLinkError: Can't load library: /tmp/openvino-native10897903401200889289/libtbb.so.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/prabod/spark/jars/spark-core_2.12-3.3.2.jar) to field java.util.regex.Pattern.pattern\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.ml import Pipeline\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "imageClassifier = LLAVAForMultiModal.loadSavedModel(str(model_dir),spark) \\\n",
        "            .setInputCols(\"image_assembler\") \\\n",
        "            .setOutputCol(\"answer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9FyBI4JGQ0x",
        "outputId": "a5593b3d-7357-47d6-c154-a4332859e601"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "imageClassifier.write().overwrite().save(\"file:///tmp/LLAVA_spark_nlp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-uJmDBSGQ0x"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# download two images to test into ./images folder\n",
        "\n",
        "url1 = \"https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/d5fbbd1a-d484-415c-88cb-9986625b7b11\"\n",
        "url2 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "\n",
        "Path(\"images\").mkdir(exist_ok=True)\n",
        "\n",
        "!wget -q -O images/image1.jpg {url1}\n",
        "!wget -q -O images/image2.jpg {url2}\n",
        "\n",
        "\n",
        "\n",
        "images_path = \"file://\" + os.getcwd() + \"/images/\"\n",
        "image_df = spark.read.format(\"image\").load(\n",
        "    path=images_path\n",
        ")\n",
        "\n",
        "test_df = image_df.withColumn(\"text\", lit(\"USER: \\n <|image|> \\n What's this picture about? \\n ASSISTANT:\\n\"))\n",
        "\n",
        "image_assembler = ImageAssembler().setInputCol(\"image\").setOutputCol(\"image_assembler\")\n",
        "\n",
        "imageClassifier = LLAVAForMultiModal.load(\"file:///tmp/LLAVA_spark_nlp\")\\\n",
        "            .setMaxOutputLength(50) \\\n",
        "            .setInputCols(\"image_assembler\") \\\n",
        "            .setOutputCol(\"answer\")\n",
        "\n",
        "pipeline = Pipeline(\n",
        "            stages=[\n",
        "                image_assembler,\n",
        "                imageClassifier,\n",
        "            ]\n",
        "        )\n",
        "\n",
        "model = pipeline.fit(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrTSor5aGQ0x",
        "outputId": "e3e803a3-a383-4bf2-f573-f56e7f8cf573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image_path: file:///home/prabod/Projects/spark-nlp/examples/python/transformers/openvino/images/image1.jpg\n",
            "[Annotation(document, 0, 207, This image features a cat comfortably laying inside a cardboard box. The cat appears to be relaxed and enjoying its cozy spot. The scene takes place on a carpeted floor, which adds to the overall warm and inv, Map(), [])]\n"
          ]
        }
      ],
      "source": [
        "light_pipeline = LightPipeline(model)\n",
        "image_path = os.getcwd() + \"/images/\" + \"image1.jpg\"\n",
        "print(\"image_path: \" + image_path)\n",
        "annotations_result = light_pipeline.fullAnnotateImage(\n",
        "    image_path,\n",
        "    \"USER: \\n <|image|> \\n What's this picture about? \\n ASSISTANT:\\n\"\n",
        ")\n",
        "\n",
        "for result in annotations_result:\n",
        "    print(result[\"answer\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llava",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}