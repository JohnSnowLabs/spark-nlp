{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en9rTz2iQUmG"
   },
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/openvino/HuggingFace_OpenVino_in_Spark_NLP_MPNetForSequenceClassification.ipynb)\n",
    "\n",
    "# Import OpenVINO MPNetForSequenceClassification models from HuggingFace ü§ó into Spark NLP üöÄ\n",
    "\n",
    "This notebook provides a detailed walkthrough on optimizing and exporting MPNetForSequenceClassification models from HuggingFace for use in Spark NLP, leveraging the various tools provided in the [Intel OpenVINO toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html) ecosystem.\n",
    "\n",
    "Let's keep in mind a few things before we start üòä\n",
    "\n",
    "- OpenVINO support was introduced in  `Spark NLP 5.4.0`, enabling high performance inference for models. Please make sure you have upgraded to the latest Spark NLP release.\n",
    "- You can import models for MPNetForSequenceClassification from MPNetForSequenceClassification   and they have to be in `Text Classification` category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2o_GAXd5QUmG"
   },
   "source": [
    "## Export and Save HuggingFace model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EG6-gP1GQUmH"
   },
   "source": [
    "- Let's install `transformers` package with the `openvino` extension and it's dependencies. You don't need `openvino` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
    "- We lock `transformers` on version `4.52.4`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully.\n",
    "- Additionally, we need to install `setfit` to load the model components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bR5wXfEZQUmH",
    "outputId": "325a91c8-303a-4e50-af75-93c86038c614"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade transformers[onnx]==4.52.4 optimum openvino setfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62JzNZIFQUmI"
   },
   "source": [
    "We'll use [rodekruis/sml-ukr-message-classifier](https://huggingface.co/rodekruis/sml-ukr-message-classifier). As this is not a pure `transformers` model, we need to export the modules separately and combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577,
     "referenced_widgets": [
      "6caee62e847e4267a941caa8da4b465f",
      "f6cf771861e7403aa3077dd05eac8db4",
      "b4076590700b426f94e8725fb0dd4f00",
      "f241aa9358d444b1af49a3e236c45787",
      "33cbca7685124426b0380a704cac31fe",
      "25290b8482994b00a469dc7aa3572e7f",
      "4ebc9097f4ad4bee82a1699a3910284b",
      "23681dee2d9a4efea5e687cbbbff64ae",
      "66aef6779f354fb3b9f6289327dc4954",
      "a42c2272631d48f58bdc33134b0cc6ba",
      "bb18a1901c324d0785f97439c0f84294",
      "c648fa173ed1451c9b52925d9701fa06",
      "4acc4536fa0f4eaab0117ca411c7f03c",
      "1a9d1d9238294f45bd0112a7bd403921",
      "467e0a498edc47e1bbebb4e722b6b3e4",
      "a3d9e4710d5a4245a068ea802492caa5",
      "5b6344b9f6b24e18adacf8d34ffd9263",
      "3755aad3cc954f49a1e2741bf1a9510a",
      "8edaa88c0bcb48278b69de4e6d13a1d6",
      "c4d7503e00364a7da2b18a6ad59db362",
      "27474281fc2e40ce8270012f8ca8ef60",
      "97a2822ecedb49919a217217bbc54241",
      "73934f6e1467465486d714aec60914ba",
      "5f1697aba045496980f644015fe5d089",
      "26e8c860ecb34a7fb7cb2349b262be8a",
      "894a5ff4c1254574bfe194fbacd2be91",
      "8a3e635a6eef45389180ddd9c74fd752",
      "3c98967402b5416685acd3d3cefaf3e1",
      "0212ea323d3c4e12b04286783911160c",
      "64df330dec1e4474b566f74ecf2bf0cc",
      "a51f4609d6f341ec98a7ae638eb13786",
      "3c1d828d24f2486cad960788bea2d57c",
      "74ef6802a71846ea8b3b97444171c7b5",
      "12393ba1cdc04df7b70c0015f90ae137",
      "00ea7bc8d04c41908983f71d4b8d15c2",
      "970eeb3ad10743b09d7f0b27c814ef35",
      "4fe8652f94dc4b27bf3768d810ba2a2d",
      "a5a2b599b0684983ac5318c12b87b2d9",
      "fbe9a57210ba4c6d9890120d54037512",
      "31c84f93cdf240678ad445e7352a87b0",
      "e870e3dc41994d068c2ceb05358a54f5",
      "323e724b7bd8420db71909e8ce385e68",
      "dd74f158544e48c3985c0f0b7f850176",
      "8342e1365a6945ad899320330b885708",
      "13846e374abb4245bd1dbf081d65df57",
      "eca1b50da4ab47909748a96707eb0e90",
      "3fd0b70233eb4db39b0ce691aa1757b3",
      "fc43013a56424833abee52ece49e22df",
      "00e99895a0774c78a8c5a73011c3c61d",
      "e16e34c9646e4a9bbf4ab2f87299c665",
      "52a38adfe6e84e2bb004758cf2f53859",
      "6f31a5ab43374053a2a444f6fb14a3af",
      "ab66974cff3d4c3087f57bf918f8b3cf",
      "df4eb307777c42ffa5d87bc6215bdde1",
      "80cf130ee9e748528d01361f4fc94c41",
      "a2b02b85b7754126a7d2d788fa8d40fa",
      "154aaaf1463a40608f4df42d25d637e3",
      "64ba1e7710794d1dbf0da71c81915cfe",
      "731ffd4cfb02490db491b7cbfba0a001",
      "e1cf7b6f452e42d68568bd3b09e73efd",
      "e533627b108142328ce15dd92a1a5fcc",
      "50971b6d2f82422baec58e13659c342a",
      "7dc518a2e02e41b6b2f0d95ebfc6fe56",
      "3d8f5ddee02e402393fd37fe6765bcb9",
      "0ac5aed5d3084d20922730c0aa202782",
      "78dc123ce17d425aa85aece804f1d2b4",
      "eca2674cf78341a0ada8f372591b1099",
      "eeaefd26aaf84edc80750395e311b623",
      "ac5564ad1fde4d6a8ca4fbcc69be4626",
      "6a2541388403444b9239d901abcc08e9",
      "88ac0c487f13466f84a7596d5cda6f8a",
      "262270ba053c4b11b6c4bbc5c9fb6d44",
      "8eed64b61fa949498ef1f9ed7d1dde6c",
      "150643b455d643d9b28754f61d8012ba",
      "89534c166cb24a23b61cb892f16f2d6d",
      "832dbd0003b149aa93969bbc5e77cb0b",
      "d9982aad25924d45a7ca6bdbc1f2ee20",
      "6cbe46d4d7ba478ebd70ab26cb6378c0",
      "4d8670b7e80b490cb263fe5f68f7a7d0",
      "4105bb2796d04ebaba6ae2c3693a5d3c",
      "2572dce1276d4224b550124af1a8881f",
      "c5726226f6d6418aa94565e787cbcdbd",
      "aab67dc430684776b816fec17aa0c861",
      "f5e34e80a40c4778942b9aa5bb538196",
      "92727180923c45c59cd95b63d22a9305",
      "1a0a88710be14d5882fc2776d8339a71",
      "5e0d1b7034f74144af76e2ff041bccb5",
      "e42fcc131841493e80b881d0f4e08953",
      "06fc59248d664b5c8ca4a2f036f80ed4",
      "45ccd806e2694a5eb8bbb4faf208c9e4",
      "09d3ef4972284cb1865eab638b1fbabc",
      "b50c75f2f98647ce880776e64baf0024",
      "583d1c7f3b8241b0abfc92446603881f",
      "212e511294714ed2a1f1d4eaf7d059a0",
      "30e8a7dc8aed4051be8a73a9c4f36a23",
      "a477d4e44cf240348fb16973048360cd",
      "41a497d79c2643ed8a099ff09ae556fd",
      "c74439fa0ea0440da1fde984d42a68f4",
      "ae99d71817304e2c84272d3a9092d951",
      "0027c1ac15124c1d8dfa9887bcd27a3d",
      "ba173dab71e1464cbcf4b460fb09b346",
      "0a95757029314479b82fce807bad186c",
      "65753ddf08f9422eaf51f27bba8a316e",
      "ff2a094107224b6c985d210c0ac43268",
      "b3c175c79e8e473fa501957ff8e9603f",
      "5adcf875719b46619aa762f0929d4258",
      "3b0466b1cdd245ecad07ae1917178126",
      "5ea5a8830e6d42d88b83b7acb955ab41",
      "c03a091b408448fb8ee6b2b7de9e4ffb",
      "b389ccdfcdf24dd8b6d763e6620470e1",
      "78b2912b82d84842971e871c543ef54d",
      "c02f72cef10e4ecf81ebe4baeb4ac908",
      "0ffd132db37a4276ba76c5ec4287a8f4",
      "579f218f6e344a739a0904a90be42be7",
      "c744d09bf04b457092ee606e0f6c2619",
      "8ce4db35c5c74c46951518458d02b357",
      "2445154c814d473cb93c4e2f73a7d919",
      "a43c9920bcc546b393bd70ad63994695",
      "13ae16dd2a95416884a1f06d658e1a0c",
      "41a4023931424a29993494d911946de5",
      "e7690963c614430a89bca8b80ddd0fa8",
      "4c7f2a0ced1548578f26d10c7e21dcae",
      "9a96d5d1d5334e6f81c83f04b26d9338",
      "d5e5c2b15c684181b4c47641cbfbe644",
      "f970fb8e94d944dc82bcfbf6aeb47fe7",
      "a09c96bc0aa74a508508b60ac21c84f8",
      "74976a2e64b04c298ec96f6947c36664",
      "fc81fcdae231436084ca0f088122df43",
      "0e7d3d5ffa6d4008bd95631adea75534",
      "f03976c1467f4c249922c525ab4dc577",
      "f46edea7ffcd44f28659f19f74eb2232",
      "3b6b4ff8dcbd4aedb695db03e56d2e80"
     ]
    },
    "id": "S2a5K57pQUmI",
    "outputId": "580f7d64-e3d4-41d6-d3e1-989e33166290"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6caee62e847e4267a941caa8da4b465f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c648fa173ed1451c9b52925d9701fa06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73934f6e1467465486d714aec60914ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12393ba1cdc04df7b70c0015f90ae137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13846e374abb4245bd1dbf081d65df57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b02b85b7754126a7d2d788fa8d40fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca2674cf78341a0ada8f372591b1099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbe46d4d7ba478ebd70ab26cb6378c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fc59248d664b5c8ca4a2f036f80ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0027c1ac15124c1d8dfa9887bcd27a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b2912b82d84842971e871c543ef54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7f2a0ced1548578f26d10c7e21dcae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_head.pkl:   0%|          | 0.00/179k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.2.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from setfit import SetFitModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"rodekruis/sml-ukr-message-classifier\"\n",
    "ONNX_MODEL = f\"onnx_models/{MODEL_NAME}\"\n",
    "\n",
    "model = SetFitModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHOl6P6vQUmI"
   },
   "source": [
    "Exporting the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05jlojpPQUmJ",
    "outputId": "689c4f38-07b0-4fd5-fdea-2e7a5936f0ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('onnx_models/rodekruis/sml-ukr-message-classifier/tokenizer_config.json',\n",
       " 'onnx_models/rodekruis/sml-ukr-message-classifier/special_tokens_map.json',\n",
       " 'onnx_models/rodekruis/sml-ukr-message-classifier/vocab.txt',\n",
       " 'onnx_models/rodekruis/sml-ukr-message-classifier/added_tokens.json',\n",
       " 'onnx_models/rodekruis/sml-ukr-message-classifier/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, export=True)\n",
    "tokenizer.save_pretrained(ONNX_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhXzO4lmQUmJ"
   },
   "source": [
    "Let's have a look inside these two directories and see what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oeq1er_9QUmJ",
    "outputId": "62259358-5056-4bda-902a-0413a838758f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 932\n",
      "-rw-r--r-- 1 root root    964 Jun 24 12:59 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root   1632 Jun 24 12:59 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root 710932 Jun 24 12:59 tokenizer.json\n",
      "-rw-r--r-- 1 root root 231536 Jun 24 12:59 vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -l {ONNX_MODEL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSn-y7naQUmJ"
   },
   "source": [
    "- As you can see, we need to move `vocab.txt` to assets folder which Spark NLP will look for\n",
    "- We also need `labels`. These are not contained in the model itself and we will have to fetch them manually. We will save this inside `labels.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5L8-R-M-QUmJ"
   },
   "outputs": [],
   "source": [
    "!mkdir -p {ONNX_MODEL}/assets && mv {ONNX_MODEL}/vocab.txt {ONNX_MODEL}/assets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Y9prrNUQUmJ",
    "outputId": "5c64f5fd-67d0-4637-c4b0-42bf3db518ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-24 12:59:57--  https://huggingface.co/rodekruis/sml-ukr-message-classifier/raw/main/label_dict.json\n",
      "Resolving huggingface.co (huggingface.co)... 65.8.243.90, 65.8.243.16, 65.8.243.46, ...\n",
      "Connecting to huggingface.co (huggingface.co)|65.8.243.90|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 589 [text/plain]\n",
      "Saving to: ‚Äòlabel_dict.json‚Äô\n",
      "\n",
      "\r\n",
      "label_dict.json       0%[                    ]       0  --.-KB/s               \r\n",
      "label_dict.json     100%[===================>]     589  --.-KB/s    in 0s      \n",
      "\n",
      "2025-06-24 12:59:57 (272 MB/s) - ‚Äòlabel_dict.json‚Äô saved [589/589]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/{MODEL_NAME}/raw/main/label_dict.json\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"label_dict.json\") as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "labels = [value for key, value in sorted(labels.items(), key=lambda x: int(x[0]))]\n",
    "\n",
    "with open(f\"{ONNX_MODEL}/assets/labels.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loz_YmZ-QUmJ",
    "outputId": "f1cc9aca-0f35-448b-8a5d-a6696562a5ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 232\n",
      "-rw-r--r-- 1 root root    337 Jun 24 12:59 labels.txt\n",
      "-rw-r--r-- 1 root root 231536 Jun 24 12:59 vocab.txt\n"
     ]
    }
   ],
   "source": [
    "ls -l {ONNX_MODEL}/assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe3RyG6RQUmJ"
   },
   "source": [
    "Voila! We have our `vocab.txt` and `labels.txt` inside assets directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hl06H2GTQUmK"
   },
   "source": [
    "## Combining and exporting the SetFit Modules\n",
    "\n",
    "The `SetFitModel` is composed of these components, we need to export:\n",
    "\n",
    "1. MPNet Embeddings Model\n",
    "2. Pooling Module\n",
    "3. Normalization Module\n",
    "4. Prediction Module\n",
    "\n",
    "We first create a custom torch module, to export it into a single ONNX graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Vfyu_QzzQUmK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SentencePredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch model wrapper for sentence-level predictions.\n",
    "    Uses pre-trained embeddings, pooling, and normalization layers from the given model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load model coefficients and intercept from the trained model head\n",
    "        self.coeffs = torch.Tensor(model.model_head.coef_)\n",
    "        self.intercept = torch.Tensor(model.model_head.intercept_)\n",
    "\n",
    "        # Extract model body components: embeddings, pooling, and normalization\n",
    "        self.embeddings, self.pooling, self.normalize = model.model_body\n",
    "\n",
    "    def predict(self, normed_embeddings):\n",
    "        \"\"\"\n",
    "        Compute logits using normalized embeddings.\n",
    "        \"\"\"\n",
    "        logits = normed_embeddings @ self.coeffs.T + self.intercept\n",
    "        return logits\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass through the model:\n",
    "        - Generate embeddings\n",
    "        - Apply pooling\n",
    "        - Normalize the pooled embeddings\n",
    "        - Compute logits\n",
    "        \"\"\"\n",
    "        input_data = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "        embeddings_out = self.embeddings(input_data)\n",
    "        pooling_out = self.pooling(embeddings_out)\n",
    "        normalize_out = self.normalize(pooling_out)\n",
    "\n",
    "        logits = self.predict(normalize_out[\"sentence_embedding\"])\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "# Instantiate the SentencePredictor model with the given pre-trained model\n",
    "sp = SentencePredictor(model)\n",
    "\n",
    "# Tokenize input sentences using the model's tokenizer\n",
    "input_data = model.model_body.tokenize([\n",
    "    \"I loved the Spiderman movie!\",\n",
    "    \"Pineapple on pizza is the worst ü§Æ\"\n",
    "])\n",
    "\n",
    "# Export the model to ONNX format for optimized inference\n",
    "torch.onnx.export(\n",
    "    sp,\n",
    "    args=input_data,\n",
    "    f=f\"{ONNX_MODEL}/model.onnx\",\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"token_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"token_length\"},\n",
    "        \"logits\": {0: \"batch_size\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTf_-LWyQUmK"
   },
   "source": [
    "Now we have the model and all necessary files to import it into Spark NLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UC2_TI8FQUmK",
    "outputId": "7945de33-cbbf-44ae-cfed-fe9469b04a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnx_models/rodekruis/sml-ukr-message-classifier:\n",
      "total 426468\n",
      "drwxr-xr-x 2 root root      4096 Jun 24 12:59 assets\n",
      "-rw-r--r-- 1 root root 435970222 Jun 24 13:00 model.onnx\n",
      "-rw-r--r-- 1 root root       964 Jun 24 12:59 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root      1632 Jun 24 12:59 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root    710932 Jun 24 12:59 tokenizer.json\n",
      "\n",
      "onnx_models/rodekruis/sml-ukr-message-classifier/assets:\n",
      "total 232\n",
      "-rw-r--r-- 1 root root    337 Jun 24 12:59 labels.txt\n",
      "-rw-r--r-- 1 root root 231536 Jun 24 12:59 vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lR {ONNX_MODEL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fj-z-EMfAf5z"
   },
   "source": [
    "Now let's convert and save the ONNX model in OpenVINO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kXi0h7TYTiB7"
   },
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "\n",
    "ov.save_model(ov.convert_model(f\"{ONNX_MODEL}/model.onnx\"), \"openvino_model.xml\")\n",
    "\n",
    "!rm -rf {ONNX_MODEL}/model.onnx\n",
    "!mv /content/openvino_model.* {ONNX_MODEL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYpWN4LEQUmK"
   },
   "source": [
    "## Import and Save MPNetForSequenceClassification in Spark NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXSE8xu0QUmK"
   },
   "source": [
    "- Install and set up Spark NLP in Google Colab\n",
    "- This example uses specific versions of `pyspark` and `spark-nlp` that have been tested with the transformer model to ensure everything runs smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kZpAEUoQUmK",
    "outputId": "0c4d16c4-d041-43ea-b047-b581fa2acde0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pyspark==3.5.4 spark-nlp==5.5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMTflUSqQUmK"
   },
   "source": [
    "Let's start Spark with Spark NLP included via our simple `start()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQ9IAdEpQUmK",
    "outputId": "ead48438-ca2a-4cd8-ada1-2fd1dda9b48c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version: 5.5.3\n",
      "Apache Spark version: 3.5.4\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: {}\".format(sparknlp.version()))\n",
    "print(\"Apache Spark version: {}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xPhT7hwQUmK"
   },
   "source": [
    "- Let's use `loadSavedModel` functon in `MPNetForSequenceClassification` which allows us to load TensorFlow model in SavedModel format\n",
    "- Most params can be set later when you are loading this model in `MPNetForSequenceClassification` in runtime like `setMaxSentenceLength`, so don't worry what you are setting them now\n",
    "- `loadSavedModel` accepts two params, first is the path to the TF SavedModel. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
    "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4UZH8_yXQUmK"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import MPNetForSequenceClassification\n",
    "\n",
    "sequenceClassifier = (\n",
    "    MPNetForSequenceClassification.loadSavedModel(ONNX_MODEL, spark)\n",
    "    .setInputCols([\"document\", \"token\"])\n",
    "    .setOutputCol(\"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5y2_o0wmQUmL"
   },
   "source": [
    "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "J5WG-CNyQUmL"
   },
   "outputs": [],
   "source": [
    "sequenceClassifier.write().overwrite().save(\"./{}_spark_nlp_onnx\".format(ONNX_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMZFJ2ugQUmL"
   },
   "source": [
    "Let's clean up stuff we don't need anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0YukPfUhQUmL"
   },
   "outputs": [],
   "source": [
    "!rm -rf {ONNX_MODEL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CYRMABhQUmL"
   },
   "source": [
    "Awesome üòé  !\n",
    "\n",
    "This is your AlbertForSequenceClassification model from HuggingFace ü§ó  loaded and saved by Spark NLP üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SlRf2pMLQUmL",
    "outputId": "012759b8-de5a-45b6-80c9-f3fc512c8dd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 213320\n",
      "drwxr-xr-x 4 root root      4096 Jun 24 13:03 fields\n",
      "drwxr-xr-x 2 root root      4096 Jun 24 13:03 metadata\n",
      "-rw-r--r-- 1 root root 218431440 Jun 24 13:03 mpnet_classification_openvino\n"
     ]
    }
   ],
   "source": [
    "! ls -l {ONNX_MODEL}_spark_nlp_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiKlUGhUQUmL"
   },
   "source": [
    "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny AlbertForSequenceClassification model üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fZzom5UKQUmL"
   },
   "outputs": [],
   "source": [
    "sequenceClassifier_loaded = (\n",
    "    MPNetForSequenceClassification.load(\"./{}_spark_nlp_onnx\".format(ONNX_MODEL))\n",
    "    .setInputCols([\"document\", \"token\"])\n",
    "    .setOutputCol(\"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IpzmYpOQUmL"
   },
   "source": [
    "You can see what labels were used to train this model via `getClasses` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wiphOA3YQUmL",
    "outputId": "d01b9560-03cb-43f1-f114-4df0f81fec98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GOODS/SERVICES',\n",
       " 'EDUCATION',\n",
       " 'SHELTER',\n",
       " 'OTHER PROGRAMS/NGOS',\n",
       " 'RC PROGRAM INFO',\n",
       " 'CVA REGISTRATION',\n",
       " 'CAR',\n",
       " 'ARMY',\n",
       " 'PSS & RFL',\n",
       " 'CVA PAYMENT',\n",
       " 'CHILDREN',\n",
       " 'CONNECTIVITY',\n",
       " 'CVA INCLUSION',\n",
       " 'FOOD',\n",
       " 'HEALTH',\n",
       " 'TRANSLATION/LANGUAGE',\n",
       " 'LEGAL',\n",
       " 'CVA PROGRAM INFO',\n",
       " 'PETS',\n",
       " 'MONEY/BANKING',\n",
       " 'WORK/JOBS',\n",
       " 'RC CONNECT WITH RED CROSS',\n",
       " 'PARCEL',\n",
       " 'TRANSPORT/MOVEMENT',\n",
       " 'NFI',\n",
       " 'ANOMALY',\n",
       " 'RC PMER/NEW PROGRAMS',\n",
       " 'WASH',\n",
       " 'SENTIMENT']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequenceClassifier_loaded.getClasses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltoOdMqkQUmO"
   },
   "source": [
    "This is how you can use your loaded classifier model in Spark NLP üöÄ pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q25VQ17NQUmP",
    "outputId": "76aadd1f-fa51-4a86-f17e-3d231ce0cd7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|              result|\n",
      "+--------------------+--------------------+\n",
      "|I love driving my...|               [CAR]|\n",
      "|The next bus will...|[TRANSPORT/MOVEMENT]|\n",
      "|Pineapple on pizz...|              [FOOD]|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import Tokenizer\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    sequenceClassifier_loaded\n",
    "])\n",
    "\n",
    "example = spark.createDataFrame([\n",
    "    [\"I love driving my car.\"],\n",
    "    [\"The next bus will arrive in 20 minutes.\"],\n",
    "    [\"Pineapple on pizza is the worst ü§Æ\"]\n",
    "]).toDF(\"text\")\n",
    "\n",
    "model = pipeline.fit(example)\n",
    "result = model.transform(example)\n",
    "\n",
    "result.select(\"text\", \"label.result\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gr0Ipn6wQUmP"
   },
   "source": [
    "That's it! You can now go wild and use hundreds of `MPNetForSequenceClassification` models from HuggingFace ü§ó in Spark NLP üöÄ\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
