{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_V5XcDCnVgSi"
   },
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/openvino/HuggingFace_OpenVINO_in_Spark_NLP_BartForZeroShotClassification.ipynb)\n",
    "\n",
    "# Import OpenVINO BartForZeroShotClassification   models from HuggingFace 🤗 into Spark NLP 🚀\n",
    "\n",
    "This notebook provides a detailed walkthrough on optimizing and exporting BartForZeroShotClassification   models from HuggingFace for use in Spark NLP, leveraging the various tools provided in the [Intel OpenVINO toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html) ecosystem.\n",
    "\n",
    "Let's keep in mind a few things before we start 😊\n",
    "\n",
    "- OpenVINO support was introduced in  `Spark NLP 5.4.0`, enabling high performance inference for models. Please make sure you have upgraded to the latest Spark NLP release.\n",
    "- You can import models for BartForZeroShotClassification   from BartForZeroShotClassification   and they have to be in `Zero Shot Classification` category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aghasVppVgSk"
   },
   "source": [
    "## 1. Export and Save the HuggingFace model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be4HsTDMVgSk"
   },
   "source": [
    "- Let's install `transformers` and `openvino` packages with other dependencies. You don't need `openvino` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
    "- We lock `transformers` on version `4.48.3`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X61N_CyMAEc5",
    "outputId": "eaed5f15-51c5-4e48-c4e9-df6f0c13ce0b"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade transformers==4.48.3 optimum[openvino]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vI7uz_6hVgSl"
   },
   "source": [
    "[Optimum Intel](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#openvino) is the interface between the Transformers library and the various model optimization and acceleration tools provided by Intel. HuggingFace models loaded with optimum-intel are automatically optimized for OpenVINO, while being compatible with the Transformers API.\n",
    "- To load a HuggingFace model directly for inference/export, just replace the `AutoModelForXxx` class with the corresponding `OVModelForXxx` class. We can use this to import and export OpenVINO models with `from_pretrained` and `save_pretrained`.\n",
    "- By setting `export=True`, the source model is converted to OpenVINO IR format on the fly.\n",
    "- We'll use [facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli) model from HuggingFace as an example and load it as a `OVModelForSequenceClassification`, representing an OpenVINO model.\n",
    "- In addition to the BartForZeroShotClassification   model, we also need to save the `AutoTokenizer`. This is the same for every model, these are assets (saved in `/assets`) needed for tokenization inside Spark NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559,
     "referenced_widgets": [
      "b738182ea21844dc8e39f1daf31a804f",
      "de9d453169ba432c9ecca15ed2e11b00",
      "8500ee2b96014db984882e166940ea3b",
      "b25b4a9981e74c5d964f9dcc17e0e727",
      "310a4cb3dc384dcaaff0087fe72f5996",
      "657d0ea508c94ac9b19f051fb0e18ca2",
      "522f2bad091147ae884ca072f4ed7616",
      "9da6fe8a006d490c82635b8e3ec1e0bb",
      "04acda1939b9480ba21539057e25d56c",
      "153688d0beb44dab9b610d5d0dc3a0f8",
      "0127b80a31ed4d9da789f86a5ee1e011",
      "8ac94136d7b94481a579c560cce4531f",
      "2c30fbe2691e49f8ae61164245b0e906",
      "b4099b3355f04c359b033e84ee447351",
      "4a252c43c39f493e8649a39333932eb1",
      "48390daf1d0f40d39f55ef4cb0bb09fb",
      "1e0cf1dfa5554c1dafb97ad065b44f60",
      "0511d2bdc38d4a7496547873b4386fe3",
      "24b7008f4b6f469899be43a188de945c",
      "96455b261a094583a3ed825b93c00db1",
      "c2131ba4365b414cb928b7a6154a3dc0",
      "0e9e9558c0764f63ba9035ff99499fdf",
      "0f0012a559714718b7c5ad27ddb1965d",
      "82e36cb65ce540408fbbbc10285f71e4",
      "0be97a07a3d145c2b6c6c19d2ccf7b71",
      "6c5f472299f94d21a435abd4238bfbbd",
      "aeee91df736f4ccba30d32b19ed22c78",
      "c275f6472da948a59c258a1c9f5e7f60",
      "3daa87d4508c4525862d605f026c36ba",
      "bf9326e77e6c4e9cb9e9b7a491c7fe67",
      "003ac5b97ce44642a0cb1bbd198629d4",
      "9c5945a9432b4cc1b9cb0d3d9942bb96",
      "e0957cfb182c4407a55ee80f02afde67",
      "3f4bcbdc5bac41f9917b4991ba15dbf4",
      "78494f31e7c14316a743ab4d9bd6e35c",
      "b3d56f89f9fd4234a90204786eac7728",
      "080885f8df254ff186e2618a89ec5be9",
      "0da09bf673144a8eb2e544eb6cac5883",
      "b74e2f749d60402bb209f96f9719bcdc",
      "06e4678db85845efb75624c1b29cc8b7",
      "8d3f6e2a0ef34a489e8a9b52a32b68e7",
      "9988d80e461f4170a5b636a700a631d4",
      "207af6b800ad4cb1af22d6db3e61f8ce",
      "e2650699de07474596b607d2df74aa2b",
      "a31bf280fe2b4727bb683120c69ed4f3",
      "50c737cd8d48470e8adfec5ee2b2a245",
      "5eeeb872f84a4055b30a50451379c5ba",
      "b8b51a1fe7b24d99873cee19ac97d797",
      "3a9f70fdc2bb42d997a8aabcd730fcf6",
      "0ce0a2555f734fe79646efec7f0cc845",
      "a0405c0c06674d70900a413ccf6ac5e9",
      "b3d305d0d95b42a0bf92461017a9d50b",
      "971187264b2640adaf24d4f7604fa8e9",
      "e27e537b6970475b9b49c0455b421857",
      "acbab56caafd4f1cb87c7306bc4aac2b",
      "60cad51d97614684828fd351a1b5ea8e",
      "f0a2dbd75ea14c29ad019270ebce5b73",
      "7a38df6989344611b19bc4a4886b9211",
      "dd13620782df4874a02a31ef15618045",
      "a81304f3ec6b40aaa92b18c8d0750bd8",
      "15826e6017004ae0b6a22b03a2793c86",
      "58502a222e724912a45cf918044a3faa",
      "5b77f65fc27844b887541e4165bab202",
      "f41a28cf2fb3454ca6fbe2a5c13a24fb",
      "e35ac0719b8642dbae949c46749f8f77",
      "b333d4f0668148b89b54abca21a1f2e5"
     ]
    },
    "id": "Jzrz_iabn6bX",
    "outputId": "483b6b2b-0e80-4628-8778-ec348243ad01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'FetchError: Could not fetch resource at https://colab.research.google.com/userdata/get?authuser=0&notebookid=1UNfbUf3s29teZUBaIey-OkzZm0A3xuKO&key=HF_TOKEN: 401  '.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b738182ea21844dc8e39f1daf31a804f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac94136d7b94481a579c560cce4531f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0012a559714718b7c5ad27ddb1965d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4bcbdc5bac41f9917b4991ba15dbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31bf280fe2b4727bb683120c69ed4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60cad51d97614684828fd351a1b5ea8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py:505: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_attn_mask_utils.py:88: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_attn_mask_utils.py:164: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('onnx_models/facebook/bart-large-mnli/tokenizer_config.json',\n",
       " 'onnx_models/facebook/bart-large-mnli/special_tokens_map.json',\n",
       " 'onnx_models/facebook/bart-large-mnli/vocab.json',\n",
       " 'onnx_models/facebook/bart-large-mnli/merges.txt',\n",
       " 'onnx_models/facebook/bart-large-mnli/added_tokens.json',\n",
       " 'onnx_models/facebook/bart-large-mnli/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.intel import OVModelForSequenceClassification\n",
    "\n",
    "MODEL_NAME = \"facebook/bart-large-mnli\"\n",
    "EXPORT_PATH = f\"ov_models/{MODEL_NAME}\"\n",
    "\n",
    "ort_model = OVModelForSequenceClassification.from_pretrained(MODEL_NAME, export=True)\n",
    "ort_model.save_pretrained(EXPORT_PATH)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.save_pretrained(EXPORT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAs-wq_wVgSn"
   },
   "source": [
    "Let's have a look inside these two directories and see what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "daFkGhN-VgSn",
    "outputId": "d291aee0-172c-4e12-f8f1-4219268a7043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1596880\n",
      "-rw-r--r-- 1 root root       1197 Jun 18 01:44 config.json\n",
      "-rw-r--r-- 1 root root     456318 Jun 18 01:44 merges.txt\n",
      "-rw-r--r-- 1 root root 1629376728 Jun 18 01:44 openvino_model.bin\n",
      "-rw-r--r-- 1 root root     988634 Jun 18 01:44 openvino_model.xml\n",
      "-rw-r--r-- 1 root root        279 Jun 18 01:44 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root       1243 Jun 18 01:44 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root    3558642 Jun 18 01:44 tokenizer.json\n",
      "-rw-r--r-- 1 root root     798293 Jun 18 01:44 vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls -l {EXPORT_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Po-SRJCp3cX"
   },
   "source": [
    "- We need to move `vocab.txt` and `merges.txt` from the tokenizer into an `assets` folder, which is where Spark NLP expects to find them. However, before doing that, we first need to convert Hugging Face’s `vocab.json` into a plain `vocab.txt` format, as Spark NLP does not support the JSON format.\n",
    "- Additionally, we need to extract the `labels` and their corresponding `ids` from the model's config. This mapping will be saved as `labels.txt` inside the same `assets` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PapWKuGpo2Dh"
   },
   "outputs": [],
   "source": [
    "!mkdir -p {EXPORT_PATH}/assets\n",
    "\n",
    "with open(f\"{EXPORT_PATH}/assets/labels.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([v for k, v in sorted(ort_model.config.id2label.items())]))\n",
    "\n",
    "import json\n",
    "with open(f\"{EXPORT_PATH}/vocab.json\") as fin, open(f\"{EXPORT_PATH}/assets/vocab.txt\", \"w\") as fout:\n",
    "    fout.writelines(f\"{k}\\n\" for k in json.load(fin))\n",
    "\n",
    "!mv {EXPORT_PATH}/merges.txt {EXPORT_PATH}/assets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8EWO6mRpLrr",
    "outputId": "688c4733-4d1d-423b-c8fe-1d2c78fb76ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 852\n",
      "-rw-r--r-- 1 root root     32 Jun 18 01:44 labels.txt\n",
      "-rw-r--r-- 1 root root 456318 Jun 18 01:44 merges.txt\n",
      "-rw-r--r-- 1 root root 407065 Jun 18 01:44 vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -l {EXPORT_PATH}/assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qJdRtwVpxnJ"
   },
   "source": [
    "- Voila! We have our `vocab.txt` and `merges.txt` inside assets directory, along with the extracted labels saved in `labels.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOUCFUjwN3xn"
   },
   "source": [
    "## Import and Save BartForZeroShotClassification in Spark NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qs1eoKAvN3xn"
   },
   "source": [
    "- Install and set up Spark NLP in Google Colab\n",
    "- This example uses specific versions of `pyspark` and `spark-nlp` that have been tested with the transformer model to ensure everything runs smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eayHES7XN3xn",
    "outputId": "59c34c7b-77ec-4a0f-8066-8df5d38f6df6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pyspark==3.5.4 spark-nlp==5.5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7VqvgjiN3xn"
   },
   "source": [
    "Let's start Spark with Spark NLP included via our simple `start()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IGJVB0X8N3xn",
    "outputId": "0657eb33-4e22-46e1-b300-cca70fdbd55a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  5.5.3\n",
      "Apache Spark version:  3.5.4\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSHrTI5lN3xn"
   },
   "source": [
    "- Let's use `loadSavedModel` functon in `BartForZeroShotClassification` which allows us to load TensorFlow model in SavedModel format\n",
    "- Most params can be set later when you are loading this model in `BartForZeroShotClassification` in runtime like `setMaxSentenceLength`, so don't worry what you are setting them now\n",
    "- `loadSavedModel` accepts two params, first is the path to the TF SavedModel. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
    "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "clNYKce5N3xo"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import BartForZeroShotClassification\n",
    "\n",
    "sequenceClassifier = BartForZeroShotClassification\\\n",
    "  .loadSavedModel(EXPORT_PATH, spark)\\\n",
    "  .setInputCols([\"document\",'token'])\\\n",
    "  .setOutputCol(\"class\")\\\n",
    "  .setCaseSensitive(False)\\\n",
    "  .setMaxSentenceLength(128)\\\n",
    "  .setCandidateLabels([\"urgent\", \"mobile\", \"travel\", \"movie\", \"music\", \"sport\", \"weather\", \"technology\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85t-OnYGN3xo"
   },
   "source": [
    "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-dfsFviKN3xo"
   },
   "outputs": [],
   "source": [
    "sequenceClassifier.write().overwrite().save(\"./{}_spark_nlp_openvino\".format(MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-o3b5WGN3xo"
   },
   "source": [
    "Let's clean up stuff we don't need anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "scZ0yARFN3xo"
   },
   "outputs": [],
   "source": [
    "!rm -rf {EXPORT_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVbO6rvPN3xo"
   },
   "source": [
    "Awesome 😎  !\n",
    "\n",
    "This is your BartForZeroShotClassification model from HuggingFace 🤗  loaded and saved by Spark NLP 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sHVo1Nz1N3xo",
    "outputId": "fbc8dbe3-fbf8-442d-c3a2-7caa8f9a0a08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1592412\n",
      "-rw-r--r-- 1 root root 1630614420 Jun 18 01:48 bart_classification_openvino\n",
      "drwxr-xr-x 5 root root       4096 Jun 18 01:47 fields\n",
      "drwxr-xr-x 2 root root       4096 Jun 18 01:47 metadata\n"
     ]
    }
   ],
   "source": [
    "! ls -l {MODEL_NAME}_spark_nlp_openvino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzD1bqT9N3xo"
   },
   "source": [
    "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny BartForZeroShotClassification model 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xF9ZV583N3xo"
   },
   "outputs": [],
   "source": [
    "sequenceClassifier_loaded = BartForZeroShotClassification.load(\"./{}_spark_nlp_openvino\".format(MODEL_NAME))\\\n",
    "  .setInputCols([\"document\",'token'])\\\n",
    "  .setOutputCol(\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3COlXkvN3xo"
   },
   "source": [
    "This is how you can use your loaded classifier model in Spark NLP 🚀 pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrxDXGXuN3xp",
    "outputId": "5a8f89d4-f65f-49a0-d3b9-5fec7a169bab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------+--------+\n",
      "|text                                                                                                          |result  |\n",
      "+--------------------------------------------------------------------------------------------------------------+--------+\n",
      "|I have a problem with my iPhone that needs to be resolved asap!!                                              |[urgent]|\n",
      "|Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.|[music] |\n",
      "|I have a phone and I love it!                                                                                 |[urgent]|\n",
      "|I really want to visit Germany and I am planning to go there next year.                                       |[urgent]|\n",
      "|Let's watch some movies tonight! I am in the mood for a horror movie.                                         |[music] |\n",
      "|Have you watched the match yesterday? It was a great game!                                                    |[urgent]|\n",
      "|We need to hurry up and get to the airport. We are going to miss our flight!                                  |[music] |\n",
      "+--------------------------------------------------------------------------------------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols(\"document\") \\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    sequenceClassifier_loaded\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame([[\"I have a problem with my iPhone that needs to be resolved asap!!\"],\n",
    "        [\"Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\"],\n",
    "        [\"I have a phone and I love it!\"],\n",
    "        [\"I really want to visit Germany and I am planning to go there next year.\"],\n",
    "        [\"Let's watch some movies tonight! I am in the mood for a horror movie.\"],\n",
    "        [\"Have you watched the match yesterday? It was a great game!\"],\n",
    "        [\"We need to hurry up and get to the airport. We are going to miss our flight!\"]], [\"text\"])\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "result = model.transform(df)\n",
    "\n",
    "result.select(\"text\", \"class.result\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA9KTNurN3xp"
   },
   "source": [
    "That's it! You can now go wild and use hundreds of `BartForZeroShotClassification` models from HuggingFace 🤗 in Spark NLP 🚀\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
