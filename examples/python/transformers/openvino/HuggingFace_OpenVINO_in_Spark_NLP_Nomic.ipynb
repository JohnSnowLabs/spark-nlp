{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvX_yCcI4W7D"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/openvino/HuggingFace_OpenVINO_in_Spark_NLP_Nomic.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J48sFcb4W7G"
      },
      "source": [
        "# Import OpenVINO Nomic models from HuggingFace ü§ó into Spark NLP üöÄ\n",
        "\n",
        "This notebook provides a detailed walkthrough on optimizing and importing Nomic models from HuggingFace  for use in Spark NLP, with [Intel OpenVINO toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html). The focus is on converting the model to the OpenVINO format and applying precision optimizations (INT8 and INT4), to enhance the performance and efficiency on CPU platforms using [Optimum Intel](https://huggingface.co/docs/optimum/main/en/intel/inference).\n",
        "\n",
        "Let's keep in mind a few things before we start üòä\n",
        "\n",
        "- OpenVINO support was introduced in  `Spark NLP 5.4.0`, enabling high performance CPU inference for models. So please make sure you have upgraded to the latest Spark NLP release.\n",
        "- Model quantization is a computationally expensive process, so it is recommended to use a runtime with more than 32GB memory for exporting the quantized model from HuggingFace.\n",
        "- You can import Nomic models via `NomicModel`. These models are usually under `Text Generation` category and have `Nomic` in their labels.\n",
        "- Some [example models](https://huggingface.co/models?search=Nomic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko24PkTd4W7H"
      },
      "source": [
        "## 1. Export and Save the HuggingFace model\n",
        "\n",
        "- Let's install `transformers` and `openvino` packages with other dependencies. You don't need `openvino` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.52.4`. This doesn't mean it won't work with the future release, but we wanted you to know which versions have been tested successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rOdslOi4W7H",
        "outputId": "8e632456-68ea-4834-8686-895d81e29af0"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade transformers[onnx]==4.52.4 optimum openvino"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twst53-c36hs"
      },
      "source": [
        "[Optimum Intel](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#openvino) is the interface between the Transformers library and the various model optimization and acceleration tools provided by Intel. HuggingFace models loaded with optimum-intel are automatically optimized for OpenVINO, while remaining compatible with the Transformers API.\n",
        "\n",
        "- We first use the `optimum-cli` tool to export the [openbmb/Nomic-2B-dpo-bf16](https://huggingface.co/openbmb/Nomic-2B-dpo-bf16) model to ONNX format for the `feature-extraction` task.\n",
        "- Then, we use `convert_model()` to convert the exported ONNX model into OpenVINO Intermediate Representation (IR) format (`.xml` and `.bin`) directly in Python.\n",
        "- The resulting OpenVINO model is saved in the specified directory (`export_openvino/hkunlp-instructor-base`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdO-QNW0Bf85"
      },
      "source": [
        "Export ONNX model using Optimum CLI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up83Ytj2Bf85"
      },
      "outputs": [],
      "source": [
        "!optimum-cli export onnx --trust-remote-code --task feature-extraction --model nomic-ai/nomic-embed-text-v1 ./onnx_models/nomic-ai/nomic-embed-text-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tksdFHxeBf86"
      },
      "source": [
        "Convert ONNX to OpenVINO IR with FP16 compression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gCnL67FqE9B9"
      },
      "outputs": [],
      "source": [
        "import openvino as ov\n",
        "\n",
        "MODEL_NAME = \"nomic-ai/nomic-embed-text-v1\"\n",
        "!mkdir -p models/$MODEL_NAME\n",
        "\n",
        "ov_model = ov.convert_model(f\"./onnx_models/{MODEL_NAME}/model.onnx\")\n",
        "ov.save_model(ov_model, f\"models/{MODEL_NAME}/openvino_model.xml\", compress_to_fp16=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qElFqttBf86"
      },
      "source": [
        "Save tokenizer vocabulary to assets folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WFXkrrHBf87"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "!mkdir -p models/nomic-ai/nomic-embed-text-v1/assets\n",
        "AutoTokenizer.from_pretrained(\"bert-base-uncased\").save_vocabulary(\"models/nomic-ai/nomic-embed-text-v1/assets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svbT3OG24W7L"
      },
      "source": [
        "## 2. Import and Save Nomic in Spark NLP\n",
        "\n",
        "- Install and set up Spark NLP in Google Colab\n",
        "- This example uses specific versions of `pyspark` and `spark-nlp` that have been tested with the transformer model to ensure everything runs smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6TWf2r14W7L",
        "outputId": "d8157a4e-43a3-46a6-f867-f53b226859f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark==3.5.4 spark-nlp==5.5.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYI03iqp4W7L"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_Oy0zMi4W7L",
        "outputId": "48d6c268-9e3b-49b7-d51f-ae763e10304e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark NLP version:  5.5.3\n",
            "Apache Spark version:  3.5.4\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXCJqb9i4W7M"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `NomicEmbeddings` which allows us to load the OpenVINO model.\n",
        "- Most params will be set automatically. They can also be set later after loading the model in `NomicEmbeddings` during runtime, so don't worry about setting them now.\n",
        "- `loadSavedModel` accepts two params, first is the path to the exported model. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "T3591W9R4W7M"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import NomicEmbeddings\n",
        "\n",
        "Nomic = NomicEmbeddings \\\n",
        "    .loadSavedModel(\"models/nomic-ai/nomic-embed-text-v1\", spark) \\\n",
        "    .setInputCols([\"documents\"]) \\\n",
        "    .setOutputCol(\"generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X3RphM-4W7M"
      },
      "source": [
        "Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T6GaugQa4W7M"
      },
      "outputs": [],
      "source": [
        "Nomic.write().overwrite().save(f\"{MODEL_NAME}_spark_nlp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0kroa6u4W7M"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BHvWriCn4W7M"
      },
      "outputs": [],
      "source": [
        "!rm -rf {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz4cU4Q54W7N"
      },
      "source": [
        "Awesome  üòé !\n",
        "\n",
        "This is your OpenVINO Nomic model from HuggingFace ü§ó  loaded and saved by Spark NLP üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17klLp1M4W7N",
        "outputId": "dfef608f-af72-4c97-c0d5-e97ac740944a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 267996\n",
            "drwxr-xr-x 3 root root      4096 Jun 23 07:09 fields\n",
            "drwxr-xr-x 2 root root      4096 Jun 23 07:09 metadata\n",
            "-rw-r--r-- 1 root root 274413907 Jun 23 07:09 nomic_openvino\n"
          ]
        }
      ],
      "source": [
        "! ls -l {MODEL_NAME}_spark_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R_rS8Fj4W7N"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny Nomic model üòä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TxmyaXyBf8-",
        "outputId": "4f0380b5-9055-410f-8357-8b58f409f72c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|          embeddings|\n",
            "+--------------------+\n",
            "|[[0.055686906, 0....|\n",
            "|[[-0.0036336272, ...|\n",
            "|[[0.004018774, 0....|\n",
            "|[[-0.018702844, 0...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "test_data = spark.createDataFrame([\n",
        "    [1, \"query: how much protein should a female eat\"],\n",
        "    [2, \"query: summit define\"],\n",
        "    [3, \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 \"\n",
        "        \"is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're \"\n",
        "        \"expecting or training for a marathon. Check out the chart below to see how much protein you should \"\n",
        "        \"be eating each day.\"],\n",
        "    [4, \"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain :\"\n",
        "        \" the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the \"\n",
        "        \"leaders of two or more governments.\"]\n",
        "]).toDF(\"id\", \"text\")\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"documents\")\n",
        "\n",
        "nomic = NomicEmbeddings \\\n",
        "    .load(f\"{MODEL_NAME}_spark_nlp\") \\\n",
        "    .setInputCols([\"documents\"]) \\\n",
        "    .setOutputCol(\"nomic\")\n",
        "\n",
        "pipeline = Pipeline().setStages([\n",
        "    document_assembler,\n",
        "    nomic\n",
        "])\n",
        "\n",
        "model = pipeline.fit(test_data)\n",
        "results = model.transform(test_data)\n",
        "\n",
        "results.select(\"nomic.embeddings\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdvQAAfo4W7N"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of Nomic models from HuggingFace ü§ó in Spark NLP üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
