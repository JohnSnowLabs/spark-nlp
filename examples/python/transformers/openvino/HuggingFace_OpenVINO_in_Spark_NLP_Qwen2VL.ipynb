{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-UfCCnfCwj_"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/openvino/HuggingFace_OpenVINO_in_Spark_NLP_Qwen2VL.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjcgKezHCwkB"
      },
      "source": [
        "# Import OpenVINO Qwen2VL models from HuggingFace ğŸ¤— into Spark NLP ğŸš€\n",
        "\n",
        "This notebook provides a detailed walkthrough on optimizing and importing Qwen2VL models from HuggingFace  for use in Spark NLP, with [Intel OpenVINO toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html). The focus is on converting the model to the OpenVINO format and applying precision optimizations (INT8 and INT4), to enhance the performance and efficiency on CPU platforms using [Optimum Intel](https://huggingface.co/docs/optimum/main/en/intel/inference).\n",
        "\n",
        "Let's keep in mind a few things before we start ğŸ˜Š\n",
        "\n",
        "- OpenVINO support was introduced in  `Spark NLP 5.4.0`, enabling high performance CPU inference for models. So please make sure you have upgraded to the latest Spark NLP release.\n",
        "- Model quantization is a computationally expensive process, so it is recommended to use a runtime with more than 32GB memory for exporting the quantized model from HuggingFace.\n",
        "- You can import Qwen2VL models via `Qwen2VL`. These models are usually under `Text Generation` category and have `Qwen2VL` in their labels.\n",
        "- Reference: [Qwen2VL](https://huggingface.co/docs/transformers/model_doc/llama#transformers.Qwen2VL)\n",
        "- Some [example models](https://huggingface.co/models?search=Qwen2VL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCR7RVVvCwkC"
      },
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "This notebook installs and configures the dependencies required to load, optimize, and run Qwen2-VL models using OpenVINO and Hugging Face Transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoMqUjRwCwkC",
        "outputId": "1c4e87d0-ed1a-4467-a95b-88042745070e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing packages: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:56<00:00, 17.70s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import subprocess\n",
        "\n",
        "def pip_install(package_list):\n",
        "    for pkg in tqdm(package_list, desc=\"Installing packages\", ncols=100):\n",
        "        subprocess.run([\"pip\", \"install\", *pkg.split()], stdout=subprocess.DEVNULL)\n",
        "\n",
        "packages = [\n",
        "    'openvino>=2024.4.0',\n",
        "    'nncf>=2.13.0',\n",
        "    'sentencepiece',\n",
        "    'tokenizers>=0.12.1',\n",
        "    'transformers==4.45.0',\n",
        "    'accelerate>=0.26.0',\n",
        "    '--pre --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly openvino openvino-tokenizers openvino-genai',\n",
        "    'torch>=2.2.1',\n",
        "    'torchvision>=0.10.2',\n",
        "    'qwen-vl-utils'\n",
        "]\n",
        "\n",
        "pip_install(packages)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "files_to_download = {\n",
        "    \"ov_qwen2_vl.py\": \"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/e0aa6c599bb9c88de0c8758aef967a6c05ad27b6/notebooks/qwen2-vl/ov_qwen2_vl.py\",\n",
        "    \"notebook_utils.py\": \"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/e0aa6c599bb9c88de0c8758aef967a6c05ad27b6/utils/notebook_utils.py\"\n",
        "}\n",
        "\n",
        "for filename, url in tqdm(files_to_download.items(), desc=\"Downloading utility files\", ncols=100):\n",
        "    if not Path(filename).exists():\n",
        "        response = requests.get(url)\n",
        "        Path(filename).write_text(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeD2DZw3V0_j",
        "outputId": "fe4e834c-e080-4d25-d590-81b3085abc64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading utility files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Third-party libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import openvino as ov\n",
        "import nncf\n",
        "\n",
        "# Transformers (Hugging Face)\n",
        "from transformers import AutoConfig, AutoProcessor\n",
        "from transformers.models.qwen2_vl.modeling_qwen2_vl import VisionRotaryEmbedding\n",
        "\n",
        "# Local project modules\n",
        "from ov_qwen2_vl import convert_qwen2vl_model, model_selector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm3WFvB9UolN",
        "outputId": "3f18af74-ab3a-4b54-bfe6-5a653d555b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/openvino/runtime/__init__.py:10: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlWioNt-CwkD"
      },
      "source": [
        "## Convert the model to OpenVino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIvUO5N2CwkD",
        "outputId": "d2278cff-930a-4ca4-a781-55c07845398b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected model: numind/NuExtract-2.0-2B\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('NuExtract-2.0-2B')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "model_id = \"numind/NuExtract-2.0-2B\"\n",
        "print(f\"Selected model: {model_id}\")\n",
        "\n",
        "pt_model_id = model_id\n",
        "model_dir = Path(model_id.split(\"/\")[-1])\n",
        "\n",
        "model_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CfFeKByCwkD",
        "outputId": "1c5f669d-9f11-4caf-bfad-717d26508336",
        "colab": {
          "referenced_widgets": [
            "6aa3ffc5e9d04d59a5eec90635b87aad",
            "f1d5ce0dfec94381b39f684f1e8af029",
            "fdbcc062c85e49a9be2fcb31c6acd44e",
            "f67a5f9874944506bd2081e34f427800",
            "fa73471c77bc44b2a47b3daeba9d1333",
            "e11883a2fb1a4fa189dded138dff4f3d",
            "aaccdfc7bd7b4b07a45288f3e9598b03",
            "3dd9d279f8d745d7ba0e4d6ee6471c9c",
            "87116d97d67d4cb28f63ccaacbc4d839",
            "af24138fab3a48dca28c84bf7dcd1446",
            "c79f3e9b845e4121b97015f709205f89",
            "da27c28d0824465bad0325cae6bf7c57",
            "f29c6385525d47a4a758d89ee0d2a501",
            "bdfbf9f70d4342199c502aecdff836d6",
            "3cd6d9b97dcb492489b4158ee07a0646",
            "7faad156adf94ae7b277f9e3ee507294",
            "2483fbbd782c438b9e15cc2f6fe29aad",
            "365fc520b57a42a380b5d6535e7770e3",
            "b251182e17c74828944b54aa342aec9b",
            "15d1eb268c7b4e88a67bd215ac322812",
            "7bfbac81298c4ff7bbc99acd9f5c55df",
            "f3d5e23870da430190d4a3741483c5eb",
            "a5410aad534343179cfd17c3bf45dba1",
            "59d352a99ab444358a5a0532a6627610",
            "a653a2e966a34d079c4444a583d05ab4",
            "f7f245d677e941e581a6de101b87a616",
            "6fa3c6929c3e49a7bb879ebd472b09f7",
            "fba1dff0499d44d3a17530c6fac4db6d",
            "4e7499c920364f1a882023713e61e6a7",
            "77064a48a37b4e6997bec8bcf1ad6c1d",
            "ad4baacb5b634fa4879dd937cc5aae01",
            "61f90bf1d6bf4abf9a098f73547d16cf",
            "2057e95e201f4f66a30a0a1fc1a80466",
            "da9a9d8f8ce84dacac53975c975513c8",
            "922273f8ea0a408eb6827add36c2cf2f",
            "672bd4da64ea4f85adad0958b6270a37",
            "efccccc14a1b48ba822bdf5af5b74d8f",
            "0189b0b367ec4695a3900d1cb5391a5d",
            "81967d33f15d415ebb16f119c76b239b",
            "8e57704326e54d34ae11a1ed7e7398dd",
            "82d183d64e1649d3849701a33d48f504",
            "490315a3b89747d29441942a70dea755",
            "384274410d294274a1bd6ba0375255ba",
            "8833642984574980be92e8caf1da31ed",
            "2d006644ee83405c80e05f5c0359ee48",
            "216a303ae8bd41c597418af24117e2f3",
            "85e1aed060454719ac359c5861f1ce71",
            "0b1a777a4e4448fd928587ff3ae6266e",
            "853b5a6a32264ceeb03db08a66cd080d",
            "95798de6e0244bbeaae99c953c6c5ce3",
            "6326d8ebde7643db8a08794f0112c9f2",
            "aa97f573971d40528a015a566b06a480",
            "441b695595954dcda29c7364be04466f",
            "70f24dfc09bc48cd98cd87b174283d92",
            "4ac07f73fd5749268551b188164c1213",
            "06f5751387ef4ea9857a0512b0a13113",
            "dfeb95d7d4ea49a0839044ac1af81088",
            "d3913884ce704a509f6f4bc4b618caab",
            "62b42b89c3b64243a193bbbc34194564",
            "7cba864ef0f642398c9793e7dd064422",
            "cb8d61ee87484389bda1d31fa4d8f623",
            "3285a0af15294292876628dec1daf19f",
            "dd68b34dfe84402a86a5c9e90e903ef4",
            "27f937415a42406eae8e6982e7630363",
            "fcbf475bccb44aff959afe2af3fa1288",
            "ecb7b16ce3a3459194fe179482a4ae87",
            "548389350d2546fd93b3f0247d7901e1",
            "255997449e854646b82dacab97a50cd1",
            "49483abcb3554a3f87cc49fc057ec094",
            "c7163f14ca394841ab44461a00942161",
            "0db362f73e384a43bdb349829c4eb006",
            "6d311028332d49b8a7cefe7321ddfe59",
            "ff410344beba415ebd555884637af462",
            "d763d9df1ffb4b889edf6c488ca8a391",
            "bf15a3a09af647a191cb46e6dc9b9e6e",
            "2a37aba4adf7468e8a032d27c7a9e66a",
            "a28d03c2e6fd42a9841093978d535987",
            "5c713d689c424a169854b49c3d152633",
            "68371fb51985466788e67289389aa243",
            "0e1bbac0c7034cd9a62a7c20fe99ac56",
            "3ad8b26423e24e21a1156e2c45383ea6",
            "fc9f0004f2b2430b94c578eb2efc92ab",
            "b93e50b4f33845f392f9ba60e50b18f8",
            "886732fc040f48dfb592993d9e4622fc",
            "d034b11b46604500ba3ba27f50935c7b",
            "e1d4feb73d9a42d887ccba49e19ae925",
            "14cceb90a6a342ad81b255950c88fd16",
            "479ebd0271bf4957b48515cb8a290b50",
            "ecfe4548c1514da19f30e70b96a87ea3",
            "ed34d0ca1a4b4c2dad9f777bcd2bc844",
            "0ccd578d73884fd08d76b4374e644ae0",
            "2f41aded4663419195371325dd1ceb9a",
            "0197e9be504b410caf71941b9aa12bc1",
            "ae6ed1c0c5144bf5b3c5ca6f962a9d7d",
            "5c523eaa79984a15929f724ab65c5132",
            "a88a25b2266c4e07b8e5fb487aaeed35",
            "721ce9cdd4ed4ebaa82f15df4098ce39",
            "08983da981f54b0fa0011e0516f0964f",
            "deed4bacf7584fbabf6e4d8bfb74dc36",
            "354854245656433ab57fb3c5620be3ee",
            "7f13751bc4554e9d9523f86de569efc6",
            "4afc429e65414208a9af21ef3175f664",
            "40bdb019619245a79ebbfea2d8469af5",
            "a3658c1e7e354840b165e0157c787f95",
            "05f5afe0cbfd448f8031d9291118219b",
            "0d03abad011e43289e36b8b80f05cd37",
            "3596b757a5d34666b0735e88e19f93be",
            "c7ea9f44268846f88b1894f4e65d92f3",
            "20fe7a83065b4ce6a760be72493b3171",
            "4149870f2535470a83e97ee499c1b918",
            "54ef5029c8b347fd9a210606e1574aca",
            "c4282913f8114b0f84f505b1b50d9a18",
            "e87eb33cf91e4eb8b4b64405bb027f10",
            "0ee68bd1ef2c45dba34ac30b64bc0688",
            "6bddb685fcc341fcb7876cca8eeec487",
            "532eb45084e44d04a682f40c49539e9c",
            "eef9a073c5f34e08a37ffbaa3dcfb7c3",
            "a9873a4562034d4bad869b13efc7996e",
            "7a199ac811014233991ebfa3313150da",
            "39201038ee4f44dcb428c75fc5062fc9",
            "1171a7f656ca41099aeb4d92687d9f9b",
            "e553016de6074f95b486d55d82293311",
            "1f2be5267f5149ac871680395f1e8ec0",
            "9994331e946549b49d1abd1dd56f30ce",
            "d12ef0d569a14a9c93bd13a9e52cbcbf"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âŒ› numind/NuExtract-2.0-2B conversion started. Be patient, it may takes some time.\n",
            "âŒ› Load Original model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6aa3ffc5e9d04d59a5eec90635b87aad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/4.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da27c28d0824465bad0325cae6bf7c57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5410aad534343179cfd17c3bf45dba1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/573 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da9a9d8f8ce84dacac53975c975513c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d006644ee83405c80e05f5c0359ee48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06f5751387ef4ea9857a0512b0a13113"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "548389350d2546fd93b3f0247d7901e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c713d689c424a169854b49c3d152633"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/392 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ecfe4548c1514da19f30e70b96a87ea3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "354854245656433ab57fb3c5620be3ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54ef5029c8b347fd9a210606e1574aca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Original model successfully loaded\n",
            "âŒ› Convert Input embedding model\n",
            "WARNING:nncf:NNCF provides best results with torch==2.7.*, while current torch version is 2.6.0+cu124. If you encounter issues, consider switching to torch==2.7.*\n",
            "âœ… Input embedding model successfully converted\n",
            "âŒ› Convert Language model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:4773: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:476: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if sequence_length != 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Language model successfully converted\n",
            "âŒ› Weights compression with int4_asym mode started\n",
            "INFO:nncf:Statistics of the bitwidth distribution:\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”‘\n",
            "â”‚ Weight compression mode   â”‚ % all parameters (layers)   â”‚ % ratio-defining parameters (layers)   â”‚\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¿â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¿â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¥\n",
            "â”‚ int8_asym                 â”‚ 15% (1 / 197)               â”‚ 0% (0 / 196)                           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ int4_asym                 â”‚ 85% (196 / 197)             â”‚ 100% (196 / 196)                       â”‚\n",
            "â”•â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”™\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e553016de6074f95b486d55d82293311"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Weights compression finished\n",
            "âŒ› Convert Image embedding model\n",
            "âŒ› Weights compression with int4_asym mode started\n",
            "INFO:nncf:Statistics of the bitwidth distribution:\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”‘\n",
            "â”‚ Weight compression mode   â”‚ % all parameters (layers)   â”‚ % ratio-defining parameters (layers)   â”‚\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¿â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¿â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¥\n",
            "â”‚ int8_asym                 â”‚ 1% (1 / 130)                â”‚ 0% (0 / 129)                           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ int4_asym                 â”‚ 99% (129 / 130)             â”‚ 100% (129 / 129)                       â”‚\n",
            "â”•â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”™\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9994331e946549b49d1abd1dd56f30ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Weights compression finished\n",
            "âœ… Image embedding model successfully converted\n",
            "âœ… numind/NuExtract-2.0-2B model conversion finished. You can find results in NuExtract-2.0-2B\n"
          ]
        }
      ],
      "source": [
        "compression_configuration = {\n",
        "    \"mode\": nncf.CompressWeightsMode.INT4_ASYM,\n",
        "    \"group_size\": 128,\n",
        "    \"ratio\": 1.0,\n",
        "}\n",
        "\n",
        "convert_qwen2vl_model(pt_model_id, model_dir, compression_configuration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoT_8kDICwkE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bbdf98d-c2b8-4e6b-da5a-6d67dd1e3e37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
            "/tmp/ipython-input-6-871166069.py:57: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
            "  t, h, w = grid_thw\n"
          ]
        }
      ],
      "source": [
        "# Patch reshape model\n",
        "class Qwen2ReshapePatches(nn.Module):\n",
        "    def __init__(self, temporal_patch_size: int = 2, merge_size: int = 2, patch_size: int = 14):\n",
        "        super().__init__()\n",
        "        self.temporal_patch_size = temporal_patch_size\n",
        "        self.merge_size = merge_size\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def forward(self, patches, repetition_factor=1):\n",
        "        patches = patches.repeat(repetition_factor, 1, 1, 1)\n",
        "        channel = patches.shape[1]\n",
        "        grid_t = patches.shape[0] // self.temporal_patch_size\n",
        "        resized_height = patches.shape[2]\n",
        "        resized_width = patches.shape[3]\n",
        "        grid_h = resized_height // self.patch_size\n",
        "        grid_w = resized_width // self.patch_size\n",
        "\n",
        "        patches = patches.reshape(\n",
        "            grid_t,\n",
        "            self.temporal_patch_size,\n",
        "            channel,\n",
        "            grid_h // self.merge_size,\n",
        "            self.merge_size,\n",
        "            self.patch_size,\n",
        "            grid_w // self.merge_size,\n",
        "            self.merge_size,\n",
        "            self.patch_size,\n",
        "        )\n",
        "        patches = patches.permute(0, 3, 6, 4, 7, 2, 1, 5, 8)\n",
        "        flatten_patches = patches.reshape(\n",
        "            grid_t * grid_h * grid_w,\n",
        "            channel * self.temporal_patch_size * self.patch_size * self.patch_size\n",
        "        )\n",
        "        return flatten_patches\n",
        "\n",
        "patch_reshape_model = Qwen2ReshapePatches()\n",
        "\n",
        "ov_model = ov.convert_model(\n",
        "    patch_reshape_model,\n",
        "    example_input={\n",
        "        \"patches\": torch.ones((1, 3, 1372, 2044), dtype=torch.float32),\n",
        "        \"repetition_factor\": torch.tensor(2),\n",
        "    }\n",
        ")\n",
        "ov.save_model(ov_model, model_dir / \"openvino_patch_reshape_model.xml\")\n",
        "\n",
        "# Rotary embedding\n",
        "config = AutoConfig.from_pretrained(model_id)\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, embed_dim, spatial_merge_size):\n",
        "        super().__init__()\n",
        "        self._rotary_pos_emb = VisionRotaryEmbedding(embed_dim)\n",
        "        self.spatial_merge_size = spatial_merge_size\n",
        "\n",
        "    def forward(self, grid_thw):\n",
        "        t, h, w = grid_thw\n",
        "        pos_ids = []\n",
        "\n",
        "        hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)\n",
        "        hpos_ids = hpos_ids.reshape(\n",
        "            h // self.spatial_merge_size,\n",
        "            self.spatial_merge_size,\n",
        "            w // self.spatial_merge_size,\n",
        "            self.spatial_merge_size,\n",
        "        )\n",
        "        hpos_ids = hpos_ids.permute(0, 2, 1, 3).flatten()\n",
        "\n",
        "        wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)\n",
        "        wpos_ids = wpos_ids.reshape(\n",
        "            h // self.spatial_merge_size,\n",
        "            self.spatial_merge_size,\n",
        "            w // self.spatial_merge_size,\n",
        "            self.spatial_merge_size,\n",
        "        )\n",
        "        wpos_ids = wpos_ids.permute(0, 2, 1, 3).flatten()\n",
        "\n",
        "        pos_ids.append(torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n",
        "        pos_ids = torch.cat(pos_ids, dim=0)\n",
        "\n",
        "        max_grid_size = grid_thw.max()\n",
        "        rotary_pos_emb_full = self._rotary_pos_emb(max_grid_size)\n",
        "        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n",
        "\n",
        "        return rotary_pos_emb\n",
        "\n",
        "vision_rotary_embedding = RotaryEmbedding(\n",
        "    config.vision_config.embed_dim // config.vision_config.num_heads // 2,\n",
        "    config.vision_config.spatial_merge_size\n",
        ")\n",
        "\n",
        "vision_embedding_ov = ov.convert_model(\n",
        "    vision_rotary_embedding,\n",
        "    example_input={\n",
        "        \"grid_thw\": torch.tensor([1, 98, 146]),\n",
        "    }\n",
        ")\n",
        "ov.save_model(vision_embedding_ov, model_dir / \"openvino_rotary_embeddings_model.xml\")\n",
        "\n",
        "# Multimodal merge module\n",
        "class MergeMultiModalInputs(nn.Module):\n",
        "    def __init__(self, image_token_index=151655):\n",
        "        super().__init__()\n",
        "        self.image_token_index = image_token_index\n",
        "\n",
        "    def forward(self, vision_embeds, inputs_embeds, input_ids):\n",
        "        image_features = vision_embeds\n",
        "        inputs_embeds = inputs_embeds\n",
        "        special_image_mask = (input_ids == self.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n",
        "        final_embedding = inputs_embeds.masked_scatter(special_image_mask, image_features)\n",
        "        return {\"inputs_embeds\": final_embedding}\n",
        "\n",
        "torch_model_merge = MergeMultiModalInputs()\n",
        "\n",
        "ov_model_merge = ov.convert_model(\n",
        "    torch_model_merge,\n",
        "    example_input={\n",
        "        \"vision_embeds\": torch.randn((3577, 1536), dtype=torch.float32),\n",
        "        \"inputs_embeds\": torch.randn((1, 3602, 1536), dtype=torch.float32),\n",
        "        \"input_ids\": torch.randint(0, 151656, (1, 3602), dtype=torch.long),\n",
        "    }\n",
        ")\n",
        "ov.save_model(ov_model_merge, model_dir / \"openvino_multimodal_merge_model.xml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0eHinHFCwkE"
      },
      "source": [
        "### 1.2 Load openvino models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSMGMPuSCwkE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dce381f0-ae36-4da6-c6c3-a600ce6ef5ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if all models are converted\n",
            "All models are converted. You can find results in NuExtract-2.0-2B\n"
          ]
        }
      ],
      "source": [
        "# Model filenames\n",
        "LANGUAGE_MODEL_NAME = \"openvino_language_model.xml\"\n",
        "IMAGE_EMBEDDING_NAME = \"openvino_vision_embeddings_model.xml\"\n",
        "IMAGE_EMBEDDING_MERGER_NAME = \"openvino_vision_embeddings_merger_model.xml\"\n",
        "TEXT_EMBEDDING_NAME = \"openvino_text_embeddings_model.xml\"\n",
        "ROTARY_EMBEDDING_NAME = \"openvino_rotary_embeddings_model.xml\"\n",
        "PATCH_RESHAPE_NAME = \"openvino_patch_reshape_model.xml\"\n",
        "\n",
        "# Load OpenVINO models\n",
        "core = ov.Core()\n",
        "model_path = model_dir\n",
        "\n",
        "language_model = core.read_model(model_path / LANGUAGE_MODEL_NAME)\n",
        "compiled_language_model = core.compile_model(language_model, \"CPU\")\n",
        "request = compiled_language_model.create_infer_request()\n",
        "\n",
        "image_embedding = core.compile_model(model_path / IMAGE_EMBEDDING_NAME, \"CPU\")\n",
        "image_embedding_merger = core.compile_model(model_path / IMAGE_EMBEDDING_MERGER_NAME, \"CPU\")\n",
        "text_embedding = core.compile_model(model_path / TEXT_EMBEDDING_NAME, \"CPU\")\n",
        "rotary_embedding = core.compile_model(model_path / ROTARY_EMBEDDING_NAME, \"CPU\")\n",
        "patch_reshape = core.compile_model(model_path / PATCH_RESHAPE_NAME, \"CPU\")\n",
        "\n",
        "# Check if all required model files exist\n",
        "print(\"Check if all models are converted\")\n",
        "\n",
        "language_model_path = model_path / LANGUAGE_MODEL_NAME\n",
        "image_embed_path = model_path / IMAGE_EMBEDDING_NAME\n",
        "image_merger_path = model_path / IMAGE_EMBEDDING_MERGER_NAME\n",
        "text_embed_path = model_path / TEXT_EMBEDDING_NAME\n",
        "rotary_embed_path = model_path / ROTARY_EMBEDDING_NAME\n",
        "patch_reshape_path = model_path / PATCH_RESHAPE_NAME\n",
        "\n",
        "if all([\n",
        "    language_model_path.exists(),\n",
        "    image_embed_path.exists(),\n",
        "    image_merger_path.exists(),\n",
        "    text_embed_path.exists(),\n",
        "    rotary_embed_path.exists(),\n",
        "    patch_reshape_path.exists()\n",
        "]):\n",
        "    print(f\"All models are converted. You can find results in {model_path}\")\n",
        "else:\n",
        "    print(\"Not all models are converted. Please check the conversion process.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5wJns-jCwkF"
      },
      "source": [
        "### 1.2 Copy assets to the assets folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mgxg8DqCwkF"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "assets_dir = model_dir / \"assets\"\n",
        "assets_dir.mkdir(exist_ok=True)\n",
        "\n",
        "for file in model_dir.glob(\"*.json\"):\n",
        "    shutil.copy(file, assets_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MTQhNpMCwkF"
      },
      "source": [
        "## Import and Save Qwen2VL in Spark NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's install and setup Spark NLP in Google Colab\n",
        "- This part is pretty easy via our simple script"
      ],
      "metadata": {
        "id": "q_JQj1vzb4SN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restart the session to run the below code in colab as it needs more RAM !!**"
      ],
      "metadata": {
        "id": "yTmRvG3Abx5g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV2EN6ekCwkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d23d4bf0-3e27-4b9b-dd2f-6f3e89c073d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing PySpark 3.4.4 and Spark NLP 6.0.5\n",
            "setup Colab for PySpark 3.4.4 and Spark NLP 6.0.5\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m311.4/311.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m718.9/718.9 kB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.2 requires pyspark[connect]~=3.5.1, but you have pyspark 3.4.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WBSbzOuCwkF"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D06jtr5pb6N3",
        "outputId": "b11be81a-5cc9-44a0-c492-660e58ed63a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version:  6.0.5\n",
            "Apache Spark version:  3.4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRHtJMqdLRPF"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `AlbertEmbeddings` which allows us to load the ONNX model\n",
        "- Most params will be set automatically. They can also be set later after loading the model in `AlbertEmbeddings` during runtime, so don't worry about setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the exported model. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- `setStorageRef` is very important. When you are training a task like NER or any Text Classification, we use this reference to bound the trained model to this specific embeddings so you won't load a different embeddings by mistake and see terrible results ğŸ˜Š\n",
        "- It's up to you what you put in `setStorageRef` but it cannot be changed later on. We usually use the name of the model to be clear, but you can get creative if you want!\n",
        "- The `dimension` param is is purely cosmetic and won't change anything. It's mostly for you to know later via `.getDimension` what is the dimension of your model. So set this accordingly.\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "model_id = \"numind/NuExtract-2.0-2B\"\n",
        "model_path = Path(model_id.split(\"/\")[-1])"
      ],
      "metadata": {
        "id": "vF0uS9yNNF2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlSvV2hxCwkF"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import Qwen2VLTransformer\n",
        "\n",
        "imageClassifier = Qwen2VLTransformer.loadSavedModel(str(model_path),spark) \\\n",
        "            .setInputCols(\"image_assembler\") \\\n",
        "            .setOutputCol(\"answer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2ywuVYLLRPF"
      },
      "source": [
        "Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lIIKYBsCwkF"
      },
      "outputs": [],
      "source": [
        "imageClassifier.write().overwrite().save(f\"{model_id.replace('-','_')}_spark_nlp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lnn2nTC1LRPH"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny ALBERT model ğŸ˜Š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQHgku2PCwkF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "\n",
        "url1 = \"https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/d5fbbd1a-d484-415c-88cb-9986625b7b11\"\n",
        "url2 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "\n",
        "Path(\"images\").mkdir(exist_ok=True)\n",
        "\n",
        "!wget -q -O images/image1.jpg {url1}\n",
        "!wget -q -O images/image2.jpg {url2}\n",
        "\n",
        "images_path = \"file://\" + os.getcwd() + \"/images/\"\n",
        "image_df = spark.read.format(\"image\").load(path=images_path)\n",
        "\n",
        "prompt = (\n",
        "    \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
        "    \"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\"\n",
        "    \"Describe this image.<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        ")\n",
        "test_df = image_df.withColumn(\"text\", lit(prompt))\n",
        "\n",
        "image_assembler = ImageAssembler() \\\n",
        "    .setInputCol(\"image\") \\\n",
        "    .setOutputCol(\"image_assembler\")\n",
        "\n",
        "imageClassifier = Qwen2VLTransformer.load(f\"{model_id.replace('-', '_')}_spark_nlp\") \\\n",
        "    .setMaxOutputLength(50) \\\n",
        "    .setInputCols(\"image_assembler\") \\\n",
        "    .setOutputCol(\"answer\")\n",
        "\n",
        "pipeline = Pipeline(stages=[image_assembler, imageClassifier])\n",
        "model = pipeline.fit(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you encounter an error at this step, try restarting the runtime,. itâ€™s likely due to low RAM."
      ],
      "metadata": {
        "id": "aMhkCvX4jZIQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMdTERCOCwkF",
        "outputId": "8c8f2125-9860-4160-d1fe-d16e290dc205",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Annotation(document, 0, 234, The image shows a cat lying inside a cardboard box. The cat has a relaxed posture, with its paws tucked under its body and its head resting on its front paws. The box is positioned on a light-colored carpet, and the background includes, Map(), [])]\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.base import LightPipeline\n",
        "\n",
        "image_path = os.path.join(os.getcwd(), \"images\", \"image1.jpg\")\n",
        "\n",
        "# Run inference with LightPipeline (for fast, local inference on small inputs)\n",
        "prompt = (\n",
        "    \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
        "    \"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\"\n",
        "    \"Describe this image.<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        ")\n",
        "\n",
        "light_pipeline = LightPipeline(model)\n",
        "annotations_result = light_pipeline.fullAnnotateImage(image_path, prompt)\n",
        "\n",
        "for result in annotations_result:\n",
        "    print(result[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW6Qc0TcLRPH"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of Qwen2VL models from HuggingFace ğŸ¤— in Spark NLP ğŸš€\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, you can zip the model and use it locally using `.load()`"
      ],
      "metadata": {
        "id": "New2FPzyhg7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "NEW_MODEL_NAME = \"nuextract_2.0_2B\"\n",
        "\n",
        "MODEL_PATH_ZIP = shutil.make_archive(\n",
        "    base_name=NEW_MODEL_NAME,\n",
        "    format='zip',\n",
        "    root_dir='/content/numind/NuExtract_2.0_2B_spark_nlp'\n",
        ")"
      ],
      "metadata": {
        "id": "ajSmpc2th7ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -l nuextract_2.0_2B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYK3Zu5Nib0J",
        "outputId": "73ef8302-b6b3-4e13-a4c6-d4d077a36562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  nuextract_2.0_2B.zip\n",
            "  Length      Date    Time    Name\n",
            "---------  ---------- -----   ----\n",
            "        0  2025-07-18 21:37   fields/\n",
            "        0  2025-07-18 21:37   metadata/\n",
            "    23572  2025-07-18 21:38   .openvino_vision_embeddings_model.xml.crc\n",
            "  3015902  2025-07-18 21:38   openvino_vision_embeddings_model.xml\n",
            "  3647056  2025-07-18 21:39   .openvino_text_embeddings_model.xml.crc\n",
            "  2747308  2025-07-18 21:38   .openvino_vision_embeddings_merger_model.xml.crc\n",
            "  7176132  2025-07-18 21:38   .openvino_language_model.xml.crc\n",
            "466821888  2025-07-18 21:39   openvino_text_embeddings_model.xml\n",
            "      200  2025-07-18 21:37   .openvino_patch_reshape_model.xml.crc\n",
            "    30738  2025-07-18 21:39   openvino_rotary_embeddings_model.xml\n",
            "      252  2025-07-18 21:39   .openvino_rotary_embeddings_model.xml.crc\n",
            "    24299  2025-07-18 21:37   openvino_patch_reshape_model.xml\n",
            "918543427  2025-07-18 21:38   openvino_language_model.xml\n",
            "    10423  2025-07-18 21:39   openvino_multimodal_merge_model.xml\n",
            "351653924  2025-07-18 21:38   openvino_vision_embeddings_merger_model.xml\n",
            "       92  2025-07-18 21:39   .openvino_multimodal_merge_model.xml.crc\n",
            "      865  2025-07-18 21:37   metadata/part-00000\n",
            "       16  2025-07-18 21:37   metadata/.part-00000.crc\n",
            "        0  2025-07-18 21:37   metadata/_SUCCESS\n",
            "        8  2025-07-18 21:37   metadata/._SUCCESS.crc\n",
            "        0  2025-07-18 21:37   fields/addedTokens/\n",
            "        0  2025-07-18 21:37   fields/generationConfig/\n",
            "        0  2025-07-18 21:37   fields/merges/\n",
            "        0  2025-07-18 21:37   fields/vocabulary/\n",
            "       95  2025-07-18 21:37   fields/generationConfig/part-00000\n",
            "       12  2025-07-18 21:37   fields/generationConfig/.part-00001.crc\n",
            "      466  2025-07-18 21:37   fields/generationConfig/part-00001\n",
            "       12  2025-07-18 21:37   fields/generationConfig/.part-00000.crc\n",
            "        0  2025-07-18 21:37   fields/generationConfig/_SUCCESS\n",
            "        8  2025-07-18 21:37   fields/generationConfig/._SUCCESS.crc\n",
            "      511  2025-07-18 21:37   fields/addedTokens/part-00000\n",
            "       12  2025-07-18 21:37   fields/addedTokens/.part-00001.crc\n",
            "      494  2025-07-18 21:37   fields/addedTokens/part-00001\n",
            "       12  2025-07-18 21:37   fields/addedTokens/.part-00000.crc\n",
            "        0  2025-07-18 21:37   fields/addedTokens/_SUCCESS\n",
            "        8  2025-07-18 21:37   fields/addedTokens/._SUCCESS.crc\n",
            "  3475222  2025-07-18 21:37   fields/vocabulary/part-00000\n",
            "    27176  2025-07-18 21:37   fields/vocabulary/.part-00001.crc\n",
            "  3476995  2025-07-18 21:37   fields/vocabulary/part-00001\n",
            "    27160  2025-07-18 21:37   fields/vocabulary/.part-00000.crc\n",
            "        0  2025-07-18 21:37   fields/vocabulary/_SUCCESS\n",
            "        8  2025-07-18 21:37   fields/vocabulary/._SUCCESS.crc\n",
            "  4152368  2025-07-18 21:37   fields/merges/part-00000\n",
            "    32448  2025-07-18 21:37   fields/merges/.part-00001.crc\n",
            "  4152243  2025-07-18 21:37   fields/merges/part-00001\n",
            "    32452  2025-07-18 21:37   fields/merges/.part-00000.crc\n",
            "        0  2025-07-18 21:37   fields/merges/_SUCCESS\n",
            "        8  2025-07-18 21:37   fields/merges/._SUCCESS.crc\n",
            "---------                     -------\n",
            "1769073812                     48 files\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}