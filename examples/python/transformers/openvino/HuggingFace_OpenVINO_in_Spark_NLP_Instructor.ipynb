{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjQoSZTMUH_5"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/openvino/HuggingFace_OpenVINO_in_Spark_NLP_Instructor.ipynb)\n",
        "\n",
        "# Import OpenVINO Instructor models from HuggingFace ğŸ¤— into Spark NLP ğŸš€\n",
        "\n",
        "This notebook provides a detailed walkthrough on optimizing and exporting Instructor models from HuggingFace for use in Spark NLP, leveraging the various tools provided in the [Intel OpenVINO toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html) ecosystem.\n",
        "\n",
        "Let's keep in mind a few things before we start ğŸ˜Š\n",
        "\n",
        "- OpenVINO support was introduced in  `Spark NLP 5.4.0`, enabling high performance inference for models. Please make sure you have upgraded to the latest Spark NLP release.\n",
        "- You can import models for Instructor from Instructor and they have to be in `Fill Mask` category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an8-RiT0UH_8"
      },
      "source": [
        "## Export and Save HuggingFace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCNlrbMWUH_8"
      },
      "source": [
        "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.31.0`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XezgP-k2UH_8",
        "outputId": "ed4ff799-4e8e-4ce9-d860-73aa170e033b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.1.1\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m453.7/453.7 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install -q --upgrade \"transformers[onnx]===4.39.3\" optimum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqoI5yIUUH_9"
      },
      "source": [
        "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to import and export ONNX models with `from_pretrained` and `save_pretrained`.\n",
        "- We'll use the [hkunlp/instructor-base](https://huggingface.co/hkunlp/instructor-base) model from HuggingFace as an example and export it with the `optimum-cli`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwylSoFOUH_9"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"hkunlp/instructor-base\"\n",
        "EXPORT_PATH = f\"export_onnx/{MODEL_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTr9oYDwUH_-",
        "outputId": "ec553c89-b0e0-4c8c-cd44-5d447f4d0862"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-26 19:09:49.332036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-26 19:09:49.358009: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-26 19:09:49.365282: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-26 19:09:50.750590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Framework not specified. Using pt to export the model.\n",
            "modules.json: 100% 461/461 [00:00<00:00, 2.19MB/s]\n",
            "config_sentence_transformers.json: 100% 122/122 [00:00<00:00, 757kB/s]\n",
            "README.md: 100% 66.2k/66.2k [00:00<00:00, 295kB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 286kB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 1.55k/1.55k [00:00<00:00, 7.14MB/s]\n",
            "pytorch_model.bin: 100% 439M/439M [00:18<00:00, 24.2MB/s]\n",
            "tokenizer_config.json: 100% 2.43k/2.43k [00:00<00:00, 13.6MB/s]\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 192MB/s]\n",
            "tokenizer.json: 100% 2.42M/2.42M [00:00<00:00, 10.8MB/s]\n",
            "special_tokens_map.json: 100% 2.20k/2.20k [00:00<00:00, 10.2MB/s]\n",
            "1_Pooling/config.json: 100% 270/270 [00:00<00:00, 1.29MB/s]\n",
            "2_Dense/config.json: 100% 115/115 [00:00<00:00, 525kB/s]\n",
            "pytorch_model.bin: 100% 2.36M/2.36M [00:00<00:00, 5.85MB/s]\n",
            "Using the export variant default. Available variants are:\n",
            "    - default: The default ONNX variant.\n",
            "\n",
            "***** Exporting submodel 1/1: SentenceTransformer *****\n",
            "Using framework PyTorch: 2.4.1+cu121\n",
            "Overriding 1 configuration item(s)\n",
            "\t- use_cache -> False\n",
            "Post-processing the exported models...\n",
            "Deduplicating shared (tied) weights...\n",
            "Could not find ONNX initializer for torch parameter 0.auto_model.encoder.embed_tokens.weight. 0.auto_model.encoder.embed_tokens.weight will not be checked for deduplication.\n",
            "Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n",
            "\t0.auto_model.encoder.embed_tokens.weight: set() --> ignored (may be a parameter from a part of the model not exported)\n",
            "\t0.auto_model.shared.weight: {'0.auto_model.shared.weight'}\n",
            "\n",
            "Validating ONNX model export_onnx/hkunlp/instructor-base/model.onnx...\n",
            "\t-[âœ“] ONNX model output names match reference model (token_embeddings, sentence_embedding)\n",
            "\t- Validating ONNX Model output \"token_embeddings\":\n",
            "\t\t-[âœ“] (2, 16, 768) matches (2, 16, 768)\n",
            "\t\t-[âœ“] all values close (atol: 1e-05)\n",
            "\t- Validating ONNX Model output \"sentence_embedding\":\n",
            "\t\t-[âœ“] (2, 768) matches (2, 768)\n",
            "\t\t-[âœ“] all values close (atol: 1e-05)\n",
            "The ONNX export succeeded and the exported model was saved at: export_onnx/hkunlp/instructor-base\n"
          ]
        }
      ],
      "source": [
        "! optimum-cli export onnx --model {MODEL_NAME} {EXPORT_PATH} --task feature-extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar_o_tJIUH_-"
      },
      "outputs": [],
      "source": [
        "! mkdir -p {EXPORT_PATH}/assets\n",
        "! mv -t {EXPORT_PATH}/assets  {EXPORT_PATH}/*.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6GW8l2fUH_-"
      },
      "source": [
        "Let's have a look inside these two directories and see what we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WYraOCfUH_-",
        "outputId": "576734eb-65f3-47a3-9992-18f98d38dcad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 433156\n",
            "drwxr-xr-x 2 root root      4096 Sep 26 19:10 assets\n",
            "-rw-r--r-- 1 root root      1545 Sep 26 19:10 config.json\n",
            "-rw-r--r-- 1 root root 441088928 Sep 26 19:10 model.onnx\n",
            "-rw-r--r-- 1 root root      2543 Sep 26 19:10 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root     20937 Sep 26 19:10 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root   2422456 Sep 26 19:10 tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukZxhGpWUH_-",
        "outputId": "748d67b2-4e6b-491d-db47-44c972afbf0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 776\n",
            "-rw-r--r-- 1 root root 791656 Sep 26 19:10 spiece.model\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}/assets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade openvino==2024.3"
      ],
      "metadata": {
        "id": "KYDNW9mN26Gl",
        "outputId": "9081871e-5433-40d2-b2aa-4c88abb685f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import openvino as ov\n",
        "model = ov.convert_model(f\"{EXPORT_PATH}/model.onnx\")"
      ],
      "metadata": {
        "id": "HlMvFM8c236C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ov.save_model(model, 'openvino_model.xml')\n",
        "\n",
        "!rm -rf {EXPORT_PATH}/model.onnx\n",
        "!mv /content/openvino_model.bin {EXPORT_PATH}\n",
        "!mv /content/openvino_model.xml {EXPORT_PATH}"
      ],
      "metadata": {
        "id": "yT-k9VQX27oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr7NE5DBUH__"
      },
      "source": [
        "## Import and Save InstructorEmbeddings  in Spark NLP\n",
        "\n",
        "- Let's install and setup Spark NLP in Google Colab\n",
        "- This part is pretty easy via our simple script\n",
        "- Additionally, we need to upgrade Spark to version 3.4.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acU9SZq-UH__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "677c8578-f753-4649-836d-d5060b68957a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing PySpark 3.2.3 and Spark NLP 5.5.0\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 5.5.0\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m620.8/620.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyspark==3.4.1\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.7 (from pyspark==3.4.1)\n",
            "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285391 sha256=822c3c043c9afcc06ff9a0d1a7cf7608ec1f6b198f047d7d83c65a9ccfc664d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.5\n",
            "    Uninstalling py4j-0.10.9.5:\n",
            "      Successfully uninstalled py4j-0.10.9.5\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.2.3\n",
            "    Uninstalling pyspark-3.2.3:\n",
            "      Successfully uninstalled pyspark-3.2.3\n",
            "Successfully installed py4j-0.10.9.7 pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash\n",
        "! pip install -U pyspark==3.4.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRUJ0CtfUH__"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kQTKjcWUH__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "446166b8-e158-48a9-ea68-bed152a0d2c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apache Spark version: 3.4.1\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "# let's start Spark with Spark NLP\n",
        "spark = sparknlp.start()\n",
        "print(\"Apache Spark version: {}\".format(spark.version))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FIOCiZxUH__"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `InstructorEmbeddings ` which allows us to load the ONNX model\n",
        "- Most params will be set automatically. They can also be set later after loading the model in `InstructorEmbeddings ` during runtime, so don't worry about setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the exported model. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wJClaqyUH__"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "\n",
        "embedding = InstructorEmbeddings.loadSavedModel(\n",
        "     EXPORT_PATH,\n",
        "     spark\n",
        " )\\\n",
        "  .setInputCols([\"document\"])\\\n",
        "  .setOutputCol(\"instructor\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8cNjLgcUH__"
      },
      "source": [
        "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqhebAObUH__"
      },
      "outputs": [],
      "source": [
        "embedding.write().overwrite().save(\"./{}_spark_nlp\".format(EXPORT_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReTnXz5pUIAA"
      },
      "source": [
        "Awesome  ğŸ˜ !\n",
        "\n",
        "This is your ONNX InstructorEmbeddings  model from HuggingFace ğŸ¤—  loaded and saved by Spark NLP ğŸš€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRG-oxWnUIAA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "445de99e-8869-41ff-8e63-348cdcfdee10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 431604\n",
            "-rw-r--r-- 1 root root 441156367 Sep 26 19:16 instructor_onnx\n",
            "-rw-r--r-- 1 root root    791656 Sep 26 19:16 instructor_spp\n",
            "drwxr-xr-x 2 root root      4096 Sep 26 19:16 metadata\n"
          ]
        }
      ],
      "source": [
        "! ls -l {EXPORT_PATH}_spark_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxvpC-hSUIAA"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny InstructorEmbeddings  model ğŸ˜Š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eglLGKeJUIAA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2981d87-67af-4019-8ebe-760b55313e60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+\n",
            "|                text|            document|          instructor|\n",
            "+--------------------+--------------------+--------------------+\n",
            "|William Henry Gat...|[{document, 0, 12...|[{sentence_embedd...|\n",
            "+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "\n",
        "document_assembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "instructor_loaded = InstructorEmbeddings.load(f\"{EXPORT_PATH}_spark_nlp\")\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"instructor\")\\\n",
        "    .setInstruction(\"Encode This:\")\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document_assembler,\n",
        "        instructor_loaded\n",
        "  ])\n",
        "\n",
        "data = spark.createDataFrame([['William Henry Gates III (born October 28, 1955) is an American business magnate, software developer, investor,and philanthropist.']]).toDF(\"text\")\n",
        "model = pipeline.fit(data)\n",
        "result = model.transform(data)\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D65GZokYUIAA"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of InstructorEmbeddings  models from HuggingFace ğŸ¤— in Spark NLP ğŸš€\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}