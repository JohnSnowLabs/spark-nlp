{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/openvino/HuggingFace_OpenVINO_in_Spark_NLP_Gemma3ForMultiModal.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import OpenVINO Gemma3 models from HuggingFace ðŸ¤— into Spark NLP ðŸš€\n",
    "\n",
    "This notebook provides a detailed walkthrough on optimizing and importing Gemma3 models from HuggingFace for use in Spark NLP, with [Intel OpenVINO toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html). The focus is on converting the model to the OpenVINO format and applying precision optimizations (INT8 and INT4), to enhance the performance and efficiency on CPU platforms using [Optimum Intel](https://huggingface.co/docs/optimum/main/en/intel/inference).\n",
    "\n",
    "Let's keep in mind a few things before we start ðŸ˜Š\n",
    "\n",
    "- OpenVINO support was introduced in `Spark NLP 5.4.0`, enabling high performance CPU inference for models. So please make sure you have upgraded to the latest Spark NLP release.\n",
    "- Model quantization is a computationally expensive process, so it is recommended to use a runtime with more than 32GB memory for exporting the quantized model from HuggingFace.\n",
    "- You can import Gemma3 models via `Gemma3`. These models are usually under the `Text Generation` category and have `Gemma3` in their labels.\n",
    "- Reference: [Gemma3](https://huggingface.co/docs/transformers/model_doc/llama#transformers.Gemma3)\n",
    "- Some [example models](https://huggingface.co/models?search=Gemma3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Installation](#setup-and-installation)\n",
    "2. [Model Configuration](#model-configuration)\n",
    "3. [Model Loading and Preparation](#model-loading-and-preparation)\n",
    "4. [Model Conversion to OpenVINO](#model-conversion-to-openvino)\n",
    "5. [Model Quantization](#model-quantization)\n",
    "6. [Model Merger Implementation](#model-merger-implementation)\n",
    "7. [Testing OpenVINO Model](#7-testing-openvino-model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install all the required dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenVINO and NNCF for model optimization\n",
    "import platform\n",
    "\n",
    "%pip install -q \"torch>=2.1\" \"torchvision\" \"Pillow\" \"gradio>=4.36\" \"opencv-python\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install  -q -U \"openvino>=2025.0.0\" \"openvino-tokenizers>=2025.0.0\" \"nncf>=2.15.0\"\n",
    "%pip install -q \"git+https://github.com/huggingface/optimum-intel.git\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q \"git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    %pip install -q \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Configuration\n",
    "\n",
    "Configure the environment to disable tokenizer parallelism for better compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Disable tokenizer parallelism to avoid potential issues\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Configuration\n",
    "\n",
    "Set up the model ID and quantization parameters for the conversion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = [\n",
    "    \"google/gemma-3-4b-it\",\n",
    "    \"google/gemma-3-12b-it\",\n",
    "    \"google/gemma-3-12b-pt\",\n",
    "    \"google/gemma-3-4b-pt\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading and Preparation\n",
    "\n",
    "Load the model processor, configuration, and prepare the model for conversion to OpenVINO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "for model_id in model_ids:\n",
    "    output_dir = f\"./models/int4/{model_id}\"\n",
    "    # check if the model is already optimized\n",
    "    if not os.path.exists(\n",
    "        f\"{output_dir}/openvino_language_model.xml\"\n",
    "    ) and not os.path.exists(f\"{output_dir}/openvino_language_model.bin\"):\n",
    "        !optimum-cli export openvino --model {model_id} --weight-format int4 {output_dir}\n",
    "    else:\n",
    "        print(f\"Model {model_id} already optimized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Conversion to OpenVINO\n",
    "\n",
    "Define paths for the converted model components and implement conversion utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id in model_ids:\n",
    "    # change vision embed avg pool to opset1\n",
    "    # this is a workaround for the issue with the Gemma3 model\n",
    "    output_dir = f\"./models/int4/{model_id}\"\n",
    "    with open(f\"{output_dir}/openvino_vision_embeddings_model.xml\", \"r\") as f:\n",
    "        xml = f.read()\n",
    "    xml = xml.replace(\"opset14\", \"opset1\")\n",
    "    with open(f\"{output_dir}/openvino_vision_embeddings_model.xml\", \"w\") as f:\n",
    "        f.write(xml)\n",
    "\n",
    "    if not os.path.exists(f\"{output_dir}/assets\"):\n",
    "        output_dir = Path(output_dir)\n",
    "        assets_dir = output_dir / \"assets\"\n",
    "        assets_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # copy all the assets to the assets directory (json files, vocab files, etc.)\n",
    "        for file in output_dir.glob(\"*.json\"):\n",
    "            shutil.copy(file, assets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_torchscript_cache():\n",
    "    \"\"\"\n",
    "    Helper function for removing cached model representation to prevent memory leaks\n",
    "    during model conversion.\n",
    "    \"\"\"\n",
    "    torch._C._jit_clear_class_registry()\n",
    "    torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "    torch.jit._state._clear_class_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Merger Implementation\n",
    "\n",
    "Implement the model merger to combine text and image components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoConfig\n",
    "import openvino as ov\n",
    "import gc\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "class MergeMultiModalInputs(torch.nn.Module):\n",
    "    def __init__(self, image_token_index=config.image_token_index):\n",
    "        \"\"\"\n",
    "        Merge multimodal inputs with the image token index.\n",
    "        Args:\n",
    "            image_token_index (int): The token index for the image token.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_token_index = image_token_index\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        vision_embeds,\n",
    "        inputs_embeds,\n",
    "        input_ids,\n",
    "    ):\n",
    "        image_features = vision_embeds\n",
    "        inputs_embeds = inputs_embeds\n",
    "        special_image_mask = (\n",
    "            (input_ids == self.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n",
    "        )\n",
    "        # image_features = image_features.to(inputs_embeds.dtype)\n",
    "        final_embedding = inputs_embeds.masked_scatter(\n",
    "            special_image_mask, image_features\n",
    "        )\n",
    "\n",
    "        return {\"final_embedding\": final_embedding}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting model google/gemma-3-4b-it merger to OpenVINO format...\n",
      "Converting model google/gemma-3-12b-it merger to OpenVINO format...\n",
      "Converting model google/gemma-3-12b-pt merger to OpenVINO format...\n",
      "Converting model google/gemma-3-4b-pt merger to OpenVINO format...\n"
     ]
    }
   ],
   "source": [
    "for model_id in model_ids:\n",
    "    print(f\"Converting model {model_id} merger to OpenVINO format...\")\n",
    "    core = ov.Core()\n",
    "    output_dir = f\"./models/int4/{model_id}\"\n",
    "    model_merger_path = f\"{output_dir}/openvino_merger_model.xml\"\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    multimodal_merger = MergeMultiModalInputs(config.image_token_index)\n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(\n",
    "            multimodal_merger,\n",
    "            example_input={\n",
    "                \"input_ids\": torch.ones([2, 1198], dtype=torch.int64),\n",
    "                \"inputs_embeds\": torch.ones(\n",
    "                    [2, 1198, config.text_config.hidden_size], dtype=torch.float32\n",
    "                ),\n",
    "                \"vision_embeds\": torch.ones(\n",
    "                    [2, config.mm_tokens_per_image, config.text_config.hidden_size],\n",
    "                    dtype=torch.float32,\n",
    "                ),\n",
    "            },\n",
    "        )\n",
    "        ov.save_model(ov_model, model_merger_path)\n",
    "        del ov_model\n",
    "        cleanup_torchscript_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Testing OpenVINO Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "device = \"CPU\"\n",
    "\n",
    "# lets pick the first model\n",
    "model_id = model_ids[0]\n",
    "output_dir = f\"./models/int4/{model_id}\"\n",
    "output_dir = Path(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths for the exported models\n",
    "image_embed_path = output_dir / \"openvino_vision_embeddings_model.xml\"\n",
    "language_model_path = output_dir / \"openvino_language_model.xml\"\n",
    "text_embeddings_path = output_dir / \"openvino_text_embeddings_model.xml\"\n",
    "model_merger_path = output_dir / \"openvino_merger_model.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the models\n",
    "language_model = core.read_model(language_model_path)\n",
    "compiled_language_model = core.compile_model(language_model, \"AUTO\")\n",
    "\n",
    "image_embed_model = core.compile_model(image_embed_path, device)\n",
    "text_embeddings_model = core.compile_model(text_embeddings_path, device)\n",
    "multimodal_merger_model = core.compile_model(model_merger_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a detailed description of the image:\n",
      "\n",
      "**Overall Impression:**\n",
      "\n",
      "The image presents a wide, scenic view of the Statue of Liberty with the New York City skyline in the background. The scene is bathed in warm, golden sunlight, suggesting either early morning or late afternoon.\n",
      "\n",
      "**Foreground:**\n",
      "\n",
      "*   **Statue of Liberty:** The iconic statue dominates the foreground. It stands proudly on a small, rocky island (Liberty Island) with a patch of green grass and a few trees surrounding its base. The statue itself is a vibrant green color, likely due to the oxidation of the copper it's made from. The details of the statue's robes, torch, and crown are clearly visible.\n",
      "*   **Water:** The water surrounding the island is a deep blue, with gentle ripples reflecting the sunlight.\n",
      "\n",
      "**Background:**\n",
      "\n",
      "*   **New York City Skyline:** A dense and impressive skyline of New York City stretches across the background. Numerous skyscrapers of varying heights and\n"
     ]
    }
   ],
   "source": [
    "from transformers.image_utils import load_image\n",
    "from transformers import AutoProcessor, TextStreamer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "# Load images\n",
    "image1 = load_image(\n",
    "    \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n",
    ")\n",
    "image2 = load_image(\n",
    "    \"https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "inputs_new = processor(text=prompt, images=[image1], return_tensors=\"pt\")\n",
    "\n",
    "request = compiled_language_model.create_infer_request()\n",
    "merge_model_request = multimodal_merger_model.create_infer_request()\n",
    "# Set the input names\n",
    "input_names = {key.get_any_name(): idx for idx, key in enumerate(language_model.inputs)}\n",
    "inputs = {}\n",
    "# Set the initial input_ids\n",
    "current_input_ids = inputs_new[\"input_ids\"]\n",
    "attention_mask = inputs_new[\"attention_mask\"]\n",
    "position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "pixel_values = inputs_new[\"pixel_values\"]\n",
    "token_type_ids = inputs_new[\"token_type_ids\"]\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"do_sample\": False,\n",
    "    \"streamer\": TextStreamer(\n",
    "        processor.tokenizer, skip_prompt=True, skip_special_tokens=True\n",
    "    ),\n",
    "}\n",
    "generated_tokens = []\n",
    "\n",
    "for i in range(generation_args[\"max_new_tokens\"]):\n",
    "    # Generate input embeds each time\n",
    "    text_embeds = torch.from_numpy(text_embeddings_model(current_input_ids)[0])\n",
    "    if current_input_ids.shape[-1] > 1:\n",
    "        vision_embeds = torch.from_numpy(\n",
    "            image_embed_model(\n",
    "                {\n",
    "                    \"pixel_values\": pixel_values,\n",
    "                }\n",
    "            )[0]\n",
    "        )\n",
    "        merge_model_request.start_async(\n",
    "            {\n",
    "                \"vision_embeds\": vision_embeds,\n",
    "                \"inputs_embeds\": text_embeds,\n",
    "                \"input_ids\": current_input_ids,\n",
    "            },\n",
    "            share_inputs=True,\n",
    "        )\n",
    "        merge_model_request.wait()\n",
    "        final_embedding = torch.from_numpy(\n",
    "            merge_model_request.get_tensor(\"final_embedding\").data\n",
    "        )\n",
    "    else:\n",
    "        final_embedding = text_embeds\n",
    "\n",
    "    if i > 0:\n",
    "        inputs = {}\n",
    "    # Prepare inputs for the model\n",
    "    inputs[\"inputs_embeds\"] = final_embedding\n",
    "    inputs[\"attention_mask\"] = attention_mask\n",
    "    inputs[\"position_ids\"] = position_ids\n",
    "    inputs[\"token_type_ids\"] = token_type_ids\n",
    "    if \"beam_idx\" in input_names:\n",
    "        inputs[\"beam_idx\"] = np.arange(attention_mask.shape[0], dtype=int)\n",
    "\n",
    "    # Start inference\n",
    "    request.start_async(inputs, share_inputs=True)\n",
    "    request.wait()\n",
    "\n",
    "    # Get the logits and find the next token\n",
    "    logits = torch.from_numpy(request.get_tensor(\"logits\").data)\n",
    "    next_token = logits.argmax(-1)[0][-1]\n",
    "\n",
    "    # Append the generated token\n",
    "    generated_tokens.append(next_token)\n",
    "\n",
    "    # Update input_ids with the new token\n",
    "    current_input_ids = torch.cat([next_token.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "\n",
    "    # update the attention mask\n",
    "    attention_mask = torch.cat(\n",
    "        [attention_mask, torch.ones_like(attention_mask[:, :1])], dim=-1\n",
    "    )\n",
    "\n",
    "    # Update inputs for the next iteration\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "    position_ids = position_ids[:, -current_input_ids.shape[1] :]\n",
    "    inputs[\"position_ids\"] = position_ids\n",
    "    token_type_ids = torch.zeros_like(current_input_ids)\n",
    "\n",
    "generated_text = processor.decode(generated_tokens, skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Import and Save in Spark NLP\n",
    "- Let's install and setup Spark NLP in Google Colab\n",
    "- This part is pretty easy via our simple script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start Spark with Spark NLP included via our simple `start()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sparknlp\n",
    "\n",
    "# let's start Spark with Spark NLP\n",
    "spark = sparknlp.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/28 03:36:26 WARN NativeLibrary: Failed to load library null: java.lang.UnsatisfiedLinkError: Can't load library: /tmp/openvino-native10904419691103163033/libtbb.so.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/prabod/spark/jars/spark-core_2.12-3.3.2.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "imageClassifier = (\n",
    "    Gemma3ForMultiModal.loadSavedModel(str(output_dir), spark)\n",
    "    .setInputCols(\"image_assembler\")\n",
    "    .setOutputCol(\"answer\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "imageClassifier.write().overwrite().save(f\"file:///tmp/{model_id}_spark_nlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml import Pipeline\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# download two images to test into ./images folder\n",
    "\n",
    "url1 = \"https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/d5fbbd1a-d484-415c-88cb-9986625b7b11\"\n",
    "url2 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "\n",
    "Path(\"images\").mkdir(exist_ok=True)\n",
    "\n",
    "!wget -q -O images/image1.jpg {url1}\n",
    "!wget -q -O images/image2.jpg {url2}\n",
    "\n",
    "\n",
    "images_path = \"file://\" + os.getcwd() + \"/images/\"\n",
    "image_df = spark.read.format(\"image\").load(path=images_path)\n",
    "\n",
    "test_df = image_df.withColumn(\n",
    "    \"text\",\n",
    "    lit(\n",
    "        \"<bos><start_of_turn>user\\nYou are a helpful assistant.\\n\\n<start_of_image>Describe this image in detail.<end_of_turn>\\n<start_of_turn>\\n\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "image_assembler = ImageAssembler().setInputCol(\"image\").setOutputCol(\"image_assembler\")\n",
    "\n",
    "imageClassifier = (\n",
    "    Gemma3ForMultiModal.load(f\"file:///tmp/{model_id}_spark_nlp\")\n",
    "    .setMaxOutputLength(50)\n",
    "    .setInputCols(\"image_assembler\")\n",
    "    .setOutputCol(\"answer\")\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        image_assembler,\n",
    "        imageClassifier,\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = pipeline.fit(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path: /mnt/research/Projects/ModelZoo/Gemma3/images/image1.jpg\n",
      "[Annotation(document, 0, 222, Okay, here's a detailed description of the image:**Overall Impression:**The image is a cozy and charming shot of a gray tabby cat completely relaxed and content inside a cardboard box. Itâ€™s a very peaceful and playful scene, Map(), [])]\n"
     ]
    }
   ],
   "source": [
    "light_pipeline = LightPipeline(model)\n",
    "image_path = os.getcwd() + \"/images/\" + \"image1.jpg\"\n",
    "print(\"image_path: \" + image_path)\n",
    "annotations_result = light_pipeline.fullAnnotateImage(\n",
    "    image_path,\n",
    "    \"<bos><start_of_turn>user\\nYou are a helpful assistant.\\n\\n<start_of_image>Describe this image in detail.<end_of_turn>\\n<start_of_turn>\",\n",
    ")\n",
    "\n",
    "for result in annotations_result:\n",
    "    print(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma32",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
