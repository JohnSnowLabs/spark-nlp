{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zva6MvJyLeWi"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/TF%20Hub%20in%20Spark%20NLP%20-%20BERT%20Sentence.ipynb)\n",
    "\n",
    "## Import BERT models for Sentenc Embeddings from TF Hub into Spark NLP ðŸš€ \n",
    "\n",
    "Let's keep in mind a few things before we start ðŸ˜Š \n",
    "\n",
    "- This feature is only in `Spark NLP 3.1.x` and after. So please make sure you have upgraded to the latest Spark NLP release\n",
    "- You can import any BERT models from TF Hub but they have to be `TF2.0 Saved Model` models. Meaning, you cannot use `BERT models for TF1` which are `DEPRECATED`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzxB-Nq6cxOA"
   },
   "source": [
    "## Save TF Hub model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNQkhyMHMgkE"
   },
   "source": [
    "- We do not need to install `tensorflow` nor `tensorflow-hub`\n",
    "- We can simple download the model and extract it\n",
    "- We'll use [small_bert/bert_uncased_L-2_H-128_A-2](https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-128_A-2/2) model from TF Hub as an example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 364,
     "status": "ok",
     "timestamp": 1626534112218,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -120
    },
    "id": "YTKqt3fnkaXH"
   },
   "outputs": [],
   "source": [
    "!rm -rf /content/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88044,
     "status": "ok",
     "timestamp": 1626534200259,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -120
    },
    "id": "SP-Op9Kirtxp",
    "outputId": "79408445-17c5-41a6-9faf-8e8241e08239"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394.3MB 36kB/s \n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9MB 35.4MB/s \n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 471kB 34.7MB/s \n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 36.5MB/s \n",
      "\u001B[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow==2.4.1 tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1626534200260,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -120
    },
    "id": "0oVoy6nrRMmk"
   },
   "outputs": [],
   "source": [
    "EXPORTED_MODEL = 'bert_en_uncased_L-2_H-128_A-2'\n",
    "TF_HUB_URL = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11757,
     "status": "ok",
     "timestamp": 1626534431104,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -120
    },
    "id": "hHXgqiWpMfCY",
    "outputId": "edcf0a15-2b18-4c19-dcce-5c3482f0046f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as keras_layer_layer_call_fn, keras_layer_layer_call_and_return_conditional_losses, keras_layer_layer_call_fn, keras_layer_layer_call_and_return_conditional_losses, keras_layer_layer_call_and_return_conditional_losses while saving (showing 5 of 170). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as keras_layer_layer_call_fn, keras_layer_layer_call_and_return_conditional_losses, keras_layer_layer_call_fn, keras_layer_layer_call_and_return_conditional_losses, keras_layer_layer_call_and_return_conditional_losses while saving (showing 5 of 170). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bert_en_uncased_L-2_H-128_A-2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bert_en_uncased_L-2_H-128_A-2/assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "encoder = hub.KerasLayer(TF_HUB_URL, trainable=False)\n",
    "\n",
    "@tf.function\n",
    "def my_module_encoder(input_mask, input_word_ids, input_type_ids):\n",
    "   inputs = {\n",
    "        'input_mask': input_mask,\n",
    "        'input_word_ids': input_word_ids,\n",
    "        'input_type_ids': input_type_ids\n",
    "   }\n",
    "   outputs = {\n",
    "        'pooler_output': encoder(inputs)['pooled_output']\n",
    "   }\n",
    "   return outputs\n",
    "\n",
    "tf.saved_model.save(\n",
    "    encoder, \n",
    "    EXPORTED_MODEL, \n",
    "    signatures=my_module_encoder.get_concrete_function(\n",
    "        input_mask=tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "        input_word_ids=tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "        input_type_ids=tf.TensorSpec(shape=(None, None), dtype=tf.int32)\n",
    "    ), \n",
    "    options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlgyZuJfS5IB"
   },
   "source": [
    "Let's have a look inside these two directories and see what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1626534438516,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -120
    },
    "id": "p2XCole7TTef",
    "outputId": "67234cb1-b3ae-4999-ce1b-949d2a5d0235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2076\n",
      "drwxr-xr-x 2 root root    4096 Jul 17 15:07 assets\n",
      "-rw-r--r-- 1 root root 2115507 Jul 17 15:07 saved_model.pb\n",
      "drwxr-xr-x 2 root root    4096 Jul 17 15:07 variables\n"
     ]
    }
   ],
   "source": [
    "!ls -l {EXPORTED_MODEL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1626534522474,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -120
    },
    "id": "r0DOGz8VUR-r",
    "outputId": "d2068cd4-6c29-40d4-ebb3-e35abd900d25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 228\n",
      "-rw-r--r-- 1 root root 231508 Jul 17 15:07 vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -l {EXPORTED_MODEL}/assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZegMvuGTmHt"
   },
   "source": [
    "- as you can see, everything needed in Spark NLP is already here, including `vocab.txt` in `assets` directory\n",
    "- we all set! We can got to Spark NLP ðŸ˜Š "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlJKd2tIU0PD"
   },
   "source": [
    "## Import and Save BERT in Spark NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0FXoxHJc5CU"
   },
   "source": [
    "- Let's install and setup Spark NLP in Google Colab\n",
    "- This part is pretty easy via our simple script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 140351,
     "status": "ok",
     "timestamp": 1626534693515,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -120
    },
    "id": "8tpW5nkMc53m",
    "outputId": "a2eb25b8-531f-4bde-c9f7-cabc8ddab485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-17 15:09:13--  http://setup.johnsnowlabs.com/colab.sh\n",
      "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
      "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
      "--2021-07-17 15:09:13--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1608 (1.6K) [text/plain]\n",
      "Saving to: â€˜STDOUTâ€™\n",
      "\n",
      "-                   100%[===================>]   1.57K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-07-17 15:09:14 (28.2 MB/s) - written to stdout [1608/1608]\n",
      "\n",
      "setup Colab for PySpark 3.0.3 and Spark NLP 3.1.2\n",
      "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
      "Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Ign:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
      "Get:11 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,418 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
      "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
      "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,221 kB]\n",
      "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,780 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,188 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,658 kB]\n",
      "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [911 kB]\n",
      "Fetched 11.4 MB in 6s (1,875 kB/s)\n",
      "Reading package lists... Done\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209.1MB 65kB/s \n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 5.8MB/s \n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204kB 53.3MB/s \n",
      "\u001B[?25h  Building wheel for pyspark (setup.py) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    }
   ],
   "source": [
    "! wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_NAgx4hdCGP"
   },
   "source": [
    "Let's start Spark with Spark NLP included via our simple `start()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 67892,
     "status": "ok",
     "timestamp": 1626534761404,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -120
    },
    "id": "cbNneAVCLU1y"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "# let's start Spark with Spark NLP\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABTu9MrdVafM"
   },
   "source": [
    "- Let's use `loadSavedModel` functon in `BertSentenceEmbeddings` which allows us to load TensorFlow model in SavedModel format\n",
    "- Unlike `BertEmbeddings` which uses `last_hidden_state` with (-1, -1, DIMENSION) shape, `BertSentenceEmbeddings` will use `pooler_output` with (-1, DIMENSION) shape for Sentence/Document embeddings. It will generate 1 vector for the entire sentence/document\n",
    "- Most params can be set later when you are loading this model in `BertSentenceEmbeddings` in runtime, so don't worry what you are setting them now\n",
    "- `loadSavedModel` accepts two params, first is the path to the TF SavedModel. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
    "- `setStorageRef` is very important. When you are training a task like NER or any Text Classification, we use this reference to bound the trained model to this specific embeddings so you won't load a different embeddings by mistake and see terrible results ðŸ˜Š\n",
    "- It's up to you what you put in `setStorageRef` but it cannot be changed later on. We usually use the name of the model to be clear, but you can get creative if you want! \n",
    "- The `dimension` param is is purely cosmetic and won't change anything. It's mostly for you to know later via `.getDimension` what is the dimension of your model. So set this accordingly.\n",
    "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 6038,
     "status": "ok",
     "timestamp": 1626534767439,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -120
    },
    "id": "8W_almibVRTj"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import *\n",
    "\n",
    "sent_bert = BertSentenceEmbeddings.loadSavedModel(\n",
    "     EXPORTED_MODEL,\n",
    "     spark\n",
    " )\\\n",
    " .setInputCols([\"sentence\",'token'])\\\n",
    " .setOutputCol(\"bert_sentence\")\\\n",
    " .setCaseSensitive(False)\\\n",
    " .setDimension(768)\\\n",
    " .setStorageRef('sent_{}'.format(EXPORTED_MODEL)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjGiq4KnXWuy"
   },
   "source": [
    "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 13160,
     "status": "ok",
     "timestamp": 1626534780950,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -120
    },
    "id": "iWu5HfbnXAlM"
   },
   "outputs": [],
   "source": [
    "sent_bert.write().overwrite().save(\"./{}_spark_nlp\".format(EXPORTED_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCrjxPhzDplN"
   },
   "source": [
    "Let's clean up stuff we don't need anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1626534780951,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -120
    },
    "id": "ZgkVIJshDtLx"
   },
   "outputs": [],
   "source": [
    "!rm -rf {EXPORTED_MODEL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TSeTRZpXqWO"
   },
   "source": [
    "Awesome ðŸ˜Ž  !\n",
    "\n",
    "This is your BERT model from HuggingFace ðŸ¤—  loaded and saved by Spark NLP ðŸš€ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1626534780951,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -120
    },
    "id": "ogpxSWxOXj3W",
    "outputId": "bfb88092-53e7-4b1e-8382-8815fef0aba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16256\n",
      "-rw-r--r-- 1 root root 16635305 Jul 17 15:13 bert_sentence_tensorflow\n",
      "drwxr-xr-x 4 root root     4096 Jul 17 15:12 fields\n",
      "drwxr-xr-x 2 root root     4096 Jul 17 15:12 metadata\n"
     ]
    }
   ],
   "source": [
    "! ls -l {EXPORTED_MODEL}_spark_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fbehje7fYTDj"
   },
   "source": [
    "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny BERT model ðŸ˜Š "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3363,
     "status": "ok",
     "timestamp": 1626534834771,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -120
    },
    "id": "1mm3CvkwYRgs"
   },
   "outputs": [],
   "source": [
    "sent_bert_loaded = BertSentenceEmbeddings.load(\"./{}_spark_nlp\".format(EXPORTED_MODEL))\\\n",
    "  .setInputCols([\"sentence\",'token'])\\\n",
    "  .setOutputCol(\"bert\")\\\n",
    "  .setCaseSensitive(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1626534834779,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -120
    },
    "id": "pGRTNISyYlnO",
    "outputId": "a7911a3f-9168-44b0-fa5b-3f71985cabe6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'sent_bert_en_uncased_L-2_H-128_A-2'"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_bert_loaded.getStorageRef()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_he2LDtBYo1h"
   },
   "source": [
    "That's it! You can now go wild and import BERT models from TF Hub in Spark NLP ðŸš€ \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMW5bG3G2TNbEse6UTXW07q",
   "collapsed_sections": [],
   "name": "TF Hub in Spark NLP - BERT Sentence.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
