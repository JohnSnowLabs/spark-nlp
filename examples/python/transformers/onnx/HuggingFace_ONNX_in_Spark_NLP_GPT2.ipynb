{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB-OotnsS-JG"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_GPT2.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRuRMH7QS-JI"
      },
      "source": [
        "## Import ONNX GPT2 models from HuggingFace ü§ó into Spark NLP üöÄ\n",
        "\n",
        "Let's keep in mind a few things before we start üòä\n",
        "\n",
        "- ONNX support was introduced in  `Spark NLP 5.0.0`, enabling high performance inference for models.\n",
        "- ONNX support for the `TFGPT2Model` is only available since in `Spark NLP 5.2.0` and after. So please make sure you have upgraded to the latest Spark NLP release\n",
        "- You can import GPT2 models via `TFGPT2Model`. These models are usually under `Text2Text Generation` category and have `GPT2` in their labels\n",
        "- This is a very computationally expensive module especially on larger sequence. The use of an accelerator such as GPU is recommended.\n",
        "- Reference: [TFGPT2Model](https://huggingface.co/docs/transformers/en/model_doc/gpt2)\n",
        "- Some [example models](https://huggingface.co/models?other=GPT2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd98DUZxS-JJ"
      },
      "source": [
        "## Export and Save HuggingFace model\n",
        "\n",
        "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.51.3`. This doesn't mean it won't work with the future releases\n",
        "- We will also need `sentencepiece` for tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFf3GagOS-JJ",
        "outputId": "02e6b35e-2390-4bfd-940c-f9b37074701b"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade transformers[onnx]==4.48.3 optimum onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX1TUzkhS-JK"
      },
      "source": [
        "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to import and export ONNX models with `from_pretrained` and `save_pretrained`.\n",
        "- We'll use [openai-community/gpt2](https://huggingface.co/openai-community/gpt2) model from HuggingFace as an example\n",
        "- In addition to `GPT2` we also need to save the tokenizer. This is the same for every model, these are assets needed for tokenization inside Spark NLP.\n",
        "- If we want to optimize the model, a GPU will be needed. Make sure to select the correct runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LCrWJEMY0wn",
        "outputId": "02f24e53-1bbf-42f3-ef8c-533ef9b01e78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_attn_mask_utils.py:116: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
            "/usr/local/lib/python3.11/dist-packages/optimum/exporters/onnx/model_patcher.py:525: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if past_key_values_length > 0:\n",
            "Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n",
            "\tlm_head.weight: {'onnx::MatMul_3447'}\n",
            "\ttransformer.wte.weight: {'transformer.wte.weight'}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('onnx_models/openai-community/gpt2/tokenizer_config.json',\n",
              " 'onnx_models/openai-community/gpt2/special_tokens_map.json',\n",
              " 'onnx_models/openai-community/gpt2/vocab.json',\n",
              " 'onnx_models/openai-community/gpt2/merges.txt',\n",
              " 'onnx_models/openai-community/gpt2/added_tokens.json',\n",
              " 'onnx_models/openai-community/gpt2/tokenizer.json')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from optimum.exporters.onnx import main_export\n",
        "\n",
        "MODEL_NAME = \"openai-community/gpt2\"\n",
        "EXPORT_PATH = f\"onnx_models/{MODEL_NAME}\"\n",
        "\n",
        "main_export(\n",
        "    model_name_or_path=MODEL_NAME,\n",
        "    output=EXPORT_PATH,\n",
        "    task=\"text-generation\",\n",
        "    opset=14\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.save_pretrained(EXPORT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDAyLDCcS-JL"
      },
      "source": [
        "Let's have a look inside these two directories and see what we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp2ssmF2S-JL",
        "outputId": "746728d6-b157-4d34-fcc9-9b210d140195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 491232\n",
            "-rw-r--r-- 1 root root       937 Jun 14 03:03 config.json\n",
            "-rw-r--r-- 1 root root       119 Jun 14 03:03 generation_config.json\n",
            "-rw-r--r-- 1 root root    456318 Jun 14 03:04 merges.txt\n",
            "-rw-r--r-- 1 root root 498186250 Jun 14 03:04 model.onnx\n",
            "-rw-r--r-- 1 root root        99 Jun 14 03:04 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root       475 Jun 14 03:04 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root   3557680 Jun 14 03:04 tokenizer.json\n",
            "-rw-r--r-- 1 root root    798156 Jun 14 03:04 vocab.json\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ-z0eSzS-JL"
      },
      "source": [
        "- We need to organize tokenizer files into an `assets` folder and convert `vocab.json` to `vocab.txt` because Spark NLP requires this format to properly load and use the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "062OnFBIS-JL"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {EXPORT_PATH}/assets && mv {EXPORT_PATH}/merges.txt {EXPORT_PATH}/assets/\n",
        "\n",
        "import json\n",
        "\n",
        "vocab = json.load(open(f\"{EXPORT_PATH}/vocab.json\"))\n",
        "with open(f\"{EXPORT_PATH}/assets/vocab.txt\", \"w\") as f:\n",
        "    f.writelines(f\"{token}\\n\" for token in vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fbDIHVFS-JL",
        "outputId": "1eb2624c-553a-47e0-dbff-888fed541082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 848\n",
            "-rw-r--r-- 1 root root 456318 Jun 14 03:04 merges.txt\n",
            "-rw-r--r-- 1 root root 406992 Jun 14 03:05 vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}/assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tmk0Y2gQm6l"
      },
      "source": [
        "All set! assets are prepped and ready for Spark NLP. We're good to go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZZqEbvvS-JM"
      },
      "source": [
        "## Import and Save GPT2 in Spark NLP\n",
        "\n",
        "- **Install and set up Spark NLP in Google Colab**\n",
        "  - This example uses specific versions of `pyspark` and `spark-nlp` that have been tested with the transformer model to ensure everything runs smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLlypPRaS-JM",
        "outputId": "dc0f8cb6-6d2b-487e-acee-1217632239d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark==3.5.4 spark-nlp==5.5.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEy-zFjnS-JM"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KOd7hwNS-JM",
        "outputId": "d628306c-7030-4806-dcc1-b1c4cbb90def"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark NLP version:  5.5.3\n",
            "Apache Spark version:  3.5.4\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgl_T39AS-JM"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `GPT2Transformer` which allows us to load the ONNX model\n",
        "- Most params will be set automatically. They can also be set later after loading the model in `GPT2Transformer` during runtime, so don't worry about setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the exported model. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ij_8ZwLxS-JM"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import GPT2Transformer\n",
        "\n",
        "gpt2 = GPT2Transformer.loadSavedModel(EXPORT_PATH, spark)\\\n",
        "    .setInputCols([\"documents\"])\\\n",
        "    .setMaxOutputLength(50)\\\n",
        "    .setDoSample(True)\\\n",
        "    .setTopK(50)\\\n",
        "    .setTemperature(0)\\\n",
        "    .setBatchSize(5)\\\n",
        "    .setNoRepeatNgramSize(3)\\\n",
        "    .setOutputCol(\"generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_eeGHNZS-JM"
      },
      "source": [
        "Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0rmW0bXLS-JM"
      },
      "outputs": [],
      "source": [
        "gpt2.write().overwrite().save(f\"{MODEL_NAME}_spark_nlp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnmGJlakS-JM"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kWkdSCjIS-JN"
      },
      "outputs": [],
      "source": [
        "!rm -rf {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9YtKl-aS-JN"
      },
      "source": [
        "Awesome  üòé !\n",
        "\n",
        "This is your ONNX GPT2 model from HuggingFace ü§ó  loaded and saved by Spark NLP üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nbzEjwWS-JN",
        "outputId": "18a32cee-651f-4e98-ac72-c715e5ab5266"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 486600\n",
            "drwxr-xr-x 4 root root      4096 Jun 14 03:10 fields\n",
            "-rw-r--r-- 1 root root 498262404 Jun 14 03:10 gpt2_onnx\n",
            "drwxr-xr-x 2 root root      4096 Jun 14 03:10 metadata\n"
          ]
        }
      ],
      "source": [
        "! ls -l {MODEL_NAME}_spark_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcNqKR7mS-JN"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny GPT2 model üòä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZyaiumUS-JN",
        "outputId": "f7734d5c-1988-4c08-b1d7-8b9fd311e2ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|result                                                                                                                                                                                                                                   |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[ Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a more general task. This approach shows that learning to learn a new task is a matter of learning to master. As described in the]|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import GPT2Transformer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "example = spark.createDataFrame([\n",
        "    [\"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a\"]\n",
        "]).toDF(\"text\")\n",
        "\n",
        "document_assembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "gpt2 = GPT2Transformer.load(f\"{MODEL_NAME}_spark_nlp\")\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"generation\")\\\n",
        "    .setMaxOutputLength(50)\\\n",
        "    .setDoSample(True)\\\n",
        "    .setTopK(50)\\\n",
        "    .setTemperature(0.7)\\\n",
        "    .setBatchSize(1)\\\n",
        "    .setNoRepeatNgramSize(3)\n",
        "\n",
        "pipeline = Pipeline().setStages([\n",
        "    document_assembler,\n",
        "    gpt2\n",
        "])\n",
        "\n",
        "result = pipeline.fit(example).transform(example)\n",
        "result.select(\"generation.result\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTnIQ3HKS-JN"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of GPT2 models from HuggingFace ü§ó in Spark NLP üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
