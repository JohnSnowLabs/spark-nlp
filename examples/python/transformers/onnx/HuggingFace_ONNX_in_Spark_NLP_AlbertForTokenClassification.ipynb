{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf3736ir5syb"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_AlbertForTokenClassification.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UWoQUsr5syc"
      },
      "source": [
        "## Import ONNX AlbertForTokenClassification models from HuggingFace ü§ó  into Spark NLP üöÄ\n",
        "\n",
        "Let's keep in mind a few things before we start üòä\n",
        "\n",
        "- ONNX support was introduced in  `Spark NLP 5.0.0`, enabling high performance inference for models.\n",
        "- `AlbertForTokenClassification` is only available since in `Spark NLP 5.1.1` and after. So please make sure you have upgraded to the latest Spark NLP release- You can import ALBERT models trained/fine-tuned for token classification via `AlbertForTokenClassification` or `TFAlbertForTokenClassification`. These models are usually under `Token Classification` category and have `albert` in their labels\n",
        "- Reference: [TFAlbertForTokenClassification](https://huggingface.co/transformers/model_doc/albert.html#tfalbertfortokenclassification)\n",
        "- Some [example models](https://huggingface.co/models?filter=albert&pipeline_tag=token-classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOI9hdop5syd"
      },
      "source": [
        "## Export and Save HuggingFace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itl2Z6H65syd"
      },
      "source": [
        "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.48.2`. This doesn't mean it won't work with the future releases\n",
        "- Albert uses SentencePiece, so we will have to install that as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7_L9qFvImb5"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade transformers[onnx]==4.48.2 optimum onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2QHud3g5sye"
      },
      "source": [
        "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to import and export ONNX models with `from_pretrained` and `save_pretrained`.\n",
        "- We'll use [HooshvareLab/albert-fa-zwnj-base-v2-ner](https://huggingface.co/HooshvareLab/albert-fa-zwnj-base-v2-ner) model from HuggingFace as an example\n",
        "- In addition to the ALBERT model, we also need to save the `AlbertTokenizer`. This is the same for every model, these are assets (saved in `/assets`) needed for tokenization inside Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UAPAvQN5sye",
        "outputId": "001675dd-ebf6-4d97-e394-696dfd8e4b45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('onnx_models/HooshvareLab/albert-fa-zwnj-base-v2-ner/tokenizer_config.json',\n",
              " 'onnx_models/HooshvareLab/albert-fa-zwnj-base-v2-ner/special_tokens_map.json',\n",
              " 'onnx_models/HooshvareLab/albert-fa-zwnj-base-v2-ner/spiece.model',\n",
              " 'onnx_models/HooshvareLab/albert-fa-zwnj-base-v2-ner/added_tokens.json',\n",
              " 'onnx_models/HooshvareLab/albert-fa-zwnj-base-v2-ner/tokenizer.json')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from optimum.onnxruntime import ORTModelForTokenClassification\n",
        "\n",
        "MODEL_NAME = 'HooshvareLab/albert-fa-zwnj-base-v2-ner'\n",
        "EXPORT_PATH = f\"onnx_models/{MODEL_NAME}\"\n",
        "\n",
        "ort_model = ORTModelForTokenClassification.from_pretrained(MODEL_NAME, export=True)\n",
        "ort_model.save_pretrained(EXPORT_PATH)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.save_pretrained(EXPORT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UwTdc_U5sye"
      },
      "source": [
        "Let's have a look inside these two directories and see what we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIsSzZaF5sye",
        "outputId": "c24628e3-d165-4d16-bad9-5f96090a5594"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 47024\n",
            "-rw-r--r-- 1 root root     1630 Jun  7 07:07 config.json\n",
            "-rw-r--r-- 1 root root 44875812 Jun  7 07:07 model.onnx\n",
            "-rw-r--r-- 1 root root      971 Jun  7 07:07 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root   857476 Jun  7 07:07 spiece.model\n",
            "-rw-r--r-- 1 root root    19227 Jun  7 07:07 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  2381031 Jun  7 07:07 tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G0gPoyg5syf"
      },
      "source": [
        "- We need to move the `spiece.model` file from the tokenizer into an assets folder, as this is where Spark NLP looks for it when working with models like Albert or other SentencePiece-based tokenizers.\n",
        "- Additionally, we need to extract the `labels` and their corresponding `ids` from the model's config. This mapping will be saved as `labels.txt` inside the same `assets` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rTI0PmQ85syf"
      },
      "outputs": [],
      "source": [
        "!mkdir {EXPORT_PATH}/assets\n",
        "\n",
        "labels = ort_model.config.label2id\n",
        "labels = sorted(labels, key=labels.get)\n",
        "\n",
        "with open(EXPORT_PATH + '/assets/labels.txt', 'w') as f:\n",
        "    f.write('\\n'.join(labels))\n",
        "\n",
        "!mv {EXPORT_PATH}/spiece.model {EXPORT_PATH}/assets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnxhTKHp5syf",
        "outputId": "418504d2-33fc-48a9-e329-65a07abe927a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "onnx_models/HooshvareLab/albert-fa-zwnj-base-v2-ner:\n",
            "total 46188\n",
            "drwxr-xr-x 2 root root     4096 Jun  7 07:07 assets\n",
            "-rw-r--r-- 1 root root     1630 Jun  7 07:07 config.json\n",
            "-rw-r--r-- 1 root root 44875812 Jun  7 07:07 model.onnx\n",
            "-rw-r--r-- 1 root root      971 Jun  7 07:07 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root    19227 Jun  7 07:07 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  2381031 Jun  7 07:07 tokenizer.json\n",
            "\n",
            "onnx_models/HooshvareLab/albert-fa-zwnj-base-v2-ner/assets:\n",
            "total 844\n",
            "-rw-r--r-- 1 root root    121 Jun  7 07:07 labels.txt\n",
            "-rw-r--r-- 1 root root 857476 Jun  7 07:07 spiece.model\n"
          ]
        }
      ],
      "source": [
        "!ls -lR {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQG_tECt-bbD",
        "outputId": "3ed1d118-90c4-4066-e076-f6fa203a14d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O\n",
            "B-DAT\n",
            "B-EVE\n",
            "B-FAC\n",
            "B-LOC\n",
            "B-MON\n",
            "B-ORG\n",
            "B-PCT\n",
            "B-PER\n",
            "B-PRO\n",
            "B-TIM\n",
            "I-DAT\n",
            "I-EVE\n",
            "I-FAC\n",
            "I-LOC\n",
            "I-MON\n",
            "I-ORG\n",
            "I-PCT\n",
            "I-PER\n",
            "I-PRO\n",
            "I-TIM"
          ]
        }
      ],
      "source": [
        "!cat {EXPORT_PATH}/assets/labels.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFYzX-2Y5syf"
      },
      "source": [
        "Voila! We have our `spiece.model` and `labels.txt` inside assets directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT_tct-15syf"
      },
      "source": [
        "## Import and Save AlbertForTokenClassification in Spark NLP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSefp1hN5syf"
      },
      "source": [
        "Let's install and setup Spark NLP in Google Colab. For this example, we'll use specific versions of `pyspark` and `spark-nlp` that we've already tested with this transformer model to make sure everything runs smoothly:\n",
        "\n",
        "If you prefer to use the latest versions, feel free to run:\n",
        "\n",
        "`!pip install -q pyspark spark-nlp`\n",
        "\n",
        "Just keep in mind that newer versions might have some changes, so you may need to tweak your code a bit if anything breaks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "AO7bzP9AKrLU"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark==3.5.4 spark-nlp==5.5.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH1QxrnD5syf"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4DE8BA85syf",
        "outputId": "412b2571-b665-4764-830e-6e67a4cab2bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning::Spark Session already created, some configs may not take.\n",
            "Spark NLP version:  5.5.3\n",
            "Apache Spark version:  3.5.4\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7ObsJbW5syg"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `AlbertForTokenClassification` which allows us to load TensorFlow model in SavedModel format\n",
        "- Most params can be set later when you are loading this model in `AlbertForTokenClassification` in runtime like `setMaxSentenceLength`, so don't worry what you are setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the TF SavedModel. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "40krPA0d5syg"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import AlbertForTokenClassification\n",
        "\n",
        "tokenClassifier = AlbertForTokenClassification\\\n",
        "  .loadSavedModel(EXPORT_PATH, spark)\\\n",
        "  .setInputCols([\"document\",'token'])\\\n",
        "  .setOutputCol(\"ner\")\\\n",
        "  .setCaseSensitive(False)\\\n",
        "  .setMaxSentenceLength(128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfbkWekB5syg"
      },
      "source": [
        "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zUK2kdxB5syg"
      },
      "outputs": [],
      "source": [
        "tokenClassifier.write().overwrite().save(\"./{}_spark_nlp_onnx\".format(MODEL_NAME))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swSkV8uB5syg"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "7I30tQe65syg"
      },
      "outputs": [],
      "source": [
        "!rm -rf {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyR8IFzO5syg"
      },
      "source": [
        "Awesome üòé  !\n",
        "\n",
        "This is your AlbertForTokenClassification model from HuggingFace ü§ó  loaded and saved by Spark NLP üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j0IRq-U5syg",
        "outputId": "b1893ba7-c6f1-45cf-82df-91518b17f2ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 44680\n",
            "-rw-r--r-- 1 root root 44882796 Jun  7 07:07 albert_classification_onnx\n",
            "-rw-r--r-- 1 root root   857476 Jun  7 07:07 albert_spp\n",
            "drwxr-xr-x 3 root root     4096 Jun  7 07:07 fields\n",
            "drwxr-xr-x 2 root root     4096 Jun  7 07:07 metadata\n"
          ]
        }
      ],
      "source": [
        "! ls -l {MODEL_NAME}_spark_nlp_onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj5VDO475syg"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny AlbertForTokenClassification model üòä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "eXqbw48x5syg"
      },
      "outputs": [],
      "source": [
        "tokenClassifier_loaded = AlbertForTokenClassification.load(\"./{}_spark_nlp_onnx\".format(MODEL_NAME))\\\n",
        "  .setInputCols([\"document\",'token'])\\\n",
        "  .setOutputCol(\"ner\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tspm-Hog5syg"
      },
      "source": [
        "You can see what labels were used to train this model via `getClasses` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKHvfTzE5syg",
        "outputId": "bde44e38-4aed-43a8-c027-a75076841f4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I-PCT',\n",
              " 'B-PRO',\n",
              " 'I-EVE',\n",
              " 'B-LOC',\n",
              " 'I-ORG',\n",
              " 'B-FAC',\n",
              " 'B-EVE',\n",
              " 'B-TIM',\n",
              " 'I-DAT',\n",
              " 'B-MON',\n",
              " 'B-PCT',\n",
              " 'I-MON',\n",
              " 'I-LOC',\n",
              " 'I-FAC',\n",
              " 'I-PRO',\n",
              " 'I-TIM',\n",
              " 'I-PER',\n",
              " 'B-DAT',\n",
              " 'B-ORG',\n",
              " 'O',\n",
              " 'B-PER']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenClassifier_loaded.getClasses()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGsJrE--5syg"
      },
      "source": [
        "This is how you can use your loaded classifier model in Spark NLP üöÄ pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF-1e5hB5syg",
        "outputId": "223ca83d-bfe5-4e9e-8730-db86e187aa68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------+--------------------------------------------------------------+\n",
            "|text                                                                        |result                                                        |\n",
            "+----------------------------------------------------------------------------+--------------------------------------------------------------+\n",
            "|ÿß€åŸÜ ÿ≥ÿ±€åÿßŸÑ ÿ®Ÿá ÿµŸàÿ±ÿ™ ÿ±ÿ≥ŸÖ€å ÿØÿ± ÿ™ÿßÿ±€åÿÆ ÿØŸáŸÖ ŸÖ€å €≤€∞€±€± ÿ™Ÿàÿ≥ÿ∑ ÿ¥ÿ®⁄©Ÿá ŸÅÿß⁄©ÿ≥ ÿ®ÿ±ÿß€å ŸæÿÆÿ¥ ÿ±ÿ≤ÿ±Ÿà ÿ¥ÿØ.|[O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, O, O, O, O, O]|\n",
            "|ÿØŸÅÿ™ÿ± ŸÖÿ±⁄©ÿ≤€å ÿ¥ÿ±⁄©ÿ™ ⁄©ÿßŸÖ€å⁄©Ÿà ÿØÿ± ÿ¥Ÿáÿ± ÿ≥ÿßÿ≥⁄©ÿßÿ™ŸàŸÜ ÿ≥ÿßÿ≥⁄©ÿß⁄ÜŸàÿßŸÜ ŸÇÿ±ÿßÿ± ÿØÿßÿ±ÿØ.                 |[O, O, B-ORG, I-ORG, O, B-LOC, I-LOC, I-LOC, O, O, O]         |\n",
            "|ÿØÿ± ÿ≥ÿßŸÑ €≤€∞€±€≥ ÿØÿ±⁄Øÿ∞ÿ¥ÿ™ Ÿà ÿ¢ŸÜÿØÿ±ÿ™€å⁄©ÿ± Ÿà ⁄©€åŸÜ ÿ®ÿ±ÿß€å ÿßŸà ŸÖÿ±ÿßÿ≥ŸÖ €åÿßÿØÿ®ŸàÿØ ⁄Øÿ±ŸÅÿ™ŸÜÿØ.            |[O, B-DAT, I-DAT, O, O, B-LOC, O, B-PER, O, O, O, O, O, O]    |\n",
            "+----------------------------------------------------------------------------+--------------------------------------------------------------+\n",
            "\n",
            "+----------------------+------+\n",
            "|text                  |entity|\n",
            "+----------------------+------+\n",
            "|ÿ¥ÿ®⁄©Ÿá ŸÅÿß⁄©ÿ≥             |ORG   |\n",
            "|ÿ¥ÿ±⁄©ÿ™ ⁄©ÿßŸÖ€å⁄©Ÿà           |ORG   |\n",
            "|ÿ¥Ÿáÿ± ÿ≥ÿßÿ≥⁄©ÿßÿ™ŸàŸÜ ÿ≥ÿßÿ≥⁄©ÿß⁄ÜŸàÿßŸÜ|LOC   |\n",
            "|ÿ≥ÿßŸÑ €≤€∞€±€≥              |DAT   |\n",
            "|ÿ¢ŸÜÿØÿ±ÿ™€å⁄©ÿ±              |LOC   |\n",
            "|⁄©€åŸÜ                   |PER   |\n",
            "+----------------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import Tokenizer, NerConverter\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols(\"document\") \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "converter = NerConverter() \\\n",
        "    .setInputCols([\"document\", \"token\", \"ner\"]) \\\n",
        "    .setOutputCol(\"ner_chunk\")\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    tokenizer,\n",
        "    tokenClassifier_loaded,\n",
        "    converter\n",
        "])\n",
        "\n",
        "example = spark.createDataFrame([\n",
        "    [\"ÿß€åŸÜ ÿ≥ÿ±€åÿßŸÑ ÿ®Ÿá ÿµŸàÿ±ÿ™ ÿ±ÿ≥ŸÖ€å ÿØÿ± ÿ™ÿßÿ±€åÿÆ ÿØŸáŸÖ ŸÖ€å €≤€∞€±€± ÿ™Ÿàÿ≥ÿ∑ ÿ¥ÿ®⁄©Ÿá ŸÅÿß⁄©ÿ≥ ÿ®ÿ±ÿß€å ŸæÿÆÿ¥ ÿ±ÿ≤ÿ±Ÿà ÿ¥ÿØ.\"],\n",
        "    [\"ÿØŸÅÿ™ÿ± ŸÖÿ±⁄©ÿ≤€å ÿ¥ÿ±⁄©ÿ™ ⁄©ÿßŸÖ€å⁄©Ÿà ÿØÿ± ÿ¥Ÿáÿ± ÿ≥ÿßÿ≥⁄©ÿßÿ™ŸàŸÜ ÿ≥ÿßÿ≥⁄©ÿß⁄ÜŸàÿßŸÜ ŸÇÿ±ÿßÿ± ÿØÿßÿ±ÿØ.\"],\n",
        "    [\"ÿØÿ± ÿ≥ÿßŸÑ €≤€∞€±€≥ ÿØÿ±⁄Øÿ∞ÿ¥ÿ™ Ÿà ÿ¢ŸÜÿØÿ±ÿ™€å⁄©ÿ± Ÿà ⁄©€åŸÜ ÿ®ÿ±ÿß€å ÿßŸà ŸÖÿ±ÿßÿ≥ŸÖ €åÿßÿØÿ®ŸàÿØ ⁄Øÿ±ŸÅÿ™ŸÜÿØ.\"]\n",
        "], [\"text\"])\n",
        "\n",
        "result = pipeline.fit(example).transform(example)\n",
        "\n",
        "result.select(\"text\", \"ner.result\").show(truncate=False)\n",
        "result.selectExpr(\"explode(ner_chunk) as chunk\").selectExpr(\n",
        "    \"chunk.result as text\",\n",
        "    \"chunk.metadata['entity'] as entity\"\n",
        ").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptDO6Zmh5syh"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of `AlbertForTokenClassification` models from HuggingFace ü§ó in Spark NLP üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
