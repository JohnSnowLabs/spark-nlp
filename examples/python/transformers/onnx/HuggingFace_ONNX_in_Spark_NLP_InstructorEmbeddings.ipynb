{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjQoSZTMUH_5"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_InstructorEmbeddings.ipynb)\n",
        "\n",
        "# Import ONNX InstructorEmbeddings  models from HuggingFace ü§ó into Spark NLP üöÄ\n",
        "\n",
        "Let's keep in mind a few things before we start üòä\n",
        "\n",
        "- ONNX support was introduced in `Spark NLP 5.1.0`, enabling high performance inference for models. Please make sure you have upgraded to the latest Spark NLP release.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an8-RiT0UH_8"
      },
      "source": [
        "## Export and Save HuggingFace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCNlrbMWUH_8"
      },
      "source": [
        "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.48.3`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XezgP-k2UH_8",
        "outputId": "5ca3d9f6-b6ed-4db1-d5b9-20ee57a1ef16"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade transformers[onnx]==4.48.3 optimum onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqoI5yIUUH_9"
      },
      "source": [
        "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to export ONNX models directly via the CLI with `!optimum-cli export onnx --model MODEL_NAME EXPORT_PATH`.\n",
        "- We'll use the [hkunlp/instructor-base](https://huggingface.co/hkunlp/instructor-base) model from HuggingFace as an example and export it with the `optimum-cli`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwylSoFOUH_9",
        "outputId": "ff64c2a1-ab4a-4ade-83e5-22761423e021"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-14 03:44:41.159023: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1749872681.505882     983 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1749872681.602460     983 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-14 03:44:42.349689: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "modules.json: 100% 461/461 [00:00<00:00, 2.79MB/s]\n",
            "config_sentence_transformers.json: 100% 122/122 [00:00<00:00, 387kB/s]\n",
            "README.md: 100% 66.2k/66.2k [00:00<00:00, 7.48MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 302kB/s]\n",
            "config.json: 100% 1.55k/1.55k [00:00<00:00, 8.47MB/s]\n",
            "pytorch_model.bin: 100% 439M/439M [00:01<00:00, 277MB/s]\n",
            "tokenizer_config.json: 100% 2.43k/2.43k [00:00<00:00, 14.5MB/s]\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 8.49MB/s]\n",
            "tokenizer.json:   0% 0.00/2.42M [00:00<?, ?B/s]\n",
            "model.safetensors:   0% 0.00/439M [00:00<?, ?B/s]\u001b[A\n",
            "model.safetensors:   2% 10.5M/439M [00:00<00:07, 59.6MB/s]\u001b[A\n",
            "model.safetensors:   7% 31.5M/439M [00:00<00:03, 123MB/s] \u001b[A\n",
            "model.safetensors:  14% 62.9M/439M [00:00<00:01, 193MB/s]\u001b[A\n",
            "model.safetensors:  22% 94.4M/439M [00:00<00:01, 227MB/s]\u001b[A\n",
            "tokenizer.json: 100% 2.42M/2.42M [00:00<00:00, 2.68MB/s]\n",
            "\n",
            "model.safetensors:  36% 157M/439M [00:00<00:01, 255MB/s]\u001b[A\n",
            "model.safetensors:  43% 189M/439M [00:00<00:01, 249MB/s]\u001b[A\n",
            "model.safetensors:  50% 220M/439M [00:00<00:00, 262MB/s]\u001b[A\n",
            "model.safetensors:  57% 252M/439M [00:01<00:00, 273MB/s]\u001b[A\n",
            "model.safetensors:  65% 283M/439M [00:01<00:00, 274MB/s]\u001b[A\n",
            "special_tokens_map.json: 100% 2.20k/2.20k [00:00<00:00, 10.6MB/s]\n",
            "\n",
            "model.safetensors:  81% 357M/439M [00:01<00:00, 292MB/s]\u001b[A\n",
            "model.safetensors:  88% 388M/439M [00:01<00:00, 264MB/s]\u001b[A\n",
            "model.safetensors: 100% 439M/439M [00:01<00:00, 222MB/s]\n",
            "config.json: 100% 270/270 [00:00<00:00, 1.83MB/s]\n",
            "config.json: 100% 115/115 [00:00<00:00, 548kB/s]\n",
            "pytorch_model.bin: 100% 2.36M/2.36M [00:00<00:00, 15.7MB/s]\n",
            "Could not find ONNX initializer for torch parameter 0.auto_model.encoder.embed_tokens.weight. 0.auto_model.encoder.embed_tokens.weight will not be checked for deduplication.\n",
            "Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n",
            "\t0.auto_model.encoder.embed_tokens.weight: set() --> ignored (may be a parameter from a part of the model not exported)\n",
            "\t0.auto_model.shared.weight: {'0.auto_model.shared.weight'}\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = \"hkunlp/instructor-base\"\n",
        "EXPORT_PATH = f\"export_onnx/{MODEL_NAME}\"\n",
        "\n",
        "! optimum-cli export onnx --model {MODEL_NAME} {EXPORT_PATH} --task feature-extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6GW8l2fUH_-"
      },
      "source": [
        "Let's have a look inside these two directories and see what we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WYraOCfUH_-",
        "outputId": "21d3275f-51ed-4c83-b544-4452c9529366"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 433936\n",
            "-rw-r--r-- 1 root root      1585 Jun 14 03:45 config.json\n",
            "-rw-r--r-- 1 root root 441093862 Jun 14 03:45 model.onnx\n",
            "-rw-r--r-- 1 root root      2543 Jun 14 03:45 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root    791656 Jun 14 03:45 spiece.model\n",
            "-rw-r--r-- 1 root root     20996 Jun 14 03:45 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root   2422434 Jun 14 03:45 tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsClRCyIq1eg"
      },
      "source": [
        "Spark NLP requires auxiliary model files, such as the model file, to be placed in a dedicated `assets` directory for proper loading and compatibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ar_o_tJIUH_-"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {EXPORT_PATH}/assets && mv -t {EXPORT_PATH}/assets  {EXPORT_PATH}/*.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukZxhGpWUH_-",
        "outputId": "a087164f-8f9e-4d46-ea69-448d46e593bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 776\n",
            "-rw-r--r-- 1 root root 791656 Jun 14 03:45 spiece.model\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}/assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr7NE5DBUH__"
      },
      "source": [
        "## Import and Save InstructorEmbeddings  in Spark NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_TJus-bhpDe"
      },
      "source": [
        "- **Install and set up Spark NLP in Google Colab**\n",
        "  - This example uses specific versions of `pyspark` and `spark-nlp` that have been tested with the transformer model to ensure everything runs smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acU9SZq-UH__",
        "outputId": "f12516de-6b07-453f-d92e-381346503e93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark==3.5.4 spark-nlp==5.5.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRUJ0CtfUH__"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kQTKjcWUH__",
        "outputId": "7ee2c93d-2ca6-450c-9d08-208b5b1b9489"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark NLP version:  5.5.3\n",
            "Apache Spark version:  3.5.4\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FIOCiZxUH__"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `InstructorEmbeddings ` which allows us to load the ONNX model\n",
        "- Most params will be set automatically. They can also be set later after loading the model in `InstructorEmbeddings ` during runtime, so don't worry about setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the exported model. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3wJClaqyUH__"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import InstructorEmbeddings\n",
        "\n",
        "embedding = InstructorEmbeddings.loadSavedModel(\n",
        "      EXPORT_PATH,\n",
        "      spark\n",
        "      )\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"instructor\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8cNjLgcUH__"
      },
      "source": [
        "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zqhebAObUH__"
      },
      "outputs": [],
      "source": [
        "embedding.write().overwrite().save(\"./{}_spark_nlp\".format(EXPORT_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReTnXz5pUIAA"
      },
      "source": [
        "Awesome  üòé !\n",
        "\n",
        "This is your ONNX InstructorEmbeddings  model from HuggingFace ü§ó  loaded and saved by Spark NLP üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRG-oxWnUIAA",
        "outputId": "6c64b08e-ca4b-474c-a662-21d9007b92d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 431608\n",
            "-rw-r--r-- 1 root root 441161306 Jun 14 03:49 instructor_onnx\n",
            "-rw-r--r-- 1 root root    791656 Jun 14 03:49 instructor_spp\n",
            "drwxr-xr-x 2 root root      4096 Jun 14 03:49 metadata\n"
          ]
        }
      ],
      "source": [
        "! ls -l {EXPORT_PATH}_spark_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxvpC-hSUIAA"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny InstructorEmbeddings  model üòä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbmurppD1tN5",
        "outputId": "f028e0ee-1fb6-4079-a128-ce231b78f425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|              result|\n",
            "+--------------------+\n",
            "|[William Henry Ga...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "document_assembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "instructor_loaded = InstructorEmbeddings.load(f\"{EXPORT_PATH}_spark_nlp\")\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"instructor\")\\\n",
        "    .setInstruction(\"Encode This:\")\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    instructor_loaded\n",
        "])\n",
        "\n",
        "data = spark.createDataFrame([[\n",
        "    \"William Henry Gates III (born October 28, 1955) is an American business magnate, software developer, investor, and philanthropist.\"\n",
        "]]).toDF(\"text\")\n",
        "\n",
        "model = pipeline.fit(data)\n",
        "result = model.transform(data)\n",
        "\n",
        "result.select(\"instructor.result\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D65GZokYUIAA"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of InstructorEmbeddings  models from HuggingFace ü§ó in Spark NLP üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
