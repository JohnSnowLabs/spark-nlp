{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcMubEra3-vj"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_CamemBertForQuestionAnswering.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwbqf2Gp3-vm"
      },
      "source": [
        "## Import ONNX CamemBertForQuestionAnswering models from HuggingFace ü§ó into Spark NLP üöÄ\n",
        "\n",
        "Let's keep in mind a few things before we start üòä\n",
        "\n",
        "- ONNX support was introduced in  `Spark NLP 5.0.0`, enabling high performance inference for models.\n",
        "- `CamemBertForQuestionAnswering` is only available since in `Spark NLP 5.2.0` and after. So please make sure you have upgraded to the latest Spark NLP release\n",
        "- You can import CamemBERT models trained/fine-tuned for question answering via `CamembertForQuestionAnswering` or `TFCamembertForQuestionAnswering`. These models are usually under `Question Answering` category and have `camembert` in their labels\n",
        "- Reference: [TFCamembertForQuestionAnswering](https://huggingface.co/docs/transformers/model_doc/camembert#transformers.TFCamembertForQuestionAnswering)\n",
        "- Some [example models](https://huggingface.co/models?other=camembert&pipeline_tag=question-answering&sort=downloads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSTvzZ2x3-vn"
      },
      "source": [
        "## Export and Save HuggingFace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s47S2V0V3-vn"
      },
      "source": [
        "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.51.3`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully.\n",
        "- CamembertTokenizer requires the `SentencePiece` library, so we install that as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kssp7YkB3-vn"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers[onnx]==4.51.3 optimum onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RylYAYX83-vp"
      },
      "source": [
        "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to import and export ONNX models with `from_pretrained` and `save_pretrained`.\n",
        "- We'll use [illuin/camembert-base-fquad](https://huggingface.co/illuin/camembert-base-fquad) model from HuggingFace as an example and load it as a `ORTModelForQuestionAnswering`, representing an ONNX model.\n",
        "- In addition to the CamemBERT model, we also need to save the `CamembertTokenizer`. This is the same for every model, these are assets (saved in `/assets`) needed for tokenization inside Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oRkFbMr3-vp",
        "outputId": "0cb46c6e-3ba5-46cf-9d26-ed0f9d7cc09d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at illuin/camembert-base-fquad were not used when initializing CamembertForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing CamembertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('onnx_models/illuin/camembert-base-fquad/tokenizer_config.json',\n",
              " 'onnx_models/illuin/camembert-base-fquad/special_tokens_map.json',\n",
              " 'onnx_models/illuin/camembert-base-fquad/sentencepiece.bpe.model',\n",
              " 'onnx_models/illuin/camembert-base-fquad/added_tokens.json')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import CamembertTokenizer\n",
        "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
        "\n",
        "MODEL_NAME = 'illuin/camembert-base-fquad'\n",
        "ONNX_MODEL = f\"onnx_models/{MODEL_NAME}\"\n",
        "\n",
        "ort_model = ORTModelForQuestionAnswering.from_pretrained(MODEL_NAME, export=True)\n",
        "ort_model.save_pretrained(ONNX_MODEL)\n",
        "\n",
        "tokenizer = CamembertTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.save_pretrained(ONNX_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJjLe9UN3-vp"
      },
      "source": [
        "Let's have a look inside these two directories and see what we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axjeqKy-3-vp",
        "outputId": "07a96da5-a11b-4acd-9c07-30bb1ab21316"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 430936\n",
            "-rw-r--r-- 1 root root        22 Jun 10 20:25 added_tokens.json\n",
            "-rw-r--r-- 1 root root       667 Jun 10 20:25 config.json\n",
            "-rw-r--r-- 1 root root 440450774 Jun 10 20:25 model.onnx\n",
            "-rw-r--r-- 1 root root    810912 Jun 10 20:25 sentencepiece.bpe.model\n",
            "-rw-r--r-- 1 root root       354 Jun 10 20:25 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1847 Jun 10 20:25 tokenizer_config.json\n"
          ]
        }
      ],
      "source": [
        "!ls -l {ONNX_MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AJUvmKY3-vq"
      },
      "source": [
        "- We need to move the `sentencepiece.bpe.model` file from the tokenizer into an assets folder, as this is where Spark NLP looks for it when working with models like Camembert or other SentencePiece-based tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xIUgsasa3-vq"
      },
      "outputs": [],
      "source": [
        "!mkdir {ONNX_MODEL}/assets && mv {ONNX_MODEL}/sentencepiece.bpe.model {ONNX_MODEL}/assets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1B4PFHw3-vq",
        "outputId": "850fe484-bea5-4732-c447-95f384054b0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "onnx_models/illuin/camembert-base-fquad:\n",
            "total 430148\n",
            "-rw-r--r-- 1 root root        22 Jun 10 20:25 added_tokens.json\n",
            "drwxr-xr-x 2 root root      4096 Jun 10 20:25 assets\n",
            "-rw-r--r-- 1 root root       667 Jun 10 20:25 config.json\n",
            "-rw-r--r-- 1 root root 440450774 Jun 10 20:25 model.onnx\n",
            "-rw-r--r-- 1 root root       354 Jun 10 20:25 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1847 Jun 10 20:25 tokenizer_config.json\n",
            "\n",
            "onnx_models/illuin/camembert-base-fquad/assets:\n",
            "total 792\n",
            "-rw-r--r-- 1 root root 810912 Jun 10 20:25 sentencepiece.bpe.model\n"
          ]
        }
      ],
      "source": [
        "!ls -lR {ONNX_MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcc-AYo33-vq"
      },
      "source": [
        "Voila! We have our `spiece.model` inside assets directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1T1jidv3-vr"
      },
      "source": [
        "## Import and Save CamemBertForQuestionAnswering in Spark NLP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgRyjfOI3-vr"
      },
      "source": [
        "- Let's install and setup Spark NLP in Google Colab. For this example, we'll use specific versions of `pyspark` and `spark-nlp` that we've already tested with this transformer model to make sure everything runs smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KNjp_FU3-vr",
        "outputId": "71180f20-4bae-4c32-f5b2-e8c9584dbf8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark==3.5.4 spark-nlp==5.5.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVGBLNPD3-vr"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAT6liSx3-vr",
        "outputId": "79ba068f-936f-4f1e-feb9-1e2dd7671786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark NLP version:  5.5.3\n",
            "Apache Spark version:  3.5.4\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TUaOvg63-vr"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `CamemBertForQuestionAnswering` which allows us to load TensorFlow model in SavedModel format\n",
        "- Most params can be set later when you are loading this model in `CamemBertForQuestionAnswering` in runtime like `setMaxSentenceLength`, so don't worry what you are setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the TF SavedModel. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8mByo2IQ3-vr"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import CamemBertForQuestionAnswering\n",
        "\n",
        "spanClassifier = CamemBertForQuestionAnswering.loadSavedModel(\n",
        "     f\"{ONNX_MODEL}\",\n",
        "     spark\n",
        " )\\\n",
        "  .setInputCols([\"document_question\",'document_context'])\\\n",
        "  .setOutputCol(\"answer\")\\\n",
        "  .setCaseSensitive(False)\\\n",
        "  .setMaxSentenceLength(512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZtqXaq03-vr"
      },
      "source": [
        "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "L337VdD13-vr"
      },
      "outputs": [],
      "source": [
        "spanClassifier.write().overwrite().save(\"./{}_spark_nlp_onnx\".format(ONNX_MODEL))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5BQT5Sb3-vs"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4U6MZ3gG3-vs"
      },
      "outputs": [],
      "source": [
        "!rm -rf {ONNX_MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9nhoYVg3-vs"
      },
      "source": [
        "Awesome üòé  !\n",
        "\n",
        "This is your CamemBertForQuestionAnswering model from HuggingFace ü§ó  loaded and saved by Spark NLP üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgwUzp-S3-vs",
        "outputId": "e4d3d392-7417-4cbf-e7b1-7976449d872c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 430996\n",
            "-rw-r--r-- 1 root root 440518118 Jun 10 20:31 camembert_classification_onnx\n",
            "-rw-r--r-- 1 root root    810912 Jun 10 20:31 camembert_spp\n",
            "drwxr-xr-x 2 root root      4096 Jun 10 20:31 metadata\n"
          ]
        }
      ],
      "source": [
        "! ls -l {ONNX_MODEL}_spark_nlp_onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkqUNE7c3-vs"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny CamemBertForQuestionAnswering model in Spark NLP üöÄ pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_0Q2xw_3-vs",
        "outputId": "5041ce7f-9349-46db-f330-8a0b37595f87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------+--------+\n",
            "|question             |result  |\n",
            "+---------------------+--------+\n",
            "|O√π est-ce que je vis?|[berlin]|\n",
            "+---------------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.base import MultiDocumentAssembler\n",
        "from sparknlp.annotator import Tokenizer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "document_assembler = MultiDocumentAssembler() \\\n",
        "    .setInputCols([\"question\", \"context\"]) \\\n",
        "    .setOutputCols([\"document_question\", \"document_context\"])\n",
        "\n",
        "spanClassifier_loaded = CamemBertForQuestionAnswering.load(f\"{ONNX_MODEL}_spark_nlp_onnx\") \\\n",
        "    .setInputCols([\"document_question\", \"document_context\"]) \\\n",
        "    .setOutputCol(\"answer\")\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    spanClassifier_loaded\n",
        "])\n",
        "\n",
        "context = \"Mon nom est Wolfgang et je vis √† Berlin\"\n",
        "question = \"O√π est-ce que je vis?\"\n",
        "example = spark.createDataFrame([[question, context]]).toDF(\"question\", \"context\")\n",
        "\n",
        "model = pipeline.fit(example)\n",
        "result = model.transform(example)\n",
        "\n",
        "result.select(\"question\", \"answer.result\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_gYVgiS3-vt"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of `CamemBertForQuestionAnswering` models from HuggingFace ü§ó in Spark NLP üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
