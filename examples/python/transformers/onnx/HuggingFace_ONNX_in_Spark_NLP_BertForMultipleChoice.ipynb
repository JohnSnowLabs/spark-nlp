{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAsu8UVGoLVf"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_BertForMultipleChoice.ipynb)\n",
        "\n",
        "## Import ONNX BertForMultipleChoice models from HuggingFace ğŸ¤— into Spark NLP ğŸš€\n",
        "\n",
        "Let's keep in mind a few things before we start ğŸ˜Š\n",
        "\n",
        "- ONNX support was introduced in  `Spark NLP 5.0.0`, enabling high performance inference for models.\n",
        "- `BertForMultipleChoice` is only available since in `Spark NLP 5.5.1` and after. So please make sure you have upgraded to the latest Spark NLP release\n",
        "- You can import BERT models trained/fine-tuned for question answering via `BertForMultipleChoice` or `TFBertForMultipleChoice`. These models are usually under `Multiple Choice` category and have `bert` in their labels\n",
        "- Reference: [BertForMultipleChoice](https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertForMultipleChoice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzijcdtQpOx9"
      },
      "source": [
        "## Export and Save HuggingFace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XZBU9Eq4gzc"
      },
      "source": [
        "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.48.2`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJWbob-kHICU",
        "outputId": "d20b7e73-ab78-4883-f98c-42cc7c61bc17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m429.3/429.3 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade transformers[onnx]==4.48.2 onnx optimum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtewR2xdOa5s"
      },
      "source": [
        "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to import and export ONNX models with `from_pretrained` and `save_pretrained`.\n",
        "- We'll use the treained model above as an example and load it as a `ORTModelForMultipleChoice`, representing an ONNX model.\n",
        "- In addition to the BERT model, we also need to save the `BertTokenizer`. This is the same for every model, these are assets (saved in `/assets`) needed for tokenization inside Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id33annImYM8",
        "outputId": "cb9b9e02-ea58-449d-85cd-61b9b236c3e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('onnx_models/bert_multiple_choice/tokenizer_config.json',\n",
              " 'onnx_models/bert_multiple_choice/special_tokens_map.json',\n",
              " 'onnx_models/bert_multiple_choice/vocab.txt',\n",
              " 'onnx_models/bert_multiple_choice/added_tokens.json',\n",
              " 'onnx_models/bert_multiple_choice/tokenizer.json')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from optimum.onnxruntime import ORTModelForMultipleChoice\n",
        "\n",
        "MODEL_NAME = \"irfanamal/bert_multiple_choice\"\n",
        "EXPORT_PATH = f\"onnx_models/bert_multiple_choice\"\n",
        "\n",
        "ort_model = ORTModelForMultipleChoice.from_pretrained(MODEL_NAME, export=True)\n",
        "ort_model.save_pretrained(EXPORT_PATH)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.save_pretrained(EXPORT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1696tiVO51u"
      },
      "source": [
        "Let's have a look inside these two directories and see what we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFamGuT4OJC2",
        "outputId": "cf756eb2-3d92-4aa1-ef57-040dcad6d4dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 419M\n",
            "-rw-r--r-- 1 root root  683 Jun  8 14:19 config.json\n",
            "-rw-r--r-- 1 root root 418M Jun  8 14:19 model.onnx\n",
            "-rw-r--r-- 1 root root  695 Jun  8 14:20 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 1.3K Jun  8 14:20 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 695K Jun  8 14:20 tokenizer.json\n",
            "-rw-r--r-- 1 root root 227K Jun  8 14:20 vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -lh {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THEhUhYRO6-y"
      },
      "source": [
        "- Now we need to move `vocab.txt` from the tokenizer into an `assets` folder, which is where Spark NLP expects to find it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_-ljjz1PVLD"
      },
      "outputs": [],
      "source": [
        "!mkdir {EXPORT_PATH}/assets && mv {EXPORT_PATH}/vocab.txt {EXPORT_PATH}/assets/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BcINpaqPmgQ",
        "outputId": "16a86d9e-c5da-4af1-f93a-597467c0e596"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "onnx_models/bert_multiple_choice:\n",
            "total 428680\n",
            "drwxr-xr-x 2 root root      4096 Jun  8 14:22 assets\n",
            "-rw-r--r-- 1 root root       683 Jun  8 14:19 config.json\n",
            "-rw-r--r-- 1 root root 438238205 Jun  8 14:19 model.onnx\n",
            "-rw-r--r-- 1 root root       695 Jun  8 14:20 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1328 Jun  8 14:20 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    711494 Jun  8 14:20 tokenizer.json\n",
            "\n",
            "onnx_models/bert_multiple_choice/assets:\n",
            "total 228\n",
            "-rw-r--r-- 1 root root 231508 Jun  8 14:20 vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -lR {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOT64bl9Ppk-"
      },
      "source": [
        "Voila! We have our `vocab.txt` inside assets directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rgd1jHMRC7q"
      },
      "source": [
        "## Import and Save BertForMultipleChoice in Spark NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0dY2lHcRG5t"
      },
      "source": [
        "Let's install and setup Spark NLP in Google Colab. For this example, we'll use specific versions of `pyspark` and `spark-nlp` that we've already tested with this transformer model to make sure everything runs smoothly:\n",
        "\n",
        "If you prefer to use the latest versions, feel free to run:\n",
        "\n",
        "`!pip install -q pyspark spark-nlp`\n",
        "\n",
        "Just keep in mind that newer versions might have some changes, so you may need to tweak your code a bit if anything breaks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28v1c7Hpx3ii",
        "outputId": "dbb30f41-5bbc-423f-a9de-3b397bd2a9ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark==3.5.4 spark-nlp==5.5.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1kTC9LQRHbg",
        "outputId": "9285ba64-7ca5-426e-fee3-e889d189df53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark NLP version:  5.5.3\n",
            "Apache Spark version:  3.5.4\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3lTxyr-R9LH"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `BertForMultipleChoice` which allows us to load TensorFlow model in SavedModel format\n",
        "- Most params can be set later when you are loading this model in `BertForMultipleChoice` in runtime like `setMaxSentenceLength`, so don't worry what you are setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the TF SavedModel. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O6v4t3HSFRU"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import BertForMultipleChoice\n",
        "\n",
        "bertMultpleChoiceClassifier = BertForMultipleChoice.loadSavedModel(\n",
        "     f\"{EXPORT_PATH}\",\n",
        "     spark\n",
        " )\\\n",
        "  .setInputCols([\"document_question\", \"document_context\"])\\\n",
        "  .setOutputCol(\"answer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmxG3UynSxFf"
      },
      "source": [
        "Let's save it on disk so it is easier to be moved around and also be used later via .load function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl9v_UCISfbJ"
      },
      "outputs": [],
      "source": [
        "bertMultpleChoiceClassifier.write().overwrite().save(\"./{}_spark_nlp_onnx\".format(MODEL_NAME))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPSFjBLuS2Lk"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spbp5G5sS2lR"
      },
      "outputs": [],
      "source": [
        "!rm -rf {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeOCQM2TS6Ew",
        "outputId": "c0175fea-6985-4a86-ca17-1be06955db75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 428048\n",
            "-rw-r--r-- 1 root root 438305209 Jun  8 14:27 bert_mc_classification_onnx\n",
            "drwxr-xr-x 3 root root      4096 Jun  8 14:27 fields\n",
            "drwxr-xr-x 2 root root      4096 Jun  8 14:27 metadata\n"
          ]
        }
      ],
      "source": [
        "! ls -l /content/irfanamal/bert_multiple_choice_spark_nlp_onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxK9WcnJS_XC"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny BertForQuestionAnswering model in Spark NLP ğŸš€ pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs3VQBACg8jm",
        "outputId": "bbe60bf3-e4a1-4cf7-9662-b94e2f77a095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------+------------------------------------------------------------------------+\n",
            "|question                                                         |choices                                                                 |\n",
            "+-----------------------------------------------------------------+------------------------------------------------------------------------+\n",
            "|In Italy, pizza served in formal settings is presented unsliced. |It is eaten with a fork and a knife, It is eaten while held in the hand.|\n",
            "|The Eiffel Tower is located in which country?                    |Germany, France, Italy                                                  |\n",
            "|Which animal is known as the king of the jungle?                 |Lion, Elephant, Tiger, Leopard                                          |\n",
            "|Water boils at what temperature?                                 |90Â°C, 120Â°C, 100Â°C                                                      |\n",
            "|Which planet is known as the Red Planet?                         |Jupiter, Mars, Venus                                                    |\n",
            "|Which language is primarily spoken in Brazil?                    |Spanish, Portuguese, English                                            |\n",
            "|The Great Wall of China was built to protect against which group?|The Greeks, The Romans, The Mongols, The Persians                       |\n",
            "|Which chemical element has the symbol 'O'?                       |Oxygen, Osmium, Ozone                                                   |\n",
            "|Which continent is the Sahara Desert located in?                 |Asia, Africa, South America                                             |\n",
            "|Which artist painted the Mona Lisa?                              |Vincent van Gogh, Leonardo da Vinci, Pablo Picasso                      |\n",
            "+-----------------------------------------------------------------+------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "testing_data = [\n",
        "    (\"In Italy, pizza served in formal settings is presented unsliced.\",\n",
        "     \"It is eaten with a fork and a knife, It is eaten while held in the hand.\"),\n",
        "    (\"The Eiffel Tower is located in which country?\", \"Germany, France, Italy\"),\n",
        "    (\"Which animal is known as the king of the jungle?\", \"Lion, Elephant, Tiger, Leopard\"),\n",
        "    (\"Water boils at what temperature?\", \"90Â°C, 120Â°C, 100Â°C\"),\n",
        "    (\"Which planet is known as the Red Planet?\", \"Jupiter, Mars, Venus\"),\n",
        "    (\"Which language is primarily spoken in Brazil?\", \"Spanish, Portuguese, English\"),\n",
        "    (\"The Great Wall of China was built to protect against which group?\", \"The Greeks, The Romans, The Mongols, The Persians\"),\n",
        "    (\"Which chemical element has the symbol 'O'?\", \"Oxygen, Osmium, Ozone\"),\n",
        "    (\"Which continent is the Sahara Desert located in?\", \"Asia, Africa, South America\"),\n",
        "    (\"Which artist painted the Mona Lisa?\", \"Vincent van Gogh, Leonardo da Vinci, Pablo Picasso\")\n",
        "]\n",
        "\n",
        "testing_df = spark.createDataFrame(testing_data, [\"question\", \"choices\"])\n",
        "testing_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IX6B1rHTNwt",
        "outputId": "ba6b585c-d14a-416c-d440-ff2f8bd1a5a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------+\n",
            "|result                               |\n",
            "+-------------------------------------+\n",
            "|[It is eaten with a fork and a knife]|\n",
            "|[Germany]                            |\n",
            "|[Lion]                               |\n",
            "|[90Â°C]                               |\n",
            "|[ Mars]                              |\n",
            "|[ Portuguese]                        |\n",
            "|[ The Mongols]                       |\n",
            "|[Oxygen]                             |\n",
            "|[ Africa]                            |\n",
            "|[Vincent van Gogh]                   |\n",
            "+-------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.base import MultiDocumentAssembler\n",
        "from sparknlp.annotator import BertForMultipleChoice\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "document_assembler = MultiDocumentAssembler() \\\n",
        "    .setInputCols([\"question\", \"choices\"]) \\\n",
        "    .setOutputCols([\"document_question\", \"document_choices\"])\n",
        "\n",
        "bert_for_multiple_choice = BertForMultipleChoice() \\\n",
        "    .load(f\"./{MODEL_NAME}_spark_nlp_onnx\") \\\n",
        "    .setInputCols([\"document_question\", \"document_choices\"]) \\\n",
        "    .setOutputCol(\"answer\") \\\n",
        "    .setBatchSize(4)\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    bert_for_multiple_choice\n",
        "])\n",
        "\n",
        "pipeline_model = pipeline.fit(testing_df)\n",
        "pipeline_df = pipeline_model.transform(testing_df)\n",
        "pipeline_df.select(\"answer.result\").show(truncate=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
