{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzB3rGwx3P9X"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_Marian.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzjK2rA43P9b"
      },
      "source": [
        "## Import ONNX Marian models from HuggingFace ü§ó into Spark NLP üöÄ\n",
        "\n",
        "Let's keep in mind a few things before we start üòä\n",
        "\n",
        "- ONNX support was introduced in  `Spark NLP 5.0.0`, enabling high performance inference for models.\n",
        "- `MarianTransformer` is only available since in `Spark NLP 5.2.0` and after. So please make sure you have upgraded to the latest Spark NLP release\n",
        "- You can import Marian models via `MarianMTModel`. These models are usually under `Text2Text Generation` category and have `marian` in their labels\n",
        "- Reference: [MarianMT](https://huggingface.co/docs/transformers/model_doc/marian)\n",
        "- Some [example models](https://huggingface.co/models?other=marian)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtghKeFT3P9b"
      },
      "source": [
        "## Export and Save HuggingFace model\n",
        "\n",
        "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.48.3`. This doesn't mean it won't work with the future releases\n",
        "- We will also need `sentencepiece` for tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KmELk3h3P9c",
        "outputId": "46cfae23-25cc-497b-d2a6-9758ccdbfd19"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade transformers[onnx]==4.48.3 optimum onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7LbwVJ43P9d"
      },
      "source": [
        "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to import and export ONNX models.\n",
        "- We'll use [Helsinki-NLP/opus-mt-en-bg](https://huggingface.co/Helsinki-NLP/opus-mt-en-bg) model from HuggingFace as an example and export it with the `optimum-cli`.\n",
        "- If we want to optimize the model, a GPU will be needed. Make sure to select the correct runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLFJgoli8qJS",
        "outputId": "81ee7184-f37f-4d83-9262-20137eb5ccef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-15 19:00:01.484188: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750014001.792926    1100 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750014001.875979    1100 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-15 19:00:02.526228: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 1.39k/1.39k [00:00<00:00, 8.27MB/s]\n",
            "pytorch_model.bin: 100% 305M/305M [00:03<00:00, 78.7MB/s]\n",
            "model.safetensors:  24% 73.4M/305M [00:01<00:02, 79.1MB/s]\n",
            "generation_config.json: 100% 293/293 [00:00<00:00, 1.03MB/s]\n",
            "model.safetensors:  34% 105M/305M [00:01<00:01, 118MB/s]  \n",
            "tokenizer_config.json: 100% 44.0/44.0 [00:00<00:00, 190kB/s]\n",
            "model.safetensors:  55% 168M/305M [00:01<00:00, 165MB/s]\n",
            "model.safetensors:  65% 199M/305M [00:01<00:00, 182MB/s]\n",
            "source.spm: 100% 791k/791k [00:00<00:00, 4.06MB/s]\n",
            "model.safetensors:  86% 262M/305M [00:01<00:00, 210MB/s]\n",
            "target.spm: 100% 999k/999k [00:00<00:00, 27.0MB/s]\n",
            "model.safetensors: 100% 305M/305M [00:02<00:00, 143MB/s]\n",
            "vocab.json: 100% 2.33M/2.33M [00:00<00:00, 79.3MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61707]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/modeling_marian.py:207: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/modeling_marian.py:214: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/modeling_marian.py:246: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_attn_mask_utils.py:88: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_attn_mask_utils.py:164: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if past_key_values_length > 0:\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/modeling_marian.py:169: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
            "Could not find ONNX initializer for torch parameter model.decoder.embed_tokens.weight. model.decoder.embed_tokens.weight will not be checked for deduplication.\n",
            "Could not find ONNX initializer for torch parameter model.encoder.embed_tokens.weight. model.encoder.embed_tokens.weight will not be checked for deduplication.\n",
            "Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n",
            "\tlm_head.weight: {'onnx::MatMul_2418'}\n",
            "\tmodel.decoder.embed_tokens.weight: set() --> ignored (may be a parameter from a part of the model not exported)\n",
            "\tmodel.encoder.embed_tokens.weight: set() --> ignored (may be a parameter from a part of the model not exported)\n",
            "\tmodel.shared.weight: {'model.shared.weight'}\n",
            "Could not find ONNX initializer for torch parameter model.decoder.embed_tokens.weight. model.decoder.embed_tokens.weight will not be checked for deduplication.\n",
            "Could not find ONNX initializer for torch parameter model.encoder.embed_tokens.weight. model.encoder.embed_tokens.weight will not be checked for deduplication.\n",
            "Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n",
            "\tlm_head.weight: {'onnx::MatMul_2036'}\n",
            "\tmodel.decoder.embed_tokens.weight: set() --> ignored (may be a parameter from a part of the model not exported)\n",
            "\tmodel.encoder.embed_tokens.weight: set() --> ignored (may be a parameter from a part of the model not exported)\n",
            "\tmodel.shared.weight: {'model.shared.weight'}\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = \"Helsinki-NLP/opus-mt-en-bg\"\n",
        "EXPORT_PATH = \"onnx_models/mt_en_bg_onnx\"\n",
        "\n",
        "# Export with optimization (O2) ‚Äî uncomment to enable\n",
        "# !optimum-cli export onnx --task text2text-generation-with-past --model {MODEL_NAME} --optimize O2 {EXPORT_PATH}\n",
        "\n",
        "# Note: Optimization (O2) may crash ONNX Runtime for T5-based models due to a known bug.\n",
        "# Workarounds:\n",
        "# 1. Manually patch ONNX Runtime (onnx_model_bert.py): comment out the head/hidden size assertion.\n",
        "# 2. Skip optimization and export as-is (recommended for T5):\n",
        "\n",
        "!optimum-cli export onnx --task text2text-generation-with-past --model {MODEL_NAME} {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4U1NKqk3P9e"
      },
      "source": [
        "Let's have a look inside these two directories and see what we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D9oBUKa3P9f",
        "outputId": "6f8ebd5d-a82c-4ff1-ea7d-d82360a62f51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 861024\n",
            "-rw-r--r-- 1 root root      1378 Jun 15 19:00 config.json\n",
            "-rw-r--r-- 1 root root 229119665 Jun 15 19:01 decoder_model_merged.onnx\n",
            "-rw-r--r-- 1 root root 228868277 Jun 15 19:00 decoder_model.onnx\n",
            "-rw-r--r-- 1 root root 216211747 Jun 15 19:01 decoder_with_past_model.onnx\n",
            "-rw-r--r-- 1 root root 203204410 Jun 15 19:00 encoder_model.onnx\n",
            "-rw-r--r-- 1 root root       288 Jun 15 19:00 generation_config.json\n",
            "-rw-r--r-- 1 root root    791438 Jun 15 19:00 source.spm\n",
            "-rw-r--r-- 1 root root        74 Jun 15 19:00 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root    999053 Jun 15 19:00 target.spm\n",
            "-rw-r--r-- 1 root root       849 Jun 15 19:00 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root   2451253 Jun 15 19:00 vocab.json\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBzRVSsO3P9f"
      },
      "source": [
        "- We need to move the sentence piece models `*.spm` from the tokenizer to assets folder which Spark NLP will look for\n",
        "- We also need to process `vocab.json` for the tokenizer vocabulary. The Spark NLP Annotator expects a `vocab.txt` with one word per line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H0sLd0yo3P9g"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {EXPORT_PATH}/assets\n",
        "!mv -t {EXPORT_PATH}/assets {EXPORT_PATH}/*.spm\n",
        "\n",
        "import json\n",
        "output_json = json.load(open(f\"{EXPORT_PATH}/vocab.json\"))\n",
        "\n",
        "with open(f\"{EXPORT_PATH}/assets/vocab.txt\", \"w\") as f:\n",
        "    for token in output_json.keys():\n",
        "        print(token, file=f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArjkmfwY3P9g",
        "outputId": "b415976c-08b3-4065-95b0-3a99e7133cea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 2528\n",
            "-rw-r--r-- 1 root root 791438 Jun 15 19:00 source.spm\n",
            "-rw-r--r-- 1 root root 999053 Jun 15 19:00 target.spm\n",
            "-rw-r--r-- 1 root root 792353 Jun 15 19:03 vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}/assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDlo6cVa3P9g"
      },
      "source": [
        "## Import and Save Marian in Spark NLP\n",
        "\n",
        "- **Install and set up Spark NLP in Google Colab**\n",
        "  - This example uses specific versions of `pyspark` and `spark-nlp` that have been tested with the transformer model to ensure everything runs smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kS4eqoY3P9g",
        "outputId": "03bee7e9-6831-412b-ea56-7bae82463692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark==3.5.4 spark-nlp==5.5.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXT7XH9d3P9h"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EawBIiz3P9h",
        "outputId": "5a3462a0-3f34-4825-bae0-545aebb1df2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark NLP version:  5.5.3\n",
            "Apache Spark version:  3.5.4\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D7JZkfb3P9h"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `MarianTransformer` which allows us to load the ONNX model\n",
        "- Most params will be set automatically. They can also be set later after loading the model in `MarianTransformer` during runtime, so don't worry about setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the exported model. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EKjvYBqR3P9h"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import MarianTransformer\n",
        "\n",
        "marian = MarianTransformer.loadSavedModel(EXPORT_PATH, spark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yaL76py3P9h"
      },
      "source": [
        "Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NtMNuq9i3P9i"
      },
      "outputs": [],
      "source": [
        "marian.write().overwrite().save(f\"{MODEL_NAME}_spark_nlp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq5B5Pxk3P9i"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DNUPMtz53P9i"
      },
      "outputs": [],
      "source": [
        "!rm -rf {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZucA1cnF3P9i"
      },
      "source": [
        "Awesome  üòé !\n",
        "\n",
        "This is your ONNX Marian model from HuggingFace ü§ó  loaded and saved by Spark NLP üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJTsT9yO3P9i",
        "outputId": "a45d6ee4-1da1-4c1e-f9a0-d51effb77498"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 424020\n",
            "-rw-r--r-- 1 root root 229154794 Jun 15 19:06 decoder.onxx\n",
            "-rw-r--r-- 1 root root 203235570 Jun 15 19:06 encoder.onxx\n",
            "-rw-r--r-- 1 root root    791438 Jun 15 19:06 marian_spp_src\n",
            "-rw-r--r-- 1 root root    999053 Jun 15 19:06 marian_spp_trg\n",
            "drwxr-xr-x 2 root root      4096 Jun 15 19:06 metadata\n"
          ]
        }
      ],
      "source": [
        "! ls -l {MODEL_NAME}_spark_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3T4adf83P9i"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny Marian model üòä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYwlgz453P9j",
        "outputId": "e11f1d1c-6c45-40ce-9325-fefc01807217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------+\n",
            "|result                                              |\n",
            "+----------------------------------------------------+\n",
            "|[(–ò—Ç–∞–ª–∏—è: –†–æ–º–∏ [;rooma] –∏) –µ —Å—Ç–æ–ª–∏—Ü–∞—Ç–∞ –Ω–∞ –ò—Ç–∞–ª–∏—è...]|\n",
            "+----------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import MarianTransformer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "test_data = spark.createDataFrame([\n",
        "    (1, \"Rome (Italian and Latin: Roma [ÀàroÀêma] ‚ìò) is the capital city of Italy...\")\n",
        "]).toDF(\"id\", \"text\")\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "marian = MarianTransformer.load(f\"{MODEL_NAME}_spark_nlp\") \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"translation\") \\\n",
        "    .setMaxInputLength(512)\n",
        "\n",
        "pipeline = Pipeline().setStages([document_assembler, marian])\n",
        "result = pipeline.fit(test_data).transform(test_data)\n",
        "\n",
        "result.select(\"translation.result\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVRcDko13P9j"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of Marian models from HuggingFace ü§ó in Spark NLP üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
