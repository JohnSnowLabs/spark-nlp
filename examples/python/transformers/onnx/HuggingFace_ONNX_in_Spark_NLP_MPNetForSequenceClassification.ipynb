{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B51jLjjF2QIT"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_MPNetForSequenceClassification.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLOMcP-q2QIW"
      },
      "source": [
        "## Import ONNX MPNetForSequenceClassification models from HuggingFace ğŸ¤—  into Spark NLP ğŸš€\n",
        "\n",
        "Let's keep in mind a few things before we start ğŸ˜Š\n",
        "\n",
        "- ONNX support was introduced in  `Spark NLP 5.0.0`, enabling high performance inference for models.\n",
        "- `MPNetForSequenceClassification` is only available since in `Spark NLP 5.2.4` and after. So please make sure you have upgraded to the latest Spark NLP release\n",
        "- You can import MPNet models trained/fine-tuned for text classification via `SetFitModel` from the `setfit` package. On huggingface, these models are usually under `Text Classification` category and have `mpnet` in their labels. Other models are currenlty not supported.\n",
        "- Some [example models](https://huggingface.co/models?pipeline_tag=text-classification&other=mpnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXtZnpZH2QIX"
      },
      "source": [
        "## Export and Save HuggingFace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PKrtdyb2QIY"
      },
      "source": [
        "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.51.3`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully.\n",
        "- Additionally, we need to install `setfit` to load the model components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8re2bU_2QIY",
        "outputId": "a2a106ad-9f94-42aa-9d10-5c6449636c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.2 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade transformers[onnx]==4.51.3 setfit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyXY-mQY2QIZ"
      },
      "source": [
        "- We'll use [rodekruis/sml-ukr-message-classifier](https://huggingface.co/rodekruis/sml-ukr-message-classifier). As this is not a pure `transformers` model, we need to export the modules separately and combine them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzzJr1g72QIa",
        "outputId": "bca780fa-d866-423e-bc47-729225c29984"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('onnx_models/rodekruis/sml-ukr-message-classifier/tokenizer_config.json',\n",
              " 'onnx_models/rodekruis/sml-ukr-message-classifier/special_tokens_map.json',\n",
              " 'onnx_models/rodekruis/sml-ukr-message-classifier/vocab.txt',\n",
              " 'onnx_models/rodekruis/sml-ukr-message-classifier/added_tokens.json',\n",
              " 'onnx_models/rodekruis/sml-ukr-message-classifier/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from setfit import SetFitModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"rodekruis/sml-ukr-message-classifier\"\n",
        "ONNX_MODEL = f\"onnx_models/{MODEL_NAME}\"\n",
        "\n",
        "model = SetFitModel.from_pretrained(MODEL_NAME)\n",
        "model.save_pretrained(ONNX_MODEL)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, export=True)\n",
        "tokenizer.save_pretrained(ONNX_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlC7GtJE2QIa"
      },
      "source": [
        "## Exporting the Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be3vYaK72QIb"
      },
      "source": [
        "Let's have a look inside these two directories and see what we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8QkdJDT2QIc",
        "outputId": "41fc1619-7ae2-46cd-df24-71398689d5bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 428848\n",
            "drwxr-xr-x 2 root root      4096 Jun 16 00:07 1_Pooling\n",
            "drwxr-xr-x 2 root root      4096 Jun 16 00:07 2_Normalize\n",
            "-rw-r--r-- 1 root root       551 Jun 16 00:07 config.json\n",
            "-rw-r--r-- 1 root root       205 Jun 16 00:07 config_sentence_transformers.json\n",
            "-rw-r--r-- 1 root root        53 Jun 16 00:07 config_setfit.json\n",
            "-rw-r--r-- 1 root root    179487 Jun 16 00:07 model_head.pkl\n",
            "-rw-r--r-- 1 root root 437967672 Jun 16 00:07 model.safetensors\n",
            "-rw-r--r-- 1 root root       349 Jun 16 00:07 modules.json\n",
            "-rw-r--r-- 1 root root      4047 Jun 16 00:07 README.md\n",
            "-rw-r--r-- 1 root root        53 Jun 16 00:07 sentence_bert_config.json\n",
            "-rw-r--r-- 1 root root       964 Jun 16 00:07 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1632 Jun 16 00:07 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    710932 Jun 16 00:07 tokenizer.json\n",
            "-rw-r--r-- 1 root root    231536 Jun 16 00:07 vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -l {ONNX_MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIFHmyYr2QIc"
      },
      "source": [
        "- As you can see, we need to move `vocab.txt` to assets folder which Spark NLP will look for\n",
        "- We also need `labels`. These are not contained in the model itself and we will have to fetch them manually. We will save this inside `labels.txt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7eWcoB02QIc",
        "outputId": "6f985848-e023-4a10-fb34-9e5f6f61c4aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-16 00:08:46--  https://huggingface.co/rodekruis/sml-ukr-message-classifier/raw/main/label_dict.json\n",
            "Resolving huggingface.co (huggingface.co)... 3.168.73.111, 3.168.73.129, 3.168.73.106, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.168.73.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 589 [text/plain]\n",
            "Saving to: â€˜label_dict.jsonâ€™\n",
            "\n",
            "\rlabel_dict.json       0%[                    ]       0  --.-KB/s               \rlabel_dict.json     100%[===================>]     589  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-16 00:08:46 (178 MB/s) - â€˜label_dict.jsonâ€™ saved [589/589]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p {ONNX_MODEL}/assets\n",
        "!mv {ONNX_MODEL}/vocab.txt {ONNX_MODEL}/assets/\n",
        "!wget https://huggingface.co/{MODEL_NAME}/raw/main/label_dict.json\n",
        "\n",
        "import json\n",
        "with open(\"label_dict.json\") as f:\n",
        "    labels = json.load(f)\n",
        "\n",
        "labels = [value for key, value in sorted(labels.items(), key=lambda x: int(x[0]))]\n",
        "\n",
        "with open(f\"{ONNX_MODEL}/assets/labels.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ajIFFDB2QId",
        "outputId": "e575bfd0-e929-4ef5-cceb-076083f42263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 232\n",
            "-rw-r--r-- 1 root root    337 Jun 16 00:08 labels.txt\n",
            "-rw-r--r-- 1 root root 231536 Jun 16 00:07 vocab.txt\n"
          ]
        }
      ],
      "source": [
        "ls -l {ONNX_MODEL}/assets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat {ONNX_MODEL}/assets/labels.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m37qMV_XEWwH",
        "outputId": "78e32bf5-b897-40e7-feb4-07b335e964d3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANOMALY\n",
            "ARMY\n",
            "CHILDREN\n",
            "CONNECTIVITY\n",
            "RC CONNECT WITH RED CROSS\n",
            "EDUCATION\n",
            "FOOD\n",
            "GOODS/SERVICES\n",
            "HEALTH\n",
            "CVA INCLUSION\n",
            "LEGAL\n",
            "MONEY/BANKING\n",
            "NFI\n",
            "OTHER PROGRAMS/NGOS\n",
            "PARCEL\n",
            "CVA PAYMENT\n",
            "PETS\n",
            "RC PMER/NEW PROGRAMS\n",
            "CVA PROGRAM INFO\n",
            "RC PROGRAM INFO\n",
            "PSS & RFL\n",
            "CVA REGISTRATION\n",
            "SENTIMENT\n",
            "SHELTER\n",
            "TRANSLATION/LANGUAGE\n",
            "CAR\n",
            "TRANSPORT/MOVEMENT\n",
            "WASH\n",
            "WORK/JOBS"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P107PJZz2QIc"
      },
      "source": [
        "Voila! We have our `vocab.txt` and `labels.txt` inside assets directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEXgFrVO2QId"
      },
      "source": [
        "## Combining and exporting the SetFit Modules\n",
        "\n",
        "The `SetFitModel` is composed of these components, we need to export:\n",
        "\n",
        "1. MPNet Embeddings Model\n",
        "2. Pooling Module\n",
        "3. Normalization Module\n",
        "4. Prediction Module\n",
        "\n",
        "We first create a custom torch module, to export it into a single ONNX graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T2bTpIIy2QId"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Define a custom model class that replicates the SetFit prediction flow\n",
        "class SentencePredictor(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "\n",
        "        # Extract linear classifier parameters\n",
        "        self.coeffs = torch.Tensor(model.model_head.coef_)\n",
        "        self.intercept = torch.Tensor(model.model_head.intercept_)\n",
        "\n",
        "        # Unpack the transformer backbone and pooling layers\n",
        "        self.embeddings, self.pooling, self.normalize = model.model_body\n",
        "\n",
        "    def predict(self, normed_embeddings):\n",
        "        # Apply linear layer manually\n",
        "        logits = normed_embeddings @ self.coeffs.T + self.intercept\n",
        "        return logits\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        input = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
        "        embeddings_out = self.embeddings(input)\n",
        "        pooled = self.pooling(embeddings_out)\n",
        "        normed = self.normalize(pooled)\n",
        "        logits = self.predict(normed[\"sentence_embedding\"])\n",
        "        return {\"logits\": logits}\n",
        "\n",
        "# Instantiate the model\n",
        "sp = SentencePredictor(model)\n",
        "\n",
        "# Prepare input batch\n",
        "input = model.model_body.tokenize([\n",
        "    \"i loved the spiderman movie!\",\n",
        "    \"pineapple on pizza is the worst ğŸ¤®\"\n",
        "])\n",
        "\n",
        "# Export the model to ONNX\n",
        "torch.onnx.export(\n",
        "    sp,\n",
        "    args=input,\n",
        "    f=f\"{ONNX_MODEL}/model.onnx\",\n",
        "    input_names=[\"input_ids\", \"attention_mask\"],\n",
        "    output_names=[\"logits\"],\n",
        "    dynamic_axes={\n",
        "        \"input_ids\": {0: \"batch_size\", 1: \"token_length\"},\n",
        "        \"attention_mask\": {0: \"batch_size\", 1: \"token_length\"},\n",
        "        \"logits\": {0: \"batch_size\"},\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ2sYnzI2QIe"
      },
      "source": [
        "Now we have the model and all necessary files to import it into Spark NLP!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVIZrRbl2QIe",
        "outputId": "c97fa64a-8d16-4119-9b2f-feb12c01c2b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "onnx_models/rodekruis/sml-ukr-message-classifier:\n",
            "total 854384\n",
            "drwxr-xr-x 2 root root      4096 Jun 16 00:07 1_Pooling\n",
            "drwxr-xr-x 2 root root      4096 Jun 16 00:07 2_Normalize\n",
            "drwxr-xr-x 2 root root      4096 Jun 16 00:08 assets\n",
            "-rw-r--r-- 1 root root       551 Jun 16 00:07 config.json\n",
            "-rw-r--r-- 1 root root       205 Jun 16 00:07 config_sentence_transformers.json\n",
            "-rw-r--r-- 1 root root        53 Jun 16 00:07 config_setfit.json\n",
            "-rw-r--r-- 1 root root    179487 Jun 16 00:07 model_head.pkl\n",
            "-rw-r--r-- 1 root root 435970222 Jun 16 00:09 model.onnx\n",
            "-rw-r--r-- 1 root root 437967672 Jun 16 00:07 model.safetensors\n",
            "-rw-r--r-- 1 root root       349 Jun 16 00:07 modules.json\n",
            "-rw-r--r-- 1 root root      4047 Jun 16 00:07 README.md\n",
            "-rw-r--r-- 1 root root        53 Jun 16 00:07 sentence_bert_config.json\n",
            "-rw-r--r-- 1 root root       964 Jun 16 00:07 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1632 Jun 16 00:07 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    710932 Jun 16 00:07 tokenizer.json\n",
            "\n",
            "onnx_models/rodekruis/sml-ukr-message-classifier/1_Pooling:\n",
            "total 4\n",
            "-rw-r--r-- 1 root root 296 Jun 16 00:07 config.json\n",
            "\n",
            "onnx_models/rodekruis/sml-ukr-message-classifier/2_Normalize:\n",
            "total 0\n",
            "\n",
            "onnx_models/rodekruis/sml-ukr-message-classifier/assets:\n",
            "total 232\n",
            "-rw-r--r-- 1 root root    337 Jun 16 00:08 labels.txt\n",
            "-rw-r--r-- 1 root root 231536 Jun 16 00:07 vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -lR {ONNX_MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaeVTnf62QIe"
      },
      "source": [
        "## Import and Save MPNetForSequenceClassification in Spark NLP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2gPiB3t2QIe"
      },
      "source": [
        "- **Install and set up Spark NLP in Google Colab**\n",
        "  - This example uses specific versions of `pyspark` and `spark-nlp` that have been tested with the transformer model to ensure everything runs smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNjqWXWp2QIe",
        "outputId": "e9706427-06f5-41c1-df40-2251d42684f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark==3.5.4 spark-nlp==5.5.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoElLU862QIe"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU94q4If2QIe",
        "outputId": "7027d125-b7ca-4400-c8d0-2de1060594ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version:  5.5.3\n",
            "Apache Spark version:  3.5.4\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC74Rdmi2QIf"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `MPNetForSequenceClassification` which allows us to load TensorFlow model in SavedModel format\n",
        "- Most params can be set later when you are loading this model in `MPNetForSequenceClassification` in runtime like `setMaxSentenceLength`, so don't worry what you are setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the TF SavedModel. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_VZE-qOH2QIf"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import MPNetForSequenceClassification\n",
        "\n",
        "sequenceClassifier = (\n",
        "    MPNetForSequenceClassification.loadSavedModel(ONNX_MODEL, spark)\n",
        "    .setInputCols([\"document\", \"token\"])\n",
        "    .setOutputCol(\"label\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6KEgTaB2QIf"
      },
      "source": [
        "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7cD8_BmG2QIf"
      },
      "outputs": [],
      "source": [
        "sequenceClassifier.write().overwrite().save(\"./{}_spark_nlp_onnx\".format(ONNX_MODEL))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtJ_ZXAG2QIf"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RaqcsUzN2QIf"
      },
      "outputs": [],
      "source": [
        "!rm -rf {ONNX_MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvUY0SQ62QIf"
      },
      "source": [
        "Awesome ğŸ˜  !\n",
        "\n",
        "This is your AlbertForSequenceClassification model from HuggingFace ğŸ¤—  loaded and saved by Spark NLP ğŸš€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfVZTsWj2QIg",
        "outputId": "0cef9746-da28-41d7-9b53-34e9234d4137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 425832\n",
            "drwxr-xr-x 4 root root      4096 Jun 16 00:17 fields\n",
            "drwxr-xr-x 2 root root      4096 Jun 16 00:17 metadata\n",
            "-rw-r--r-- 1 root root 436036881 Jun 16 00:17 mpnet_classification_onnx\n"
          ]
        }
      ],
      "source": [
        "! ls -l {ONNX_MODEL}_spark_nlp_onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVVCkfAn2QIg"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny AlbertForSequenceClassification model ğŸ˜Š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Z3sscDfE2QIg"
      },
      "outputs": [],
      "source": [
        "sequenceClassifier_loaded = (\n",
        "    MPNetForSequenceClassification.load(\"./{}_spark_nlp_onnx\".format(ONNX_MODEL))\n",
        "    .setInputCols([\"document\", \"token\"])\n",
        "    .setOutputCol(\"label\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9hFQRaN2QIg"
      },
      "source": [
        "You can see what labels were used to train this model via `getClasses` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5x5W_7z2QIg",
        "outputId": "27e17883-8c39-46d4-dc0b-a859a32c6fb3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['GOODS/SERVICES',\n",
              " 'EDUCATION',\n",
              " 'SHELTER',\n",
              " 'OTHER PROGRAMS/NGOS',\n",
              " 'RC PROGRAM INFO',\n",
              " 'CVA REGISTRATION',\n",
              " 'CAR',\n",
              " 'ARMY',\n",
              " 'PSS & RFL',\n",
              " 'CVA PAYMENT',\n",
              " 'CHILDREN',\n",
              " 'CONNECTIVITY',\n",
              " 'CVA INCLUSION',\n",
              " 'FOOD',\n",
              " 'HEALTH',\n",
              " 'TRANSLATION/LANGUAGE',\n",
              " 'LEGAL',\n",
              " 'CVA PROGRAM INFO',\n",
              " 'PETS',\n",
              " 'MONEY/BANKING',\n",
              " 'WORK/JOBS',\n",
              " 'RC CONNECT WITH RED CROSS',\n",
              " 'PARCEL',\n",
              " 'TRANSPORT/MOVEMENT',\n",
              " 'NFI',\n",
              " 'ANOMALY',\n",
              " 'RC PMER/NEW PROGRAMS',\n",
              " 'WASH',\n",
              " 'SENTIMENT']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "sequenceClassifier_loaded.getClasses()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfYIqnI92QIk"
      },
      "source": [
        "This is how you can use your loaded classifier model in Spark NLP ğŸš€ pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ifqbvxq2QIk",
        "outputId": "335299c5-7d78-4446-cd06-6f487a4ec0ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------+--------------+-------------+\n",
            "|text                                    |expected_label|result       |\n",
            "+----------------------------------------+--------------+-------------+\n",
            "|Where can I find food today?            |FOOD          |[FOOD]       |\n",
            "|I need a safe place to sleep tonight.   |SHELTER       |[SHELTER]    |\n",
            "|My payment didnâ€™t arrive, can you check?|CVA PAYMENT   |[CVA PAYMENT]|\n",
            "+----------------------------------------+--------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import Tokenizer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    tokenizer,\n",
        "    sequenceClassifier_loaded\n",
        "])\n",
        "\n",
        "data = [\n",
        "    (\"Where can I find food today?\", \"FOOD\"),\n",
        "    (\"I need a safe place to sleep tonight.\", \"SHELTER\"),\n",
        "    (\"My payment didnâ€™t arrive, can you check?\", \"CVA PAYMENT\"),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"text\", \"expected_label\"])\n",
        "\n",
        "result = pipeline.fit(df).transform(df)\n",
        "result.select(\"text\", \"expected_label\", \"label.result\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ3Jsy2l2QIk"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of `MPNetForSequenceClassification` models from HuggingFace ğŸ¤— in Spark NLP ğŸš€\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}