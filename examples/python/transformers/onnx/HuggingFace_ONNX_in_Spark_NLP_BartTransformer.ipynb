{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEdJynTH3L0x"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_BartForZeroShot.ipynb)\n",
        "\n",
        "# Import ONNX BartTransformer models from HuggingFace ðŸ¤— into Spark NLP ðŸš€\n",
        "\n",
        "Let's keep in mind a few things before we start ðŸ˜Š\n",
        "\n",
        "- ONNX support was introduced in `Spark NLP 5.0.0`, enabling high performance inference for models. Please make sure you have upgraded to the latest Spark NLP release.\n",
        "- The BartForZeroShot model was introduced in `Spark NLP 5.1.0 and requires Spark version 3.4.1 and up.`\n",
        "- Official models are supported, but not all custom models may work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfiBPTV83L0y"
      },
      "source": [
        "## Export and Save HuggingFace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhUUhv8h3L0z"
      },
      "source": [
        "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.31.0`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy9Ig4tY3L0z",
        "outputId": "2104230a-63f9-4778-f218-484fa5eebe8f"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade transformers optimum  onnx onnxruntime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_WSgW9w3L00"
      },
      "source": [
        "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to import and export ONNX models with `from_pretrained` and `save_pretrained`.\n",
        "- We'll use the [facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli) model from HuggingFace as an example and export it with the `optimum-cli`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ar3GeeF43L00"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"sshleifer/distilbart-xsum-12-6\"\n",
        "EXPORT_PATH = f\"export_onnx/{MODEL_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F7dqTBe3L01",
        "outputId": "04ec1dfb-0327-42c2-e058-e7ca52c847a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-13 14:31:06.182461: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747146666.544555    1430 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747146666.638646    1430 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-13 14:31:07.367006: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 1.59k/1.59k [00:00<00:00, 9.61MB/s]\n",
            "pytorch_model.bin: 100% 611M/611M [00:02<00:00, 249MB/s]\n",
            "model.safetensors:  60% 367M/611M [00:02<00:01, 222MB/s]\n",
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 164kB/s]\n",
            "model.safetensors:  65% 398M/611M [00:02<00:00, 227MB/s]\n",
            "model.safetensors:  75% 461M/611M [00:02<00:00, 224MB/s]\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 4.83MB/s]\n",
            "model.safetensors:  81% 493M/611M [00:02<00:00, 222MB/s]\n",
            "model.safetensors:  86% 524M/611M [00:02<00:00, 225MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 3.76MB/s]\n",
            "model.safetensors: 100% 611M/611M [00:03<00:00, 197MB/s]\n",
            "Moving the following attributes in the config to the generation config: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'length_penalty': 0.5, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py:232: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py:239: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py:271: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_attn_mask_utils.py:88: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_attn_mask_utils.py:164: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if past_key_values_length > 0:\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py:194: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
            "Could not find ONNX initializer for torch parameter model.encoder.embed_tokens.weight. model.encoder.embed_tokens.weight will not be checked for deduplication.\n",
            "Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n",
            "\tlm_head.weight: {'onnx::MatMul_2577'}\n",
            "\tmodel.decoder.embed_tokens.weight: {'model.shared.weight'}\n",
            "\tmodel.encoder.embed_tokens.weight: set() --> ignored (may be a parameter from a part of the model not exported)\n",
            "\tmodel.shared.weight: {'model.shared.weight'}\n",
            "Could not find ONNX initializer for torch parameter model.encoder.embed_tokens.weight. model.encoder.embed_tokens.weight will not be checked for deduplication.\n",
            "Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n",
            "\tlm_head.weight: {'onnx::MatMul_2198'}\n",
            "\tmodel.decoder.embed_tokens.weight: {'model.shared.weight'}\n",
            "\tmodel.encoder.embed_tokens.weight: set() --> ignored (may be a parameter from a part of the model not exported)\n",
            "\tmodel.shared.weight: {'model.shared.weight'}\n",
            "\t\t-[x] values not close enough, max diff: 3.0517578125e-05 (atol: 1e-05)\n",
            "The ONNX export succeeded with the warning: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 1e-05:\n",
            "- logits: max diff = 3.0517578125e-05.\n",
            " The exported model was saved at: export_onnx/sshleifer/distilbart-xsum-12-6\n"
          ]
        }
      ],
      "source": [
        "!optimum-cli export onnx --task text2text-generation-with-past --model {MODEL_NAME} {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jrTPqhE3L01"
      },
      "source": [
        "We have to move additional model assets into a seperate folder, so that Spark NLP can load it properly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqXo5KCK3L02"
      },
      "source": [
        "Let's have a look inside these two directories and see what we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CyHyF5Pr3L02"
      },
      "outputs": [],
      "source": [
        "!mkdir {EXPORT_PATH}/assets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nCc4TrW3IVCs"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "output_json = json.load(open(f\"{EXPORT_PATH}/vocab.json\"))\n",
        "\n",
        "with open(f\"{EXPORT_PATH}/assets/vocab.txt\", \"w\") as f:\n",
        "    for key in output_json.keys():\n",
        "        print(key, file=f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgrbr5_iISGa",
        "outputId": "042b47be-d0c0-4ae6-892a-d48a280f9d59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'export_onnx/sshleifer/distilbart-xsum-12-6/merges.txt': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mv {EXPORT_PATH}/merges.txt {EXPORT_PATH}/assets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFXX_acJ3L03",
        "outputId": "09206d0e-c68c-472c-9f0c-9a433f59ea0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 3152352\n",
            "drwxr-xr-x 2 root root      4096 May 13 14:34 assets\n",
            "-rw-r--r-- 1 root root      1662 May 13 14:31 config.json\n",
            "-rw-r--r-- 1 root root 819866018 May 13 14:33 decoder_model_merged.onnx\n",
            "-rw-r--r-- 1 root root 819603498 May 13 14:33 decoder_model.onnx\n",
            "-rw-r--r-- 1 root root 769174126 May 13 14:33 decoder_with_past_model.onnx\n",
            "-rw-r--r-- 1 root root 814962649 May 13 14:31 encoder_model.onnx\n",
            "-rw-r--r-- 1 root root       329 May 13 14:31 generation_config.json\n",
            "-rw-r--r-- 1 root root       957 May 13 14:31 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1243 May 13 14:31 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root   3558642 May 13 14:31 tokenizer.json\n",
            "-rw-r--r-- 1 root root    798293 May 13 14:31 vocab.json\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk28iNof8WR5"
      },
      "source": [
        "## Import and Save BartTransformer  in Spark NLP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J__aVVu48WR5"
      },
      "source": [
        "- Let's install and setup Spark NLP in Google Colab\n",
        "- This part is pretty easy via our simple script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "udnbTHNj8WR6",
        "outputId": "413b9a26-5121-4835-8929-bd051ba7293f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing PySpark 3.2.3 and Spark NLP 6.0.0\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 6.0.0\n",
            "Collecting pyspark==3.4.0\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.7 (from pyspark==3.4.0)\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317124 sha256=91bca7965dd64906d3a58c9b58f1f64b0b02c8104e1b12e87ab21bb943352206\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/49/ad/5c21e362b2cc9fb6785cdf03f7864b96d8ca6521f1947e3e25\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.5\n",
            "    Uninstalling py4j-0.10.9.5:\n",
            "      Successfully uninstalled py4j-0.10.9.5\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.2.3\n",
            "    Uninstalling pyspark-3.2.3:\n",
            "      Successfully uninstalled pyspark-3.2.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.7.2 requires pyspark[connect]>=3.5, but you have pyspark 3.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed py4j-0.10.9.7 pyspark-3.4.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "a0a7f59038d342b4a4cd8ee099c8f6a3",
              "pip_warning": {
                "packages": [
                  "py4j",
                  "pyspark"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash\n",
        "!pip install pyspark==3.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u9B2ldj8WR6"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "twQ6BHyo8WR6"
      },
      "outputs": [],
      "source": [
        "import sparknlp\n",
        "# let's start Spark with Spark NLP\n",
        "spark = sparknlp.start()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOEy0EXR8WR7"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `BartTransformer` which allows us to load TensorFlow model in SavedModel format\n",
        "- Most params can be set later when you are loading this model in `BartTransformer` in runtime like `setMaxSentenceLength`, so don't worry what you are setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the TF SavedModel. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lcqReFJO8WR7"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "\n",
        "EXPORT_PATH = f\"export_onnx/{MODEL_NAME}\"\n",
        "\n",
        "zero_shot_classifier = BartTransformer.loadSavedModel(\n",
        "    EXPORT_PATH,\n",
        "    spark\n",
        "    )\\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmHVmBCo8WR9"
      },
      "source": [
        "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9RBvw6p58WR9"
      },
      "outputs": [],
      "source": [
        "zero_shot_classifier.write().overwrite().save(\"./{}_spark_nlp\".format(EXPORT_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgUg2p0v8WR9"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cdBziZhw8WR-"
      },
      "outputs": [],
      "source": [
        "!rm -rf {MODEL_NAME}_tokenizer {MODEL_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iwYIQ6U8WR-"
      },
      "source": [
        "Awesome ðŸ˜Ž  !\n",
        "\n",
        "This is your BartTransformer model from HuggingFace ðŸ¤—  loaded and saved by Spark NLP ðŸš€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JAkr3438WR-",
        "outputId": "a72e816a-aee8-48bb-9267-cd4292962239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access 'sshleifer/distilbart-xsum-12-6_spark_nlp': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "! ls -l {MODEL_NAME}_spark_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5c2xWtt8WR-"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny BertForSequenceClassiBartTransformerfication model ðŸ˜Š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JjxWoPhW8WR_"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp\n",
        "\n",
        "zero_shot_classifier_loaded = BartTransformer.load(\"./{}_spark_nlp\".format(EXPORT_PATH))\\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAITDhUg8WSA"
      },
      "source": [
        "This is how you can use your loaded classifier model in Spark NLP ðŸš€ pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4svOlV88WSA",
        "outputId": "28f4dade-bb78-4c75-d9d5-01cc2ae9ffdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|              result|\n",
            "+--------------------+\n",
            "|[In this paper, w...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols(\"document\").setOutputCol(\"token\")\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    zero_shot_classifier_loaded\n",
        "])\n",
        "\n",
        "test_data = spark.createDataFrame([\n",
        "    [\"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a \" +\n",
        "       \"downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness\" +\n",
        "       \" of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this \" +\n",
        "       \"paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework \" +\n",
        "       \"that converts all text-based language problems into a text-to-text format. Our systematic study compares \" +\n",
        "       \"pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens \" +\n",
        "       \"of language understanding tasks. By combining the insights from our exploration with scale and our new \" +\n",
        "       \"Colossal Clean Crawled Corpus, we achieve state-of-the-art results on many benchmarks covering \" +\n",
        "       \"summarization, question answering, text classification, and more. To facilitate future work on transfer \" +\n",
        "       \"learning for NLP, we release our data set, pre-trained models, and code.\"]\n",
        "]).toDF(\"text\")\n",
        "\n",
        "model = pipeline.fit(test_data)\n",
        "model.transform(test_data).select(\"generation.result\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26gEdXR28WSB"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of\n",
        "`BartForZeroShotClassification` models as zero-shot classifiers from HuggingFace ðŸ¤— in Spark NLP ðŸš€"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
