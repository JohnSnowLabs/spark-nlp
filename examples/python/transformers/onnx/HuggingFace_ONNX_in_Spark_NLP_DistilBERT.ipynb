{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yugO26c-AYU"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_DistilBERT.ipynb)\n",
        "\n",
        "# Import ONNX DistilBERT models from HuggingFace ü§ó into Spark NLP üöÄ\n",
        "\n",
        "Let's keep in mind a few things before we start üòä\n",
        "\n",
        "- ONNX support was introduced in  `Spark NLP 5.0.0`, enabling high performance inference for models. Please make sure you have upgraded to the latest Spark NLP release.\n",
        "- You can import models for DistilBERT from HuggingFace and they have to be in `Fill Mask` category. Meaning, you cannot use DistilBERT models trained/fine-tuned on a specific task such as token/sequence classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvBDTE4--AYW"
      },
      "source": [
        "## Export and Save HuggingFace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saVtBgpq-AYW"
      },
      "source": [
        "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.48.3`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r6vZYxj1-AYX"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade transformers[onnx]==4.48.3 optimum onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vZqOJ0V-AYY"
      },
      "source": [
        "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to import and export ONNX models with `from_pretrained` and `save_pretrained`.\n",
        "- We'll use [distilbert-base-cased](https://huggingface.co/distilbert-base-cased) model from HuggingFace as an example and load it as a `ORTModelForFeatureExtraction`, representing an ONNX model.\n",
        "- In addition to the DistilBERT model, we also need to save the `DistilBertTokenizer`. This is the same for every model, these are assets (saved in `/assets`) needed for tokenization inside Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "O_42JzavT4JS",
        "outputId": "d8eb9371-c09a-4c33-d214-dc747999af98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model distilbert-base-cased was already converted to ONNX but got `export=True`, the model will be converted to ONNX once again. Don't forget to save the resulting model with `.save_pretrained()`\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('onnx_models/distilbert-base-cased/tokenizer_config.json',\n",
              " 'onnx_models/distilbert-base-cased/special_tokens_map.json',\n",
              " 'onnx_models/distilbert-base-cased/vocab.txt',\n",
              " 'onnx_models/distilbert-base-cased/added_tokens.json')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
        "\n",
        "MODEL_NAME = \"distilbert-base-cased\"\n",
        "EXPORT_PATH = f\"onnx_models/{MODEL_NAME}\"\n",
        "\n",
        "ort_model = ORTModelForFeatureExtraction.from_pretrained(MODEL_NAME, export=True)\n",
        "ort_model.save_pretrained(EXPORT_PATH)\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.save_pretrained(EXPORT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s1WzdYu-AYZ"
      },
      "source": [
        "Let's have a look inside these two directories and see what we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JIKzqSlk-AYZ",
        "outputId": "ec9a87e8-16b8-4dc6-de73-6ae2561aaa92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 254992\n",
            "-rw-r--r-- 1 root root       545 Jun 13 03:28 config.json\n",
            "-rw-r--r-- 1 root root 260878383 Jun 13 03:28 model.onnx\n",
            "-rw-r--r-- 1 root root       125 Jun 13 03:28 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1279 Jun 13 03:28 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    213450 Jun 13 03:28 vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LiSrje6UQen"
      },
      "source": [
        "- We need to move the `vocab.txt` file from the tokenizer into an assets folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "f1XwYmgv-Oth"
      },
      "outputs": [],
      "source": [
        "!mkdir {EXPORT_PATH}/assets && mv {EXPORT_PATH}/vocab.txt {EXPORT_PATH}/assets/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "obZpfqRp-AYa",
        "outputId": "730288bb-088e-40df-ef14-c6b3ca7dd55e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 212\n",
            "-rw-r--r-- 1 root root 213450 Jun 13 03:28 vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}/assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF2xrJui-AYa"
      },
      "source": [
        "## Import and Save DistilBERT in Spark NLP\n",
        "\n",
        "- **Install and set up Spark NLP in Google Colab**\n",
        "  - This example uses specific versions of `pyspark` and `spark-nlp` that have been tested with the transformer model to ensure everything runs smoothly.\n",
        "\n",
        "- **Optional: Use the latest versions**\n",
        "  - If you prefer to use the latest versions instead, you can install them with:\n",
        "    ```bash\n",
        "    !wget http://setup.johnsnowlabs.com/colab.sh -O - | bash\n",
        "    ```\n",
        "  - Note: The latest versions may introduce breaking changes, so you might need to adjust the code accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FI5PNYy2-AYb",
        "outputId": "f180ad4c-7401-4af0-d24a-cf0e963ecb87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark==3.5.4 spark-nlp==5.5.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeJuIHJ9-AYb"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jSQ_1H4h-AYb",
        "outputId": "c7d94b32-f612-4ab4-ae64-73e785637cb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark NLP version:  5.5.3\n",
            "Apache Spark version:  3.5.4\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsV1WZWG-AYc"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `BertEmbeddings` which allows us to load the ONNX model\n",
        "- Most params will be set automatically. They can also be set later after loading the model in `BertEmbeddings` during runtime, so don't worry about setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the exported model. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- `setStorageRef` is very important. When you are training a task like NER or any Text Classification, we use this reference to bound the trained model to this specific embeddings so you won't load a different embeddings by mistake and see terrible results üòä\n",
        "- It's up to you what you put in `setStorageRef` but it cannot be changed later on. We usually use the name of the model to be clear, but you can get creative if you want!\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nEUHxN0E-AYc"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import DistilBertEmbeddings\n",
        "\n",
        "distilbert = DistilBertEmbeddings.loadSavedModel(f\"{EXPORT_PATH}\", spark)\\\n",
        "    .setInputCols([\"document\",'token'])\\\n",
        "    .setOutputCol(\"distilbert\")\\\n",
        "    .setCaseSensitive(True)\\\n",
        "    .setStorageRef('distilbert_base_cased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPvFZXUK-AYc"
      },
      "source": [
        "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-QXtOR_P-AYc"
      },
      "outputs": [],
      "source": [
        "distilbert.write().overwrite().save(f\"{MODEL_NAME}_spark_nlp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJt-wlNo-AYc"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yp6mWy4R-AYd"
      },
      "outputs": [],
      "source": [
        "!rm -rf {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0b27DAU-AYd"
      },
      "source": [
        "Awesome  üòé !\n",
        "\n",
        "This is your ONNX DistilBERT model from HuggingFace ü§ó  loaded and saved by Spark NLP üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "POIedCsw-AYd",
        "outputId": "ffc1a8ed-435e-4c97-ecbe-689592be82ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 254816\n",
            "-rw-r--r-- 1 root root 260918327 Jun 13 03:33 distilbert_onnx\n",
            "drwxr-xr-x 3 root root      4096 Jun 13 03:33 fields\n",
            "drwxr-xr-x 2 root root      4096 Jun 13 03:33 metadata\n"
          ]
        }
      ],
      "source": [
        "! ls -l {MODEL_NAME}_spark_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwFu5nrn-AYd"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny DistilBERT model üòä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Kjx7MwEA-AYd",
        "outputId": "0c10d889-b1a0-4978-8bb8-71f1d65ed418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|          embeddings|\n",
            "+--------------------+\n",
            "|[-0.079205066, -0...|\n",
            "|[-0.008524727, -0...|\n",
            "|[0.06860065, -0.1...|\n",
            "|[-0.054616462, -0...|\n",
            "|[-0.29974172, -0....|\n",
            "|[0.0888023, -0.20...|\n",
            "|[0.3460091, -0.49...|\n",
            "|[-0.0032464452, -...|\n",
            "|[0.6970492, -0.12...|\n",
            "|[-0.45469904, 0.2...|\n",
            "|[0.41657686, 0.18...|\n",
            "|[0.09718056, 0.17...|\n",
            "|[0.1975824, 0.163...|\n",
            "|[0.20108229, -0.1...|\n",
            "|[0.6448232, 0.120...|\n",
            "|[0.28228003, -0.2...|\n",
            "|[0.0451998, 0.476...|\n",
            "|[0.4543021, 0.238...|\n",
            "|[0.14045055, 0.05...|\n",
            "|[0.05716961, 0.29...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import Tokenizer\n",
        "\n",
        "document_assembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer()\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "distilbert_loaded = DistilBertEmbeddings.load(f\"{MODEL_NAME}_spark_nlp\")\\\n",
        "    .setInputCols([\"document\", \"token\"])\\\n",
        "    .setOutputCol(\"distilbert\")\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    tokenizer,\n",
        "    distilbert_loaded\n",
        "])\n",
        "\n",
        "data = spark.createDataFrame([[\n",
        "    'William Henry Gates III (born October 28, 1955) is an American business magnate, software developer, investor, and philanthropist.'\n",
        "]]).toDF(\"text\")\n",
        "\n",
        "model = pipeline.fit(data)\n",
        "result = model.transform(data)\n",
        "\n",
        "result.selectExpr(\"explode(distilbert.embeddings) as embeddings\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3ONEY3r-AYe"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of DistilBERT models from HuggingFace ü§ó in Spark NLP üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
