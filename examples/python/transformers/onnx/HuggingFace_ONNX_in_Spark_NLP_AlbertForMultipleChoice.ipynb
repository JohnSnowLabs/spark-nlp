{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAsu8UVGoLVf"
   },
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_AlbertForMultipleChoice.ipynb)\n",
    "\n",
    "## Import ONNX AlbertForMultipleChoice models from HuggingFace ðŸ¤— into Spark NLP ðŸš€\n",
    "\n",
    "Let's keep in mind a few things before we start ðŸ˜Š\n",
    "\n",
    "- ONNX support was introduced in  `Spark NLP 5.0.0`, enabling high performance inference for models.\n",
    "- `AlbertForMultipleChoice` is only available since in `Spark NLP 5.6.0` and after. So please make sure you have upgraded to the latest Spark NLP release\n",
    "- You can import ALBERT models trained/fine-tuned for question answering via `AlbertForMultipleChoice` or `AlbertForMultipleChoice`. These models are usually under `Multiple Choice` category and have `bert` in their labels\n",
    "- Reference: [AlbertForMultipleChoice](https://huggingface.co/docs/transformers/main/en/model_doc/albert#transformers.AlbertForMultipleChoice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzijcdtQpOx9"
   },
   "source": [
    "## Export and Save HuggingFace model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlgoClMXpSg4"
   },
   "source": [
    "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJWbob-kHICU",
    "outputId": "d05b0dac-d342-40b4-aafc-f8ebd52d97a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m424.1/424.1 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
      "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.2 which is incompatible.\n",
      "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
      "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade transformers[onnx] optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtewR2xdOa5s"
   },
   "source": [
    "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to import and export ONNX models with `from_pretrained` and `save_pretrained`.\n",
    "- We'll use the treained model above as an example and load it as a `ORTModelForMultipleChoice`, representing an ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "87VKKCh1N-Ut"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade transformers[onnx] optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Id33annImYM8"
   },
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForMultipleChoice\n",
    "\n",
    "MODEL_NAME = \"Ariffiq99/CRAB_COPA_KUCI_e_care_albert_Base_Finetuned\"\n",
    "ONNX_MODEL_PATH = f\"onnx_models/albert_multiple_choice\"\n",
    "\n",
    "ort_model = ORTModelForMultipleChoice.from_pretrained(MODEL_NAME, export=True)\n",
    "ort_model.save_pretrained(ONNX_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1696tiVO51u"
   },
   "source": [
    "Let's have a look inside these two directories and see what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NFamGuT4OJC2",
    "outputId": "724401e5-2d11-4c89-ba0b-d995e6276ba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48M\n",
      "-rw-r--r-- 1 root root  871 Dec 27 19:03 config.json\n",
      "-rw-r--r-- 1 root root  45M Dec 27 19:03 model.onnx\n",
      "-rw-r--r-- 1 root root  970 Dec 27 19:03 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root 743K Dec 27 19:03 spiece.model\n",
      "-rw-r--r-- 1 root root 1.5K Dec 27 19:03 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root 2.2M Dec 27 19:03 tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {ONNX_MODEL_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THEhUhYRO6-y"
   },
   "source": [
    "We need the `spiece.model` for the Tokenizer. This is the same for every model, these are assets (saved in /assets) needed for tokenization inside Spark NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "N_-ljjz1PVLD"
   },
   "outputs": [],
   "source": [
    "!mkdir {ONNX_MODEL_PATH}/assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MI0KJCcJPjoX"
   },
   "outputs": [],
   "source": [
    "!mv {ONNX_MODEL_PATH}/spiece.model {ONNX_MODEL_PATH}/assets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOT64bl9Ppk-"
   },
   "source": [
    "Voila! We have our vocab.txt inside assets directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BcINpaqPmgQ",
    "outputId": "a705bff5-f98d-405b-dcea-0449b0383d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnx_models/albert_multiple_choice:\n",
      "total 48312\n",
      "drwxr-xr-x 2 root root     4096 Dec 27 19:04 assets\n",
      "-rw-r--r-- 1 root root      871 Dec 27 19:03 config.json\n",
      "-rw-r--r-- 1 root root 47180962 Dec 27 19:03 model.onnx\n",
      "-rw-r--r-- 1 root root      970 Dec 27 19:03 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root     1442 Dec 27 19:03 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root  2272611 Dec 27 19:03 tokenizer.json\n",
      "\n",
      "onnx_models/albert_multiple_choice/assets:\n",
      "total 744\n",
      "-rw-r--r-- 1 root root 760289 Dec 27 19:03 spiece.model\n"
     ]
    }
   ],
   "source": [
    "!ls -lR {ONNX_MODEL_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rgd1jHMRC7q"
   },
   "source": [
    "## Import and Save AlbertForMultipleChoice in Spark NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0dY2lHcRG5t"
   },
   "source": [
    "- Let's install and setup Spark NLP in Google Colab\n",
    "- This part is pretty easy via our simple script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ld2osF6STCv"
   },
   "outputs": [],
   "source": [
    "!wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u1kTC9LQRHbg",
    "outputId": "6add9710-c8ee-4323-9944-960ff9fcfd65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark version: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "# let's start Spark with Spark NLP\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3lTxyr-R9LH"
   },
   "source": [
    "- Let's use `loadSavedModel` functon in `AlbertForMultipleChoice` which allows us to load TensorFlow model in SavedModel format\n",
    "- Most params can be set later when you are loading this model in `AlbertForMultipleChoice` in runtime like `setMaxSentenceLength`, so don't worry what you are setting them now\n",
    "- `loadSavedModel` accepts two params, first is the path to the TF SavedModel. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
    "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6O6v4t3HSFRU"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "\n",
    "albertMultpleChoiceClassifier = AlbertForMultipleChoice.loadSavedModel(\n",
    "     f\"{ONNX_MODEL_PATH}\",\n",
    "     spark\n",
    " )\\\n",
    "  .setInputCols([\"document_question\", \"document_context\"])\\\n",
    "  .setOutputCol(\"answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmxG3UynSxFf"
   },
   "source": [
    "Let's save it on disk so it is easier to be moved around and also be used later via .load function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "dl9v_UCISfbJ"
   },
   "outputs": [],
   "source": [
    "albertMultpleChoiceClassifier.write().overwrite().save(\"./{}_spark_nlp_onnx\".format(MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPSFjBLuS2Lk"
   },
   "source": [
    "Let's clean up stuff we don't need anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "spbp5G5sS2lR"
   },
   "outputs": [],
   "source": [
    "!rm -rf {ONNX_MODEL_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxK9WcnJS_XC"
   },
   "source": [
    "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny `AlbertForMultipleChoice` model in Spark NLP ðŸš€ pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Gs3VQBACg8jm"
   },
   "outputs": [],
   "source": [
    " testing_data = [\n",
    "            (\"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\",\n",
    "             \"It is eaten with a fork and a knife, It is eaten while held in the hand.\"),\n",
    "\n",
    "            (\"The Eiffel Tower is located in which country?\",\n",
    "             \"Germany, France, Italy\"),\n",
    "\n",
    "            (\"Which animal is known as the king of the jungle?\",\n",
    "             \"Lion, Elephant, Tiger, Leopard\"),\n",
    "\n",
    "            (\"Water boils at what temperature?\",\n",
    "             \"90Â°C, 120Â°C, 100Â°C\"),\n",
    "\n",
    "            (\"Which planet is known as the Red Planet?\",\n",
    "             \"Jupiter, Mars, Venus\"),\n",
    "\n",
    "            (\"Which language is primarily spoken in Brazil?\",\n",
    "             \"Spanish, Portuguese, English\"),\n",
    "\n",
    "            (\"The Great Wall of China was built to protect against invasions from which group?\",\n",
    "             \"The Greeks, The Romans, The Mongols, The Persians\"),\n",
    "\n",
    "            (\"Which chemical element has the symbol 'O'?\",\n",
    "             \"Oxygenm, Osmium, Ozone\"),\n",
    "\n",
    "            (\"Which continent is the Sahara Desert located in?\",\n",
    "             \"Asia, Africa, South America\"),\n",
    "\n",
    "            (\"Which artist painted the Mona Lisa?\",\n",
    "             \"Vincent van Gogh, Leonardo da Vinci, Pablo Picasso\")\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQ-hmCBSPCsU",
    "outputId": "929d3ea1-193c-409a-eb00-8ada21e3b18f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+------------------------------------------------------------------------+\n",
      "|question                                                                                  |choices                                                                 |\n",
      "+------------------------------------------------------------------------------------------+------------------------------------------------------------------------+\n",
      "|In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.|It is eaten with a fork and a knife, It is eaten while held in the hand.|\n",
      "|The Eiffel Tower is located in which country?                                             |Germany, France, Italy                                                  |\n",
      "|Which animal is known as the king of the jungle?                                          |Lion, Elephant, Tiger, Leopard                                          |\n",
      "|Water boils at what temperature?                                                          |90Â°C, 120Â°C, 100Â°C                                                      |\n",
      "|Which planet is known as the Red Planet?                                                  |Jupiter, Mars, Venus                                                    |\n",
      "|Which language is primarily spoken in Brazil?                                             |Spanish, Portuguese, English                                            |\n",
      "|The Great Wall of China was built to protect against invasions from which group?          |The Greeks, The Romans, The Mongols, The Persians                       |\n",
      "|Which chemical element has the symbol 'O'?                                                |Oxygenm, Osmium, Ozone                                                  |\n",
      "|Which continent is the Sahara Desert located in?                                          |Asia, Africa, South America                                             |\n",
      "|Which artist painted the Mona Lisa?                                                       |Vincent van Gogh, Leonardo da Vinci, Pablo Picasso                      |\n",
      "+------------------------------------------------------------------------------------------+------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing_df = spark.createDataFrame(testing_data, [\"question\", \"choices\"])\n",
    "testing_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8IX6B1rHTNwt",
    "outputId": "b5d5d8b1-5d3e-42e2-b219-9dc6cd80017d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------+\n",
      "|answer                                                                                                      |\n",
      "+------------------------------------------------------------------------------------------------------------+\n",
      "|[{chunk, 0, 35,  It is eaten while held in the hand., {sentence -> 0, chunk -> 0, score -> 0.55574197}, []}]|\n",
      "|[{chunk, 0, 5,  Italy, {sentence -> 0, chunk -> 0, score -> 0.3497405}, []}]                                |\n",
      "|[{chunk, 0, 8,  Elephant, {sentence -> 0, chunk -> 0, score -> 0.28558698}, []}]                            |\n",
      "|[{chunk, 0, 5,  100Â°C, {sentence -> 0, chunk -> 0, score -> 0.34499714}, []}]                               |\n",
      "|[{chunk, 0, 4,  Mars, {sentence -> 0, chunk -> 0, score -> 0.3803456}, []}]                                 |\n",
      "|[{chunk, 0, 10,  Portuguese, {sentence -> 0, chunk -> 0, score -> 0.36515844}, []}]                         |\n",
      "|[{chunk, 0, 11,  The Mongols, {sentence -> 0, chunk -> 0, score -> 0.2663425}, []}]                         |\n",
      "|[{chunk, 0, 6,  Osmium, {sentence -> 0, chunk -> 0, score -> 0.35382026}, []}]                              |\n",
      "|[{chunk, 0, 13,  South America, {sentence -> 0, chunk -> 0, score -> 0.38049418}, []}]                      |\n",
      "|[{chunk, 0, 13,  Pablo Picasso, {sentence -> 0, chunk -> 0, score -> 0.3762705}, []}]                       |\n",
      "+------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_assembler = MultiDocumentAssembler() \\\n",
    "            .setInputCols([\"question\", \"choices\"]) \\\n",
    "            .setOutputCols([\"document_question\", \"document_choices\"])\n",
    "\n",
    "albert_for_multiple_choice = AlbertForMultipleChoice() \\\n",
    "    .load(\"./{}_spark_nlp_onnx\".format(MODEL_NAME)) \\\n",
    "    .setInputCols([\"document_question\", \"document_choices\"])\\\n",
    "    .setOutputCol(\"answer\") \\\n",
    "    .setBatchSize(4)\n",
    "\n",
    "pipeline = Pipeline(stages=[document_assembler, albert_for_multiple_choice])\n",
    "pipeline_model = pipeline.fit(testing_df)\n",
    "\n",
    "pipeline_df = pipeline_model.transform(testing_df)\n",
    "\n",
    "pipeline_df.select(\"answer\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
