{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MqjWWyKO75Q"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_RoBertaForSequenceClassification.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YNxPhkpO75S"
      },
      "source": [
        "## Import ONNX RoBertaForSequenceClassification models from HuggingFace ü§ó  into Spark NLP üöÄ\n",
        "\n",
        "Let's keep in mind a few things before we start üòä\n",
        "\n",
        "- ONNX support was introduced in  `Spark NLP 5.0.0`, enabling high performance inference for models.\n",
        "- `RoBertaForSequenceClassification` is only available since in `Spark NLP 5.1.4` and after. So please make sure you have upgraded to the latest Spark NLP release\n",
        "- You can import RoBERTa models trained/fine-tuned for sequence classification via `RobertaForSequenceClassification` or `TFRobertaForSequenceClassification`. These models are usually under `Text Classification` category and have `roberta` in their labels\n",
        "- Reference: [TFRobertaForSequenceClassification](https://huggingface.co/docs/transformers/model_doc/roberta#transformers.TFRobertaForSequenceClassification)\n",
        "- Some [example models](https://huggingface.co/models?filter=roberta&pipeline_tag=text-classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pPuG0BQO75T"
      },
      "source": [
        "## Export and Save HuggingFace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFGIGL0AO75U"
      },
      "source": [
        "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.52.3`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSUnpqGMO75V",
        "outputId": "d77e8b1e-f10f-4a65-e1b2-31bdefe335dc"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade transformers[onnx]==4.52.3 optimum onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkDFFkVGO75W"
      },
      "source": [
        "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to import and export ONNX models with `from_pretrained` and `save_pretrained`.\n",
        "- We'll use [arpanghoshal/EmoRoBERTa](https://huggingface.co/arpanghoshal/EmoRoBERTa) model from HuggingFace as an example and load it as a `ORTModelForSequenceClassification`, representing an ONNX model.\n",
        "- In addition to the RoBERTa model, we also need to save the tokenizer. This is the same for every model, these are assets (saved in `/assets`) needed for tokenization inside Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HJ0upOs01o_",
        "outputId": "c2e0ef43-a944-494e-e2e2-fc6ad40d81a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No ONNX files were found for cardiffnlp/twitter-roberta-base-sentiment-latest, setting `export=True` to convert the model to ONNX. Don't forget to save the resulting model with `.save_pretrained()`\n",
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('onnx_models/cardiffnlp/twitter-roberta-base-sentiment-latest/tokenizer_config.json',\n",
              " 'onnx_models/cardiffnlp/twitter-roberta-base-sentiment-latest/special_tokens_map.json',\n",
              " 'onnx_models/cardiffnlp/twitter-roberta-base-sentiment-latest/vocab.json',\n",
              " 'onnx_models/cardiffnlp/twitter-roberta-base-sentiment-latest/merges.txt',\n",
              " 'onnx_models/cardiffnlp/twitter-roberta-base-sentiment-latest/added_tokens.json')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import RobertaTokenizer\n",
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "\n",
        "MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
        "ONNX_MODEL = f\"onnx_models/{MODEL_NAME}\"\n",
        "\n",
        "ort_model = ORTModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "ort_model.save_pretrained(ONNX_MODEL)\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.save_pretrained(ONNX_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5cXgoC-O75Y"
      },
      "source": [
        "Let's have a look inside these two directories and see what we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPFktVF4O75Z",
        "outputId": "71ce6b5d-986d-4330-8f62-0a8fbd4d4018"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 488660\n",
            "-rw-r--r-- 1 root root       844 Jun 16 16:38 config.json\n",
            "-rw-r--r-- 1 root root    456318 Jun 16 16:39 merges.txt\n",
            "-rw-r--r-- 1 root root 498911192 Jun 16 16:39 model.onnx\n",
            "-rw-r--r-- 1 root root       958 Jun 16 16:39 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1250 Jun 16 16:39 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    999355 Jun 16 16:39 vocab.json\n"
          ]
        }
      ],
      "source": [
        "!ls -l {ONNX_MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbMZq12r1g9j"
      },
      "source": [
        "- We need to convert `vocab.json` to a plain `vocab.txt` format, as required by Spark NLP.\n",
        "- Move both `vocab.txt` and `merges.txt` into the assets folder.\n",
        "- Additionally, we need to extract label-to-ID mappings from the model config and save them as `labels.txt` in the same folder for Spark NLP to use during inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Qoo3dwykmO74"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "!mkdir -p {ONNX_MODEL}/assets && mv {ONNX_MODEL}/merges.txt {ONNX_MODEL}/assets/\n",
        "\n",
        "with open(f\"{ONNX_MODEL}/vocab.json\") as f, open(f\"{ONNX_MODEL}/assets/vocab.txt\", \"w\") as out:\n",
        "    out.write(\"\\n\".join(json.load(f)))\n",
        "\n",
        "with open(f\"{ONNX_MODEL}/assets/labels.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(ort_model.config.id2label[k] for k in sorted(ort_model.config.id2label)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARYqrWPl2bVm",
        "outputId": "cf844c90-cd46-4cb9-9791-88b870ba1941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "onnx_models/cardiffnlp/twitter-roberta-base-sentiment-latest:\n",
            "total 488216\n",
            "drwxr-xr-x 2 root root      4096 Jun 16 16:39 assets\n",
            "-rw-r--r-- 1 root root       844 Jun 16 16:38 config.json\n",
            "-rw-r--r-- 1 root root 498911192 Jun 16 16:39 model.onnx\n",
            "-rw-r--r-- 1 root root       958 Jun 16 16:39 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1250 Jun 16 16:39 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    999355 Jun 16 16:39 vocab.json\n",
            "\n",
            "onnx_models/cardiffnlp/twitter-roberta-base-sentiment-latest/assets:\n",
            "total 852\n",
            "-rw-r--r-- 1 root root     25 Jun 16 16:39 labels.txt\n",
            "-rw-r--r-- 1 root root 456318 Jun 16 16:39 merges.txt\n",
            "-rw-r--r-- 1 root root 407064 Jun 16 16:39 vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -lR {ONNX_MODEL}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXJnzNU02Mzp",
        "outputId": "007c9376-8fab-47e4-fc9e-6af00b56eaa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "negative\n",
            "neutral\n",
            "positive"
          ]
        }
      ],
      "source": [
        "!cat {ONNX_MODEL}/assets/labels.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXcHwPxAO75a"
      },
      "source": [
        "Voila! We have our `vocab.txt`, `merges.txt` and `labels.txt` inside assets directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_gVE_rDO75b"
      },
      "source": [
        "## Import and Save RoBertaForSequenceClassification in Spark NLP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPEPvm25O75b"
      },
      "source": [
        "- **Install and set up Spark NLP in Google Colab**\n",
        "  - This example uses specific versions of `pyspark` and `spark-nlp` that have been tested with the transformer model to ensure everything runs smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqynCNmYO75c",
        "outputId": "adb34bef-9482-40bf-c816-9afe00527d2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m831.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark==3.5.4 spark-nlp==5.5.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz_ANjp1O75c"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flmq4vloO75c",
        "outputId": "1837d070-ee9d-4918-ee90-6c7ddb3b045a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark NLP version: 5.5.3\n",
            "Apache Spark version: 3.5.4\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: {}\".format(sparknlp.version()))\n",
        "print(\"Apache Spark version: {}\".format(spark.version))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EohL2eY0O75c"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `RoBertaForSequenceClassification` which allows us to load TensorFlow model in SavedModel format\n",
        "- Most params can be set later when you are loading this model in `RoBertaForSequenceClassification` in runtime like `setMaxSentenceLength`, so don't worry what you are setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the TF SavedModel. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uTAe0RJwO75d"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import RoBertaForSequenceClassification\n",
        "\n",
        "sequenceClassifier = RoBertaForSequenceClassification.loadSavedModel(\n",
        "    ONNX_MODEL,\n",
        "    spark\n",
        ")\\\n",
        "  .setInputCols([\"document\",'token'])\\\n",
        "  .setOutputCol(\"class\")\\\n",
        "  .setCaseSensitive(True)\\\n",
        "  .setMaxSentenceLength(128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ro3JpgUO75d"
      },
      "source": [
        "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "n7p21VZ_O75d"
      },
      "outputs": [],
      "source": [
        "sequenceClassifier.write().overwrite().save(\"./{}_spark_nlp_onnx\".format(ONNX_MODEL))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F-kMx2ZO75d"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9G1-SZKxO75d"
      },
      "outputs": [],
      "source": [
        "!rm -rf {ONNX_MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnplO9XoO75d"
      },
      "source": [
        "Awesome üòé  !\n",
        "\n",
        "This is your RoBertaForSequenceClassification model from HuggingFace ü§ó  loaded and saved by Spark NLP üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yws9LXLO75d",
        "outputId": "e26232c6-bcd4-44c4-d0d6-b04be92d3d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 487308\n",
            "drwxr-xr-x 5 root root      4096 Jun 16 16:42 fields\n",
            "drwxr-xr-x 2 root root      4096 Jun 16 16:42 metadata\n",
            "-rw-r--r-- 1 root root 498987456 Jun 16 16:42 roberta_classification_onnx\n"
          ]
        }
      ],
      "source": [
        "! ls -l {ONNX_MODEL}_spark_nlp_onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11cMxMFyO75e"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny RoBertaForSequenceClassification model üòä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1KAE0i27O75e"
      },
      "outputs": [],
      "source": [
        "sequenceClassifier_loaded = RoBertaForSequenceClassification.load(\"./{}_spark_nlp_onnx\".format(ONNX_MODEL))\\\n",
        "    .setInputCols([\"document\",'token'])\\\n",
        "    .setOutputCol(\"class\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33bj888GO75e"
      },
      "source": [
        "You can see what labels were used to train this model via `getClasses` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NgQBiYjO75e",
        "outputId": "d42f7ea6-b70e-4f1f-b67e-a8e1c62006fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['neutral', 'positive', 'negative']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequenceClassifier_loaded.getClasses()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX0-nI2XO75f"
      },
      "source": [
        "This is how you can use your loaded classifier model in Spark NLP üöÄ pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySrUr5ODO75f",
        "outputId": "439b8933-05e2-46ad-b0d1-733cc1df3e7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+----------+\n",
            "|text         |result    |\n",
            "+-------------+----------+\n",
            "|I love you!  |[negative]|\n",
            "|Kill yourself|[neutral] |\n",
            "+-------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import Tokenizer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    tokenizer,\n",
        "    sequenceClassifier_loaded\n",
        "])\n",
        "\n",
        "data = spark.createDataFrame([\n",
        "    [\"I love you!\"],\n",
        "    [\"Kill yourself\"]\n",
        "], [\"text\"])\n",
        "\n",
        "result = pipeline.fit(data).transform(data)\n",
        "result.select(\"text\", \"class.result\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abtb9pl0O75g"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of `RoBertaForSequenceClassification` models from HuggingFace ü§ó in Spark NLP üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
