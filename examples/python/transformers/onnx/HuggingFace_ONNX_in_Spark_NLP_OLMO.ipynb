{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB-OotnsS-JG"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_OLMO.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRuRMH7QS-JI"
      },
      "source": [
        "## Import ONNX OLMO models from HuggingFace 🤗 into Spark NLP 🚀\n",
        "\n",
        "Let's keep in mind a few things before we start 😊\n",
        "\n",
        "- ONNX support was introduced in  `Spark NLP 5.0.0`, enabling high performance inference for models.\n",
        "- You can import OLMO models via `OLMOModel`. These models are usually under `Text2Text Generation` category and have `OLMO` in their labels\n",
        "- This is a very computationally expensive module especially on larger sequence. The use of an accelerator such as GPU is recommended.\n",
        "- Reference: [OLMOModel](https://huggingface.co/docs/transformers/en/model_doc/OLMO)\n",
        "- Some [example models](https://huggingface.co/models?other=OLMO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd98DUZxS-JJ"
      },
      "source": [
        "## Export and Save HuggingFace model\n",
        "\n",
        "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.41.0`. This doesn't mean it won't work with the future releases\n",
        "- We will also need `sentencepiece` for tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wFf3GagOS-JJ",
        "outputId": "78b6529d-afad-414c-baa3-e8087061072f"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade transformers[onnx]==4.41.0\n",
        "!pip install optimum sentencepiece onnx onnxruntime ai2-olmo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX1TUzkhS-JK"
      },
      "source": [
        "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to import and export ONNX models with `from_pretrained` and `save_pretrained`.\n",
        "- We'll use [allenai/OLMo-1B-hf](https://huggingface.co/allenai/OLMo-1B-hf) model from HuggingFace as an example\n",
        "- In addition to `OLMO` we also need to save the tokenizer. This is the same for every model, these are assets needed for tokenization inside Spark NLP.\n",
        "- If we want to optimize the model, a GPU will be needed. Make sure to select the correct runtime.\n",
        "0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-ibF3eK_S-JK"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "MODEL_NAME = \"allenai/OLMo-1B-hf\"\n",
        "\n",
        "\n",
        "# Path to store the exported models\n",
        "EXPORT_PATH = f\"onnx_models/{MODEL_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kDH5EpwnS-JK",
        "outputId": "e32328ad-45b3-4d6c-d5d6-d2da88dbbd4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/prabod/anaconda3/envs/olmo/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100%|█████████████████████████████| 632/632 [00:00<00:00, 38.6kB/s]\n",
            "model.safetensors: 100%|███████████████████| 4.71G/4.71G [03:24<00:00, 23.1MB/s]\n",
            "generation_config.json: 100%|██████████████████| 116/116 [00:00<00:00, 12.9kB/s]\n",
            "tokenizer_config.json: 100%|████████████████| 5.37k/5.37k [00:00<00:00, 698kB/s]\n",
            "tokenizer.json: 100%|██████████████████████| 2.12M/2.12M [00:00<00:00, 2.45MB/s]\n",
            "special_tokens_map.json: 100%|███████████████| 65.0/65.0 [00:00<00:00, 25.5kB/s]\n",
            "/home/prabod/anaconda3/envs/olmo/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/home/prabod/anaconda3/envs/olmo/lib/python3.9/site-packages/transformers/models/olmo/modeling_olmo.py:1039: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if sequence_length != 1:\n",
            "Weight deduplication check in the ONNX export requires accelerate. Please install accelerate to run it.\n",
            "\t\t-[x] values not close enough, max diff: 0.0007228851318359375 (atol: 0.0001)\n",
            "The ONNX export succeeded with the warning: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 0.0001:\n",
            "- logits: max diff = 0.0007228851318359375.\n",
            " The exported model was saved at: onnx_models/allenai/OLMo-1B-hf\n"
          ]
        }
      ],
      "source": [
        "!optimum-cli export onnx  --trust-remote-code --task text-generation --model {MODEL_NAME} {EXPORT_PATH} "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDAyLDCcS-JL"
      },
      "source": [
        "Let's have a look inside these two directories and see what we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jp2ssmF2S-JL",
        "outputId": "7c3379db-18cd-4990-de7e-51e9b4eada8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 5001720\n",
            "-rw-rw-r-- 1 prabod prabod        646 Feb 12 03:51 config.json\n",
            "-rw-rw-r-- 1 prabod prabod        111 Feb 12 03:51 generation_config.json\n",
            "-rw-rw-r-- 1 prabod prabod     468660 Feb 12 03:52 model.onnx\n",
            "-rw-rw-r-- 1 prabod prabod 5119148032 Feb 12 03:52 model.onnx_data\n",
            "-rw-rw-r-- 1 prabod prabod        293 Feb 12 03:51 special_tokens_map.json\n",
            "-rw-rw-r-- 1 prabod prabod       5372 Feb 12 03:51 tokenizer_config.json\n",
            "-rw-rw-r-- 1 prabod prabod    2115417 Feb 12 03:51 tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ-z0eSzS-JL"
      },
      "source": [
        "- As you can see, we need to move the sentence piece models `spiece.model` from the tokenizer to assets folder which Spark NLP will look for"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/prabod/anaconda3/envs/olmo/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('onnx_models/allenai/OLMo-1B-hf/assets/tokenizer_config.json',\n",
              " 'onnx_models/allenai/OLMo-1B-hf/assets/special_tokens_map.json',\n",
              " 'onnx_models/allenai/OLMo-1B-hf/assets/tokenizer.json')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "from pathlib import Path\n",
        "model_id = 'allenai/OLMo-1B-hf'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)\n",
        "config = AutoConfig.from_pretrained(model_id,trust_remote_code=True)\n",
        "\n",
        "\n",
        "ASSETS_PATH = f\"{EXPORT_PATH}/assets\"\n",
        "\n",
        "\n",
        "\n",
        "# make sure the directory exists\n",
        "Path(ASSETS_PATH).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "config.save_pretrained(ASSETS_PATH)\n",
        "tokenizer.save_vocabulary(ASSETS_PATH)\n",
        "\n",
        "tokenizer.save_pretrained(ASSETS_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "062OnFBIS-JL"
      },
      "outputs": [],
      "source": [
        "! mkdir -p {EXPORT_PATH}/assets\n",
        "! mv -t {EXPORT_PATH}/assets {EXPORT_PATH}/merges.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xZMCq14PUdrG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open(f\"{ASSETS_PATH}/vocab.json\", \"r\") as F:\n",
        "    vocab_json = json.load(F)\n",
        "    vocab = [\"\" for i in range(len(vocab_json))]\n",
        "    for word in vocab_json:\n",
        "        vocab[vocab_json[word]] = word\n",
        "    with open(f\"{ASSETS_PATH}/vocab.txt\", \"w\") as F2:\n",
        "        F2.writelines(map(lambda x: str(x) + \"\\n\", vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3fbDIHVFS-JL",
        "outputId": "ebe0a435-3c5c-4c20-df51-534397802fbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 3716\n",
            "-rw-rw-r-- 1 prabod prabod     673 Feb 12 03:59 config.json\n",
            "-rw-rw-r-- 1 prabod prabod  456598 Feb 12 03:59 merges.txt\n",
            "-rw-rw-r-- 1 prabod prabod     293 Feb 12 03:59 special_tokens_map.json\n",
            "-rw-rw-r-- 1 prabod prabod    5372 Feb 12 03:59 tokenizer_config.json\n",
            "-rw-rw-r-- 1 prabod prabod 2115417 Feb 12 03:59 tokenizer.json\n",
            "-rw-rw-r-- 1 prabod prabod  799451 Feb 12 03:59 vocab.json\n",
            "-rw-rw-r-- 1 prabod prabod  407614 Feb 12 04:00 vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}/assets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-12 04:30:03,971 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:03,994 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:03,995 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:04,016 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:04,017 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:04,039 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:04,041 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/self_attn/rotary_emb/MatMul ...\n",
            "2025-02-12 04:30:04,042 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:04,043 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/self_attn/MatMul ...\n",
            "2025-02-12 04:30:04,045 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:04,046 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/self_attn/MatMul_1 ...\n",
            "2025-02-12 04:30:04,047 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:04,048 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:04,073 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:04,074 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:04,186 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:04,192 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:04,279 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:04,283 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:04,370 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:04,373 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:04,402 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:04,403 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:04,422 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:04,423 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:04,442 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:04,444 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/self_attn/MatMul ...\n",
            "2025-02-12 04:30:04,445 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:04,446 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/self_attn/MatMul_1 ...\n",
            "2025-02-12 04:30:04,447 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:04,448 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:04,470 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:04,471 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:04,569 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:04,573 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:04,672 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:04,676 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:04,775 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:04,779 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:04,807 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:04,808 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:04,827 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:04,828 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:04,848 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:04,849 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/self_attn/MatMul ...\n",
            "2025-02-12 04:30:04,850 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:04,851 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/self_attn/MatMul_1 ...\n",
            "2025-02-12 04:30:04,852 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:04,855 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:04,874 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:04,875 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:04,964 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:04,968 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:05,057 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:05,060 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:05,151 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:05,155 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:05,183 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:05,184 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:05,203 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:05,204 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:05,223 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:05,224 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/self_attn/MatMul ...\n",
            "2025-02-12 04:30:05,225 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:05,226 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/self_attn/MatMul_1 ...\n",
            "2025-02-12 04:30:05,227 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:05,228 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:05,250 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:05,251 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:05,348 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:05,352 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:05,459 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:05,464 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:05,564 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:05,568 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:05,601 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:05,602 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:05,623 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:05,624 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:05,645 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:05,646 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/self_attn/MatMul ...\n",
            "2025-02-12 04:30:05,647 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:05,649 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/self_attn/MatMul_1 ...\n",
            "2025-02-12 04:30:05,650 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:05,651 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:05,671 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:05,672 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:05,768 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:05,772 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:05,859 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:05,863 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:05,952 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:05,956 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:05,989 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:05,990 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:06,010 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:06,011 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:06,032 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:06,033 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/self_attn/MatMul ...\n",
            "2025-02-12 04:30:06,034 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:06,036 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/self_attn/MatMul_1 ...\n",
            "2025-02-12 04:30:06,037 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:06,038 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:06,061 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:06,062 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:06,175 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:06,182 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:06,268 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:06,272 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:06,368 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:06,375 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:06,403 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:06,404 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:06,423 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:06,424 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:06,443 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:06,445 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/self_attn/MatMul ...\n",
            "2025-02-12 04:30:06,446 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:06,447 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/self_attn/MatMul_1 ...\n",
            "2025-02-12 04:30:06,448 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:06,449 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:06,469 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:06,470 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:06,555 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:06,559 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:06,652 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:06,655 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:06,743 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:06,747 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:06,775 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:06,776 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:06,795 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:06,796 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:06,815 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:06,816 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/self_attn/MatMul ...\n",
            "2025-02-12 04:30:06,818 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:06,819 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/self_attn/MatMul_1 ...\n",
            "2025-02-12 04:30:06,820 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:06,821 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:06,844 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:06,846 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:06,947 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:06,952 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:07,053 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:07,058 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:07,161 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:07,166 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:07,198 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:07,199 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:07,220 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:07,221 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:07,241 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:07,243 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/self_attn/MatMul ...\n",
            "2025-02-12 04:30:07,244 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:07,245 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/self_attn/MatMul_1 ...\n",
            "2025-02-12 04:30:07,246 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:07,247 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:07,268 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:07,269 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:07,356 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:07,360 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:07,445 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:07,449 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:07,540 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:07,544 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:07,571 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:07,572 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:07,591 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:07,592 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:07,613 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:07,615 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/self_attn/MatMul ...\n",
            "2025-02-12 04:30:07,616 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:07,617 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/self_attn/MatMul_1 ...\n",
            "2025-02-12 04:30:07,618 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:07,619 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:07,640 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:07,641 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:07,734 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:07,739 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:07,844 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:07,849 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:07,948 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:07,951 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:07,980 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:07,981 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:08,001 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:08,002 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:08,022 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:08,023 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/self_attn/MatMul ...\n",
            "2025-02-12 04:30:08,025 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:08,026 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/self_attn/MatMul_1 ...\n",
            "2025-02-12 04:30:08,027 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:08,028 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:08,047 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:08,048 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:08,135 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:08,141 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:08,226 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:08,230 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:08,315 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:08,319 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:08,348 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:08,349 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:08,368 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:08,369 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:08,388 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:08,389 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/self_attn/MatMul ...\n",
            "2025-02-12 04:30:08,391 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:08,392 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/self_attn/MatMul_1 ...\n",
            "2025-02-12 04:30:08,393 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:08,394 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:08,415 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:08,416 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:08,521 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:08,525 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:08,630 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:08,634 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:08,738 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:08,742 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:08,775 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:08,776 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:08,797 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:08,798 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:08,818 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:08,820 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/self_attn/MatMul ...\n",
            "2025-02-12 04:30:08,821 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:08,822 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/self_attn/MatMul_1 ...\n",
            "2025-02-12 04:30:08,823 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:08,824 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:08,846 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:08,847 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:08,929 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:08,933 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:09,025 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:09,029 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:09,118 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:09,122 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:09,151 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:09,152 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:09,171 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:09,172 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:09,191 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:09,193 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/self_attn/MatMul ...\n",
            "2025-02-12 04:30:09,194 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:09,195 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/self_attn/MatMul_1 ...\n",
            "2025-02-12 04:30:09,197 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:09,198 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:09,219 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:09,220 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:09,308 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:09,311 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:09,399 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:09,402 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:09,489 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:09,492 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:09,520 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:09,521 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:09,540 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:09,541 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:09,560 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:09,561 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/self_attn/MatMul ...\n",
            "2025-02-12 04:30:09,563 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:09,564 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/self_attn/MatMul_1 ...\n",
            "2025-02-12 04:30:09,565 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:09,566 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:09,587 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:09,588 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:09,713 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:09,717 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:09,842 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:09,847 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:09,973 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:09,976 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:10,004 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/self_attn/q_proj/MatMul ...\n",
            "2025-02-12 04:30:10,005 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:10,024 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/self_attn/k_proj/MatMul ...\n",
            "2025-02-12 04:30:10,025 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:10,044 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/self_attn/v_proj/MatMul ...\n",
            "2025-02-12 04:30:10,046 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/self_attn/MatMul ...\n",
            "2025-02-12 04:30:10,047 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:10,048 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/self_attn/MatMul_1 ...\n",
            "2025-02-12 04:30:10,050 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - MatMul doesn't have const weight. Skip to quantize\n",
            "2025-02-12 04:30:10,051 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:10,072 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/self_attn/o_proj/MatMul ...\n",
            "2025-02-12 04:30:10,073 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:10,193 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/gate_proj/MatMul ...\n",
            "2025-02-12 04:30:10,198 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:10,326 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/up_proj/MatMul ...\n",
            "2025-02-12 04:30:10,331 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:10,456 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/down_proj/MatMul ...\n",
            "2025-02-12 04:30:10,462 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /lm_head/MatMul ...\n",
            "2025-02-12 04:30:11,248 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /lm_head/MatMul ...\n"
          ]
        }
      ],
      "source": [
        "import onnx\n",
        "# from onnxruntime import quantization as ort_quantization\n",
        "from onnxruntime.quantization.matmul_4bits_quantizer import MatMul4BitsQuantizer\n",
        "\n",
        "Path(f'onnx_models/{model_id}_int4').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model = onnx.load_model(f\"onnx_models/{model_id}/model.onnx\", load_external_data=True)\n",
        "quant = MatMul4BitsQuantizer(\n",
        "    model=model,\n",
        "    block_size=32,\n",
        "    is_symmetric=True,\n",
        "    nodes_to_exclude=[],\n",
        ")\n",
        "quant.process()\n",
        "quant.model.save_model_to_file(f'onnx_models/{model_id}_int4/model.onnx', use_external_data_format=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_id = 'allenai/OLMo-1B-hf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import onnx\n",
        "model = onnx.load(f\"onnx_models/{model_id}_int4/model.onnx\")\n",
        "EXPORT_PATH = f\"onnx_models/{model_id}_int4\"\n",
        "onnx.save_model(model, f\"{EXPORT_PATH}/decoder_model.onnx\", save_as_external_data=True, all_tensors_to_one_file=True, location=\"_olmo_decoder_model.onnx_data\", size_threshold=1024, convert_attribute=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm -rf {EXPORT_PATH}/model.onnx {EXPORT_PATH}/model.onnx_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#copy the assets\n",
        "!cp -r onnx_models/{model_id}/assets onnx_models/{model_id}_int4/assets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /home/prabod/Projects/spark-nlp/python/dist/spark_nlp-5.5.3-py2.py3-none-any.whl\n",
            "Collecting pyspark==3.2.3\n",
            "  Using cached pyspark-3.2.3.tar.gz (281.5 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting py4j==0.10.9.5 (from pyspark==3.2.3)\n",
            "  Using cached py4j-0.10.9.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "spark-nlp is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
            "Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.3-py2.py3-none-any.whl size=281990715 sha256=ec075358b0ed3cc8cae95e6699c93f9e9949e54045ca13ced0d05052e0143361\n",
            "  Stored in directory: /home/prabod/.cache/pip/wheels/cc/f4/8d/dfbbd536587311afde33711613a0c193f18e7d90b120801108\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install /home/prabod/Projects/spark-nlp/python/dist/spark_nlp-5.5.3-py2.py3-none-any.whl pyspark==3.2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZZqEbvvS-JM"
      },
      "source": [
        "## Import and Save OLMO in Spark NLP\n",
        "\n",
        "- Let's install and setup Spark NLP in Google Colab\n",
        "- This part is pretty easy via our simple script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SLlypPRaS-JM",
        "outputId": "54ab8af5-a1cb-4c29-f982-2f5aac5e6e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing PySpark 3.2.3 and Spark NLP 5.4.2\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 5.4.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m579.5/579.5 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEy-zFjnS-JM"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0KOd7hwNS-JM",
        "outputId": "8e408b69-db08-42f5-9d14-c163034f9c04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spark-nlp==5.5.0rc1\n",
            "  Downloading spark_nlp-5.5.0rc1-py2.py3-none-any.whl.metadata (55 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spark_nlp-5.5.0rc1-py2.py3-none-any.whl (629 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/629.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m624.6/629.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m629.6/629.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "  Attempting uninstall: spark-nlp\n",
            "    Found existing installation: spark-nlp 5.4.2\n",
            "    Uninstalling spark-nlp-5.4.2:\n",
            "      Successfully uninstalled spark-nlp-5.4.2\n",
            "Successfully installed spark-nlp-5.5.0rc1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "# let's start Spark with Spark NLP\n",
        "spark = sparknlp.start()\n",
        "print(\"Apache Spark version: {}\".format(spark.version))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgl_T39AS-JM"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `OLMOTransformer` which allows us to load the ONNX model\n",
        "- Most params will be set automatically. They can also be set later after loading the model in `OLMOTransformer` during runtime, so don't worry about setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the exported model. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ij_8ZwLxS-JM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Could not extract bos_token_id from config.json, assigning default value -1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/prabod/spark/jars/spark-core_2.12-3.3.2.jar) to field java.util.regex.Pattern.pattern\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.annotator import *\n",
        "\n",
        "olmo = OLMoTransformer.loadSavedModel(EXPORT_PATH, spark)\\\n",
        "  .setInputCols([\"documents\"])\\\n",
        "  .setMaxOutputLength(100)\\\n",
        "  .setDoSample(False)\\\n",
        "  .setOutputCol(\"generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_eeGHNZS-JM"
      },
      "source": [
        "Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0rmW0bXLS-JM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "olmo.write().overwrite().save(f\"/tmp/{MODEL_NAME}_spark_nlp_int4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnmGJlakS-JM"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kWkdSCjIS-JN"
      },
      "outputs": [],
      "source": [
        "!rm -rf {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9YtKl-aS-JN"
      },
      "source": [
        "Awesome  😎 !\n",
        "\n",
        "This is your ONNX OLMO model from HuggingFace 🤗  loaded and saved by Spark NLP 🚀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9nbzEjwWS-JN",
        "outputId": "4b20ba7c-41c5-440f-89c8-fd4e6a0ec541"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 1121168\n",
            "-rw-r--r-- 1 prabod prabod     496159 Feb 12 11:54 decoder_model.onnx\n",
            "drwxr-xr-x 5 prabod prabod       4096 Feb 12 11:54 fields\n",
            "drwxr-xr-x 2 prabod prabod       4096 Feb 12 11:54 metadata\n",
            "-rw-r--r-- 1 prabod prabod 1147568128 Feb 12 11:54 _olmo_decoder_model.onnx_data\n"
          ]
        }
      ],
      "source": [
        "! ls -l /tmp/{MODEL_NAME}_spark_nlp_int4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcNqKR7mS-JN"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny OLMO model 😊"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DZyaiumUS-JN",
        "outputId": "d7db52cb-b85d-4d9a-fd94-24e5b0af7f4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CPUs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 21:======================================================> (30 + 1) / 31]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |document                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |generation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new Colossal Clean Crawled Corpus, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.|[{document, 0, 1008, Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new Colossal Clean Crawled Corpus, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code., {sentence -> 0}, []}]|[{document, 0, 1195, Transfer learning , where a model is first pre - trained on a data - rich task before being fine - tuned on a downstream task , has emerged as a powerful technique in natural language processing ( NLP ). The effectiveness of transfer learning has given rise to a diversity of approaches , methodology , and practice . In this paper , we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text - based language problems into a text - to - text format . Our systematic study compares pre - training objectives , architectures , unlabeled data sets , transfer approaches , and other factors on dozens of language understanding tasks . By combining the insights from our exploration with scale and our new Colossal Clean Crawled Corpus , we achieve state - of - the - art results on many benchmarks covering summarization , question answering , text classification , and more . To facilitate future work on transfer learning for NLP , we release our data set , pre - trained models , and code . We also release the Colossala testset and a full report on our results , which we provide for researchers . The paper is available at https ., {sentence -> 0}, []}]|\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "test_data = spark.createDataFrame([\n",
        "    [\"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a \" +\n",
        "       \"downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness\" +\n",
        "       \" of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this \" +\n",
        "       \"paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework \" +\n",
        "       \"that converts all text-based language problems into a text-to-text format. Our systematic study compares \" +\n",
        "       \"pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens \" +\n",
        "       \"of language understanding tasks. By combining the insights from our exploration with scale and our new \" +\n",
        "       \"Colossal Clean Crawled Corpus, we achieve state-of-the-art results on many benchmarks covering \" +\n",
        "       \"summarization, question answering, text classification, and more. To facilitate future work on transfer \" +\n",
        "       \"learning for NLP, we release our data set, pre-trained models, and code.\"]\n",
        "]).toDF(\"text\")\n",
        "\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "olmo = OLMoTransformer.load(f\"file:///tmp/{MODEL_NAME}_spark_nlp_int4\")\\\n",
        "      .setInputCols([\"document\"])\\\n",
        "      .setMaxOutputLength(50)\\\n",
        "      .setDoSample(True)\\\n",
        "      .setTopK(50)\\\n",
        "      .setTemperature(0)\\\n",
        "      .setBatchSize(5)\\\n",
        "      .setNoRepeatNgramSize(3)\\\n",
        "      .setOutputCol(\"generation\")\n",
        "\n",
        "pipeline = Pipeline().setStages([document_assembler, olmo])\n",
        "\n",
        "result = pipeline.fit(test_data).transform(test_data)\n",
        "result.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTnIQ3HKS-JN"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of OLMO models from HuggingFace 🤗 in Spark NLP 🚀\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "olmo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
