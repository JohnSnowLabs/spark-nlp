{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPBGFPQY-Pti"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_T5.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXVi9YX8-Ptj"
      },
      "source": [
        "## Import ONNX T5 models from HuggingFace ü§ó into Spark NLP üöÄ\n",
        "\n",
        "Let's keep in mind a few things before we start üòä\n",
        "\n",
        "- ONNX support was introduced in  `Spark NLP 5.0.0`, enabling high performance inference for models.\n",
        "- ONNX support for the `T5Transformer` is only available since in `Spark NLP 5.2.0` and after. So please make sure you have upgraded to the latest Spark NLP release\n",
        "- You can import T5 models via `T5Model`. These models are usually under `Text2Text Generation` category and have `T5` in their labels\n",
        "- This is a very computationally expensive module especially on larger sequence. The use of an accelerator such as GPU is recommended.\n",
        "- Reference: [T5Model](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model)\n",
        "- Some [example models](https://huggingface.co/models?other=T5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqjyZc25-Ptj"
      },
      "source": [
        "## Export and Save HuggingFace model\n",
        "\n",
        "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.52.3`. This doesn't mean it won't work with the future releases\n",
        "- We will also need `sentencepiece` for tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi96KLA2j5Sf",
        "outputId": "d8bcac12-471a-4ea1-c641-7307cc384274"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade transformers[onnx]==4.52.3 optimum onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76aT0S2L-Ptk"
      },
      "source": [
        "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to import and export ONNX models\n",
        "- We'll use [google/flan-t5-base](https://huggingface.co/google/flan-t5-base) model from HuggingFace as an example\n",
        "- In addition to `T5Model` we also need to save the tokenizer. This is the same for every model, these are assets needed for tokenization inside Spark NLP.\n",
        "- If we want to optimize the model, a GPU will be needed. Make sure to select the correct runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHN_mkPu-Ptl"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"google/flan-t5-base\"\n",
        "EXPORT_PATH = f\"onnx_models/{MODEL_NAME}\"\n",
        "\n",
        "# Option 1: Export with optimization (O2)\n",
        "# NOTE: May cause issues for T5-small or its variants due to a known ONNX Runtime bug.\n",
        "# To fix it:\n",
        "#  - Option A: Manually edit onnxruntime's source (not recommended in Colab)\n",
        "#  - Option B: Skip optimization (recommended workaround)\n",
        "# !optimum-cli export onnx --task text2text-generation-with-past --model $MODEL_NAME --optimize O2 $EXPORT_PATH\n",
        "\n",
        "# Option 2: Export without optimization (safe for all models)\n",
        "!optimum-cli export onnx --task text2text-generation-with-past --model $MODEL_NAME $EXPORT_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjRL7kWF-Ptl"
      },
      "source": [
        "Let's have a look inside these two directories and see what we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nlwl38WY-Ptm",
        "outputId": "85857de3-c764-43d9-a684-80e6f30497d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 2283468\n",
            "-rw-r--r-- 1 root root      1515 Jun 16 22:01 config.json\n",
            "-rw-r--r-- 1 root root 651215877 Jun 16 22:03 decoder_model_merged.onnx\n",
            "-rw-r--r-- 1 root root 650868767 Jun 16 22:02 decoder_model.onnx\n",
            "-rw-r--r-- 1 root root 594210416 Jun 16 22:02 decoder_with_past_model.onnx\n",
            "-rw-r--r-- 1 root root 438701789 Jun 16 22:01 encoder_model.onnx\n",
            "-rw-r--r-- 1 root root       142 Jun 16 22:01 generation_config.json\n",
            "-rw-r--r-- 1 root root      2543 Jun 16 22:01 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root    791656 Jun 16 22:01 spiece.model\n",
            "-rw-r--r-- 1 root root     20830 Jun 16 22:01 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root   2422234 Jun 16 22:01 tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57oZPy6b-Ptm"
      },
      "source": [
        "- As you can see, we need to move the sentence piece models `spiece.model` from the tokenizer to assets folder which Spark NLP will look for"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x254dmHd-Ptm"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {EXPORT_PATH}/assets && mv -t {EXPORT_PATH}/assets {EXPORT_PATH}/spiece.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkJeK_Ik-Ptm",
        "outputId": "846ac1ee-1cd4-4714-dcd8-01c50ab4115d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 776\n",
            "-rw-r--r-- 1 root root 791656 Jun 16 22:01 spiece.model\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}/assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udCYwISA-Ptn"
      },
      "source": [
        "## Import and Save T5 in Spark NLP\n",
        "\n",
        "- **Install and set up Spark NLP in Google Colab**\n",
        "  - This example uses specific versions of `pyspark` and `spark-nlp` that have been tested with the transformer model to ensure everything runs smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHTvexlp-Ptn",
        "outputId": "11885771-4a1a-40df-f1c6-c27a941f9637"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark==3.5.4 spark-nlp==5.5.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RrMrJ4N-Ptn"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYf0j_it-Ptn",
        "outputId": "dfc84466-1c44-4525-9528-90a8cb3f8e0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark NLP version:  5.5.3\n",
            "Apache Spark version:  3.5.4\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb4xDsIK-Ptn"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `T5Transformer` which allows us to load the ONNX model\n",
        "- Most params will be set automatically. They can also be set later after loading the model in `T5Transformer` during runtime, so don't worry about setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the exported model. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EK1NjP75-Ptn"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import T5Transformer\n",
        "\n",
        "T5 = T5Transformer.loadSavedModel(EXPORT_PATH, spark)\\\n",
        "  .setUseCache(True) \\\n",
        "  .setTask(\"summarize:\") \\\n",
        "  .setMaxOutputLength(200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6gphHNg-Pto"
      },
      "source": [
        "Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XW-NHEVk-Pto"
      },
      "outputs": [],
      "source": [
        "T5.write().overwrite().save(f\"{MODEL_NAME}_spark_nlp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WMzBrET-Pto"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Xnfaj70p-Pto"
      },
      "outputs": [],
      "source": [
        "!rm -rf {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC9p2-s3-Pto"
      },
      "source": [
        "Awesome  üòé !\n",
        "\n",
        "This is your ONNX T5 model from HuggingFace ü§ó  loaded and saved by Spark NLP üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRsMHUF2-Pto",
        "outputId": "0082d621-50e7-4f76-f3c6-1b9059fbe1cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 1065328\n",
            "-rw-r--r-- 1 root root 651315411 Jun 16 22:12 decoder.onxx\n",
            "-rw-r--r-- 1 root root 438768884 Jun 16 22:12 encoder.onxx\n",
            "drwxr-xr-x 2 root root      4096 Jun 16 22:12 metadata\n",
            "-rw-r--r-- 1 root root    791656 Jun 16 22:12 t5_spp\n"
          ]
        }
      ],
      "source": [
        "! ls -l {MODEL_NAME}_spark_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LjFBjbj-Pto"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny T5 model üòä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11qXEdZN-Pto",
        "outputId": "e14143fa-2c78-4465-a034-82858b1879e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------+\n",
            "|result                                                                                                     |\n",
            "+-----------------------------------------------------------------------------------------------------------+\n",
            "|[We introduce a unified framework that converts text-to-text language problems into a text-to-text format.]|\n",
            "+-----------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "test_data = spark.createDataFrame([[\n",
        "    \"\"\"\n",
        "    Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task,\n",
        "    has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has\n",
        "    given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer\n",
        "    learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a\n",
        "    text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer\n",
        "    approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration\n",
        "    with scale and our new Colossal Clean Crawled Corpus, we achieve state-of-the-art results on many benchmarks covering\n",
        "    summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP,\n",
        "    we release our data set, pre-trained models, and code.\n",
        "    \"\"\"\n",
        "]], [\"text\"])\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "T5 = T5Transformer.load(f\"{MODEL_NAME}_spark_nlp\") \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"summary\")\n",
        "\n",
        "pipeline = Pipeline().setStages([\n",
        "    document_assembler,\n",
        "    T5\n",
        "])\n",
        "\n",
        "result = pipeline.fit(test_data).transform(test_data)\n",
        "result.select(\"summary.result\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4KHg4n4-Ptp"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of T5 models from HuggingFace ü§ó in Spark NLP üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
