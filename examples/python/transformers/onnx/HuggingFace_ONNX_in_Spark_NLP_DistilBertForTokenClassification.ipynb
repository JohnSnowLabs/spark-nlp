{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E_VGHzde-dO"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_DistilBertForTokenClassification.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4i6NVs8e-dS"
      },
      "source": [
        "## Import ONNX DistilBertForTokenClassification models from HuggingFace ðŸ¤—  into Spark NLP ðŸš€\n",
        "\n",
        "Let's keep in mind a few things before we start ðŸ˜Š\n",
        "\n",
        "- ONNX support was introduced in  `Spark NLP 5.0.0`, enabling high performance inference for models.\n",
        "- `DistilBertForTokenClassification` is only available since in `Spark NLP 5.1.3` and after. So please make sure you have upgraded to the latest Spark NLP release\n",
        "- You can import DistilBERT models trained/fine-tuned for token classification via `DistilBertForTokenClassification` or `TFDistilBertForTokenClassification`. These models are usually under `Token Classification` category and have `bert` in their labels\n",
        "- Reference: [TFDistilBertForTokenClassification](https://huggingface.co/transformers/model_doc/distilbert.html#tfdistilbertfortokenclassification)\n",
        "- Some [example models](https://huggingface.co/models?filter=distilbert&pipeline_tag=token-classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nACOQNzle-dT"
      },
      "source": [
        "## Export and Save HuggingFace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDBksbKEe-dT"
      },
      "source": [
        "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.52.4`. This doesn't mean it won't work with the future releases\n",
        "- Albert uses SentencePiece, so we will have to install that as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w813X530e-dU",
        "outputId": "b17d65c5-738e-4c96-ec10-15f2a4342571",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m424.6/424.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m451.2/451.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires flatbuffers>=24.3.25, but you have flatbuffers 2.0.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade transformers[onnx]==4.52.4 optimum onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQxXnjlMe-dV"
      },
      "source": [
        "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to import and export ONNX models with `from_pretrained` and `save_pretrained`.\n",
        "- We'll use [elastic/distilbert-base-cased-finetuned-conll03-english](https://huggingface.co/elastic/distilbert-base-cased-finetuned-conll03-english) model from HuggingFace as an example\n",
        "- In addition to `TFDistilBertForTokenClassification` we also need to save the `DistilBertTokenizer`. This is the same for every model, these are assets needed for tokenization inside Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "from optimum.onnxruntime import ORTModelForTokenClassification\n",
        "\n",
        "MODEL_NAME = 'elastic/distilbert-base-cased-finetuned-conll03-english'\n",
        "ONNX_MODEL = f\"onnx_models/{MODEL_NAME}\"\n",
        "\n",
        "ort_model = ORTModelForTokenClassification.from_pretrained(MODEL_NAME, export=True)\n",
        "ort_model.save_pretrained(ONNX_MODEL)\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.save_pretrained(ONNX_MODEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "0a9d39906619438ca8dfde8a7bb9d4f0",
            "aad68f816d644df8978ad01d09113056",
            "83bae24290c34a628aeb5c825720d7f0",
            "bd5c6a816c8e4f8e970c06886dc0abe6",
            "f86a578466e245bcaf70f7165e2dfb67",
            "c3f7a304bad34588a0fa3e215123bb1a",
            "ef25ce911cbe40b68814c4b9311b6d8d",
            "f426cf0e25684450880bc05302dd704b",
            "7290cf9a323e40e7898635f549d28ff5",
            "e1cd75c48ebd4375a7b3e5c8959bf4a0",
            "468c6b23b6184cf2ab5fccaef2861f46",
            "71888ce752e34fc6b8d83d6dee04f04d",
            "d7a0fb9f3f0744c5874d0db6d4c0470e",
            "65a05b06702d4293a967bc588b902cfa",
            "b3f62b13eaf54379a162a847bb05046b",
            "57d996d820314569a5729f7b4b8b38a9",
            "27d87f3555ea43c7b6a6aa43ae5643f0",
            "40bedeb5cf4246338ff196505ebeb947",
            "9f833d5a884a4cd2bd8ddec51796f6d4",
            "e3ee00f399b2422bac6d2dabc5232665",
            "f8ee9d9123c04af48328e9c4db9e9b7e",
            "61d815603523438a8e502a1395997e94",
            "7bde8c52c83b4a84b3c49e5f0e67eb27",
            "4fb8c9a316224fc98c45ab6ce234b1b9",
            "76133f1072ba496b9c8c012eb0c6fe49",
            "900b2e2001ae4cf0838e1577b1abf913",
            "67177f1ed2fb43ea8c81379ccee51641",
            "ced05cf47d7240f5a76bfccc93e17e29",
            "8d1c13d982414ea3ac97ade4b3008e39",
            "91a07cbdb21b4d72ae30832880156838",
            "4e0df09a36de4462bf5ae2cc4c721026",
            "55ccbde5b72c48e9a1c3d0e4a5704ff3",
            "9686319251fa4f19b7c64e2a330c7571",
            "f07d1678f12d498fa021bfda05f26f6d",
            "61c2774bda5d45ac8b3fa00a938559de",
            "774d1fd307494ef886207fc579e3a480",
            "8a5533acc12043b8aab35ec4d89dccce",
            "ef8a473414ec4ba88a9dd4c100da48b4",
            "68b1338e370d4e9492a3c3b6d8821f09",
            "fcf77172624f41a394ed11e02ba6ea2a",
            "53b706c94dd149d0aa8413da71374195",
            "f8b795bd671c4db7a8f5d20013a3d221",
            "f650447e7a3e494e8a7d8fefacc3b8ba",
            "1ef4b6660e1d4961b953d93a1a6a5a4a",
            "e1cf90e1724742568a4a2e6bcce77a2f",
            "16624ef49d804fb992c5785e006e694d",
            "994163e6891b457fb5acb3fbe46ec1ca",
            "717af669e03846d89fda0b3e91551d73",
            "1e07e349a7aa481d9ecf93a1a0fd5df8",
            "2731c61a3bd14892a2b48d438bdecf38",
            "a6eda088dddf475081a36fd5f3bcd828",
            "49c0cbbf07934a5eb5027988cbb2c820",
            "25f4c68363e844ae93048698ac424f62",
            "cef0d201ba914f1fae24eb78dbe1a479",
            "605666b1dbe24bf1a846fbf6acbee947"
          ]
        },
        "id": "n3l2dt012U7L",
        "outputId": "2a80a7be-274c-4ee1-e853-3db35e034942"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'TypeError: Failed to fetch'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/954 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a9d39906619438ca8dfde8a7bb9d4f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71888ce752e34fc6b8d83d6dee04f04d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/257 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bde8c52c83b4a84b3c49e5f0e67eb27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f07d1678f12d498fa021bfda05f26f6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1cf90e1724742568a4a2e6bcce77a2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('onnx_models/elastic/distilbert-base-cased-finetuned-conll03-english/tokenizer_config.json',\n",
              " 'onnx_models/elastic/distilbert-base-cased-finetuned-conll03-english/special_tokens_map.json',\n",
              " 'onnx_models/elastic/distilbert-base-cased-finetuned-conll03-english/vocab.txt',\n",
              " 'onnx_models/elastic/distilbert-base-cased-finetuned-conll03-english/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F63hmVite-dW"
      },
      "source": [
        "Let's have a look inside these two directories and see what we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t0apDi-5e-dX",
        "outputId": "4936b75a-3633-4808-b7a6-56c8ff1f1c80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 255044\n",
            "-rw-r--r-- 1 root root       882 Jun 14 00:39 config.json\n",
            "-rw-r--r-- 1 root root 260928908 Jun 14 00:39 model.onnx\n",
            "-rw-r--r-- 1 root root       125 Jun 14 00:39 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1279 Jun 14 00:39 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    213450 Jun 14 00:39 vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -l {ONNX_MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdWDavmje-dX"
      },
      "source": [
        "- As you can see, we need to move `vocab.txt` from the tokenizer to assets folder which Spark NLP will look for\n",
        "- We also need `labels` and their `ids` which is saved inside the model's config. We will save this inside `labels.txt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EnLzR1Gje-dY"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {ONNX_MODEL}/assets\n",
        "\n",
        "labels = ort_model.config.label2id\n",
        "labels = sorted(labels, key=labels.get)\n",
        "\n",
        "with open(f\"{ONNX_MODEL}/assets/labels.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(labels))\n",
        "\n",
        "!mv {ONNX_MODEL}/vocab.txt {ONNX_MODEL}/assets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tiQBkvkCe-dY",
        "outputId": "5b0231db-e94e-4748-c377-5f58616c01b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "onnx_models/elastic/distilbert-base-cased-finetuned-conll03-english:\n",
            "total 254836\n",
            "drwxr-xr-x 2 root root      4096 Jun 14 00:42 assets\n",
            "-rw-r--r-- 1 root root       882 Jun 14 00:39 config.json\n",
            "-rw-r--r-- 1 root root 260928908 Jun 14 00:39 model.onnx\n",
            "-rw-r--r-- 1 root root       125 Jun 14 00:39 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1279 Jun 14 00:39 tokenizer_config.json\n",
            "\n",
            "onnx_models/elastic/distilbert-base-cased-finetuned-conll03-english/assets:\n",
            "total 216\n",
            "-rw-r--r-- 1 root root     51 Jun 14 00:42 labels.txt\n",
            "-rw-r--r-- 1 root root 213450 Jun 14 00:39 vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -lR {ONNX_MODEL}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat {ONNX_MODEL}/assets/labels.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9Uf9HIm2zj2",
        "outputId": "7d12f909-b028-4b28-c58b-847aba66ec66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O\n",
            "B-PER\n",
            "I-PER\n",
            "B-ORG\n",
            "I-ORG\n",
            "B-LOC\n",
            "I-LOC\n",
            "B-MISC\n",
            "I-MISC"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xYe11Zse-dY"
      },
      "source": [
        "Voila! We have our `vocab.txt` and `labels.txt` inside assets directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnOO_JY_e-dZ"
      },
      "source": [
        "## Import and Save DistilBertForTokenClassification in Spark NLP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPuN--_Oe-dZ"
      },
      "source": [
        "- **Install and set up Spark NLP in Google Colab**\n",
        "  - This example uses specific versions of `pyspark` and `spark-nlp` that have been tested with the transformer model to ensure everything runs smoothly.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark==3.5.4 spark-nlp==5.5.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRUUsdZTVThk",
        "outputId": "7e511963-0792-4d69-c263-02426a975856"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGvmRlQJe-dZ"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9EF7jU90e-dZ",
        "outputId": "a619a505-53fb-4942-d44a-7dcd1950b3b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version: 5.5.3\n",
            "Apache Spark version: 3.5.4\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: {}\".format(sparknlp.version()))\n",
        "print(\"Apache Spark version: {}\".format(spark.version))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGJmJNjMe-dZ"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `DistilBertForTokenClassification` which allows us to load TensorFlow model in SavedModel format\n",
        "- Most params can be set later when you are loading this model in `DistilBertForTokenClassification` in runtime like `setMaxSentenceLength`, so don't worry what you are setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the TF SavedModel. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VuLgAt5Le-da"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import DistilBertForTokenClassification\n",
        "\n",
        "tokenClassifier = DistilBertForTokenClassification.loadSavedModel(\n",
        "      ONNX_MODEL,\n",
        "      spark\n",
        "    )\\\n",
        "    .setInputCols([\"document\",'token'])\\\n",
        "    .setOutputCol(\"ner\")\\\n",
        "    .setCaseSensitive(True)\\\n",
        "    .setMaxSentenceLength(128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxrvWd0-e-da"
      },
      "source": [
        "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SSMJHekye-da"
      },
      "outputs": [],
      "source": [
        "tokenClassifier.write().overwrite().save(\"./{}_spark_nlp_onnx\".format(ONNX_MODEL))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBDFWCaEe-da"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "scXC-vrde-da"
      },
      "outputs": [],
      "source": [
        "!rm -rf {ONNX_MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXiOpvpZe-da"
      },
      "source": [
        "Awesome ðŸ˜Ž  !\n",
        "\n",
        "This is your DistilBertForTokenClassification model from HuggingFace ðŸ¤—  loaded and saved by Spark NLP ðŸš€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Kr5EUuxpe-da",
        "outputId": "4af4420a-cf7c-4f3e-a54b-5606da0373de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 254864\n",
            "-rw-r--r-- 1 root root 260968857 Jun 14 00:45 distilbert_classification_onnx\n",
            "drwxr-xr-x 4 root root      4096 Jun 14 00:45 fields\n",
            "drwxr-xr-x 2 root root      4096 Jun 14 00:45 metadata\n"
          ]
        }
      ],
      "source": [
        "! ls -l {ONNX_MODEL}_spark_nlp_onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GgjYsy2e-da"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny DitilBertForTokenClassification model ðŸ˜Š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZwLY5Fw4e-db"
      },
      "outputs": [],
      "source": [
        "tokenClassifier_loaded = DistilBertForTokenClassification.load(\"./{}_spark_nlp_onnx\".format(ONNX_MODEL))\\\n",
        "  .setInputCols([\"document\",'token'])\\\n",
        "  .setOutputCol(\"ner\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqO_e-hHe-db"
      },
      "source": [
        "You can see what labels were used to train this model via `getClasses` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "u6Ouj53Ae-db",
        "outputId": "8d43d5ae-8250-4f81-f4a2-2600b8da9a05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['B-LOC', 'I-ORG', 'I-MISC', 'I-LOC', 'I-PER', 'B-MISC', 'B-ORG', 'O', 'B-PER']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "tokenClassifier_loaded.getClasses()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl35zZD_e-db"
      },
      "source": [
        "This is how you can use your loaded classifier model in Spark NLP ðŸš€ pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "k-umCL6Fe-db",
        "outputId": "b8f4b812-41dc-4099-9f4b-c292114eb7e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------+------------------------------------------------------------------+\n",
            "|text                                                                         |result                                                            |\n",
            "+-----------------------------------------------------------------------------+------------------------------------------------------------------+\n",
            "|Barack Obama was born in Hawaii and served as President of the United States.|[B-PER, I-PER, O, O, O, B-LOC, O, O, O, O, O, O, B-LOC, I-LOC, O] |\n",
            "|Apple Inc. is based in Cupertino and was founded by Steve Jobs.              |[B-ORG, I-ORG, I-ORG, O, O, O, B-LOC, O, O, O, O, B-PER, I-PER, O]|\n",
            "|Cristiano Ronaldo plays for Al-Nassr and has won multiple Ballon d'Or awards.|[B-PER, I-PER, O, O, B-ORG, O, O, O, O, B-ORG, I-MISC, O, O]      |\n",
            "+-----------------------------------------------------------------------------+------------------------------------------------------------------+\n",
            "\n",
            "+-----------------+------+\n",
            "|text             |entity|\n",
            "+-----------------+------+\n",
            "|Barack Obama     |PER   |\n",
            "|Hawaii           |LOC   |\n",
            "|United States    |LOC   |\n",
            "|Apple Inc.       |ORG   |\n",
            "|Cupertino        |LOC   |\n",
            "|Steve Jobs       |PER   |\n",
            "|Cristiano Ronaldo|PER   |\n",
            "|Al-Nassr         |ORG   |\n",
            "|Ballon d'Or      |ORG   |\n",
            "+-----------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import Tokenizer, NerConverter\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "converter = NerConverter() \\\n",
        "    .setInputCols([\"document\", \"token\", \"ner\"]) \\\n",
        "    .setOutputCol(\"ner_chunk\")\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    tokenizer,\n",
        "    tokenClassifier_loaded,\n",
        "    converter\n",
        "])\n",
        "\n",
        "example = spark.createDataFrame([\n",
        "    [\"Barack Obama was born in Hawaii and served as President of the United States.\"],\n",
        "    [\"Apple Inc. is based in Cupertino and was founded by Steve Jobs.\"],\n",
        "    [\"Cristiano Ronaldo plays for Al-Nassr and has won multiple Ballon d'Or awards.\"]\n",
        "]).toDF(\"text\")\n",
        "\n",
        "result = pipeline.fit(example).transform(example)\n",
        "result.select(\"text\", \"ner.result\").show(truncate=False)\n",
        "\n",
        "result.selectExpr(\"explode(ner_chunk) as chunk\").selectExpr(\n",
        "    \"chunk.result as text\",\n",
        "    \"chunk.metadata['entity'] as entity\"\n",
        ").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DOxTJ8le-db"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of `DistlBertForTokenClassification` models from HuggingFace ðŸ¤— in Spark NLP ðŸš€\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}