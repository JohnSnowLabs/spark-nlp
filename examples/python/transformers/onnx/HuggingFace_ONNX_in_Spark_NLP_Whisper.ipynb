{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEdJynTH3L0x"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_Whisper.ipynb)\n",
        "\n",
        "# Import ONNX Whisper models from HuggingFace ğŸ¤— into Spark NLP ğŸš€\n",
        "\n",
        "Let's keep in mind a few things before we start ğŸ˜Š\n",
        "\n",
        "- ONNX support was introduced in `Spark NLP 5.0.0`, enabling high performance inference for models. Please make sure you have upgraded to the latest Spark NLP release.\n",
        "- The Whisper model was introduced in `Spark NLP 5.1.0 and requires Spark version 3.4.1 and up.`\n",
        "- Official models are supported, but not all custom models may work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfiBPTV83L0y"
      },
      "source": [
        "## Export and Save HuggingFace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhUUhv8h3L0z"
      },
      "source": [
        "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.31.0`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sUIG8ym_3ZdY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yy9Ig4tY3L0z",
        "outputId": "648a6286-3ad1-4656-f8fa-06d156549d41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/7.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.3/7.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.2/7.4 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.0/410.0 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade \"transformers[onnx]==4.31.0\" optimum tensorflow onnx==1.14.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_WSgW9w3L00"
      },
      "source": [
        "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to import and export ONNX models with `from_pretrained` and `save_pretrained`.\n",
        "- We'll use the [whisper-tiny](https://huggingface.co/openai/whisper-tiny) model from HuggingFace as an example and export it with the `optimum-cli`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ar3GeeF43L00"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"openai/whisper-tiny\"\n",
        "EXPORT_PATH = f\"export_onnx/{MODEL_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1F7dqTBe3L01",
        "outputId": "9a9d903b-a829-4d9d-d2ed-48948e476e73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-12 10:35:18.194732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Framework not specified. Using pt to export the model.\n",
            "config.json: 100% 1.98k/1.98k [00:00<00:00, 7.28MB/s]\n",
            "model.safetensors: 100% 151M/151M [00:00<00:00, 311MB/s]\n",
            "generation_config.json: 100% 3.75k/3.75k [00:00<00:00, 17.5MB/s]\n",
            "Automatic task detection to automatic-speech-recognition-with-past (possible synonyms are: speech2seq-lm-with-past).\n",
            "tokenizer_config.json: 100% 283k/283k [00:00<00:00, 30.7MB/s]\n",
            "vocab.json: 100% 836k/836k [00:00<00:00, 20.0MB/s]\n",
            "tokenizer.json: 100% 2.48M/2.48M [00:00<00:00, 5.49MB/s]\n",
            "merges.txt: 100% 494k/494k [00:00<00:00, 22.1MB/s]\n",
            "normalizer.json: 100% 52.7k/52.7k [00:00<00:00, 115MB/s]\n",
            "added_tokens.json: 100% 34.6k/34.6k [00:00<00:00, 79.4MB/s]\n",
            "special_tokens_map.json: 100% 2.19k/2.19k [00:00<00:00, 8.36MB/s]\n",
            "preprocessor_config.json: 100% 185k/185k [00:00<00:00, 41.6MB/s]\n",
            "Using the export variant default. Available variants are:\n",
            "    - default: The default ONNX variant.\n",
            "Using framework PyTorch: 2.2.1+cu121\n",
            "Overriding 1 configuration item(s)\n",
            "\t- use_cache -> False\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py:410: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py:449: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
            "Using framework PyTorch: 2.2.1+cu121\n",
            "Overriding 1 configuration item(s)\n",
            "\t- use_cache -> True\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py:1004: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if input_shape[-1] > 1:\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py:417: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
            "Using framework PyTorch: 2.2.1+cu121\n",
            "Overriding 1 configuration item(s)\n",
            "\t- use_cache -> True\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py:372: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
            "Post-processing the exported models...\n",
            "Weight deduplication check in the ONNX export requires accelerate. Please install accelerate to run it.\n",
            "The two models proto have different outputs (17 and 9 outputs). Constant outputs will be added to unify the two models outputs.\n",
            "Adding a constant output for present.0.encoder.key of shape [0, 6, 1, 64] in model2.\n",
            "Adding a constant output for present.0.encoder.value of shape [0, 6, 1, 64] in model2.\n",
            "Adding a constant output for present.1.encoder.key of shape [0, 6, 1, 64] in model2.\n",
            "Adding a constant output for present.1.encoder.value of shape [0, 6, 1, 64] in model2.\n",
            "Adding a constant output for present.2.encoder.key of shape [0, 6, 1, 64] in model2.\n",
            "Adding a constant output for present.2.encoder.value of shape [0, 6, 1, 64] in model2.\n",
            "Adding a constant output for present.3.encoder.key of shape [0, 6, 1, 64] in model2.\n",
            "Adding a constant output for present.3.encoder.value of shape [0, 6, 1, 64] in model2.\n",
            "Validating ONNX model export_onnx/openai/whisper-tiny/encoder_model.onnx...\n",
            "\t-[âœ“] ONNX model output names match reference model (last_hidden_state)\n",
            "\t- Validating ONNX Model output \"last_hidden_state\":\n",
            "\t\t-[âœ“] (2, 1500, 384) matches (2, 1500, 384)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "Validating ONNX model export_onnx/openai/whisper-tiny/decoder_model_merged.onnx...\n",
            "\u001b[0;93m2024-04-12 10:35:46.461625726 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Shape_4_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:46.461718257 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_23_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:46.478143231 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_1_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:46.478201698 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:46.478215371 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Unsqueeze_192'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:46.478645008 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_11_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:46.478663150 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Unsqueeze_211'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:46.478686496 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_12_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:46.478693914 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_14_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:46.478705872 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_2_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:46.478714456 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_13_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:46.478728444 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Unsqueeze_180'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:46.478740137 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_10_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\t-[âœ“] ONNX model output names match reference model (present.2.decoder.key, present.3.encoder.key, present.0.encoder.key, present.1.decoder.key, present.2.decoder.value, present.1.encoder.key, present.3.decoder.key, present.0.decoder.key, logits, present.2.encoder.key, present.3.decoder.value, present.2.encoder.value, present.1.encoder.value, present.0.decoder.value, present.1.decoder.value, present.3.encoder.value, present.0.encoder.value)\n",
            "\t- Validating ONNX Model output \"logits\":\n",
            "\t\t-[âœ“] (2, 16, 51865) matches (2, 16, 51865)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.0.decoder.key\":\n",
            "\t\t-[âœ“] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.0.decoder.value\":\n",
            "\t\t-[âœ“] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.0.encoder.key\":\n",
            "\t\t-[âœ“] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.0.encoder.value\":\n",
            "\t\t-[âœ“] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.1.decoder.key\":\n",
            "\t\t-[âœ“] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.1.decoder.value\":\n",
            "\t\t-[âœ“] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.1.encoder.key\":\n",
            "\t\t-[âœ“] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.1.encoder.value\":\n",
            "\t\t-[âœ“] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.2.decoder.key\":\n",
            "\t\t-[âœ“] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.2.decoder.value\":\n",
            "\t\t-[âœ“] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.2.encoder.key\":\n",
            "\t\t-[âœ“] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.2.encoder.value\":\n",
            "\t\t-[âœ“] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.3.decoder.key\":\n",
            "\t\t-[âœ“] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.3.decoder.value\":\n",
            "\t\t-[âœ“] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.3.encoder.key\":\n",
            "\t\t-[âœ“] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.3.encoder.value\":\n",
            "\t\t-[âœ“] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "Validating ONNX model export_onnx/openai/whisper-tiny/decoder_model_merged.onnx...\n",
            "\u001b[0;93m2024-04-12 10:35:47.393181620 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Shape_4_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:47.393302485 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_23_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:47.411905010 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_1_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:47.411952338 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:47.411978486 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Unsqueeze_192'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:47.412527152 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_11_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:47.412551044 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Unsqueeze_211'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:47.412584655 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_12_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:47.412597581 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_14_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:47.412615576 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_2_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:47.412631568 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_13_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:47.412656271 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Unsqueeze_180'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\u001b[0;93m2024-04-12 10:35:47.412676295 [W:onnxruntime:, graph.cc:3593 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_10_output_0'. It is not used by any node and should be removed from the model.\u001b[m\n",
            "\t-[âœ“] ONNX model output names match reference model (present.2.decoder.key, present.1.decoder.key, present.2.decoder.value, present.3.decoder.key, present.3.decoder.value, present.0.decoder.key, logits, present.0.decoder.value, present.1.decoder.value)\n",
            "\t- Validating ONNX Model output \"logits\":\n",
            "\t\t-[âœ“] (2, 1, 51865) matches (2, 1, 51865)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.0.decoder.key\":\n",
            "\t\t-[âœ“] (2, 6, 17, 64) matches (2, 6, 17, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.0.decoder.value\":\n",
            "\t\t-[âœ“] (2, 6, 17, 64) matches (2, 6, 17, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.1.decoder.key\":\n",
            "\t\t-[âœ“] (2, 6, 17, 64) matches (2, 6, 17, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.1.decoder.value\":\n",
            "\t\t-[âœ“] (2, 6, 17, 64) matches (2, 6, 17, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.2.decoder.key\":\n",
            "\t\t-[âœ“] (2, 6, 17, 64) matches (2, 6, 17, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.2.decoder.value\":\n",
            "\t\t-[âœ“] (2, 6, 17, 64) matches (2, 6, 17, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.3.decoder.key\":\n",
            "\t\t-[âœ“] (2, 6, 17, 64) matches (2, 6, 17, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "\t- Validating ONNX Model output \"present.3.decoder.value\":\n",
            "\t\t-[âœ“] (2, 6, 17, 64) matches (2, 6, 17, 64)\n",
            "\t\t-[âœ“] all values close (atol: 0.001)\n",
            "The ONNX export succeeded and the exported model was saved at: export_onnx/openai/whisper-tiny\n"
          ]
        }
      ],
      "source": [
        "! optimum-cli export onnx --model {MODEL_NAME} {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jrTPqhE3L01"
      },
      "source": [
        "We have to move additional model assets into a seperate folder, so that Spark NLP can load it properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CyHyF5Pr3L02"
      },
      "outputs": [],
      "source": [
        "! mkdir -p {EXPORT_PATH}/assets\n",
        "! mv -t {EXPORT_PATH}/assets {EXPORT_PATH}/*.json {EXPORT_PATH}/*.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqXo5KCK3L02"
      },
      "source": [
        "Let's have a look inside these two directories and see what we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qFXX_acJ3L03",
        "outputId": "e82587f4-e280-4288-de3a-087d8c1f7aaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 607916\n",
            "drwxr-xr-x 2 root root      4096 Apr 12 10:35 assets\n",
            "-rw-r--r-- 1 root root 198217996 Apr 12 10:35 decoder_model_merged.onnx\n",
            "-rw-r--r-- 1 root root 198061779 Apr 12 10:35 decoder_model.onnx\n",
            "-rw-r--r-- 1 root root 193303540 Apr 12 10:35 decoder_with_past_model.onnx\n",
            "-rw-r--r-- 1 root root  32904958 Apr 12 10:35 encoder_model.onnx\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-lbCcSP13L03",
        "outputId": "9b37ce06-a335-4c82-8899-55fb9197815a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4308\n",
            "-rw-r--r-- 1 root root   34604 Apr 12 10:35 added_tokens.json\n",
            "-rw-r--r-- 1 root root    2243 Apr 12 10:35 config.json\n",
            "-rw-r--r-- 1 root root    3742 Apr 12 10:35 generation_config.json\n",
            "-rw-r--r-- 1 root root  493869 Apr 12 10:35 merges.txt\n",
            "-rw-r--r-- 1 root root   52666 Apr 12 10:35 normalizer.json\n",
            "-rw-r--r-- 1 root root     339 Apr 12 10:35 preprocessor_config.json\n",
            "-rw-r--r-- 1 root root    2194 Apr 12 10:35 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root  283277 Apr 12 10:35 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 2480466 Apr 12 10:35 tokenizer.json\n",
            "-rw-r--r-- 1 root root 1036584 Apr 12 10:35 vocab.json\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}/assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q63ic12h3L04"
      },
      "source": [
        "## Import and Save Whisper in Spark NLP\n",
        "\n",
        "- Let's install and setup Spark NLP in Google Colab\n",
        "- This part is pretty easy via our simple script\n",
        "- Additionally, we need to upgrade Spark to version 3.4.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZKZ_tizZ3L04",
        "outputId": "2d7801ea-99fd-44ac-a963-e98f8feb0c06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing PySpark 3.2.3 and Spark NLP 5.3.3\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 5.3.3\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m568.4/568.4 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyspark==3.4.1\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.7 (from pyspark==3.4.1)\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285388 sha256=35520bb723dd6a52ac228a8c249191033e27475dc70be0af064dde9b1b780d3c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.5\n",
            "    Uninstalling py4j-0.10.9.5:\n",
            "      Successfully uninstalled py4j-0.10.9.5\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.2.3\n",
            "    Uninstalling pyspark-3.2.3:\n",
            "      Successfully uninstalled pyspark-3.2.3\n",
            "Successfully installed py4j-0.10.9.7 pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash\n",
        "! pip install -U pyspark==3.4.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EiV3v3D3L05"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HKzEZfQn3L05"
      },
      "outputs": [],
      "source": [
        "import sparknlp\n",
        "\n",
        "# let's start Spark with Spark NLP\n",
        "spark = sparknlp.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UCXtwOd3L05"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `WhisperForCTC` which allows us to load the ONNX model\n",
        "- Most params will be set automatically. They can also be set later after loading the model in `WhisperForCTC` during runtime, so don't worry about setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the exported model. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fZNPXuQP3L05"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import *\n",
        "\n",
        "# All these params should be identical to the original ONNX model\n",
        "whisper = (\n",
        "    WhisperForCTC.loadSavedModel(f\"{EXPORT_PATH}\", spark)\n",
        "    .setInputCols(\"audio_assembler\")\n",
        "    .setOutputCol(\"text\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tSlzbOR3L06"
      },
      "source": [
        "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nkP_gWrt3L06"
      },
      "outputs": [],
      "source": [
        "whisper.write().overwrite().save(f\"{MODEL_NAME}_spark_nlp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKqHPm903L06"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6Dfa7zDK3L06"
      },
      "outputs": [],
      "source": [
        "!rm -rf {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ecbVmq73L06"
      },
      "source": [
        "Awesome  ğŸ˜ !\n",
        "\n",
        "This is your ONNX Whisper model from HuggingFace ğŸ¤—  loaded and saved by Spark NLP ğŸš€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WKxyiCOi3L07",
        "outputId": "2eae5016-aa01-4da2-f6f9-574b8f4136fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 414404\n",
            "-rw-r--r-- 1 root root 198092144 Apr 12 10:38 decoder_model\n",
            "-rw-r--r-- 1 root root 193333200 Apr 12 10:38 decoder_with_past_model\n",
            "-rw-r--r-- 1 root root  32910123 Apr 12 10:38 encoder_model\n",
            "drwxr-xr-x 6 root root      4096 Apr 12 10:38 fields\n",
            "drwxr-xr-x 2 root root      4096 Apr 12 10:38 metadata\n"
          ]
        }
      ],
      "source": [
        "! ls -l {MODEL_NAME}_spark_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VEQV_Cv3L07"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny Whisper model ğŸ˜Š"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/audio/txt/librispeech_asr_0.txt"
      ],
      "metadata": {
        "id": "KzAIXRki4kRQ",
        "outputId": "c926a754-3bb1-4790-b3cf-ec10a93a0ebc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-12 10:39:27--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/audio/txt/librispeech_asr_0.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2199992 (2.1M) [text/plain]\n",
            "Saving to: â€˜librispeech_asr_0.txtâ€™\n",
            "\n",
            "\rlibrispeech_asr_0.t   0%[                    ]       0  --.-KB/s               \rlibrispeech_asr_0.t 100%[===================>]   2.10M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-04-12 10:39:27 (143 MB/s) - â€˜librispeech_asr_0.txtâ€™ saved [2199992/2199992]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "L9hjHeKs3L07",
        "outputId": "65c2a3cb-675f-4873-e786-3a644ffe0b88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|result                                                                                    |\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|[ Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.]|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "audioAssembler = AudioAssembler() \\\n",
        "    .setInputCol(\"audio_content\") \\\n",
        "    .setOutputCol(\"audio_assembler\")\n",
        "\n",
        "speechToText = WhisperForCTC.load(f\"{MODEL_NAME}_spark_nlp\")\n",
        "\n",
        "pipeline = Pipeline().setStages([audioAssembler, speechToText])\n",
        "\n",
        "audio_path = \"librispeech_asr_0.txt\"\n",
        "with open(audio_path) as file:\n",
        "    raw_floats = [float(data) for data in file.read().strip().split(\"\\n\")]\n",
        "\n",
        "processedAudioFloats = spark.createDataFrame([[raw_floats]]).toDF(\"audio_content\")\n",
        "\n",
        "result = pipeline.fit(processedAudioFloats).transform(processedAudioFloats)\n",
        "result.select(\"text.result\").show(truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_uVMnSS3L07"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of Whisper models from HuggingFace ğŸ¤— in Spark NLP ğŸš€\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}