{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_Whisper.ipynb)\n",
    "\n",
    "# Import ONNX Whisper models from HuggingFace 🤗 into Spark NLP 🚀\n",
    "\n",
    "Let's keep in mind a few things before we start 😊\n",
    "\n",
    "- ONNX support was introduced in `Spark NLP 5.0.0`, enabling high performance inference for models. Please make sure you have upgraded to the latest Spark NLP release.\n",
    "- The Whisper model was introduced in `Spark NLP 5.1.0 and requires Spark version 3.4.1 and up.`\n",
    "- Official models are supported, but not all custom models may work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export and Save HuggingFace model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
    "- We lock `transformers` on version `4.31.0`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.2/364.2 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.2/451.2 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade \"transformers[onnx]==4.31.0\" optimum tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to import and export ONNX models with `from_pretrained` and `save_pretrained`.\n",
    "- We'll use the [whisper-tiny](https://huggingface.co/openai/whisper-tiny) model from HuggingFace as an example and export it with the `optimum-cli`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"openai/whisper-tiny\"\n",
    "EXPORT_PATH = f\"export_onnx/{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-14 13:53:19.500633: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Framework not specified. Using pt to export to ONNX.\n",
      "Downloading (…)lve/main/config.json: 100% 1.98k/1.98k [00:00<00:00, 3.50MB/s]\n",
      "Downloading model.safetensors: 100% 151M/151M [00:05<00:00, 28.6MB/s]\n",
      "Downloading (…)neration_config.json: 100% 3.72k/3.72k [00:00<00:00, 17.7MB/s]\n",
      "Automatic task detection to automatic-speech-recognition-with-past (possible synonyms are: speech2seq-lm-with-past).\n",
      "Downloading (…)okenizer_config.json: 100% 841/841 [00:00<00:00, 2.66MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100% 1.04M/1.04M [00:00<00:00, 10.8MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100% 2.20M/2.20M [00:00<00:00, 25.4MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100% 494k/494k [00:00<00:00, 18.1MB/s]\n",
      "Downloading (…)main/normalizer.json: 100% 52.7k/52.7k [00:00<00:00, 73.6MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100% 2.08k/2.08k [00:00<00:00, 7.43MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100% 2.08k/2.08k [00:00<00:00, 7.93MB/s]\n",
      "Downloading (…)rocessor_config.json: 100% 185k/185k [00:00<00:00, 7.36MB/s]\n",
      "Using framework PyTorch: 2.0.1+cu118\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py:410: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py:449: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Using framework PyTorch: 2.0.1+cu118\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py:1004: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py:417: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Using framework PyTorch: 2.0.1+cu118\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "Asked a sequence length of 16, but a sequence length of 1 will be used with use_past == True for `decoder_input_ids`.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py:372: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Post-processing the exported models...\n",
      "The two models proto have different outputs (17 and 9 outputs). Constant outputs will be added to unify the two models outputs.\n",
      "Addind a constant output for present.0.encoder.key of shape [0, 6, 1, 64] in model2.\n",
      "Addind a constant output for present.0.encoder.value of shape [0, 6, 1, 64] in model2.\n",
      "Addind a constant output for present.1.encoder.key of shape [0, 6, 1, 64] in model2.\n",
      "Addind a constant output for present.1.encoder.value of shape [0, 6, 1, 64] in model2.\n",
      "Addind a constant output for present.2.encoder.key of shape [0, 6, 1, 64] in model2.\n",
      "Addind a constant output for present.2.encoder.value of shape [0, 6, 1, 64] in model2.\n",
      "Addind a constant output for present.3.encoder.key of shape [0, 6, 1, 64] in model2.\n",
      "Addind a constant output for present.3.encoder.value of shape [0, 6, 1, 64] in model2.\n",
      "Validating models in subprocesses...\n",
      "2023-08-14 13:53:53.825862: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Validating ONNX model export_onnx/openai/whisper-tiny/encoder_model.onnx...\n",
      "\t-[✓] ONNX model output names match reference model (last_hidden_state)\n",
      "\t- Validating ONNX Model output \"last_hidden_state\":\n",
      "\t\t-[✓] (2, 1500, 384) matches (2, 1500, 384)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "2023-08-14 13:54:09.277640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Validating ONNX model export_onnx/openai/whisper-tiny/decoder_model_merged.onnx...\n",
      "2023-08-14 13:54:11.873629973 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Shape_4_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-08-14 13:54:11.873793574 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_16_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-08-14 13:54:11.892860734 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-08-14 13:54:11.892952656 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-08-14 13:54:11.893552082 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_10_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-08-14 13:54:11.893587646 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-08-14 13:54:11.893601631 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_12_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-08-14 13:54:11.893631102 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_9_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-08-14 13:54:11.893696170 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_11_output_0'. It is not used by any node and should be removed from the model.\n",
      "\t-[✓] ONNX model output names match reference model (present.1.decoder.key, present.2.encoder.value, present.3.encoder.key, present.0.decoder.value, present.1.decoder.value, present.0.encoder.key, present.2.decoder.key, present.3.decoder.value, present.2.encoder.key, present.2.decoder.value, present.1.encoder.key, present.3.encoder.value, present.1.encoder.value, present.0.decoder.key, present.3.decoder.key, logits, present.0.encoder.value)\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 16, 51865) matches (2, 16, 51865)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.0.decoder.key\":\n",
      "\t\t-[✓] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.0.decoder.value\":\n",
      "\t\t-[✓] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.0.encoder.key\":\n",
      "\t\t-[✓] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.0.encoder.value\":\n",
      "\t\t-[✓] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.1.decoder.key\":\n",
      "\t\t-[✓] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.1.decoder.value\":\n",
      "\t\t-[✓] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.1.encoder.key\":\n",
      "\t\t-[✓] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.1.encoder.value\":\n",
      "\t\t-[✓] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.2.decoder.key\":\n",
      "\t\t-[✓] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.2.decoder.value\":\n",
      "\t\t-[✓] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.2.encoder.key\":\n",
      "\t\t-[✓] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.2.encoder.value\":\n",
      "\t\t-[✓] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.3.decoder.key\":\n",
      "\t\t-[✓] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.3.decoder.value\":\n",
      "\t\t-[✓] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.3.encoder.key\":\n",
      "\t\t-[✓] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.3.encoder.value\":\n",
      "\t\t-[✓] (2, 6, 16, 64) matches (2, 6, 16, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "2023-08-14 13:54:20.179734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Validating ONNX model export_onnx/openai/whisper-tiny/decoder_model_merged.onnx...\n",
      "Asked a sequence length of 16, but a sequence length of 1 will be used with use_past == True for `decoder_input_ids`.\n",
      "2023-08-14 13:54:23.118265457 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Shape_4_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-08-14 13:54:23.118402025 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_16_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-08-14 13:54:23.134562875 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-08-14 13:54:23.134629569 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-08-14 13:54:23.135051085 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_10_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-08-14 13:54:23.135074933 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-08-14 13:54:23.135088187 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_12_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-08-14 13:54:23.135109430 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_9_output_0'. It is not used by any node and should be removed from the model.\n",
      "2023-08-14 13:54:23.135158578 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/model/decoder/Constant_11_output_0'. It is not used by any node and should be removed from the model.\n",
      "\t-[✓] ONNX model output names match reference model (present.0.decoder.key, present.3.decoder.key, present.1.decoder.key, present.1.decoder.value, logits, present.3.decoder.value, present.2.decoder.value, present.2.decoder.key, present.0.decoder.value)\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 1, 51865) matches (2, 1, 51865)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.0.decoder.key\":\n",
      "\t\t-[✓] (2, 6, 17, 64) matches (2, 6, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.0.decoder.value\":\n",
      "\t\t-[✓] (2, 6, 17, 64) matches (2, 6, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.1.decoder.key\":\n",
      "\t\t-[✓] (2, 6, 17, 64) matches (2, 6, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.1.decoder.value\":\n",
      "\t\t-[✓] (2, 6, 17, 64) matches (2, 6, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.2.decoder.key\":\n",
      "\t\t-[✓] (2, 6, 17, 64) matches (2, 6, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.2.decoder.value\":\n",
      "\t\t-[✓] (2, 6, 17, 64) matches (2, 6, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.3.decoder.key\":\n",
      "\t\t-[✓] (2, 6, 17, 64) matches (2, 6, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "\t- Validating ONNX Model output \"present.3.decoder.value\":\n",
      "\t\t-[✓] (2, 6, 17, 64) matches (2, 6, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "The ONNX export succeeded and the exported model was saved at: export_onnx/openai/whisper-tiny\n"
     ]
    }
   ],
   "source": [
    "! optimum-cli export onnx --model {MODEL_NAME} {EXPORT_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to move additional model assets into a seperate folder, so that Spark NLP can load it properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p {EXPORT_PATH}/assets\n",
    "! mv -t {EXPORT_PATH}/assets {EXPORT_PATH}/*.json {EXPORT_PATH}/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look inside these two directories and see what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 607868\n",
      "drwxr-xr-x 2 root root      4096 Aug 14 13:55 assets\n",
      "-rw-r--r-- 1 root root 198197526 Aug 14 13:53 decoder_model_merged.onnx\n",
      "-rw-r--r-- 1 root root 198049530 Aug 14 13:53 decoder_model.onnx\n",
      "-rw-r--r-- 1 root root 193295315 Aug 14 13:53 decoder_with_past_model.onnx\n",
      "-rw-r--r-- 1 root root  32900723 Aug 14 13:53 encoder_model.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls -l {EXPORT_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3728\n",
      "-rw-r--r-- 1 root root    2082 Aug 14 13:53 added_tokens.json\n",
      "-rw-r--r-- 1 root root    2243 Aug 14 13:53 config.json\n",
      "-rw-r--r-- 1 root root    3711 Aug 14 13:53 generation_config.json\n",
      "-rw-r--r-- 1 root root  493864 Aug 14 13:53 merges.txt\n",
      "-rw-r--r-- 1 root root   52666 Aug 14 13:53 normalizer.json\n",
      "-rw-r--r-- 1 root root     339 Aug 14 13:53 preprocessor_config.json\n",
      "-rw-r--r-- 1 root root    2077 Aug 14 13:53 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root     835 Aug 14 13:53 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root 2203267 Aug 14 13:53 tokenizer.json\n",
      "-rw-r--r-- 1 root root 1036584 Aug 14 13:53 vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls -l {EXPORT_PATH}/assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Save Whisper in Spark NLP\n",
    "\n",
    "- Let's install and setup Spark NLP in Google Colab\n",
    "- This part is pretty easy via our simple script\n",
    "- Additionally, we need to upgrade Spark to version 3.4.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash\n",
    "! pip install -U pyspark==3.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start Spark with Spark NLP included via our simple `start()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "\n",
    "# let's start Spark with Spark NLP\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's use `loadSavedModel` functon in `WhisperForCTC` which allows us to load the ONNX model\n",
    "- Most params will be set automatically. They can also be set later after loading the model in `WhisperForCTC` during runtime, so don't worry about setting them now\n",
    "- `loadSavedModel` accepts two params, first is the path to the exported model. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
    "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import *\n",
    "\n",
    "# All these params should be identical to the original ONNX model\n",
    "whisper = (\n",
    "    WhisperForCTC.loadSavedModel(f\"{EXPORT_PATH}\", spark)\n",
    "    .setInputCols(\"audio_assembler\")\n",
    "    .setOutputCol(\"text\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper.write().overwrite().save(f\"{MODEL_NAME}_spark_nlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up stuff we don't need anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {EXPORT_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome  😎 !\n",
    "\n",
    "This is your ONNX Whisper model from HuggingFace 🤗  loaded and saved by Spark NLP 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 414388\n",
      "-rw-r--r-- 1 root root 198079914 Aug 14 14:03 decoder_model_whisper_ctc\n",
      "-rw-r--r-- 1 root root 193324994 Aug 14 14:03 decoder_with_past_model_whisper_ctc\n",
      "-rw-r--r-- 1 root root  32905912 Aug 14 14:03 encoder_model_whisper_ctc\n",
      "drwxr-xr-x 6 root root      4096 Aug 14 14:03 fields\n",
      "drwxr-xr-x 2 root root      4096 Aug 14 14:03 metadata\n"
     ]
    }
   ],
   "source": [
    "! ls -l {MODEL_NAME}_spark_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny Whisper model 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|result                                                                                    |\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|[ Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.]|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "audioAssembler = AudioAssembler() \\\n",
    "    .setInputCol(\"audio_content\") \\\n",
    "    .setOutputCol(\"audio_assembler\")\n",
    "\n",
    "speechToText = WhisperForCTC.load(f\"{MODEL_NAME}_spark_nlp\")\n",
    "\n",
    "pipeline = Pipeline().setStages([audioAssembler, speechToText])\n",
    "\n",
    "audio_path = \"../../../../src/test/resources/audio/txt/librispeech_asr_0.txt\"\n",
    "with open(audio_path) as file:\n",
    "    raw_floats = [float(data) for data in file.read().strip().split(\"\\n\")]\n",
    "\n",
    "processedAudioFloats = spark.createDataFrame([[raw_floats]]).toDF(\"audio_content\")\n",
    "\n",
    "result = pipeline.fit(processedAudioFloats).transform(processedAudioFloats)\n",
    "result.select(\"text.result\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You can now go wild and use hundreds of Whisper models from HuggingFace 🤗 in Spark NLP 🚀\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}