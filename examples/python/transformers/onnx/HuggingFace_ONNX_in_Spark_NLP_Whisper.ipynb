{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEdJynTH3L0x"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_Whisper.ipynb)\n",
        "\n",
        "# Import ONNX Whisper models from HuggingFace ðŸ¤— into Spark NLP ðŸš€\n",
        "\n",
        "Let's keep in mind a few things before we start ðŸ˜Š\n",
        "\n",
        "- ONNX support was introduced in `Spark NLP 5.0.0`, enabling high performance inference for models. Please make sure you have upgraded to the latest Spark NLP release.\n",
        "- The Whisper model was introduced in `Spark NLP 5.1.0 and requires Spark version 3.4.1 and up.`\n",
        "- Official models are supported, but not all custom models may work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfiBPTV83L0y"
      },
      "source": [
        "## Export and Save HuggingFace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhUUhv8h3L0z"
      },
      "source": [
        "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.31.0`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy9Ig4tY3L0z",
        "outputId": "e6eb1aaf-fe7a-4496-9cc4-a4967adc1b5b"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade transformers[onnx]==4.52.3 optimum onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_WSgW9w3L00"
      },
      "source": [
        "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to import and export ONNX models\n",
        "- We'll use the [whisper-tiny](https://huggingface.co/openai/whisper-tiny) model from HuggingFace as an example and export it with the `optimum-cli`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar3GeeF43L00"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"openai/whisper-tiny\"\n",
        "EXPORT_PATH = f\"export_onnx/{MODEL_NAME}\"\n",
        "\n",
        "! optimum-cli export onnx --model {MODEL_NAME} {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqXo5KCK3L02"
      },
      "source": [
        "Let's have a look inside these two directories and see what we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFXX_acJ3L03",
        "outputId": "d891be5a-8343-42c2-ca33-b73d10ed4dc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 380152\n",
            "-rw-r--r-- 1 root root     34604 Jun 26 06:12 added_tokens.json\n",
            "-rw-r--r-- 1 root root      1327 Jun 26 06:12 config.json\n",
            "-rw-r--r-- 1 root root 118509614 Jun 26 06:12 decoder_model_merged.onnx\n",
            "-rw-r--r-- 1 root root 118364554 Jun 26 06:12 decoder_model.onnx\n",
            "-rw-r--r-- 1 root root 113627714 Jun 26 06:12 decoder_with_past_model.onnx\n",
            "-rw-r--r-- 1 root root  32894170 Jun 26 06:12 encoder_model.onnx\n",
            "-rw-r--r-- 1 root root      3742 Jun 26 06:12 generation_config.json\n",
            "-rw-r--r-- 1 root root    493869 Jun 26 06:12 merges.txt\n",
            "-rw-r--r-- 1 root root     52666 Jun 26 06:12 normalizer.json\n",
            "-rw-r--r-- 1 root root       356 Jun 26 06:12 preprocessor_config.json\n",
            "-rw-r--r-- 1 root root      2194 Jun 26 06:12 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root    282713 Jun 26 06:12 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root   3930494 Jun 26 06:12 tokenizer.json\n",
            "-rw-r--r-- 1 root root   1036584 Jun 26 06:12 vocab.json\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jrTPqhE3L01"
      },
      "source": [
        "We have to move additional model assets into a seperate folder, so that Spark NLP can load it properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CyHyF5Pr3L02"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {EXPORT_PATH}/assets && mv -t {EXPORT_PATH}/assets {EXPORT_PATH}/*.json {EXPORT_PATH}/*.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lbCcSP13L03",
        "outputId": "6791f090-e4a4-4c14-8267-3596c64b3132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 5724\n",
            "-rw-r--r-- 1 root root   34604 Jun 26 06:12 added_tokens.json\n",
            "-rw-r--r-- 1 root root    1327 Jun 26 06:12 config.json\n",
            "-rw-r--r-- 1 root root    3742 Jun 26 06:12 generation_config.json\n",
            "-rw-r--r-- 1 root root  493869 Jun 26 06:12 merges.txt\n",
            "-rw-r--r-- 1 root root   52666 Jun 26 06:12 normalizer.json\n",
            "-rw-r--r-- 1 root root     356 Jun 26 06:12 preprocessor_config.json\n",
            "-rw-r--r-- 1 root root    2194 Jun 26 06:12 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root  282713 Jun 26 06:12 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 3930494 Jun 26 06:12 tokenizer.json\n",
            "-rw-r--r-- 1 root root 1036584 Jun 26 06:12 vocab.json\n"
          ]
        }
      ],
      "source": [
        "!ls -l {EXPORT_PATH}/assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q63ic12h3L04"
      },
      "source": [
        "## Import and Save Whisper in Spark NLP\n",
        "\n",
        "- Install and set up Spark NLP in Google Colab\n",
        "- This example uses specific versions of `pyspark` and `spark-nlp` that have been tested with the transformer model to ensure everything runs smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKZ_tizZ3L04",
        "outputId": "6a7cabe0-938b-49ad-c499-a2c8a086ba38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark==3.5.4 spark-nlp==5.5.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EiV3v3D3L05"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKzEZfQn3L05",
        "outputId": "2b89e564-7e91-433b-88ea-b2c8c4ad2503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark NLP version:  5.5.3\n",
            "Apache Spark version: 3.5.4\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: {}\".format(spark.version))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UCXtwOd3L05"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `WhisperForCTC` which allows us to load the ONNX model\n",
        "- Most params will be set automatically. They can also be set later after loading the model in `WhisperForCTC` during runtime, so don't worry about setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the exported model. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fZNPXuQP3L05"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import WhisperForCTC\n",
        "\n",
        "whisper = (\n",
        "    WhisperForCTC.loadSavedModel(f\"{EXPORT_PATH}\", spark)\n",
        "    .setInputCols(\"audio_assembler\")\n",
        "    .setOutputCol(\"text\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tSlzbOR3L06"
      },
      "source": [
        "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nkP_gWrt3L06"
      },
      "outputs": [],
      "source": [
        "whisper.write().overwrite().save(f\"{MODEL_NAME}_spark_nlp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKqHPm903L06"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6Dfa7zDK3L06"
      },
      "outputs": [],
      "source": [
        "!rm -rf {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ecbVmq73L06"
      },
      "source": [
        "Awesome  ðŸ˜Ž !\n",
        "\n",
        "This is your ONNX Whisper model from HuggingFace ðŸ¤—  loaded and saved by Spark NLP ðŸš€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKxyiCOi3L07",
        "outputId": "8c58bf94-1be3-408f-c337-63df5626342a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 258736\n",
            "-rw-r--r-- 1 root root 118382769 Jun 26 06:15 decoder_model\n",
            "-rw-r--r-- 1 root root 113645224 Jun 26 06:15 decoder_with_past_model\n",
            "-rw-r--r-- 1 root root  32899340 Jun 26 06:15 encoder_model\n",
            "drwxr-xr-x 6 root root      4096 Jun 26 06:15 fields\n",
            "drwxr-xr-x 2 root root      4096 Jun 26 06:15 metadata\n"
          ]
        }
      ],
      "source": [
        "! ls -l {MODEL_NAME}_spark_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VEQV_Cv3L07"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny Whisper model ðŸ˜Š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzAIXRki4kRQ",
        "outputId": "c5e5b316-af74-44ac-a371-8d22865a7751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-06-26 06:15:29--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/audio/txt/librispeech_asr_0.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2199992 (2.1M) [text/plain]\n",
            "Saving to: â€˜librispeech_asr_0.txtâ€™\n",
            "\n",
            "librispeech_asr_0.t 100%[===================>]   2.10M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-06-26 06:15:30 (32.0 MB/s) - â€˜librispeech_asr_0.txtâ€™ saved [2199992/2199992]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/audio/txt/librispeech_asr_0.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9hjHeKs3L07",
        "outputId": "99450efe-a2ee-4ffc-a138-283043fa2b30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------+\n",
            "|transcription                                                                           |\n",
            "+----------------------------------------------------------------------------------------+\n",
            "| Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.|\n",
            "+----------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.base import AudioAssembler\n",
        "from sparknlp.annotator import WhisperForCTC\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "audio_assembler = AudioAssembler() \\\n",
        "    .setInputCol(\"audio_content\") \\\n",
        "    .setOutputCol(\"audio_assembler\")\n",
        "\n",
        "whisper_model = WhisperForCTC.load(f\"{MODEL_NAME}_spark_nlp\") \\\n",
        "    .setInputCols([\"audio_assembler\"]) \\\n",
        "    .setOutputCol(\"text\")\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    audio_assembler,\n",
        "    whisper_model\n",
        "])\n",
        "\n",
        "with open(\"librispeech_asr_0.txt\") as f:\n",
        "    raw_floats = [float(x) for x in f.read().strip().split(\"\\n\")]\n",
        "\n",
        "df = spark.createDataFrame([[raw_floats]], [\"audio_content\"])\n",
        "\n",
        "model = pipeline.fit(df)\n",
        "result = model.transform(df)\n",
        "\n",
        "result.selectExpr(\"text.result[0] as transcription\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_uVMnSS3L07"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of Whisper models from HuggingFace ðŸ¤— in Spark NLP ðŸš€\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
