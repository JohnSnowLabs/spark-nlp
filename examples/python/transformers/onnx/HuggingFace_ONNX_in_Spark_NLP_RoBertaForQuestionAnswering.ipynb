{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5EKaurq9JS4"
      },
      "source": [
        "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/onnx/HuggingFace_ONNX_in_Spark_NLP_RoBertaForQuestionAnswering.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPdC5pDh9JS9"
      },
      "source": [
        "## Import ONNX RoBertaForQuestionAnswering models from HuggingFace ü§ó into Spark NLP üöÄ\n",
        "\n",
        "Let's keep in mind a few things before we start üòä\n",
        "\n",
        "- ONNX support was introduced in  `Spark NLP 5.0.0`, enabling high performance inference for models.\n",
        "- `RoBertaForQuestionAnswering` is only available since in `Spark NLP 5.1.4` and after. So please make sure you have upgraded to the latest Spark NLP release\n",
        "- You can import RoBERTa models trained/fine-tuned for question answering via `RobertaForQuestionAnswering` or `TFRobertaForQuestionAnswering`. These models are usually under `Question Answering` category and have `roberta` in their labels\n",
        "- Reference: [TFRobertaForQuestionAnswering](https://huggingface.co/docs/transformers/model_doc/roberta#transformers.TFRobertaForQuestionAnswering)\n",
        "- Some [example models](https://huggingface.co/models?filter=roberta&pipeline_tag=question-answering)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdUaKpFC9JS-"
      },
      "source": [
        "## Export and Save HuggingFace model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xte6cCYV9JS_"
      },
      "source": [
        "- Let's install `transformers` package with the `onnx` extension and it's dependencies. You don't need `onnx` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
        "- We lock `transformers` on version `4.52.3`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully.\n",
        "- Albert uses SentencePiece, so we will have to install that as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl0_xEjd9JTA",
        "outputId": "8af67cfc-9ba6-40eb-957a-ca1b58e31860"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade transformers[onnx]==4.52.3 optimum onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vaztlY89JTD"
      },
      "source": [
        "- HuggingFace has an extension called Optimum which offers specialized model inference, including ONNX. We can use this to import and export ONNX models.\n",
        "- We'll use [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2) model from HuggingFace as an example as an example and export it with the `optimum-cli`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wykq9OBJ7lcT",
        "outputId": "0d2760bc-2d3f-4785-a7ff-f58e603ad786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-16 04:48:10.059060: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750049290.388172    9439 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750049290.478072    9439 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = 'deepset/roberta-base-squad2'\n",
        "ONNX_MODEL = f\"onnx_models/{MODEL_NAME}\"\n",
        "\n",
        "!optimum-cli export onnx --model {MODEL_NAME} {ONNX_MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMOCHXfs9JTE"
      },
      "source": [
        "Let's have a look inside these two directories and see what we are dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrRRKQ7w9JTF",
        "outputId": "6d46d4d0-f883-4d14-f3c1-63b30f81c92c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 489628\n",
            "-rw-r--r-- 1 root root       727 Jun 16 04:48 config.json\n",
            "-rw-r--r-- 1 root root    456318 Jun 16 04:48 merges.txt\n",
            "-rw-r--r-- 1 root root 496545753 Jun 16 04:48 model.onnx\n",
            "-rw-r--r-- 1 root root       957 Jun 16 04:48 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1302 Jun 16 04:48 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root   3558642 Jun 16 04:48 tokenizer.json\n",
            "-rw-r--r-- 1 root root    798293 Jun 16 04:48 vocab.json\n"
          ]
        }
      ],
      "source": [
        "!ls -l {ONNX_MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvDj-6Mn_-G_"
      },
      "source": [
        "- Spark NLP needs the vocab as a plain `vocab.txt` (so we have to convert it from Hugging Face‚Äôs `vocab.json`) and expects both `vocab.txt` and `merges.txt` to be inside an assets folder to work properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "m9Nxb7l99JTE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(f'{ONNX_MODEL}/vocab.json') as f, open(f'{ONNX_MODEL}/vocab.txt', 'w') as out:\n",
        "    out.writelines(f\"{k}\\n\" for k in json.load(f).keys())\n",
        "\n",
        "!mkdir -p {ONNX_MODEL}/assets\n",
        "!mv {ONNX_MODEL}/vocab.txt {ONNX_MODEL}/merges.txt {ONNX_MODEL}/assets/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0R4IN899JTI",
        "outputId": "55ae9eb3-5332-483b-f167-17922a7a0d01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "onnx_models/deepset/roberta-base-squad2:\n",
            "total 489184\n",
            "drwxr-xr-x 2 root root      4096 Jun 16 04:48 assets\n",
            "-rw-r--r-- 1 root root       727 Jun 16 04:48 config.json\n",
            "-rw-r--r-- 1 root root 496545753 Jun 16 04:48 model.onnx\n",
            "-rw-r--r-- 1 root root       957 Jun 16 04:48 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1302 Jun 16 04:48 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root   3558642 Jun 16 04:48 tokenizer.json\n",
            "-rw-r--r-- 1 root root    798293 Jun 16 04:48 vocab.json\n",
            "\n",
            "onnx_models/deepset/roberta-base-squad2/assets:\n",
            "total 848\n",
            "-rw-r--r-- 1 root root 456318 Jun 16 04:48 merges.txt\n",
            "-rw-r--r-- 1 root root 407065 Jun 16 04:48 vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -lR {ONNX_MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBgQ2NAx9JTI"
      },
      "source": [
        "Voila! We have our `vocab.txt` and `merges.txt` inside assets directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bQxasXu9JTJ"
      },
      "source": [
        "## Import and Save RoBertaForQuestionAnswering in Spark NLP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_omNRyX9JTJ"
      },
      "source": [
        "- **Install and set up Spark NLP in Google Colab**\n",
        "  - This example uses specific versions of `pyspark` and `spark-nlp` that have been tested with the transformer model to ensure everything runs smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_t5hbw69JTJ",
        "outputId": "4b7bb539-f285-4f9e-c105-ee03e8497771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark==3.5.4 spark-nlp==5.5.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWqYWhB19JTJ"
      },
      "source": [
        "Let's start Spark with Spark NLP included via our simple `start()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ngs4RBWk9JTJ",
        "outputId": "ac2bfd1d-8a58-4da5-dca4-38e43e8cedf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark NLP version: 5.5.3\n",
            "Apache Spark version: 3.5.4\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: {}\".format(sparknlp.version()))\n",
        "print(\"Apache Spark version: {}\".format(spark.version))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Trg267-S9JTK"
      },
      "source": [
        "- Let's use `loadSavedModel` functon in `RoBertaForQuestionAnswering` which allows us to load TensorFlow model in SavedModel format\n",
        "- Most params can be set later when you are loading this model in `RoBertaForQuestionAnswering` in runtime like `setMaxSentenceLength`, so don't worry what you are setting them now\n",
        "- `loadSavedModel` accepts two params, first is the path to the TF SavedModel. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jvVaNwo99JTK"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import RoBertaForQuestionAnswering\n",
        "\n",
        "spanClassifier = RoBertaForQuestionAnswering.loadSavedModel(\n",
        "     ONNX_MODEL,\n",
        "     spark\n",
        " )\\\n",
        "  .setInputCols([\"document_question\",'document_context'])\\\n",
        "  .setOutputCol(\"answer\")\\\n",
        "  .setCaseSensitive(True)\\\n",
        "  .setMaxSentenceLength(512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTurRtGE9JTK"
      },
      "source": [
        "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "22EdDX9R9JTK"
      },
      "outputs": [],
      "source": [
        "spanClassifier.write().overwrite().save(\"./{}_spark_nlp_onnx\".format(ONNX_MODEL))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC77xQiV9JTL"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "C3kwTXkf9JTL"
      },
      "outputs": [],
      "source": [
        "!rm -rf {ONNX_MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20VS_Qnn9JTL"
      },
      "source": [
        "Awesome üòé  !\n",
        "\n",
        "This is your RoBertaForQuestionAnswering model from HuggingFace ü§ó  loaded and saved by Spark NLP üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9bdNY_g9JTL",
        "outputId": "8b168203-f821-4332-9db7-18ce41f4a0cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 484996\n",
            "drwxr-xr-x 4 root root      4096 Jun 16 04:48 fields\n",
            "drwxr-xr-x 2 root root      4096 Jun 16 04:48 metadata\n",
            "-rw-r--r-- 1 root root 496621657 Jun 16 04:49 roberta_classification_onnx\n"
          ]
        }
      ],
      "source": [
        "! ls -l {ONNX_MODEL}_spark_nlp_onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAag4dpP9JTM"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny RoBertaForQuestionAnswering model in Spark NLP üöÄ pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7iAicOO9JTM",
        "outputId": "7f281896-a781-453c-fbde-72ccf907948c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------+---------------------+\n",
            "|question                           |result               |\n",
            "+-----------------------------------+---------------------+\n",
            "|What is the boiling point of water?|[100 degrees Celsius]|\n",
            "|Who is the president of the USA?   |[Joe Biden]          |\n",
            "|What color is the sky?             |[blue]               |\n",
            "+-----------------------------------+---------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.base import MultiDocumentAssembler\n",
        "from sparknlp.annotator import RoBertaForQuestionAnswering\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "document_assembler = MultiDocumentAssembler() \\\n",
        "    .setInputCols([\"question\", \"context\"]) \\\n",
        "    .setOutputCols([\"document_question\", \"document_context\"])\n",
        "\n",
        "qa_model = RoBertaForQuestionAnswering.load(f\"./{ONNX_MODEL}_spark_nlp_onnx\") \\\n",
        "    .setInputCols([\"document_question\", \"document_context\"]) \\\n",
        "    .setOutputCol(\"answer\")\n",
        "\n",
        "pipeline = Pipeline().setStages([\n",
        "    document_assembler,\n",
        "    qa_model\n",
        "])\n",
        "\n",
        "data = [\n",
        "    (\"What is the boiling point of water?\", \"Water boils at 100 degrees Celsius.\"),\n",
        "    (\"Who is the president of the USA?\", \"The president of the USA is Joe Biden.\"),\n",
        "    (\"What color is the sky?\", \"The sky is blue on a clear day.\"),\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"question\", \"context\"])\n",
        "\n",
        "result = pipeline.fit(df).transform(df)\n",
        "result.select(\"question\", \"answer.result\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMBHamBA9JTM"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of `RoBertaForQuestionAnswering` models from HuggingFace ü§ó in Spark NLP üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
