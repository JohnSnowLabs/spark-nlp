{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10gTaH0m6ANK"
   },
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/HuggingFace%20in%20Spark%20NLP%20-%20ALBERT%20-%20S3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GL3z0zmy6ANQ"
   },
   "source": [
    "## Import ALBERT models from HuggingFace ðŸ¤—  into Spark NLP ðŸš€ with S3\n",
    "\n",
    "Let's keep in mind a few things before we start ðŸ˜Š\n",
    "\n",
    "- In Spark NLP you can configure the store repository to save these models. Starting in Spark NLP 5.1.0, we can define a local file system, or a distributed file system (DBFS) like HDFS or Databricks FS. You can also set an S3 bucket, Azure Storage or GCP Storage.\n",
    "- You can import models for ALBERT from HuggingFace but they have to be compatible with `TensorFlow` and they have to be in `Fill Mask` category. Meaning, you cannot use ALBERT models trained/fine-tuned on a specific task such as token/sequence classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to see the steps required to use an external S3 repository. To do this, we need to configure the spark session with the required settings for Spark NLP and Spark ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark NLP Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark NLP requires the following configuration:\n",
    "- S3 Region: We need the region to upload a file on your S3 bucket. This is defined in the config `spark.jsl.settings.aws.region`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark ML Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration will depend on your S3 bucket and AWS configuration. In this notebook a connection through **Temporary Security Credentials** is showcased. **Please contact your administrator to choose the right setup, as well as, the required keys/tokens.**\n",
    "\n",
    "Spark ML requires the following configuration to save a model in S3 using *Temporary Security Credentials*:\n",
    "\n",
    "1. Authenticating with S3: This is needed to interact with external S3 buckets, and it will require an access key, a secret key, and a session token. Define the values in these configs:\n",
    "\n",
    "- `spark.hadoop.fs.s3a.access.key`\n",
    "- `spark.hadoop.fs.s3a.secret.key`\n",
    "- `spark.hadoop.fs.s3a.session.token`\n",
    "2. Credential Provider: You need to define the Hadoop provider that will handle this connection. Since in this notebook, *Temporary Security Credentials* is used we need to use the provider `TemporaryAWSCredentialsProvider` from `hadoop-aws` package, and set it up in the config below:\n",
    "\n",
    "- `spark.hadoop.fs.s3a.aws.credentials.provider`\n",
    "3. AWS packages: S3A depends upon two JARs, alongside `hadoop-common` and its dependencies, which are `hadoop-aws` and `aws-java-sdk` packages. So, you will need to either add these dependencies in your application or to your spark session. Since we are using a notebook, we will add these packages while creating the spark session in the following config:\n",
    "\n",
    "- `spark.jars.packages`\n",
    "4. AWS File System: Defining S3AFileSystem it's also required for interacting S3 with AWS SDK. Define the value in this config:\n",
    "\n",
    "- `spark.hadoop.fs.s3a.impl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter your AWS Access Key:\")\n",
    "MY_ACCESS_KEY = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter your AWS Secret Key:\")\n",
    "MY_SECRET_KEY = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter your AWS Session Key:\")\n",
    "MY_SESSION_KEY = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter your AWS Region:\")\n",
    "MY_AWS_REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S3 Storage configuration\n",
    "s3_params = {\n",
    "    \"spark.hadoop.fs.s3a.aws.credentials.provider\": \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\",\n",
    "    \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "    \"spark.hadoop.fs.s3a.path.style.access\": \"true\",\n",
    "    \"spark.hadoop.fs.s3a.access.key\": MY_ACCESS_KEY,\n",
    "    \"spark.hadoop.fs.s3a.secret.key\": MY_SECRET_KEY,\n",
    "    \"spark.hadoop.fs.s3a.session.token\": MY_SESSION_KEY,\n",
    "    \"spark.jsl.settings.aws.region\": MY_AWS_REGION,\n",
    "    \"spark.jars.packages\": \"org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk:1.11.901\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LJh0lsD6ANR"
   },
   "source": [
    "## Export and Save HuggingFace model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeVoOo4x6ANS"
   },
   "source": [
    "- Let's install `HuggingFace` and `TensorFlow`. You don't need `TensorFlow` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
    "- We lock TensorFlow on `2.11.0` version and Transformers on `4.25.1`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully.\n",
    "- AlbertTokenizer requires the `SentencePiece` library, so we install that as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ror03LDu6ANT"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.25.1 tensorflow==2.11.0 sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iA9RZAk6ANU"
   },
   "source": [
    "- HuggingFace comes with a native `saved_model` feature inside `save_pretrained` function for TensorFlow based models. We will use that to save it as TF `SavedModel`.\n",
    "- We'll use [albert-base-v2](https://huggingface.co/albert-base-v2) model from HuggingFace as an example\n",
    "- In addition to `TFAlbertModel` we also need to save the `AlbertTokenizer`. This is the same for every model, these are assets needed for tokenization inside Spark NLP.\n",
    "- Since `albert-base-v2` model is PyTorch we will use `from_pt=True` param to convert it to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4WLv5Qd46ANV",
    "outputId": "362594f5-2ab1-4ee7-b2e9-771b2507ca60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try downloading TF weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at albert-base-v2 were not used when initializing TFAlbertModel: ['predictions']\n",
      "- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFAlbertModel were initialized from the model checkpoint at albert-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertTokenizer, TFAlbertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# albert-base-v2\n",
    "MODEL_NAME = 'albert-base-v2'\n",
    "\n",
    "AlbertTokenizer.from_pretrained(MODEL_NAME, return_tensors=\"pt\").save_pretrained(\"./{}_tokenizer\".format(MODEL_NAME))\n",
    "\n",
    "# just in case if there is no TF/Keras file provided in the model\n",
    "# we can just use `from_pt` and convert PyTorch to TensorFlow\n",
    "try:\n",
    "  print('try downloading TF weights')\n",
    "  model = TFAlbertModel.from_pretrained(MODEL_NAME)\n",
    "except:\n",
    "  print('try downloading PyTorch weights')\n",
    "  model = TFAlbertModel.from_pretrained(MODEL_NAME, from_pt=True)\n",
    "\n",
    "# Define TF Signature\n",
    "@tf.function(\n",
    "  input_signature=[\n",
    "      {\n",
    "          \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\n",
    "          \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),\n",
    "          \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),\n",
    "      }\n",
    "  ]\n",
    ")\n",
    "def serving_fn(input):\n",
    "    return model(input)\n",
    "\n",
    "model.save_pretrained(\"./{}\".format(MODEL_NAME), saved_model=True, signatures={\"serving_default\": serving_fn})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOBx9dTF6ANW"
   },
   "source": [
    "Let's have a look inside these two directories and see what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ap30faa6ANX",
    "outputId": "03d3dc26-4c4a-4c5e-d4d3-917791d12844"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 45684\n",
      "-rw-r--r-- 1 root root      792 Jun 19 22:26 config.json\n",
      "drwxr-xr-x 3 root root     4096 Jun 19 22:26 saved_model\n",
      "-rw-r--r-- 1 root root 46771352 Jun 19 22:26 tf_model.h5\n"
     ]
    }
   ],
   "source": [
    "!ls -l {MODEL_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3I7CPpLS6ANZ",
    "outputId": "85ad5058-976b-4b8d-c089-a6dd38aa521a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 10048\n",
      "drwxr-xr-x 2 root root     4096 Jun 19 22:26 assets\n",
      "-rw-r--r-- 1 root root       54 Jun 19 22:26 fingerprint.pb\n",
      "-rw-r--r-- 1 root root    24303 Jun 19 22:26 keras_metadata.pb\n",
      "-rw-r--r-- 1 root root 10248326 Jun 19 22:26 saved_model.pb\n",
      "drwxr-xr-x 2 root root     4096 Jun 19 22:26 variables\n"
     ]
    }
   ],
   "source": [
    "!ls -l {MODEL_NAME}/saved_model/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jUg5qhym6ANZ",
    "outputId": "2b0589d4-6e3b-4d70-b8c2-61f80bf1ba9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 752\n",
      "-rw-r--r-- 1 root root    286 Jun 19 22:25 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root 760289 Jun 19 22:25 spiece.model\n",
      "-rw-r--r-- 1 root root    577 Jun 19 22:25 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "!ls -l {MODEL_NAME}_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMMfK6dT6ANa"
   },
   "source": [
    "- as you can see, we need the SavedModel from `saved_model/1/` path\n",
    "- we also be needing `spiece.model` file from the tokenizer\n",
    "- all we need is to copy `spiece.model` file into `saved_model/1/assets` which Spark NLP will look for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpPFTKt56ANa"
   },
   "outputs": [],
   "source": [
    "# let's copy spiece.model file to saved_model/1/assets\n",
    "!cp {MODEL_NAME}_tokenizer/spiece.model {MODEL_NAME}/saved_model/1/assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now upload the model in your S3 repository. You just need to copy all the contents inside `{MODEL_NAME}/saved_model/1` to an S3 bucket. For example in `s3://my_s3_path/saved_model/albert-base/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter your S3 path to store these models:\")\n",
    "S3_MODEL_PATH = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6ljWc6H6ANb"
   },
   "source": [
    "## Import and Save ALBERT in Spark NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biZWVzsq6ANb"
   },
   "source": [
    "- Let's install and setup Spark NLP in Google Colab\n",
    "- This part is pretty easy via our simple script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjYo-piN6ANb"
   },
   "outputs": [],
   "source": [
    "! pip install -q pyspark==3.2.3 spark-nlp==4.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8meip9m46ANc"
   },
   "source": [
    "Let's start Spark with Spark NLP included via our simple `start()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start(params=s3_params)\n",
    "\n",
    "print(\"Apache Spark version: {}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yKwXaDe6ANc"
   },
   "source": [
    "- Let's use `loadSavedModel` functon in `AlbertEmbeddings` which allows us to load TensorFlow model in SavedModel format\n",
    "- Most params can be set later when you are loading this model in `AlbertEmbeddings` in runtime, so don't worry what you are setting them now\n",
    "- `loadSavedModel` accepts two params, first is the path to the TF SavedModel. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
    "- `setStorageRef` is very important. When you are training a task like NER or any Text Classification, we use this reference to bound the trained model to this specific embeddings so you won't load a different embeddings by mistake and see terrible results ðŸ˜Š\n",
    "- It's up to you what you put in `setStorageRef` but it cannot be changed later on. We usually use the name of the model to be clear, but you can get creative if you want!\n",
    "- The `dimension` param is is purely cosmetic and won't change anything. It's mostly for you to know later via `.getDimension` what is the dimension of your model. So set this accordingly.\n",
    "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBQtwQzP6ANd"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import *\n",
    "\n",
    "albert = AlbertEmbeddings.loadSavedModel(\n",
    "     '{}/saved_model/1'.format(MODEL_NAME),\n",
    "     spark\n",
    " )\\\n",
    " .setInputCols([\"sentence\",'token'])\\\n",
    " .setOutputCol(\"embeddings\")\\\n",
    " .setCaseSensitive(False)\\\n",
    " .setDimension(768)\\\n",
    " .setStorageRef('albert_base_uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHJ9guQm6ANd"
   },
   "source": [
    "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4Tre-q96ANe"
   },
   "outputs": [],
   "source": [
    "albert.write().overwrite().save(\"./{}_spark_nlp\".format(MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhpTt6rb6ANe"
   },
   "source": [
    "Let's clean up stuff we don't need anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWX9a6Xi6ANe"
   },
   "outputs": [],
   "source": [
    "!rm -rf {MODEL_NAME}_tokenizer {MODEL_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXWYSQYI6ANf"
   },
   "source": [
    "Awesome ðŸ˜Ž  !\n",
    "\n",
    "This is your ALBERT model from HuggingFace ðŸ¤— loaded and saved by Spark NLP ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nMQgQsJ16ANf",
    "outputId": "09355ef1-fe9b-4b2d-c15a-cbff45403a13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 56376\n",
      "-rw-r--r-- 1 root root   760289 Jun 19 22:27 albert_spp\n",
      "-rw-r--r-- 1 root root 56957780 Jun 19 22:27 albert_tensorflow\n",
      "drwxr-xr-x 3 root root     4096 Jun 19 22:27 fields\n",
      "drwxr-xr-x 2 root root     4096 Jun 19 22:27 metadata\n"
     ]
    }
   ],
   "source": [
    "! ls -l {MODEL_NAME}_spark_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC5m929Y6ANf"
   },
   "source": [
    "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny RoBERTa model ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQq2nPu46ANg"
   },
   "outputs": [],
   "source": [
    "albert_loaded = AlbertEmbeddings.load(\"./{}_spark_nlp\".format(MODEL_NAME))\\\n",
    "  .setInputCols([\"sentence\",'token'])\\\n",
    "  .setOutputCol(\"embeddings\")\\\n",
    "  .setCaseSensitive(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cTBhsFof6ANh",
    "outputId": "468a256c-5ef2-4958-ca97-814ceeecb28d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'albert_base_uncased'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_loaded.getStorageRef()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOxncom56ANi"
   },
   "source": [
    "That's it! You can now go wild and use hundreds of ALBERT models from HuggingFace ðŸ¤— in Spark NLP ðŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTvRX7dvNrr5"
   },
   "outputs": [],
   "source": [
    "albert = AlbertEmbeddings.loadSavedModel(S3_MODEL_PATH, spark)\\\n",
    " .setInputCols([\"sentence\",'token'])\\\n",
    " .setOutputCol(\"embeddings\")\\\n",
    " .setCaseSensitive(False)\\\n",
    " .setDimension(768)\\\n",
    " .setStorageRef('albert_base_uncased')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
