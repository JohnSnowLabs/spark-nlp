{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vXYNX2lQROB"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20CamemBertForSequenceClassification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zva6MvJyLeWi"
   },
   "source": [
    "## Import CamemBertForSequenceClassification models from HuggingFace ðŸ¤—  into Spark NLP ðŸš€ \n",
    "\n",
    "Let's keep in mind a few things before we start ðŸ˜Š \n",
    "\n",
    "- This feature is only in `Spark NLP 4.2.5` and after. So please make sure you have upgraded to the latest Spark NLP release\n",
    "- You can import CamemBERT models trained/fine-tuned for sequence classification via `CamembertForSequenceClassification` or `TFCamembertForSequenceClassification`. These models are usually under `Sequence Classification` category and have `camembert` in their labels\n",
    "- Reference: [TFCamembertForSequenceClassification](https://huggingface.co/docs/transformers/model_doc/camembert#transformers.TFCamembertForSequenceClassification)\n",
    "- Some [example models](https://huggingface.co/models?other=camembert&pipeline_tag=token-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzxB-Nq6cxOA"
   },
   "source": [
    "## Export and Save HuggingFace model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yNQkhyMHMgkE"
   },
   "source": [
    "- Let's install `HuggingFace` and `TensorFlow`. You don't need `TensorFlow` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n",
    "- We lock TensorFlow on `2.11.0` version and Transformers on `4.25.1`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully.\n",
    "- CamembertTokenizer requires the `SentencePiece` library, so we install that as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95771,
     "status": "ok",
     "timestamp": 1640707909485,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -60
    },
    "id": "hHXgqiWpMfCY",
    "outputId": "3134cc48-78bc-4e03-a79f-748292f7d0a1"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.25.1 tensorflow==2.11.0 sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3AM6bj4P3NS"
   },
   "source": [
    "- HuggingFace comes with a native `saved_model` feature inside `save_pretrained` function for TensorFlow based models. We will use that to save it as TF `SavedModel`.\n",
    "- We'll use [tblard/tf-allocine](https://huggingface.co/tblard/tf-allocine) model from HuggingFace as an example\n",
    "- In addition to `TFCamembertForSequenceClassification` we also need to save the `CamembertTokenizer`. This is the same for every model, these are assets needed for tokenization inside Spark NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 352,
     "status": "ok",
     "timestamp": 1640708841457,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -60
    },
    "id": "ZaiirlSKNhVD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try downloading TF weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFCamembertForSequenceClassification.\n",
      "\n",
      "All the layers of TFCamembertForSequenceClassification were initialized from the model checkpoint at tblard/tf-allocine.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCamembertForSequenceClassification for predictions without further training.\n",
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 422). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tblard/tf-allocine/saved_model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tblard/tf-allocine/saved_model/1/assets\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFCamembertForSequenceClassification, CamembertTokenizer \n",
    "import tensorflow as tf\n",
    "\n",
    "MODEL_NAME = 'tblard/tf-allocine'\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.save_pretrained('./{}_tokenizer/'.format(MODEL_NAME))\n",
    "\n",
    "# just in case if there is no TF/Keras file provided in the model\n",
    "# we can just use `from_pt` and convert PyTorch to TensorFlow\n",
    "try:\n",
    "  print('try downloading TF weights')\n",
    "  model = TFCamembertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "except:\n",
    "  print('try downloading PyTorch weights')\n",
    "  model = TFCamembertForSequenceClassification.from_pretrained(MODEL_NAME, from_pt=True)\n",
    "\n",
    "# Define TF Signature\n",
    "# NOTE: CamemBertForSequenceClassification in Spakr NLP requires int64\n",
    "@tf.function(\n",
    "  input_signature=[\n",
    "      {\n",
    "          \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),\n",
    "          \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\")       \n",
    "      }\n",
    "  ]\n",
    ")\n",
    "def serving_fn(input):\n",
    "    return model(input)\n",
    "\n",
    "model.save_pretrained(\"./{}\".format(MODEL_NAME), saved_model=True, signatures={\"serving_default\": serving_fn})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlgyZuJfS5IB"
   },
   "source": [
    "Let's have a look inside these two directories and see what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 494,
     "status": "ok",
     "timestamp": 1640708154100,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -60
    },
    "id": "p2XCole7TTef",
    "outputId": "7bd16979-4e59-4f6e-d685-4b0f882b5bcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 864824\n",
      "-rw-r--r--  1 maziyar  staff        833 Dec 15 10:17 config.json\n",
      "drwxr-xr-x  3 maziyar  staff         96 Dec 15 10:17 \u001B[34msaved_model\u001B[m\u001B[m\n",
      "-rw-r--r--  1 maziyar  staff  442783552 Dec 15 10:17 tf_model.h5\n"
     ]
    }
   ],
   "source": [
    "!ls -l {MODEL_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 511,
     "status": "ok",
     "timestamp": 1640708154608,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -60
    },
    "id": "r0DOGz8VUR-r",
    "outputId": "49b86052-ec5c-4a97-959d-c2aa5c3b8df5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 18928\n",
      "drwxr-xr-x  2 maziyar  staff       64 Dec 15 10:17 \u001B[34massets\u001B[m\u001B[m\n",
      "-rw-r--r--  1 maziyar  staff       55 Dec 15 10:17 fingerprint.pb\n",
      "-rw-r--r--  1 maziyar  staff   167585 Dec 15 10:17 keras_metadata.pb\n",
      "-rw-r--r--  1 maziyar  staff  9518269 Dec 15 10:17 saved_model.pb\n",
      "drwxr-xr-x  4 maziyar  staff      128 Dec 15 10:17 \u001B[34mvariables\u001B[m\u001B[m\n"
     ]
    }
   ],
   "source": [
    "!ls -l {MODEL_NAME}/saved_model/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1640708154609,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -60
    },
    "id": "Mcm2UpNxUUQN",
    "outputId": "5068af51-5a09-4a60-866b-96b4f4bdd083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1600\n",
      "-rw-r--r--  1 maziyar  staff  810912 Dec 15 10:16 sentencepiece.bpe.model\n",
      "-rw-r--r--  1 maziyar  staff     241 Dec 15 10:16 special_tokens_map.json\n",
      "-rw-r--r--  1 maziyar  staff     717 Dec 15 10:16 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "!ls -l {MODEL_NAME}_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZegMvuGTmHt"
   },
   "source": [
    "- as you can see, we need the SavedModel from `saved_model/1/` path\n",
    "- we also be needing `sentencepiece.bpe.model` file from the tokenizer\n",
    "- all we need is to copy `sentencepiece.bpe.model` file into `saved_model/1/assets` which Spark NLP will look for\n",
    "- in addition to vocabs, we also need `labels` and their `ids` which is saved inside the model's config. We will save this inside `labels.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ez6MT-RTT7ss"
   },
   "outputs": [],
   "source": [
    "asset_path = '{}/saved_model/1/assets'.format(MODEL_NAME)\n",
    "\n",
    "# let's copy sentencepiece.bpe.model file to saved_model/1/assets\n",
    "!cp {MODEL_NAME}_tokenizer/sentencepiece.bpe.model {asset_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vcg_5YP1-vfC"
   },
   "outputs": [],
   "source": [
    "# get label2id dictionary \n",
    "labels = model.config.label2id\n",
    "# sort the dictionary based on the id\n",
    "labels = sorted(labels, key=labels.get)\n",
    "\n",
    "with open(asset_path+'/labels.txt', 'w') as f:\n",
    "    f.write('\\n'.join(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBq7ztzlACYO"
   },
   "source": [
    "Voila! We have our `vocab.txt` and `labels.txt` inside assets directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1640708155273,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -60
    },
    "id": "OYnT5U8N9dxT",
    "outputId": "89764651-6a64-4b11-aaaa-f031a4284e1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1592\n",
      "-rw-r--r--  1 maziyar  staff      17 Dec 15 10:18 labels.txt\n",
      "-rw-r--r--  1 maziyar  staff  810912 Dec 15 10:18 sentencepiece.bpe.model\n"
     ]
    }
   ],
   "source": [
    "! ls -l {asset_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlJKd2tIU0PD"
   },
   "source": [
    "## Import and Save CamemBertForSequenceClassification in Spark NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0FXoxHJc5CU"
   },
   "source": [
    "- Let's install and setup Spark NLP in Google Colab\n",
    "- This part is pretty easy via our simple script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7553,
     "status": "ok",
     "timestamp": 1640708780913,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -60
    },
    "id": "8tpW5nkMc53m",
    "outputId": "2677b2fd-477a-4530-c98b-a8a1ccbd2baa"
   },
   "outputs": [],
   "source": [
    "! wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_NAgx4hdCGP"
   },
   "source": [
    "Let's start Spark with Spark NLP included via our simple `start()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 33750,
     "status": "ok",
     "timestamp": 1640708814657,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -60
    },
    "id": "cbNneAVCLU1y"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "# let's start Spark with Spark NLP\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABTu9MrdVafM"
   },
   "source": [
    "- Let's use `loadSavedModel` functon in `CamemBertForSequenceClassification` which allows us to load TensorFlow model in SavedModel format\n",
    "- Most params can be set later when you are loading this model in `CamemBertForSequenceClassification` in runtime like `setMaxSentenceLength`, so don't worry what you are setting them now\n",
    "- `loadSavedModel` accepts two params, first is the path to the TF SavedModel. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
    "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1640708858933,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -60
    },
    "id": "8W_almibVRTj"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "sequenceClassifier = CamemBertForSequenceClassification\\\n",
    "  .loadSavedModel('{}/saved_model/1'.format(MODEL_NAME), spark)\\\n",
    "  .setInputCols([\"document\",'token'])\\\n",
    "  .setOutputCol(\"class\")\\\n",
    "  .setCaseSensitive(True)\\\n",
    "  .setMaxSentenceLength(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjGiq4KnXWuy"
   },
   "source": [
    "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWu5HfbnXAlM"
   },
   "outputs": [],
   "source": [
    "sequenceClassifier.write().overwrite().save(\"./{}_spark_nlp\".format(MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCrjxPhzDplN"
   },
   "source": [
    "Let's clean up stuff we don't need anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgkVIJshDtLx"
   },
   "outputs": [],
   "source": [
    "! rm -rf {MODEL_NAME}_tokenizer {MODEL_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TSeTRZpXqWO"
   },
   "source": [
    "Awesome ðŸ˜Ž  !\n",
    "\n",
    "This is your CamemBertForTokenClassification model from HuggingFace ðŸ¤—  loaded and saved by Spark NLP ðŸš€ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1640708814658,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -60
    },
    "id": "ogpxSWxOXj3W",
    "outputId": "7fc4e69f-3ab2-4ddc-a3b0-6de95f018c91"
   },
   "outputs": [],
   "source": [
    "! ls -l {MODEL_NAME}_spark_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fbehje7fYTDj"
   },
   "source": [
    "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny CamemBertForTokenClassification model ðŸ˜Š "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 88864,
     "status": "ok",
     "timestamp": 1640708950792,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -60
    },
    "id": "1mm3CvkwYRgs"
   },
   "outputs": [],
   "source": [
    "sequenceClassifier_loaded = CamemBertForSequenceClassification.load(\"./{}_spark_nlp\".format(MODEL_NAME))\\\n",
    "  .setInputCols([\"document\",'token'])\\\n",
    "  .setOutputCol(\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDWNWdBlBpHi"
   },
   "source": [
    "You can see what labels were used to train this model via `getClasses` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGRTNISyYlnO"
   },
   "outputs": [],
   "source": [
    "# .getClasses was introduced in spark-nlp==3.4.0\n",
    "sequenceClassifier_loaded.getClasses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvRBsP2SBpHi"
   },
   "source": [
    "This is how you can use your loaded classifier model in Spark NLP ðŸš€ pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15729,
     "status": "ok",
     "timestamp": 1640708966516,
     "user": {
      "displayName": "Maziyar Panahi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64",
      "userId": "06037986691777662786"
     },
     "user_tz": -60
    },
    "id": "MysnSyi8BpHi",
    "outputId": "c13a1827-770f-48a6-bba6-eda25077f8ef"
   },
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    tokenizer,\n",
    "    sequenceClassifier_loaded    \n",
    "])\n",
    "\n",
    "# couple of simple examples\n",
    "example = spark.createDataFrame([[\"Je m'appelle jean-baptiste et je vis Ã  montrÃ©al\"], ['george washington est allÃ© Ã  washington']]).toDF(\"text\")\n",
    "\n",
    "result = pipeline.fit(example).transform(example)\n",
    "\n",
    "# result is a DataFrame\n",
    "result.select(\"text\", \"class.result\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_he2LDtBYo1h"
   },
   "source": [
    "That's it! You can now go wild and use hundreds of `CamemBertForSequenceClassification` models from HuggingFace ðŸ¤— in Spark NLP ðŸš€ \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HuggingFace in Spark NLP - XlmRoBertaForTokenClassification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 ('transformers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "59794f394f79a45d9851d6706177d59b9a5e9d735b0369dbae4b76bccf016251"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
