
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>sparknlp.annotator.T5Transformer &#8212; Spark NLP 3.2.0 documentation</title>
    
  <link href="../../static/css/theme.css" rel="stylesheet" />
  <link href="../../static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../../static/css/custom.css" />
    
  <link rel="preload" as="script" href="../../static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../static/documentation_options.js"></script>
    <script src="../../static/jquery.js"></script>
    <script src="../../static/underscore.js"></script>
    <script src="../../static/doctools.js"></script>
    <script src="../../static/toggleprompt.js"></script>
    <link rel="shortcut icon" href="../../static/fav.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="sparknlp.annotator.TextMatcher" href="sparknlp.annotator.TextMatcher.html" />
    <link rel="prev" title="sparknlp.annotator.SymmetricDeleteModel" href="sparknlp.annotator.SymmetricDeleteModel.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../index.html">
  <img src="../../static/logo.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../getting_started/index.html">
  Getting Started
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../user_guide/index.html">
  User Guide
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  API Reference
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="sparknlp.annotator.html">
   sparknlp.annotator
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.AlbertEmbeddings.html">
     sparknlp.annotator.AlbertEmbeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.BertEmbeddings.html">
     sparknlp.annotator.BertEmbeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.BertForTokenClassification.html">
     sparknlp.annotator.BertForTokenClassification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.BertSentenceEmbeddings.html">
     sparknlp.annotator.BertSentenceEmbeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.BigTextMatcher.html">
     sparknlp.annotator.BigTextMatcher
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.BigTextMatcherModel.html">
     sparknlp.annotator.BigTextMatcherModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.ChunkEmbeddings.html">
     sparknlp.annotator.ChunkEmbeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.ChunkTokenizer.html">
     sparknlp.annotator.ChunkTokenizer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.ChunkTokenizerModel.html">
     sparknlp.annotator.ChunkTokenizerModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.Chunker.html">
     sparknlp.annotator.Chunker
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.ClassifierDLApproach.html">
     sparknlp.annotator.ClassifierDLApproach
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.ClassifierDLModel.html">
     sparknlp.annotator.ClassifierDLModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.ContextSpellCheckerApproach.html">
     sparknlp.annotator.ContextSpellCheckerApproach
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.ContextSpellCheckerModel.html">
     sparknlp.annotator.ContextSpellCheckerModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.DateMatcher.html">
     sparknlp.annotator.DateMatcher
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.DateMatcherUtils.html">
     sparknlp.annotator.DateMatcherUtils
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.DependencyParserApproach.html">
     sparknlp.annotator.DependencyParserApproach
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.DependencyParserModel.html">
     sparknlp.annotator.DependencyParserModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.DistilBertEmbeddings.html">
     sparknlp.annotator.DistilBertEmbeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.DistilBertForTokenClassification.html">
     sparknlp.annotator.DistilBertForTokenClassification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.DocumentNormalizer.html">
     sparknlp.annotator.DocumentNormalizer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.ElmoEmbeddings.html">
     sparknlp.annotator.ElmoEmbeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.GraphExtraction.html">
     sparknlp.annotator.GraphExtraction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.LanguageDetectorDL.html">
     sparknlp.annotator.LanguageDetectorDL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.Lemmatizer.html">
     sparknlp.annotator.Lemmatizer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.LemmatizerModel.html">
     sparknlp.annotator.LemmatizerModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.LongformerEmbeddings.html">
     sparknlp.annotator.LongformerEmbeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.MarianTransformer.html">
     sparknlp.annotator.MarianTransformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.MultiClassifierDLApproach.html">
     sparknlp.annotator.MultiClassifierDLApproach
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.MultiClassifierDLModel.html">
     sparknlp.annotator.MultiClassifierDLModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.MultiDateMatcher.html">
     sparknlp.annotator.MultiDateMatcher
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.NGramGenerator.html">
     sparknlp.annotator.NGramGenerator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.NerApproach.html">
     sparknlp.annotator.NerApproach
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.NerConverter.html">
     sparknlp.annotator.NerConverter
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.NerCrfApproach.html">
     sparknlp.annotator.NerCrfApproach
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.NerCrfModel.html">
     sparknlp.annotator.NerCrfModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.NerDLApproach.html">
     sparknlp.annotator.NerDLApproach
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.NerDLModel.html">
     sparknlp.annotator.NerDLModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.NerOverwriter.html">
     sparknlp.annotator.NerOverwriter
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.Normalizer.html">
     sparknlp.annotator.Normalizer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.NormalizerModel.html">
     sparknlp.annotator.NormalizerModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.NorvigSweetingApproach.html">
     sparknlp.annotator.NorvigSweetingApproach
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.NorvigSweetingModel.html">
     sparknlp.annotator.NorvigSweetingModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.PerceptronApproach.html">
     sparknlp.annotator.PerceptronApproach
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.PerceptronModel.html">
     sparknlp.annotator.PerceptronModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.RecursiveTokenizer.html">
     sparknlp.annotator.RecursiveTokenizer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.RecursiveTokenizerModel.html">
     sparknlp.annotator.RecursiveTokenizerModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.RegexMatcher.html">
     sparknlp.annotator.RegexMatcher
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.RegexMatcherModel.html">
     sparknlp.annotator.RegexMatcherModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.RegexTokenizer.html">
     sparknlp.annotator.RegexTokenizer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.RoBertaEmbeddings.html">
     sparknlp.annotator.RoBertaEmbeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.SentenceDetector.html">
     sparknlp.annotator.SentenceDetector
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.SentenceDetectorDLApproach.html">
     sparknlp.annotator.SentenceDetectorDLApproach
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.SentenceDetectorDLModel.html">
     sparknlp.annotator.SentenceDetectorDLModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.SentenceDetectorParams.html">
     sparknlp.annotator.SentenceDetectorParams
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.SentenceEmbeddings.html">
     sparknlp.annotator.SentenceEmbeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.SentimentDLApproach.html">
     sparknlp.annotator.SentimentDLApproach
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.SentimentDLModel.html">
     sparknlp.annotator.SentimentDLModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.SentimentDetector.html">
     sparknlp.annotator.SentimentDetector
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.SentimentDetectorModel.html">
     sparknlp.annotator.SentimentDetectorModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.Stemmer.html">
     sparknlp.annotator.Stemmer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.StopWordsCleaner.html">
     sparknlp.annotator.StopWordsCleaner
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.SymmetricDeleteApproach.html">
     sparknlp.annotator.SymmetricDeleteApproach
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.SymmetricDeleteModel.html">
     sparknlp.annotator.SymmetricDeleteModel
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     sparknlp.annotator.T5Transformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.TextMatcher.html">
     sparknlp.annotator.TextMatcher
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.TextMatcherModel.html">
     sparknlp.annotator.TextMatcherModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.Token2Chunk.html">
     sparknlp.annotator.Token2Chunk
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.Tokenizer.html">
     sparknlp.annotator.Tokenizer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.TokenizerModel.html">
     sparknlp.annotator.TokenizerModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.TypedDependencyParserApproach.html">
     sparknlp.annotator.TypedDependencyParserApproach
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.TypedDependencyParserModel.html">
     sparknlp.annotator.TypedDependencyParserModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.UniversalSentenceEncoder.html">
     sparknlp.annotator.UniversalSentenceEncoder
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.ViveknSentimentApproach.html">
     sparknlp.annotator.ViveknSentimentApproach
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.ViveknSentimentModel.html">
     sparknlp.annotator.ViveknSentimentModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.WordEmbeddings.html">
     sparknlp.annotator.WordEmbeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.WordEmbeddingsModel.html">
     sparknlp.annotator.WordEmbeddingsModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.WordSegmenterApproach.html">
     sparknlp.annotator.WordSegmenterApproach
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.WordSegmenterModel.html">
     sparknlp.annotator.WordSegmenterModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.XlmRoBertaEmbeddings.html">
     sparknlp.annotator.XlmRoBertaEmbeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.XlnetEmbeddings.html">
     sparknlp.annotator.XlnetEmbeddings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.annotator.YakeModel.html">
     sparknlp.annotator.YakeModel
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="sparknlp.base.html">
   sparknlp.base
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.base.Chunk2Doc.html">
     sparknlp.base.Chunk2Doc
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.base.Doc2Chunk.html">
     sparknlp.base.Doc2Chunk
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.base.DocumentAssembler.html">
     sparknlp.base.DocumentAssembler
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.base.EmbeddingsFinisher.html">
     sparknlp.base.EmbeddingsFinisher
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.base.Finisher.html">
     sparknlp.base.Finisher
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.base.GraphFinisher.html">
     sparknlp.base.GraphFinisher
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.base.HasRecursiveFit.html">
     sparknlp.base.HasRecursiveFit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.base.HasRecursiveTransform.html">
     sparknlp.base.HasRecursiveTransform
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.base.LightPipeline.html">
     sparknlp.base.LightPipeline
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.base.RecursivePipeline.html">
     sparknlp.base.RecursivePipeline
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.base.RecursivePipelineModel.html">
     sparknlp.base.RecursivePipelineModel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.base.TokenAssembler.html">
     sparknlp.base.TokenAssembler
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="sparknlp.functions.html">
   sparknlp.functions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.functions.explode_annotations_col.html">
     sparknlp.functions.explode_annotations_col
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.functions.filter_by_annotations_col.html">
     sparknlp.functions.filter_by_annotations_col
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.functions.map_annotations.html">
     sparknlp.functions.map_annotations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.functions.map_annotations_array.html">
     sparknlp.functions.map_annotations_array
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.functions.map_annotations_col.html">
     sparknlp.functions.map_annotations_col
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.functions.map_annotations_cols.html">
     sparknlp.functions.map_annotations_cols
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.functions.map_annotations_strict.html">
     sparknlp.functions.map_annotations_strict
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="sparknlp.training.html">
   sparknlp.training
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.training.CoNLL.html">
     sparknlp.training.CoNLL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.training.CoNLLU.html">
     sparknlp.training.CoNLLU
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.training.POS.html">
     sparknlp.training.POS
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sparknlp.training.PubTator.html">
     sparknlp.training.PubTator
    </a>
   </li>
  </ul>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                

<nav id="bd-toc-nav">
    
</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="sparknlp-annotator-t5transformer">
<h1>sparknlp.annotator.T5Transformer<a class="headerlink" href="#sparknlp-annotator-t5transformer" title="Permalink to this headline">¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sparknlp.annotator.</span></span><span class="sig-name descname"><span class="pre">T5Transformer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">classname</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'com.johnsnowlabs.nlp.annotators.seq2seq.T5Transformer'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">java_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../modules/sparknlp/annotator.html#T5Transformer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp.annotator.T5Transformer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sparknlp.common.AnnotatorModel</span></code></p>
<p>T5: the Text-To-Text Transfer Transformer</p>
<p>T5 reconsiders all NLP tasks into a unified text-to-text-format where the
input and output are always text strings, in contrast to BERT-style models
that can only output either a class label or a span of the input. The
text-to-text framework is able to use the same model, loss function, and
hyper-parameters on any NLP task, including machine translation, document
summarization, question answering, and classification tasks (e.g., sentiment
analysis). T5 can even apply to regression tasks by training it to predict
the string representation of a number instead of the number itself.</p>
<p>Pretrained models can be loaded with <a class="reference internal" href="#sparknlp.annotator.T5Transformer.pretrained" title="sparknlp.annotator.T5Transformer.pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pretrained()</span></code></a> of the companion
object:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t5</span> <span class="o">=</span> <span class="n">T5Transformer</span><span class="o">.</span><span class="n">pretrained</span><span class="p">()</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">setTask</span><span class="p">(</span><span class="s2">&quot;summarize:&quot;</span><span class="p">)</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s2">&quot;document&quot;</span><span class="p">])</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s2">&quot;summaries&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The default model is <code class="docutils literal notranslate"><span class="pre">&quot;t5_small&quot;</span></code>, if no name is provided. For available
pretrained models please see the <a class="reference external" href="https://nlp.johnsnowlabs.com/models?q=t5">Models Hub</a>.</p>
<p>For extended examples of usage, see the <a class="reference external" href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/10.T5_Workshop_with_Spark_NLP.ipynb">Spark NLP Workshop</a>.</p>
<table class="table">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Input Annotation types</p></th>
<th class="head"><p>Output Annotation type</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">DOCUMENT</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">DOCUMENT</span></code></p></td>
</tr>
</tbody>
</table>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>configProtoBytes</strong></dt><dd><p>ConfigProto from tensorflow, serialized into byte array.</p>
</dd>
<dt><strong>task</strong></dt><dd><p>Transformer’s task, e.g. <code class="docutils literal notranslate"><span class="pre">summarize:</span></code></p>
</dd>
<dt><strong>minOutputLength</strong></dt><dd><p>Minimum length of the sequence to be generated</p>
</dd>
<dt><strong>maxOutputLength</strong></dt><dd><p>Maximum length of output text</p>
</dd>
<dt><strong>doSample</strong></dt><dd><p>Whether or not to use sampling; use greedy decoding otherwise</p>
</dd>
<dt><strong>temperature</strong></dt><dd><p>The value used to module the next token probabilities</p>
</dd>
<dt><strong>topK</strong></dt><dd><p>The number of highest probability vocabulary tokens to keep for
top-k-filtering</p>
</dd>
<dt><strong>topP</strong></dt><dd><p>Top cumulative probability for vocabulary tokens</p>
<p>If set to float &lt; 1, only the most probable tokens with probabilities
that add up to <code class="docutils literal notranslate"><span class="pre">topP</span></code> or higher are kept for generation.</p>
</dd>
<dt><strong>repetitionPenalty</strong></dt><dd><p>The parameter for repetition penalty. 1.0 means no penalty.</p>
</dd>
<dt><strong>noRepeatNgramSize</strong></dt><dd><p>If set to int &gt; 0, all ngrams of that size can only occur once</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This is a very computationally expensive module especially on larger
sequence. The use of an accelerator such as GPU is recommended.</p>
<p class="rubric">References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html">Exploring Transfer Learning with T5: the Text-To-Text Transfer
Transformer</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text
Transformer</a></p></li>
<li><p><a class="reference external" href="https://github.com/google-research/text-to-text-transfer-transformer">https://github.com/google-research/text-to-text-transfer-transformer</a></p></li>
</ul>
<p><strong>Paper Abstract:</strong></p>
<p><em>Transfer learning, where a model is first pre-trained on a data-rich task
before being fine-tuned on a downstream task, has emerged as a powerful
technique in natural language processing (NLP). The effectiveness of
transfer learning has given rise to a diversity of approaches, methodology,
and practice. In this paper, we explore the landscape of transfer learning
techniques for NLP by introducing a unified framework that converts all
text-based language problems into a text-to-text format. Our systematic
study compares pre-training objectives, architectures, unlabeled data sets,
transfer approaches, and other factors on dozens of language understanding
tasks. By combining the insights from our exploration with scale and our new
Colossal Clean Crawled Corpus, we achieve state-of-the-art results on many
benchmarks covering summarization, question answering, text classification,
and more. To facilitate future work on transfer learning for NLP, we release
our data set, pre-trained models, and code.</em></p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s2">&quot;text&quot;</span><span class="p">)</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s2">&quot;documents&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t5</span> <span class="o">=</span> <span class="n">T5Transformer</span><span class="o">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s2">&quot;t5_small&quot;</span><span class="p">)</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">setTask</span><span class="p">(</span><span class="s2">&quot;summarize:&quot;</span><span class="p">)</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s2">&quot;documents&quot;</span><span class="p">])</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">setMaxOutputLength</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span> \
<span class="gp">... </span>    <span class="o">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s2">&quot;summaries&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span><span class="o">.</span><span class="n">setStages</span><span class="p">([</span><span class="n">documentAssembler</span><span class="p">,</span> <span class="n">t5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([[</span>
<span class="gp">... </span>    <span class="s2">&quot;Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a &quot;</span> <span class="o">+</span>
<span class="gp">... </span>    <span class="s2">&quot;downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness&quot;</span> <span class="o">+</span>
<span class="gp">... </span>    <span class="s2">&quot; of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this &quot;</span> <span class="o">+</span>
<span class="gp">... </span>    <span class="s2">&quot;paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework &quot;</span> <span class="o">+</span>
<span class="gp">... </span>    <span class="s2">&quot;that converts all text-based language problems into a text-to-text format. Our systematic study compares &quot;</span> <span class="o">+</span>
<span class="gp">... </span>    <span class="s2">&quot;pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens &quot;</span> <span class="o">+</span>
<span class="gp">... </span>    <span class="s2">&quot;of language understanding tasks. By combining the insights from our exploration with scale and our new &quot;</span> <span class="o">+</span>
<span class="gp">... </span>    <span class="s2">&quot;Colossal Clean Crawled Corpus, we achieve state-of-the-art results on many benchmarks covering &quot;</span> <span class="o">+</span>
<span class="gp">... </span>    <span class="s2">&quot;summarization, question answering, text classification, and more. To facilitate future work on transfer &quot;</span> <span class="o">+</span>
<span class="gp">... </span>    <span class="s2">&quot;learning for NLP, we release our data set, pre-trained models, and code.&quot;</span>
<span class="gp">... </span><span class="p">]])</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="s2">&quot;text&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;summaries.result&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span class="go">|result                                                                                                                                                                                                        |</span>
<span class="go">+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span class="go">|[transfer learning has emerged as a powerful technique in natural language processing (NLP) the effectiveness of transfer learning has given rise to a diversity of approaches, methodologies, and practice .]|</span>
<span class="go">--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
</pre></div>
</div>
<p class="rubric">Methods</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">__init__</span></code>([classname, java_model])</p></td>
<td><p>Initialize this instance with a Java model object.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.clear" title="sparknlp.annotator.T5Transformer.clear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clear</span></code></a>(param)</p></td>
<td><p>Clears a param from the param map if it has been explicitly set.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.copy" title="sparknlp.annotator.T5Transformer.copy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copy</span></code></a>([extra])</p></td>
<td><p>Creates a copy of this instance with the same uid and some extra params.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.explainParam" title="sparknlp.annotator.T5Transformer.explainParam"><code class="xref py py-obj docutils literal notranslate"><span class="pre">explainParam</span></code></a>(param)</p></td>
<td><p>Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.explainParams" title="sparknlp.annotator.T5Transformer.explainParams"><code class="xref py py-obj docutils literal notranslate"><span class="pre">explainParams</span></code></a>()</p></td>
<td><p>Returns the documentation of all params with their optionally default values and user-supplied values.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.extractParamMap" title="sparknlp.annotator.T5Transformer.extractParamMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">extractParamMap</span></code></a>([extra])</p></td>
<td><p>Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values &lt; user-supplied values &lt; extra.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.getInputCols" title="sparknlp.annotator.T5Transformer.getInputCols"><code class="xref py py-obj docutils literal notranslate"><span class="pre">getInputCols</span></code></a>()</p></td>
<td><p>Gets current column names of input annotations.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.getLazyAnnotator" title="sparknlp.annotator.T5Transformer.getLazyAnnotator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">getLazyAnnotator</span></code></a>()</p></td>
<td><p>Gets whether Annotator should be evaluated lazily in a RecursivePipeline.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.getOrDefault" title="sparknlp.annotator.T5Transformer.getOrDefault"><code class="xref py py-obj docutils literal notranslate"><span class="pre">getOrDefault</span></code></a>(param)</p></td>
<td><p>Gets the value of a param in the user-supplied param map or its default value.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.getOutputCol" title="sparknlp.annotator.T5Transformer.getOutputCol"><code class="xref py py-obj docutils literal notranslate"><span class="pre">getOutputCol</span></code></a>()</p></td>
<td><p>Gets output column name of annotations.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.getParam" title="sparknlp.annotator.T5Transformer.getParam"><code class="xref py py-obj docutils literal notranslate"><span class="pre">getParam</span></code></a>(paramName)</p></td>
<td><p>Gets a param by its name.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.getParamValue" title="sparknlp.annotator.T5Transformer.getParamValue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">getParamValue</span></code></a>(paramName)</p></td>
<td><p>Gets the value of a parameter.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.hasDefault" title="sparknlp.annotator.T5Transformer.hasDefault"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hasDefault</span></code></a>(param)</p></td>
<td><p>Checks whether a param has a default value.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.hasParam" title="sparknlp.annotator.T5Transformer.hasParam"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hasParam</span></code></a>(paramName)</p></td>
<td><p>Tests whether this instance contains a param with a given (string) name.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.isDefined" title="sparknlp.annotator.T5Transformer.isDefined"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isDefined</span></code></a>(param)</p></td>
<td><p>Checks whether a param is explicitly set by user or has a default value.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.isSet" title="sparknlp.annotator.T5Transformer.isSet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isSet</span></code></a>(param)</p></td>
<td><p>Checks whether a param is explicitly set by user.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.load" title="sparknlp.annotator.T5Transformer.load"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load</span></code></a>(path)</p></td>
<td><p>Reads an ML instance from the input path, a shortcut of <cite>read().load(path)</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.loadSavedModel" title="sparknlp.annotator.T5Transformer.loadSavedModel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">loadSavedModel</span></code></a>(folder, spark_session)</p></td>
<td><p>Loads a locally saved model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.pretrained" title="sparknlp.annotator.T5Transformer.pretrained"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained</span></code></a>([name, lang, remote_loc])</p></td>
<td><p>Downloads and loads a pretrained model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.read" title="sparknlp.annotator.T5Transformer.read"><code class="xref py py-obj docutils literal notranslate"><span class="pre">read</span></code></a>()</p></td>
<td><p>Returns an MLReader instance for this class.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.save" title="sparknlp.annotator.T5Transformer.save"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save</span></code></a>(path)</p></td>
<td><p>Save this ML instance to the given path, a shortcut of ‘write().save(path)’.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.set" title="sparknlp.annotator.T5Transformer.set"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set</span></code></a>(param, value)</p></td>
<td><p>Sets a parameter in the embedded param map.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.setConfigProtoBytes" title="sparknlp.annotator.T5Transformer.setConfigProtoBytes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setConfigProtoBytes</span></code></a>(b)</p></td>
<td><p>Sets configProto from tensorflow, serialized into byte array.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.setDoSample" title="sparknlp.annotator.T5Transformer.setDoSample"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setDoSample</span></code></a>(value)</p></td>
<td><p>Sets whether or not to use sampling, use greedy decoding otherwise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.setInputCols" title="sparknlp.annotator.T5Transformer.setInputCols"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setInputCols</span></code></a>(*value)</p></td>
<td><p>Sets column names of input annotations.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.setLazyAnnotator" title="sparknlp.annotator.T5Transformer.setLazyAnnotator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setLazyAnnotator</span></code></a>(value)</p></td>
<td><p>Sets whether Annotator should be evaluated lazily in a RecursivePipeline.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.setMaxOutputLength" title="sparknlp.annotator.T5Transformer.setMaxOutputLength"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setMaxOutputLength</span></code></a>(value)</p></td>
<td><p>Sets maximum length of output text.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.setMinOutputLength" title="sparknlp.annotator.T5Transformer.setMinOutputLength"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setMinOutputLength</span></code></a>(value)</p></td>
<td><p>Sets minimum length of the sequence to be generated.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.setNoRepeatNgramSize" title="sparknlp.annotator.T5Transformer.setNoRepeatNgramSize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setNoRepeatNgramSize</span></code></a>(value)</p></td>
<td><p>Sets size of n-grams that can only occur once.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.setOutputCol" title="sparknlp.annotator.T5Transformer.setOutputCol"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setOutputCol</span></code></a>(value)</p></td>
<td><p>Sets output column name of annotations.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.setParamValue" title="sparknlp.annotator.T5Transformer.setParamValue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setParamValue</span></code></a>(paramName)</p></td>
<td><p>Sets the value of a parameter.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">setParams</span></code>()</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.setRepetitionPenalty" title="sparknlp.annotator.T5Transformer.setRepetitionPenalty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setRepetitionPenalty</span></code></a>(value)</p></td>
<td><p>Sets the parameter for repetition penalty.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.setTask" title="sparknlp.annotator.T5Transformer.setTask"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setTask</span></code></a>(value)</p></td>
<td><p>Sets the transformer’s task, e.g.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.setTemperature" title="sparknlp.annotator.T5Transformer.setTemperature"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setTemperature</span></code></a>(value)</p></td>
<td><p>Sets the value used to module the next token probabilities.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.setTopK" title="sparknlp.annotator.T5Transformer.setTopK"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setTopK</span></code></a>(value)</p></td>
<td><p>Sets the number of highest probability vocabulary tokens to keep for top-k-filtering.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.setTopP" title="sparknlp.annotator.T5Transformer.setTopP"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setTopP</span></code></a>(value)</p></td>
<td><p>Sets the top cumulative probability for vocabulary tokens.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.transform" title="sparknlp.annotator.T5Transformer.transform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transform</span></code></a>(dataset[, params])</p></td>
<td><p>Transforms the input dataset with optional parameters.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.write" title="sparknlp.annotator.T5Transformer.write"><code class="xref py py-obj docutils literal notranslate"><span class="pre">write</span></code></a>()</p></td>
<td><p>Returns an MLWriter instance for this ML instance.</p></td>
</tr>
</tbody>
</table>
<p class="rubric">Attributes</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">configProtoBytes</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">doSample</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">getter_attrs</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">inputCols</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">lazyAnnotator</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">maxOutputLength</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">minOutputLength</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">name</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">noRepeatNgramSize</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">outputCol</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp.annotator.T5Transformer.params" title="sparknlp.annotator.T5Transformer.params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">params</span></code></a></p></td>
<td><p>Returns all params ordered by name.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">repetitionPenalty</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">task</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">temperature</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">topK</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">topP</span></code></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.clear">
<span class="sig-name descname"><span class="pre">clear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.clear" title="Permalink to this definition">¶</a></dt>
<dd><p>Clears a param from the param map if it has been explicitly set.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.copy">
<span class="sig-name descname"><span class="pre">copy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">extra</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a copy of this instance with the same uid and some
extra params. This implementation first calls Params.copy and
then make a copy of the companion Java pipeline component with
extra params. So both the Python wrapper and the Java pipeline
component get copied.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>extra</strong> – Extra parameters to copy to the new instance</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Copy of this instance</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.explainParam">
<span class="sig-name descname"><span class="pre">explainParam</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.explainParam" title="Permalink to this definition">¶</a></dt>
<dd><p>Explains a single param and returns its name, doc, and optional
default value and user-supplied value in a string.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.explainParams">
<span class="sig-name descname"><span class="pre">explainParams</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.explainParams" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the documentation of all params with their optionally
default values and user-supplied values.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.extractParamMap">
<span class="sig-name descname"><span class="pre">extractParamMap</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">extra</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.extractParamMap" title="Permalink to this definition">¶</a></dt>
<dd><p>Extracts the embedded default param values and user-supplied
values, and then merges them with extra values from input into
a flat param map, where the latter value is used if there exist
conflicts, i.e., with ordering: default param values &lt;
user-supplied values &lt; extra.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>extra</strong> – extra param values</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>merged param map</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.getInputCols">
<span class="sig-name descname"><span class="pre">getInputCols</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.getInputCols" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets current column names of input annotations.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.getLazyAnnotator">
<span class="sig-name descname"><span class="pre">getLazyAnnotator</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.getLazyAnnotator" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets whether Annotator should be evaluated lazily in a
RecursivePipeline.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.getOrDefault">
<span class="sig-name descname"><span class="pre">getOrDefault</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.getOrDefault" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the value of a param in the user-supplied param map or its
default value. Raises an error if neither is set.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.getOutputCol">
<span class="sig-name descname"><span class="pre">getOutputCol</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.getOutputCol" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets output column name of annotations.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.getParam">
<span class="sig-name descname"><span class="pre">getParam</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">paramName</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.getParam" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets a param by its name.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.getParamValue">
<span class="sig-name descname"><span class="pre">getParamValue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">paramName</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.getParamValue" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the value of a parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>paramName</strong><span class="classifier">str</span></dt><dd><p>Name of the parameter</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.hasDefault">
<span class="sig-name descname"><span class="pre">hasDefault</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.hasDefault" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks whether a param has a default value.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.hasParam">
<span class="sig-name descname"><span class="pre">hasParam</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">paramName</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.hasParam" title="Permalink to this definition">¶</a></dt>
<dd><p>Tests whether this instance contains a param with a given
(string) name.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.isDefined">
<span class="sig-name descname"><span class="pre">isDefined</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.isDefined" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks whether a param is explicitly set by user or has
a default value.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.isSet">
<span class="sig-name descname"><span class="pre">isSet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.isSet" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks whether a param is explicitly set by user.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.load">
<em class="property"><span class="pre">classmethod</span> </em><span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Reads an ML instance from the input path, a shortcut of <cite>read().load(path)</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.loadSavedModel">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">loadSavedModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">folder</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spark_session</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../modules/sparknlp/annotator.html#T5Transformer.loadSavedModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp.annotator.T5Transformer.loadSavedModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a locally saved model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>folder</strong><span class="classifier">str</span></dt><dd><p>Folder of the saved model</p>
</dd>
<dt><strong>spark_session</strong><span class="classifier">pyspark.sql.SparkSession</span></dt><dd><p>The current SparkSession</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>T5Transformer</dt><dd><p>The restored model</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.params">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">params</span></span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.params" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns all params ordered by name. The default implementation
uses <code class="xref py py-func docutils literal notranslate"><span class="pre">dir()</span></code> to get all attributes of type
<code class="xref py py-class docutils literal notranslate"><span class="pre">Param</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.pretrained">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'t5_small'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lang</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'en'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remote_loc</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../modules/sparknlp/annotator.html#T5Transformer.pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp.annotator.T5Transformer.pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Downloads and loads a pretrained model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>name</strong><span class="classifier">str, optional</span></dt><dd><p>Name of the pretrained model, by default “t5_small”</p>
</dd>
<dt><strong>lang</strong><span class="classifier">str, optional</span></dt><dd><p>Language of the pretrained model, by default “en”</p>
</dd>
<dt><strong>remote_loc</strong><span class="classifier">str, optional</span></dt><dd><p>Optional remote address of the resource, by default None. Will use
Spark NLPs repositories otherwise.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>T5Transformer</dt><dd><p>The restored model</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.read">
<em class="property"><span class="pre">classmethod</span> </em><span class="sig-name descname"><span class="pre">read</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.read" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an MLReader instance for this class.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save this ML instance to the given path, a shortcut of ‘write().save(path)’.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.set">
<span class="sig-name descname"><span class="pre">set</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.set" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets a parameter in the embedded param map.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.setConfigProtoBytes">
<span class="sig-name descname"><span class="pre">setConfigProtoBytes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">b</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../modules/sparknlp/annotator.html#T5Transformer.setConfigProtoBytes"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp.annotator.T5Transformer.setConfigProtoBytes" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets configProto from tensorflow, serialized into byte array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>b</strong><span class="classifier">List[str]</span></dt><dd><p>ConfigProto from tensorflow, serialized into byte array</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.setDoSample">
<span class="sig-name descname"><span class="pre">setDoSample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../modules/sparknlp/annotator.html#T5Transformer.setDoSample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp.annotator.T5Transformer.setDoSample" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets whether or not to use sampling, use greedy decoding otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>value</strong><span class="classifier">bool</span></dt><dd><p>Whether or not to use sampling; use greedy decoding otherwise</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.setInputCols">
<span class="sig-name descname"><span class="pre">setInputCols</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.setInputCols" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets column names of input annotations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>*value</strong><span class="classifier">str</span></dt><dd><p>Input columns for the annotator</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.setLazyAnnotator">
<span class="sig-name descname"><span class="pre">setLazyAnnotator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.setLazyAnnotator" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets whether Annotator should be evaluated lazily in a
RecursivePipeline.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>value</strong><span class="classifier">bool</span></dt><dd><p>Whether Annotator should be evaluated lazily in a
RecursivePipeline</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.setMaxOutputLength">
<span class="sig-name descname"><span class="pre">setMaxOutputLength</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../modules/sparknlp/annotator.html#T5Transformer.setMaxOutputLength"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp.annotator.T5Transformer.setMaxOutputLength" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets maximum length of output text.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>value</strong><span class="classifier">int</span></dt><dd><p>Maximum length of output text</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.setMinOutputLength">
<span class="sig-name descname"><span class="pre">setMinOutputLength</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../modules/sparknlp/annotator.html#T5Transformer.setMinOutputLength"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp.annotator.T5Transformer.setMinOutputLength" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets minimum length of the sequence to be generated.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>value</strong><span class="classifier">int</span></dt><dd><p>Minimum length of the sequence to be generated</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.setNoRepeatNgramSize">
<span class="sig-name descname"><span class="pre">setNoRepeatNgramSize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../modules/sparknlp/annotator.html#T5Transformer.setNoRepeatNgramSize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp.annotator.T5Transformer.setNoRepeatNgramSize" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets size of n-grams that can only occur once.</p>
<p>If set to int &gt; 0, all ngrams of that size can only occur once.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>value</strong><span class="classifier">int</span></dt><dd><p>N-gram size can only occur once</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.setOutputCol">
<span class="sig-name descname"><span class="pre">setOutputCol</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.setOutputCol" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets output column name of annotations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>value</strong><span class="classifier">str</span></dt><dd><p>Name of output column</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.setParamValue">
<span class="sig-name descname"><span class="pre">setParamValue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">paramName</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.setParamValue" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the value of a parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>paramName</strong><span class="classifier">str</span></dt><dd><p>Name of the parameter</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.setRepetitionPenalty">
<span class="sig-name descname"><span class="pre">setRepetitionPenalty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../modules/sparknlp/annotator.html#T5Transformer.setRepetitionPenalty"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp.annotator.T5Transformer.setRepetitionPenalty" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the parameter for repetition penalty. 1.0 means no penalty.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>value</strong><span class="classifier">float</span></dt><dd><p>The repetition penalty</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<p>See <a class="reference external" href="https://arxiv.org/pdf/1909.05858.pdf">Ctrl: A Conditional Transformer Language Model For Controllable
Generation</a> for more details.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.setTask">
<span class="sig-name descname"><span class="pre">setTask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../modules/sparknlp/annotator.html#T5Transformer.setTask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp.annotator.T5Transformer.setTask" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the transformer’s task, e.g. <code class="docutils literal notranslate"><span class="pre">summarize:</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>value</strong><span class="classifier">str</span></dt><dd><p>The transformer’s task</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.setTemperature">
<span class="sig-name descname"><span class="pre">setTemperature</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../modules/sparknlp/annotator.html#T5Transformer.setTemperature"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp.annotator.T5Transformer.setTemperature" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the value used to module the next token probabilities.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>value</strong><span class="classifier">float</span></dt><dd><p>The value used to module the next token probabilities</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.setTopK">
<span class="sig-name descname"><span class="pre">setTopK</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../modules/sparknlp/annotator.html#T5Transformer.setTopK"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp.annotator.T5Transformer.setTopK" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the number of highest probability vocabulary tokens to keep for
top-k-filtering.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>value</strong><span class="classifier">int</span></dt><dd><p>Number of highest probability vocabulary tokens to keep</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.setTopP">
<span class="sig-name descname"><span class="pre">setTopP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../modules/sparknlp/annotator.html#T5Transformer.setTopP"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp.annotator.T5Transformer.setTopP" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the top cumulative probability for vocabulary tokens.</p>
<p>If set to float &lt; 1, only the most probable tokens with probabilities
that add up to <code class="docutils literal notranslate"><span class="pre">topP</span></code> or higher are kept for generation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>value</strong><span class="classifier">float</span></dt><dd><p>Cumulative probability for vocabulary tokens</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.transform">
<span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transforms the input dataset with optional parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> – input dataset, which is an instance of <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame" title="(in PySpark vmaster)"><code class="xref py py-class docutils literal notranslate"><span class="pre">pyspark.sql.DataFrame</span></code></a></p></li>
<li><p><strong>params</strong> – an optional param map that overrides embedded params.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>transformed dataset</p>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.3.0.</span></p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.uid">
<span class="sig-name descname"><span class="pre">uid</span></span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.uid" title="Permalink to this definition">¶</a></dt>
<dd><p>A unique id for the object.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp.annotator.T5Transformer.write">
<span class="sig-name descname"><span class="pre">write</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sparknlp.annotator.T5Transformer.write" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an MLWriter instance for this ML instance.</p>
</dd></dl>

</dd></dl>

</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="sparknlp.annotator.SymmetricDeleteModel.html" title="previous page">sparknlp.annotator.SymmetricDeleteModel</a>
    <a class='right-next' id="next-link" href="sparknlp.annotator.TextMatcher.html" title="next page">sparknlp.annotator.TextMatcher</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, John Snow Labs.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.2.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>