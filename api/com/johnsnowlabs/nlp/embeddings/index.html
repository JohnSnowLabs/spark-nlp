<!DOCTYPE html >
<html>
        <head>
          <meta http-equiv="X-UA-Compatible" content="IE=edge" />
          <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
          <title>Spark NLP 3.1.2 ScalaDoc  - com.johnsnowlabs.nlp.embeddings</title>
          <meta name="description" content="Spark NLP 3.1.2 ScalaDoc - com.johnsnowlabs.nlp.embeddings" />
          <meta name="keywords" content="Spark NLP 3.1.2 ScalaDoc com.johnsnowlabs.nlp.embeddings" />
          <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
          
      
      <link href="../../../../lib/index.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../../../lib/template.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../../../lib/diagrams.css" media="screen" type="text/css" rel="stylesheet" id="diagrams-css" />
      <script type="text/javascript" src="../../../../lib/jquery.min.js"></script>
      <script type="text/javascript" src="../../../../lib/jquery.panzoom.min.js"></script>
      <script type="text/javascript" src="../../../../lib/jquery.mousewheel.min.js"></script>
      <script type="text/javascript" src="../../../../lib/index.js"></script>
      <script type="text/javascript" src="../../../../index.js"></script>
      <script type="text/javascript" src="../../../../lib/scheduler.js"></script>
      <script type="text/javascript" src="../../../../lib/template.js"></script>
      
      <script type="text/javascript">
        /* this variable can be used by the JS to determine the path to the root document */
        var toRoot = '../../../../';
      </script>
    
        </head>
        <body>
      <div id="search">
        <span id="doc-title">Spark NLP 3.1.2 ScalaDoc<span id="doc-version"></span></span>
        <span class="close-results"><span class="left">&lt;</span> Back</span>
        <div id="textfilter">
          <span class="input">
            <input autocapitalize="none" placeholder="Search" id="index-input" type="text" accesskey="/" />
            <i class="clear material-icons"></i>
            <i id="search-icon" class="material-icons"></i>
          </span>
        </div>
    </div>
      <div id="search-results">
        <div id="search-progress">
          <div id="progress-fill"></div>
        </div>
        <div id="results-content">
          <div id="entity-results"></div>
          <div id="member-results"></div>
        </div>
      </div>
      <div id="content-scroll-container" style="-webkit-overflow-scrolling: touch;">
        <div id="content-container" style="-webkit-overflow-scrolling: touch;">
          <div id="subpackage-spacer">
            <div id="packages">
              <h1>Packages</h1>
              <ul>
                <li name="_root_.root" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="_root_"></a><a id="root:_root_"></a>
      <span class="permalink">
      <a href="../../../../index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../../../../index.html"><span class="name">root</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../../../../index.html" class="extype" name="_root_">root</a></dd></dl></div>
    </li><li name="_root_.com" visbl="pub" class="indented1 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="com"></a><a id="com:com"></a>
      <span class="permalink">
      <a href="../../../../com/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../../../index.html"><span class="name">com</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../../../../index.html" class="extype" name="_root_">root</a></dd></dl></div>
    </li><li name="com.johnsnowlabs" visbl="pub" class="indented2 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="johnsnowlabs"></a><a id="johnsnowlabs:johnsnowlabs"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../../index.html"><span class="name">johnsnowlabs</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../../../index.html" class="extype" name="com">com</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp" visbl="pub" class="indented3 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="nlp"></a><a id="nlp:nlp"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../index.html"><span class="name">nlp</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../../index.html" class="extype" name="com.johnsnowlabs">johnsnowlabs</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators" visbl="pub" class="indented4 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="annotators"></a><a id="annotators:annotators"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/annotators/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../annotators/index.html"><span class="name">annotators</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings" visbl="pub" class="indented4 current" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="embeddings"></a><a id="embeddings:embeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <span class="name">embeddings</span>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></dd></dl></div>
    </li><li class="current-entities indented4">
                        <a class="object" href="AlbertEmbeddings$.html" title="This is the companion object of AlbertEmbeddings."></a>
                        <a class="class" href="AlbertEmbeddings.html" title="ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS - Google Research, Toyota Technological Institute at Chicago"></a>
                        <a href="AlbertEmbeddings.html" title="ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS - Google Research, Toyota Technological Institute at Chicago">AlbertEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="BertEmbeddings$.html" title="This is the companion object of BertEmbeddings."></a>
                        <a class="class" href="BertEmbeddings.html" title="Token-level embeddings using BERT."></a>
                        <a href="BertEmbeddings.html" title="Token-level embeddings using BERT.">BertEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="BertSentenceEmbeddings$.html" title="This is the companion object of BertSentenceEmbeddings."></a>
                        <a class="class" href="BertSentenceEmbeddings.html" title="Sentence-level embeddings using BERT."></a>
                        <a href="BertSentenceEmbeddings.html" title="Sentence-level embeddings using BERT.">BertSentenceEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="ChunkEmbeddings$.html" title="This is the companion object of ChunkEmbeddings."></a>
                        <a class="class" href="ChunkEmbeddings.html" title="This annotator utilizes WordEmbeddings, BertEmbeddings etc."></a>
                        <a href="ChunkEmbeddings.html" title="This annotator utilizes WordEmbeddings, BertEmbeddings etc.">ChunkEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="DistilBertEmbeddings$.html" title="This is the companion object of DistilBertEmbeddings."></a>
                        <a class="class" href="DistilBertEmbeddings.html" title="DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base."></a>
                        <a href="DistilBertEmbeddings.html" title="DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base.">DistilBertEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="ElmoEmbeddings$.html" title="This is the companion object of ElmoEmbeddings."></a>
                        <a class="class" href="ElmoEmbeddings.html" title="Word embeddings from ELMo (Embeddings from Language Models), a language model trained on the 1 Billion Word Benchmark."></a>
                        <a href="ElmoEmbeddings.html" title="Word embeddings from ELMo (Embeddings from Language Models), a language model trained on the 1 Billion Word Benchmark.">ElmoEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="EmbeddingsCoverage.html" title=""></a>
                        <a href="EmbeddingsCoverage.html" title="">EmbeddingsCoverage</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="HasEmbeddingsProperties.html" title=""></a>
                        <a href="HasEmbeddingsProperties.html" title="">HasEmbeddingsProperties</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="object" href="PoolingStrategy$.html" title=""></a>
                        <a href="PoolingStrategy$.html" title="">PoolingStrategy</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadAlbertTensorflowModel.html" title=""></a>
                        <a href="ReadAlbertTensorflowModel.html" title="">ReadAlbertTensorflowModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadBertSentenceTensorflowModel.html" title=""></a>
                        <a href="ReadBertSentenceTensorflowModel.html" title="">ReadBertSentenceTensorflowModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadBertTensorflowModel.html" title=""></a>
                        <a href="ReadBertTensorflowModel.html" title="">ReadBertTensorflowModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadDistilBertTensorflowModel.html" title=""></a>
                        <a href="ReadDistilBertTensorflowModel.html" title="">ReadDistilBertTensorflowModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadElmoTensorflowModel.html" title=""></a>
                        <a href="ReadElmoTensorflowModel.html" title="">ReadElmoTensorflowModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadRobertaTensorflowModel.html" title=""></a>
                        <a href="ReadRobertaTensorflowModel.html" title="">ReadRobertaTensorflowModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadUSETensorflowModel.html" title=""></a>
                        <a href="ReadUSETensorflowModel.html" title="">ReadUSETensorflowModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadXlmRobertaTensorflowModel.html" title=""></a>
                        <a href="ReadXlmRobertaTensorflowModel.html" title="">ReadXlmRobertaTensorflowModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadXlnetTensorflowModel.html" title=""></a>
                        <a href="ReadXlnetTensorflowModel.html" title="">ReadXlnetTensorflowModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedAlbertModel.html" title=""></a>
                        <a href="ReadablePretrainedAlbertModel.html" title="">ReadablePretrainedAlbertModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedBertModel.html" title=""></a>
                        <a href="ReadablePretrainedBertModel.html" title="">ReadablePretrainedBertModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedBertSentenceModel.html" title=""></a>
                        <a href="ReadablePretrainedBertSentenceModel.html" title="">ReadablePretrainedBertSentenceModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedDistilBertModel.html" title=""></a>
                        <a href="ReadablePretrainedDistilBertModel.html" title="">ReadablePretrainedDistilBertModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedElmoModel.html" title=""></a>
                        <a href="ReadablePretrainedElmoModel.html" title="">ReadablePretrainedElmoModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedRobertaModel.html" title=""></a>
                        <a href="ReadablePretrainedRobertaModel.html" title="">ReadablePretrainedRobertaModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedUSEModel.html" title=""></a>
                        <a href="ReadablePretrainedUSEModel.html" title="">ReadablePretrainedUSEModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedWordEmbeddings.html" title=""></a>
                        <a href="ReadablePretrainedWordEmbeddings.html" title="">ReadablePretrainedWordEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedXlmRobertaModel.html" title=""></a>
                        <a href="ReadablePretrainedXlmRobertaModel.html" title="">ReadablePretrainedXlmRobertaModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedXlnetModel.html" title=""></a>
                        <a href="ReadablePretrainedXlnetModel.html" title="">ReadablePretrainedXlnetModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadsFromBytes.html" title=""></a>
                        <a href="ReadsFromBytes.html" title="">ReadsFromBytes</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="RoBertaEmbeddings$.html" title="This is the companion object of RoBertaEmbeddings."></a>
                        <a class="class" href="RoBertaEmbeddings.html" title="The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov."></a>
                        <a href="RoBertaEmbeddings.html" title="The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.">RoBertaEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="SentenceEmbeddings$.html" title="This is the companion object of SentenceEmbeddings."></a>
                        <a class="class" href="SentenceEmbeddings.html" title="Converts the results from WordEmbeddings, BertEmbeddings, or ElmoEmbeddings into sentence or document embeddings by either summing up or averaging all the word embeddings in a sentence or a document (depending on the inputCols)."></a>
                        <a href="SentenceEmbeddings.html" title="Converts the results from WordEmbeddings, BertEmbeddings, or ElmoEmbeddings into sentence or document embeddings by either summing up or averaging all the word embeddings in a sentence or a document (depending on the inputCols).">SentenceEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="UniversalSentenceEncoder$.html" title="This is the companion object of UniversalSentenceEncoder."></a>
                        <a class="class" href="UniversalSentenceEncoder.html" title="The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks."></a>
                        <a href="UniversalSentenceEncoder.html" title="The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.">UniversalSentenceEncoder</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="WordEmbeddings$.html" title="This is the companion object of WordEmbeddings."></a>
                        <a class="class" href="WordEmbeddings.html" title="Word Embeddings lookup annotator that maps tokens to vectors."></a>
                        <a href="WordEmbeddings.html" title="Word Embeddings lookup annotator that maps tokens to vectors.">WordEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="object" href="WordEmbeddingsBinaryIndexer$.html" title=""></a>
                        <a href="WordEmbeddingsBinaryIndexer$.html" title="">WordEmbeddingsBinaryIndexer</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="WordEmbeddingsModel$.html" title="This is the companion object of WordEmbeddingsModel."></a>
                        <a class="class" href="WordEmbeddingsModel.html" title="Word Embeddings lookup annotator that maps tokens to vectors"></a>
                        <a href="WordEmbeddingsModel.html" title="Word Embeddings lookup annotator that maps tokens to vectors">WordEmbeddingsModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="class" href="WordEmbeddingsReader.html" title=""></a>
                        <a href="WordEmbeddingsReader.html" title="">WordEmbeddingsReader</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="object" href="WordEmbeddingsTextIndexer$.html" title=""></a>
                        <a href="WordEmbeddingsTextIndexer$.html" title="">WordEmbeddingsTextIndexer</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="class" href="WordEmbeddingsWriter.html" title=""></a>
                        <a href="WordEmbeddingsWriter.html" title="">WordEmbeddingsWriter</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="XlmRoBertaEmbeddings$.html" title=""></a>
                        <a class="class" href="XlmRoBertaEmbeddings.html" title="The XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov."></a>
                        <a href="XlmRoBertaEmbeddings.html" title="The XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.">XlmRoBertaEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="XlnetEmbeddings$.html" title="This is the companion object of XlnetEmbeddings."></a>
                        <a class="class" href="XlnetEmbeddings.html" title="XlnetEmbeddings (XLNet): Generalized Autoregressive Pretraining for Language Understanding"></a>
                        <a href="XlnetEmbeddings.html" title="XlnetEmbeddings (XLNet): Generalized Autoregressive Pretraining for Language Understanding">XlnetEmbeddings</a>
                      </li><li name="com.johnsnowlabs.nlp.pretrained" visbl="pub" class="indented4 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="pretrained"></a><a id="pretrained:pretrained"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/pretrained/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../pretrained/index.html"><span class="name">pretrained</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.recursive" visbl="pub" class="indented4 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="recursive"></a><a id="recursive:recursive"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/recursive/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../recursive/index.html"><span class="name">recursive</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.serialization" visbl="pub" class="indented4 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="serialization"></a><a id="serialization:serialization"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/serialization/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../serialization/index.html"><span class="name">serialization</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.training" visbl="pub" class="indented4 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="training"></a><a id="training:training"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/training/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../training/index.html"><span class="name">training</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.util" visbl="pub" class="indented4 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="util"></a><a id="util:util"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/util/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../util/index.html"><span class="name">util</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></dd></dl></div>
    </li>
              </ul>
            </div>
          </div>
          <div id="content">
            <body class="package value">
      <div id="definition">
        <div class="big-circle package">p</div>
        <p id="owner"><a href="../../../index.html" class="extype" name="com">com</a>.<a href="../../index.html" class="extype" name="com.johnsnowlabs">johnsnowlabs</a>.<a href="../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></p>
        <h1>embeddings<span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span></h1>
        
      </div>

      <h4 id="signature" class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <span class="name">embeddings</span>
      </span>
      </h4>

      
          <div id="comment" class="fullcommenttop"></div>
        

      <div id="mbrsel">
        <div class="toggle"></div>
        <div id="memberfilter">
          <i class="material-icons arrow"></i>
          <span class="input">
            <input id="mbrsel-input" placeholder="Filter all members" type="text" accesskey="/" />
          </span>
          <i class="clear material-icons"></i>
        </div>
        <div id="filterby">
          <div id="order">
            <span class="filtertype">Ordering</span>
            <ol>
              
              <li class="alpha in"><span>Alphabetic</span></li>
              
            </ol>
          </div>
          
          <div id="visbl">
              <span class="filtertype">Visibility</span>
              <ol><li class="public in"><span>Public</span></li><li class="all out"><span>All</span></li></ol>
            </div>
        </div>
      </div>

      <div id="template">
        <div id="allMembers">
        

        <div id="types" class="types members">
              <h3>Type Members</h3>
              <ol><li name="com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="AlbertEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings]withWriteTensorflowModelwithWriteSentencePieceModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitiveProperties"></a><a id="AlbertEmbeddings:AlbertEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/AlbertEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS - Google Research, Toyota Technological Institute at Chicago" href="AlbertEmbeddings.html"><span class="name">AlbertEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="AlbertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings">AlbertEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="AlbertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings">AlbertEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/WriteSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.WriteSentencePieceModel">WriteSentencePieceModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a></span>
      </span>
      
      <p class="shortcomment cmt">ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS - Google Research, Toyota Technological Institute at Chicago</p><div class="fullcomment"><div class="comment cmt"><p>ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS - Google Research, Toyota Technological Institute at Chicago</p><p>These word embeddings represent the outputs generated by the Albert model.
All official Albert releases by google in TF-HUB are supported with this Albert Wrapper:</p><p><b>Ported TF-Hub Models:</b></p><p><code>&quot;albert_base_uncased&quot;</code>    | <a href="https://tfhub.dev/google/albert_base/3" target="_blank">albert_base</a>       |  768-embed-dim,   12-layer,  12-heads, 12M parameters</p><p><code>&quot;albert_large_uncased&quot;</code>   | <a href="https://tfhub.dev/google/albert_large/3" target="_blank">albert_large</a>     |  1024-embed-dim,  24-layer,  16-heads, 18M parameters</p><p><code>&quot;albert_xlarge_uncased&quot;</code>  | <a href="https://tfhub.dev/google/albert_xlarge/3" target="_blank">albert_xlarge</a>   |  2048-embed-dim,  24-layer,  32-heads, 60M parameters</p><p><code>&quot;albert_xxlarge_uncased&quot;</code> | <a href="https://tfhub.dev/google/albert_xxlarge/3" target="_blank">albert_xxlarge</a> |  4096-embed-dim,  12-layer,  64-heads, 235M parameters</p><p>This model requires input tokenization with SentencePiece model, which is provided by Spark-NLP (See tokenizers package).</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = AlbertEmbeddings.pretrained()
 .setInputCols(<span class="lit">"sentence"</span>, <span class="lit">"token"</span>)
 .setOutputCol(<span class="lit">"embeddings"</span>)</pre><p>The default model is <code>&quot;albert_base_uncased&quot;</code>, if no name is provided.</p><p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/dl-ner/ner_albert.ipynb" target="_blank">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/AlbertEmbeddingsTestSpec.scala" target="_blank">AlbertEmbeddingsTestSpec</a>.
Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. The Spark NLP Workshop
example shows how to import them <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a>.</p><p><b>Sources:</b></p><p><a href="https://arxiv.org/pdf/1909.11942.pdf" target="_blank">ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS</a></p><p><a href="https://github.com/google-research/ALBERT" target="_blank">https://github.com/google-research/ALBERT</a></p><p><a href="https://tfhub.dev/s?q=albert" target="_blank">https://tfhub.dev/s?q=albert</a></p><p><b>Paper abstract:</b></p><p><i>Increasing model size when pretraining natural language representations often results in improved performance on
downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and
longer training times. To address these problems, we present two parameter reduction techniques to lower memory
consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows
that our proposed methods lead to models that scale much better compared to
the original BERT. We also use a self-supervised loss that focuses on modeling
inter-sentence coherence, and show it consistently helps downstream tasks with
multi-sentence inputs. As a result, our best model establishes new state-of-the-art
results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large.</i></p><p><b>Tips:</b>
ALBERT uses repeating layers which results in a small memory footprint,
however the computational cost remains similar to a BERT-like architecture with
the same number of hidden layers as it has to iterate through the same number of (repeating) layers.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = AlbertEmbeddings.pretrained()
  .setInputCols(<span class="lit">"token"</span>, <span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  documentAssembler,
  tokenizer,
  embeddings,
  embeddingsFinisher
))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[<span class="num">1.1342473030090332</span>,-<span class="num">1.3855540752410889</span>,<span class="num">0.9818322062492371</span>,-<span class="num">0.784737348556518</span>...|
|[<span class="num">0.847029983997345</span>,-<span class="num">1.047153353691101</span>,-<span class="num">0.1520637571811676</span>,-<span class="num">0.6245765686035156</span>...|
|[-<span class="num">0.009860038757324219</span>,-<span class="num">0.13450059294700623</span>,<span class="num">2.707749128341675</span>,<span class="num">1.2916892766952</span>...|
|[-<span class="num">0.04192575812339783</span>,-<span class="num">0.5764210224151611</span>,-<span class="num">0.3196685314178467</span>,-<span class="num">0.527840495109</span>...|
|[<span class="num">0.15583214163780212</span>,-<span class="num">0.1614152491092682</span>,-<span class="num">0.28423872590065</span>,-<span class="num">0.135491415858268</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p><a href="https://nlp.johnsnowlabs.com/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="BertEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.BertEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.BertEmbeddings]withWriteTensorflowModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitiveProperties"></a><a id="BertEmbeddings:BertEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/BertEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Token-level embeddings using BERT." href="BertEmbeddings.html"><span class="name">BertEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a></span>
      </span>
      
      <p class="shortcomment cmt">Token-level embeddings using BERT.</p><div class="fullcomment"><div class="comment cmt"><p>Token-level embeddings using BERT. BERT (Bidirectional Encoder Representations from Transformers) provides dense
vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = BertEmbeddings.pretrained()
  .setInputCols(<span class="lit">"token"</span>, <span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"bert_embeddings"</span>)</pre><p>The default model is <code>&quot;small_bert_L2_768&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/blogposts/3.NER_with_BERT.ipynb" target="_blank">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/BertEmbeddingsTestSpec.scala" target="_blank">BertEmbeddingsTestSpec</a>.
Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. The Spark NLP Workshop
example shows how to import them <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a>.</p><p><b>Sources</b> :</p><p><a href="https://arxiv.org/abs/1810.04805" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p><p><a href="https://github.com/google-research/bert" target="_blank">https://github.com/google-research/bert</a></p><p><b> Paper abstract </b></p><p><i>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations
from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional
representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a
result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering and language inference, without
substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It
obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score
to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1
question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point
absolute improvement).</i></p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.BertEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = BertEmbeddings.pretrained(<span class="lit">"small_bert_L2_128"</span>, <span class="lit">"en"</span>)
  .setInputCols(<span class="lit">"token"</span>, <span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"bert_embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"bert_embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  documentAssembler,
  tokenizer,
  embeddings,
  embeddingsFinisher
))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[-<span class="num">2.3497989177703857</span>,<span class="num">0.480538547039032</span>,-<span class="num">0.3238905668258667</span>,-<span class="num">1.612930893898010</span>...|
|[-<span class="num">2.1357314586639404</span>,<span class="num">0.32984697818756104</span>,-<span class="num">0.6032363176345825</span>,-<span class="num">1.6791689395904</span>...|
|[-<span class="num">1.8244884014129639</span>,-<span class="num">0.27088963985443115</span>,-<span class="num">1.059438943862915</span>,-<span class="num">0.9817547798156</span>...|
|[-<span class="num">1.1648050546646118</span>,-<span class="num">0.4725411534309387</span>,-<span class="num">0.5938255786895752</span>,-<span class="num">1.5780693292617</span>...|
|[-<span class="num">0.9125322699546814</span>,<span class="num">0.4563939869403839</span>,-<span class="num">0.3975459933280945</span>,-<span class="num">1.81611204147338</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p><a href="BertSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings">BertSentenceEmbeddings</a> for sentence-level embeddings</p></span><span class="cmt"><p><a href="https://nlp.johnsnowlabs.com/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="BertSentenceEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings]withWriteTensorflowModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitiveProperties"></a><a id="BertSentenceEmbeddings:BertSentenceEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/BertSentenceEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Sentence-level embeddings using BERT." href="BertSentenceEmbeddings.html"><span class="name">BertSentenceEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="BertSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings">BertSentenceEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="BertSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings">BertSentenceEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a></span>
      </span>
      
      <p class="shortcomment cmt">Sentence-level embeddings using BERT.</p><div class="fullcomment"><div class="comment cmt"><p>Sentence-level embeddings using BERT. BERT (Bidirectional Encoder Representations from Transformers) provides dense
vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = BertSentenceEmbeddings.pretrained()
  .setInputCols(<span class="lit">"sentence"</span>)
  .setOutputCol(<span class="lit">"sentence_bert_embeddings"</span>)</pre><p>The default model is <code>&quot;sent_small_bert_L2_768&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BERT%20Sentence.ipynb" target="_blank">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/BertSentenceEmbeddingsTestSpec.scala" target="_blank">BertSentenceEmbeddingsTestSpec</a>.</p><p><b>Sources</b> :</p><p><a href="https://arxiv.org/abs/1810.04805" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p><p><a href="https://github.com/google-research/bert" target="_blank">https://github.com/google-research/bert</a></p><p><b> Paper abstract </b></p><p><i>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations
from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional
representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a
result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering and language inference, without
substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It
obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score
to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1
question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point
absolute improvement).</i></p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator.SentenceDetector
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> sentence = <span class="kw">new</span> SentenceDetector()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"sentence"</span>)

<span class="kw">val</span> embeddings = BertSentenceEmbeddings.pretrained(<span class="lit">"sent_small_bert_L2_128"</span>)
  .setInputCols(<span class="lit">"sentence"</span>)
  .setOutputCol(<span class="lit">"sentence_bert_embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"sentence_bert_embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  documentAssembler,
  sentence,
  embeddings,
  embeddingsFinisher
))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"John loves apples. Mary loves oranges. John loves Mary."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[-<span class="num">0.8951074481010437</span>,<span class="num">0.13753940165042877</span>,<span class="num">0.3108254075050354</span>,-<span class="num">1.65693199634552</span>...|
|[-<span class="num">0.6180210709571838</span>,-<span class="num">0.12179657071828842</span>,-<span class="num">0.191165953874588</span>,-<span class="num">1.4497021436691</span>...|
|[-<span class="num">0.822715163230896</span>,<span class="num">0.7568016648292542</span>,-<span class="num">0.1165061742067337</span>,-<span class="num">1.59048593044281</span>,...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p><a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a> for token-level embeddings</p></span><span class="cmt"><p><a href="https://nlp.johnsnowlabs.com/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ChunkEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings]withHasSimpleAnnotate[com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings]"></a><a id="ChunkEmbeddings:ChunkEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ChunkEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="This annotator utilizes WordEmbeddings, BertEmbeddings etc." href="ChunkEmbeddings.html"><span class="name">ChunkEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="ChunkEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings">ChunkEmbeddings</a>] with <a href="../HasSimpleAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasSimpleAnnotate">HasSimpleAnnotate</a>[<a href="ChunkEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings">ChunkEmbeddings</a>]</span>
      </span>
      
      <p class="shortcomment cmt">This annotator utilizes <a href="WordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings">WordEmbeddings</a>, <a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a> etc.</p><div class="fullcomment"><div class="comment cmt"><p>This annotator utilizes <a href="WordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings">WordEmbeddings</a>, <a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a> etc. to generate chunk embeddings from either
<a href="../annotators/Chunker.html" class="extype" name="com.johnsnowlabs.nlp.annotators.Chunker">Chunker</a>, <a href="../annotators/NGramGenerator.html" class="extype" name="com.johnsnowlabs.nlp.annotators.NGramGenerator">NGramGenerator</a>,
or <a href="../annotators/ner/NerConverter.html" class="extype" name="com.johnsnowlabs.nlp.annotators.ner.NerConverter">NerConverter</a> outputs.</p><p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/databricks_notebooks/3.SparkNLP_Pretrained_Models_v3.0.ipynb" target="_blank">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/ChunkEmbeddingsTestSpec.scala" target="_blank">ChunkEmbeddingsTestSpec</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.{NGramGenerator, Tokenizer}
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="cmt">// Extract the Embeddings from the NGrams</span>
<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> sentence = <span class="kw">new</span> SentenceDetector()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"sentence"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"sentence"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> nGrams = <span class="kw">new</span> NGramGenerator()
  .setInputCols(<span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"chunk"</span>)
  .setN(<span class="num">2</span>)

<span class="kw">val</span> embeddings = WordEmbeddingsModel.pretrained()
  .setInputCols(<span class="lit">"sentence"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)
  .setCaseSensitive(<span class="kw">false</span>)

<span class="cmt">// Convert the NGram chunks into Word Embeddings</span>
<span class="kw">val</span> chunkEmbeddings = <span class="kw">new</span> ChunkEmbeddings()
  .setInputCols(<span class="lit">"chunk"</span>, <span class="lit">"embeddings"</span>)
  .setOutputCol(<span class="lit">"chunk_embeddings"</span>)
  .setPoolingStrategy(<span class="lit">"AVERAGE"</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    sentence,
    tokenizer,
    nGrams,
    embeddings,
    chunkEmbeddings
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(chunk_embeddings) as result"</span>)
  .select(<span class="lit">"result.annotatorType"</span>, <span class="lit">"result.result"</span>, <span class="lit">"result.embeddings"</span>)
  .show(<span class="num">5</span>, <span class="num">80</span>)
+---------------+----------+--------------------------------------------------------------------------------+
|  annotatorType|    result|                                                                      embeddings|
+---------------+----------+--------------------------------------------------------------------------------+
|word_embeddings|   This is|[-<span class="num">0.55661</span>, <span class="num">0.42829502</span>, <span class="num">0.86661</span>, -<span class="num">0.409785</span>, <span class="num">0.06316501</span>, <span class="num">0.120775</span>, -<span class="num">0.0732005</span>, ...|
|word_embeddings|      is a|[-<span class="num">0.40674996</span>, <span class="num">0.22938299</span>, <span class="num">0.50597</span>, -<span class="num">0.288195</span>, <span class="num">0.555655</span>, <span class="num">0.465145</span>, <span class="num">0.140118</span>, <span class="num">0.</span>..|
|word_embeddings|a sentence|[<span class="num">0.17417</span>, <span class="num">0.095253006</span>, -<span class="num">0.0530925</span>, -<span class="num">0.218465</span>, <span class="num">0.714395</span>, <span class="num">0.79860497</span>, <span class="num">0.0129999</span>...|
|word_embeddings|sentence .|[<span class="num">0.139705</span>, <span class="num">0.177955</span>, <span class="num">0.1887775</span>, -<span class="num">0.45545</span>, <span class="num">0.20030999</span>, <span class="num">0.461557</span>, -<span class="num">0.07891501</span>, ...|
+---------------+----------+--------------------------------------------------------------------------------+</pre></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="DistilBertEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings]withWriteTensorflowModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitiveProperties"></a><a id="DistilBertEmbeddings:DistilBertEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/DistilBertEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base." href="DistilBertEmbeddings.html"><span class="name">DistilBertEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="DistilBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings">DistilBertEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="DistilBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings">DistilBertEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a></span>
      </span>
      
      <p class="shortcomment cmt">DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base.</p><div class="fullcomment"><div class="comment cmt"><p>DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than
<code>bert-base-uncased</code>, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = DistilBertEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)</pre><p>The default model is <code>&quot;distilbert_base_cased&quot;</code>, if no name is provided.
For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20DistilBERT.ipynb" target="_blank">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/DistilBertEmbeddingsTestSpec.scala" target="_blank">DistilBertEmbeddingsTestSpec</a>.
Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. The Spark NLP Workshop
example shows how to import them <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a>.</p><p>The DistilBERT model was proposed in the paper
<a href="https://arxiv.org/abs/1910.01108" target="_blank">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a>.</p><p><b>Paper Abstract:</b></p><p><i>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP),
operating these large models in on-the-edge and/or under constrained computational training or inference budgets
remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation
model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger
counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage
knowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by
40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive
biases learned by larger models during pretraining, we introduce a triple loss combining language modeling,
distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we
demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device
study.</i></p><p>Tips:</p><ul><li>DistilBERT doesn't have <code>:obj:token_type_ids</code>, you don't need to indicate which token belongs to which segment. Just
    separate your segments with the separation token <code>:obj:tokenizer.sep_token</code> (or <code>:obj:[SEP]</code>).</li><li>DistilBERT doesn't have options to select the input positions (<code>:obj:position_ids</code> input). This could be added if
    necessary though, just let us know if you need this option.</li></ul><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = DistilBertEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)
  .setCaseSensitive(<span class="kw">true</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    tokenizer,
    embeddings,
    embeddingsFinisher
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[<span class="num">0.1127224713563919</span>,-<span class="num">0.1982710212469101</span>,<span class="num">0.5360898375511169</span>,-<span class="num">0.272536993026733</span>...|
|[<span class="num">0.35534414649009705</span>,<span class="num">0.13215228915214539</span>,<span class="num">0.40981462597846985</span>,<span class="num">0.14036104083061</span>...|
|[<span class="num">0.328085333108902</span>,-<span class="num">0.06269335001707077</span>,-<span class="num">0.017595693469047546</span>,-<span class="num">0.024373905733</span>...|
|[<span class="num">0.15617232024669647</span>,<span class="num">0.2967822253704071</span>,<span class="num">0.22324979305267334</span>,-<span class="num">0.04568954557180</span>...|
|[<span class="num">0.45411425828933716</span>,<span class="num">0.01173491682857275</span>,<span class="num">0.190129816532135</span>,<span class="num">0.1178255230188369</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p><a href="https://nlp.johnsnowlabs.com/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ElmoEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings]withHasSimpleAnnotate[com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings]withWriteTensorflowModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitiveProperties"></a><a id="ElmoEmbeddings:ElmoEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ElmoEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Word embeddings from ELMo (Embeddings from Language Models), a language model trained on the 1 Billion Word Benchmark." href="ElmoEmbeddings.html"><span class="name">ElmoEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="ElmoEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings">ElmoEmbeddings</a>] with <a href="../HasSimpleAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasSimpleAnnotate">HasSimpleAnnotate</a>[<a href="ElmoEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings">ElmoEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a></span>
      </span>
      
      <p class="shortcomment cmt">Word embeddings from ELMo (Embeddings from Language Models), a language model trained on the 1 Billion Word Benchmark.</p><div class="fullcomment"><div class="comment cmt"><p>Word embeddings from ELMo (Embeddings from Language Models), a language model trained on the 1 Billion Word Benchmark.</p><p>Note that this is a very computationally expensive module compared to word embedding modules that only perform
embedding lookups. The use of an accelerator is recommended.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = ElmoEmbeddings.pretrained()
  .setInputCols(<span class="lit">"sentence"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"elmo_embeddings"</span>)</pre><p>The default model is <code>&quot;elmo&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>The pooling layer can be set with <code>setPoolingLayer</code> to the following values:</p><ul><li><code>&quot;word_emb&quot;</code>: the character-based word representations with shape <code>[batch_size, max_length, 512]</code>.</li><li><code>&quot;lstm_outputs1&quot;</code>: the first LSTM hidden state with shape <code>[batch_size, max_length, 1024]</code>.</li><li><code>&quot;lstm_outputs2&quot;</code>: the second LSTM hidden state with shape <code>[batch_size, max_length, 1024]</code>.</li><li><code>&quot;elmo&quot;</code>: the weighted sum of the 3 layers, where the weights are trainable. This tensor has shape <code>[batch_size, max_length, 1024]</code>.</li></ul><p>For extended examples of usage, see the
<a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/dl-ner/ner_elmo.ipynb" target="_blank">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/ElmoEmbeddingsTestSpec.scala" target="_blank">ElmoEmbeddingsTestSpec</a>.</p><p><b>Sources:</b></p><p><a href="https://tfhub.dev/google/elmo/3" target="_blank">https://tfhub.dev/google/elmo/3</a></p><p><a href="https://arxiv.org/abs/1802.05365" target="_blank">Deep contextualized word representations</a></p><p><b> Paper abstract:</b></p><p><i>We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of
word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model
polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model
(biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to
existing models and significantly improve the state of the art across six challenging NLP problems, including
question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the
deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of
semi-supervision signals.</i></p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = ElmoEmbeddings.pretrained()
  .setPoolingLayer(<span class="lit">"word_emb"</span>)
  .setInputCols(<span class="lit">"token"</span>, <span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  documentAssembler,
  tokenizer,
  embeddings,
  embeddingsFinisher
))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[<span class="num">6.662458181381226E-4</span>,-<span class="num">0.2541114091873169</span>,-<span class="num">0.6275503039360046</span>,<span class="num">0.5787073969841</span>...|
|[<span class="num">0.19154725968837738</span>,<span class="num">0.22998669743537903</span>,-<span class="num">0.2894386649131775</span>,<span class="num">0.21524395048618</span>...|
|[<span class="num">0.10400570929050446</span>,<span class="num">0.12288510054349899</span>,-<span class="num">0.07056470215320587</span>,-<span class="num">0.246389418840</span>...|
|[<span class="num">0.49932169914245605</span>,-<span class="num">0.12706467509269714</span>,<span class="num">0.30969417095184326</span>,<span class="num">0.2643227577209</span>...|
|[-<span class="num">0.8871506452560425</span>,-<span class="num">0.20039963722229004</span>,-<span class="num">1.0601330995559692</span>,<span class="num">0.0348707810044</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p><a href="https://nlp.johnsnowlabs.com/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of other transformer based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.EmbeddingsCoverage" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="EmbeddingsCoverageextendsAnyRef"></a><a id="EmbeddingsCoverage:EmbeddingsCoverage"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/EmbeddingsCoverage.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="EmbeddingsCoverage.html"><span class="name">EmbeddingsCoverage</span></a><span class="result"> extends <span class="extype" name="scala.AnyRef">AnyRef</span></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="HasEmbeddingsPropertiesextendsParams"></a><a id="HasEmbeddingsProperties:HasEmbeddingsProperties"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/HasEmbeddingsProperties.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="HasEmbeddingsProperties.html"><span class="name">HasEmbeddingsProperties</span></a><span class="result"> extends <span class="extype" name="org.apache.spark.ml.param.Params">Params</span></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadAlbertTensorflowModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadAlbertTensorflowModelextendsReadTensorflowModelwithReadSentencePieceModel"></a><a id="ReadAlbertTensorflowModel:ReadAlbertTensorflowModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadAlbertTensorflowModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadAlbertTensorflowModel.html"><span class="name">ReadAlbertTensorflowModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/ReadSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.ReadSentencePieceModel">ReadSentencePieceModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadBertSentenceTensorflowModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadBertSentenceTensorflowModelextendsReadTensorflowModel"></a><a id="ReadBertSentenceTensorflowModel:ReadBertSentenceTensorflowModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadBertSentenceTensorflowModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadBertSentenceTensorflowModel.html"><span class="name">ReadBertSentenceTensorflowModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadBertTensorflowModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadBertTensorflowModelextendsReadTensorflowModel"></a><a id="ReadBertTensorflowModel:ReadBertTensorflowModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadBertTensorflowModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadBertTensorflowModel.html"><span class="name">ReadBertTensorflowModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadDistilBertTensorflowModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadDistilBertTensorflowModelextendsReadTensorflowModel"></a><a id="ReadDistilBertTensorflowModel:ReadDistilBertTensorflowModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadDistilBertTensorflowModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadDistilBertTensorflowModel.html"><span class="name">ReadDistilBertTensorflowModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadElmoTensorflowModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadElmoTensorflowModelextendsReadTensorflowModel"></a><a id="ReadElmoTensorflowModel:ReadElmoTensorflowModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadElmoTensorflowModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadElmoTensorflowModel.html"><span class="name">ReadElmoTensorflowModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadRobertaTensorflowModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadRobertaTensorflowModelextendsReadTensorflowModel"></a><a id="ReadRobertaTensorflowModel:ReadRobertaTensorflowModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadRobertaTensorflowModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadRobertaTensorflowModel.html"><span class="name">ReadRobertaTensorflowModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadUSETensorflowModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadUSETensorflowModelextendsReadTensorflowModel"></a><a id="ReadUSETensorflowModel:ReadUSETensorflowModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadUSETensorflowModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadUSETensorflowModel.html"><span class="name">ReadUSETensorflowModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadXlmRobertaTensorflowModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadXlmRobertaTensorflowModelextendsReadTensorflowModelwithReadSentencePieceModel"></a><a id="ReadXlmRobertaTensorflowModel:ReadXlmRobertaTensorflowModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadXlmRobertaTensorflowModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadXlmRobertaTensorflowModel.html"><span class="name">ReadXlmRobertaTensorflowModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/ReadSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.ReadSentencePieceModel">ReadSentencePieceModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadXlnetTensorflowModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadXlnetTensorflowModelextendsReadTensorflowModelwithReadSentencePieceModel"></a><a id="ReadXlnetTensorflowModel:ReadXlnetTensorflowModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadXlnetTensorflowModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadXlnetTensorflowModel.html"><span class="name">ReadXlnetTensorflowModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/ReadSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.ReadSentencePieceModel">ReadSentencePieceModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedAlbertModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedAlbertModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings]"></a><a id="ReadablePretrainedAlbertModel:ReadablePretrainedAlbertModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedAlbertModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedAlbertModel.html"><span class="name">ReadablePretrainedAlbertModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="AlbertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings">AlbertEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="AlbertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings">AlbertEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedBertModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedBertModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.BertEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.BertEmbeddings]"></a><a id="ReadablePretrainedBertModel:ReadablePretrainedBertModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedBertModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedBertModel.html"><span class="name">ReadablePretrainedBertModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedBertSentenceModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedBertSentenceModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings]"></a><a id="ReadablePretrainedBertSentenceModel:ReadablePretrainedBertSentenceModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedBertSentenceModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedBertSentenceModel.html"><span class="name">ReadablePretrainedBertSentenceModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="BertSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings">BertSentenceEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="BertSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings">BertSentenceEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedDistilBertModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedDistilBertModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings]"></a><a id="ReadablePretrainedDistilBertModel:ReadablePretrainedDistilBertModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedDistilBertModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedDistilBertModel.html"><span class="name">ReadablePretrainedDistilBertModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="DistilBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings">DistilBertEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="DistilBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings">DistilBertEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedElmoModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedElmoModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings]"></a><a id="ReadablePretrainedElmoModel:ReadablePretrainedElmoModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedElmoModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedElmoModel.html"><span class="name">ReadablePretrainedElmoModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="ElmoEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings">ElmoEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="ElmoEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings">ElmoEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedRobertaModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedRobertaModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings]"></a><a id="ReadablePretrainedRobertaModel:ReadablePretrainedRobertaModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedRobertaModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedRobertaModel.html"><span class="name">ReadablePretrainedRobertaModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="RoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings">RoBertaEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="RoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings">RoBertaEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedUSEModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedUSEModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder]withHasPretrained[com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder]"></a><a id="ReadablePretrainedUSEModel:ReadablePretrainedUSEModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedUSEModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedUSEModel.html"><span class="name">ReadablePretrainedUSEModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="UniversalSentenceEncoder.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder">UniversalSentenceEncoder</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="UniversalSentenceEncoder.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder">UniversalSentenceEncoder</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedWordEmbeddings" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedWordEmbeddingsextendsStorageReadable[com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel]withHasPretrained[com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel]"></a><a id="ReadablePretrainedWordEmbeddings:ReadablePretrainedWordEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedWordEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedWordEmbeddings.html"><span class="name">ReadablePretrainedWordEmbeddings</span></a><span class="result"> extends <a href="../../storage/StorageReadable.html" class="extype" name="com.johnsnowlabs.storage.StorageReadable">StorageReadable</a>[<a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedXlmRobertaModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedXlmRobertaModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings]"></a><a id="ReadablePretrainedXlmRobertaModel:ReadablePretrainedXlmRobertaModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedXlmRobertaModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedXlmRobertaModel.html"><span class="name">ReadablePretrainedXlmRobertaModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="XlmRoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings">XlmRoBertaEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="XlmRoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings">XlmRoBertaEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedXlnetModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedXlnetModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings]"></a><a id="ReadablePretrainedXlnetModel:ReadablePretrainedXlnetModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedXlnetModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedXlnetModel.html"><span class="name">ReadablePretrainedXlnetModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="XlnetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings">XlnetEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="XlnetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings">XlnetEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadsFromBytes" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadsFromBytesextendsAnyRef"></a><a id="ReadsFromBytes:ReadsFromBytes"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadsFromBytes.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadsFromBytes.html"><span class="name">ReadsFromBytes</span></a><span class="result"> extends <span class="extype" name="scala.AnyRef">AnyRef</span></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="RoBertaEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings]withWriteTensorflowModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitiveProperties"></a><a id="RoBertaEmbeddings:RoBertaEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/RoBertaEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov." href="RoBertaEmbeddings.html"><span class="name">RoBertaEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="RoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings">RoBertaEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="RoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings">RoBertaEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a></span>
      </span>
      
      <p class="shortcomment cmt">The RoBERTa model was proposed in <a href="https://arxiv.org/abs/1907.11692" target="_blank">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>
by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.</p><div class="fullcomment"><div class="comment cmt"><p>The RoBERTa model was proposed in <a href="https://arxiv.org/abs/1907.11692" target="_blank">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>
by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.
It is based on Google's BERT model released in 2018.</p><p>It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = RoBertaEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)</pre><p>The default model is <code>&quot;roberta_base&quot;</code>, if no name is provided.
For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20RoBERTa.ipynb" target="_blank">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/RoBertaEmbeddingsTestSpec.scala" target="_blank">RoBertaEmbeddingsTestSpec</a>.
Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. The Spark NLP Workshop
example shows how to import them <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a>.</p><p><b>Paper Abstract:</b></p><p><i>Language model pretraining has led to significant performance gains but careful comparison between different
approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes,
and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication
study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and
training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every
model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results
highlight the importance of previously overlooked design choices, and raise questions about the source of recently
reported improvements. We release our models and code.</i></p><p>Tips:</p><ul><li>RoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a different pretraining scheme.</li><li>RoBERTa doesn't have :obj:<code>token_type_ids</code>, you don't need to indicate which token belongs to which segment. Just separate your segments with the separation token :obj:<code>tokenizer.sep_token</code> (or :obj:<code>&lt;/s&gt;</code>)</li></ul><p>The original code can be found <code><code><code>here</code></code></code> <a href="https://github.com/pytorch/fairseq/tree/master/examples/roberta" target="_blank">https://github.com/pytorch/fairseq/tree/master/examples/roberta</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = RoBertaEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)
  .setCaseSensitive(<span class="kw">true</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    tokenizer,
    embeddings,
    embeddingsFinisher
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[<span class="num">0.18792399764060974</span>,-<span class="num">0.14591649174690247</span>,<span class="num">0.20547787845134735</span>,<span class="num">0.1468472778797</span>...|
|[<span class="num">0.22845706343650818</span>,<span class="num">0.18073144555091858</span>,<span class="num">0.09725798666477203</span>,-<span class="num">0.0417917296290</span>...|
|[<span class="num">0.07037967443466187</span>,-<span class="num">0.14801117777824402</span>,-<span class="num">0.03603338822722435</span>,-<span class="num">0.17893412709</span>...|
|[-<span class="num">0.08734266459941864</span>,<span class="num">0.2486150562763214</span>,-<span class="num">0.009067727252840996</span>,-<span class="num">0.24408400058</span>...|
|[<span class="num">0.22409197688102722</span>,-<span class="num">0.4312366545200348</span>,<span class="num">0.1401449590921402</span>,<span class="num">0.356410235166549</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p><a href="https://nlp.johnsnowlabs.com/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="SentenceEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings]withHasSimpleAnnotate[com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings]withHasEmbeddingsPropertieswithHasStorageRef"></a><a id="SentenceEmbeddings:SentenceEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/SentenceEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Converts the results from WordEmbeddings, BertEmbeddings, or ElmoEmbeddings into sentence or document embeddings by either summing up or averaging all the word embeddings in a sentence or a document (depending on the inputCols)." href="SentenceEmbeddings.html"><span class="name">SentenceEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="SentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings">SentenceEmbeddings</a>] with <a href="../HasSimpleAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasSimpleAnnotate">HasSimpleAnnotate</a>[<a href="SentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings">SentenceEmbeddings</a>] with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a></span>
      </span>
      
      <p class="shortcomment cmt">Converts the results from <a href="WordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings">WordEmbeddings</a>, <a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a>, or <a href="ElmoEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings">ElmoEmbeddings</a> into sentence
or document embeddings by either summing up or averaging all the word embeddings in a sentence or a document
(depending on the inputCols).</p><div class="fullcomment"><div class="comment cmt"><p>Converts the results from <a href="WordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings">WordEmbeddings</a>, <a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a>, or <a href="ElmoEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings">ElmoEmbeddings</a> into sentence
or document embeddings by either summing up or averaging all the word embeddings in a sentence or a document
(depending on the inputCols).</p><p>This can be configured with <code>setPoolingStrategy</code>, which either be <code>&quot;AVERAGE&quot;</code> or <code>&quot;SUM&quot;</code>.</p><p>For more extended examples see the
<a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/databricks_notebooks/12.%20Named_Entity_Disambiguation_v3.0.ipynb" target="_blank">Spark NLP Workshop</a>.
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/SentenceEmbeddingsTestSpec.scala" target="_blank">SentenceEmbeddingsTestSpec</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = WordEmbeddingsModel.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)

<span class="kw">val</span> embeddingsSentence = <span class="kw">new</span> SentenceEmbeddings()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>, <span class="lit">"embeddings"</span>))
  .setOutputCol(<span class="lit">"sentence_embeddings"</span>)
  .setPoolingStrategy(<span class="lit">"AVERAGE"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"sentence_embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    tokenizer,
    embeddings,
    embeddingsSentence,
    embeddingsFinisher
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[-<span class="num">0.22093398869037628</span>,<span class="num">0.25130119919776917</span>,<span class="num">0.41810303926467896</span>,-<span class="num">0.380883991718</span>...|
+--------------------------------------------------------------------------------+</pre></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="UniversalSentenceEncoderextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder]withHasSimpleAnnotate[com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder]withHasEmbeddingsPropertieswithHasStorageRefwithWriteTensorflowModel"></a><a id="UniversalSentenceEncoder:UniversalSentenceEncoder"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/UniversalSentenceEncoder.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks." href="UniversalSentenceEncoder.html"><span class="name">UniversalSentenceEncoder</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="UniversalSentenceEncoder.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder">UniversalSentenceEncoder</a>] with <a href="../HasSimpleAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasSimpleAnnotate">HasSimpleAnnotate</a>[<a href="UniversalSentenceEncoder.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder">UniversalSentenceEncoder</a>] with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a></span>
      </span>
      
      <p class="shortcomment cmt">The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.</p><div class="fullcomment"><div class="comment cmt"><p>The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> useEmbeddings = UniversalSentenceEncoder.pretrained()
  .setInputCols(<span class="lit">"sentence"</span>)
  .setOutputCol(<span class="lit">"sentence_embeddings"</span>)</pre><p>The default model is <code>&quot;tfhub_use&quot;</code>, if no name is provided.
For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/databricks_notebooks/2.4/3.SparkNLP_Pretrained_Models.ipynb" target="_blank">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/UniversalSentenceEncoderTestSpec.scala" target="_blank">UniversalSentenceEncoderTestSpec</a>.</p><p><b>Sources:</b></p><p><a href="https://arxiv.org/abs/1803.11175" target="_blank">Universal Sentence Encoder</a></p><p><a href="https://tfhub.dev/google/universal-sentence-encoder/2" target="_blank">https://tfhub.dev/google/universal-sentence-encoder/2</a></p><p><b>Paper abstract:</b></p><p><i>We present models for encoding sentences into embedding vectors that specifically target transfer learning to other
NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the
encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and
report the relationship between model complexity, resource consumption, the availability of transfer task training
data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained
word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence
embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe
surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain
encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained
sentence encoding models are made freely available for download and on TF Hub.</i></p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator.SentenceDetector
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> sentence = <span class="kw">new</span> SentenceDetector()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"sentence"</span>)

<span class="kw">val</span> embeddings = UniversalSentenceEncoder.pretrained()
  .setInputCols(<span class="lit">"sentence"</span>)
  .setOutputCol(<span class="lit">"sentence_embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"sentence_embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    sentence,
    embeddings,
    embeddingsFinisher
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[<span class="num">0.04616805538535118</span>,<span class="num">0.022307956591248512</span>,-<span class="num">0.044395286589860916</span>,-<span class="num">0.0016493503</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p><a href="https://nlp.johnsnowlabs.com/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="WordEmbeddingsextendsAnnotatorApproach[com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel]withHasStoragewithHasEmbeddingsProperties"></a><a id="WordEmbeddings:WordEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/WordEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Word Embeddings lookup annotator that maps tokens to vectors." href="WordEmbeddings.html"><span class="name">WordEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorApproach.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorApproach">AnnotatorApproach</a>[<a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel</a>] with <a href="../../storage/HasStorage.html" class="extype" name="com.johnsnowlabs.storage.HasStorage">HasStorage</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a></span>
      </span>
      
      <p class="shortcomment cmt">Word Embeddings lookup annotator that maps tokens to vectors.</p><div class="fullcomment"><div class="comment cmt"><p>Word Embeddings lookup annotator that maps tokens to vectors.</p><p>For instantiated/pretrained models, see <a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel</a>.</p><p>A custom token lookup dictionary for embeddings can be set with <code>setStoragePath</code>.
Each line of the provided file needs to have a token, followed by their vector representation, delimited by a spaces.</p><pre>...
are <span class="num">0.39658191506190343</span> <span class="num">0.630968081620067</span> <span class="num">0.5393722253731201</span> <span class="num">0.8428180123359783</span>
were <span class="num">0.7535235923631415</span> <span class="num">0.9699218875629833</span> <span class="num">0.10397182122983872</span> <span class="num">0.11833962569383116</span>
stress <span class="num">0.0492683418305907</span> <span class="num">0.9415954572751959</span> <span class="num">0.47624463167525755</span> <span class="num">0.16790967216778263</span>
induced <span class="num">0.1535748762292387</span> <span class="num">0.33498936903209897</span> <span class="num">0.9235178224122094</span> <span class="num">0.1158772920395934</span>
...</pre><p>If a token is not found in the dictionary, then the result will be a zero vector of the same dimension.
Statistics about the rate of converted tokens, can be retrieved with <a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel.withCoverageColumn</a>
and <a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel.overallCoverage</a>.</p><p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/databricks_notebooks/2.4/3.SparkNLP_Pretrained_Models.ipynb" target="_blank">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/WordEmbeddingsTestSpec.scala" target="_blank">WordEmbeddingsTestSpec</a>.</p><h4>Example</h4><p>In this example, the file <code>random_embeddings_dim4.txt</code> has the form of the content above.</p><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.WordEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.util.io.ReadAs
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = <span class="kw">new</span> WordEmbeddings()
  .setStoragePath(<span class="lit">"src/test/resources/random_embeddings_dim4.txt"</span>, ReadAs.TEXT)
  .setStorageRef(<span class="lit">"glove_4d"</span>)
  .setDimension(<span class="num">4</span>)
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    tokenizer,
    embeddings,
    embeddingsFinisher
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"The patient was diagnosed with diabetes."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="kw">false</span>)
+----------------------------------------------------------------------------------+
|result                                                                            |
+----------------------------------------------------------------------------------+
|[<span class="num">0.9439099431037903</span>,<span class="num">0.4707513153553009</span>,<span class="num">0.806300163269043</span>,<span class="num">0.16176554560661316</span>]     |
|[<span class="num">0.7966810464859009</span>,<span class="num">0.5551124811172485</span>,<span class="num">0.8861005902290344</span>,<span class="num">0.28284206986427307</span>]    |
|[<span class="num">0.025029370561242104</span>,<span class="num">0.35177749395370483</span>,<span class="num">0.052506182342767715</span>,<span class="num">0.1887107789516449</span>]|
|[<span class="num">0.08617766946554184</span>,<span class="num">0.8399239182472229</span>,<span class="num">0.5395117998123169</span>,<span class="num">0.7864698767662048</span>]    |
|[<span class="num">0.6599600911140442</span>,<span class="num">0.16109347343444824</span>,<span class="num">0.6041093468666077</span>,<span class="num">0.8913561105728149</span>]    |
|[<span class="num">0.5955275893211365</span>,<span class="num">0.01899011991918087</span>,<span class="num">0.4397728443145752</span>,<span class="num">0.8911281824111938</span>]    |
|[<span class="num">0.9840458631515503</span>,<span class="num">0.7599489092826843</span>,<span class="num">0.9417727589607239</span>,<span class="num">0.8624503016471863</span>]     |
+----------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p><a href="SentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings">SentenceEmbeddings</a> to combine embeddings into a sentence-level representation</p></span><span class="cmt"><p><a href="https://nlp.johnsnowlabs.com/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="WordEmbeddingsModelextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel]withHasSimpleAnnotate[com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel]withHasEmbeddingsPropertieswithHasStorageModelwithParamsAndFeaturesWritable"></a><a id="WordEmbeddingsModel:WordEmbeddingsModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/WordEmbeddingsModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Word Embeddings lookup annotator that maps tokens to vectors" href="WordEmbeddingsModel.html"><span class="name">WordEmbeddingsModel</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel</a>] with <a href="../HasSimpleAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasSimpleAnnotate">HasSimpleAnnotate</a>[<a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel</a>] with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageModel.html" class="extype" name="com.johnsnowlabs.storage.HasStorageModel">HasStorageModel</a> with <a href="../ParamsAndFeaturesWritable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesWritable">ParamsAndFeaturesWritable</a></span>
      </span>
      
      <p class="shortcomment cmt">Word Embeddings lookup annotator that maps tokens to vectors</p><div class="fullcomment"><div class="comment cmt"><p>Word Embeddings lookup annotator that maps tokens to vectors</p><p>This is the instantiated model of <a href="WordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings">WordEmbeddings</a>.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = WordEmbeddingsModel.pretrained()
    .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
    .setOutputCol(<span class="lit">"embeddings"</span>)</pre><p>The default model is <code>&quot;glove_100d&quot;</code>, if no name is provided.
For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>There are also two convenient functions to retrieve the embeddings coverage with respect to the transformed dataset:</p><ul><li><code>withCoverageColumn(dataset, embeddingsCol, outputCol)</code>:
    Adds a custom column with word coverage stats for the embedded field:
    (<code>coveredWords</code>, <code>totalWords</code>, <code>coveragePercentage</code>). This creates a new column with statistics for each row.</li></ul><pre><span class="kw">val</span> wordsCoverage = WordEmbeddingsModel.withCoverageColumn(resultDF, <span class="lit">"embeddings"</span>, <span class="lit">"cov_embeddings"</span>)
wordsCoverage.select(<span class="lit">"text"</span>,<span class="lit">"cov_embeddings"</span>).show(<span class="kw">false</span>)
+-------------------+--------------+
|text               |cov_embeddings|
+-------------------+--------------+
|This is a sentence.|[<span class="num">5</span>, <span class="num">5</span>, <span class="num">1.0</span>]   |
+-------------------+--------------+</pre><ul><li><code>overallCoverage(dataset, embeddingsCol)</code>:
    Calculates overall word coverage for the whole data in the embedded field.
    This returns a single coverage object considering all rows in the field.</li></ul><pre><span class="kw">val</span> wordsOverallCoverage = WordEmbeddingsModel.overallCoverage(wordsCoverage,<span class="lit">"embeddings"</span>).percentage
<span class="num">1.0</span></pre><p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/databricks_notebooks/2.4/3.SparkNLP_Pretrained_Models.ipynb" target="_blank">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/WordEmbeddingsTestSpec.scala" target="_blank">WordEmbeddingsTestSpec</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = WordEmbeddingsModel.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    tokenizer,
    embeddings,
    embeddingsFinisher
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[-<span class="num">0.570580005645752</span>,<span class="num">0.44183000922203064</span>,<span class="num">0.7010200023651123</span>,-<span class="num">0.417129993438720</span>...|
|[-<span class="num">0.542639970779419</span>,<span class="num">0.4147599935531616</span>,<span class="num">1.0321999788284302</span>,-<span class="num">0.4024400115013122</span>...|
|[-<span class="num">0.2708599865436554</span>,<span class="num">0.04400600120425224</span>,-<span class="num">0.020260000601410866</span>,-<span class="num">0.17395000159</span>...|
|[<span class="num">0.6191999912261963</span>,<span class="num">0.14650000631809235</span>,-<span class="num">0.08592499792575836</span>,-<span class="num">0.2629800140857</span>...|
|[-<span class="num">0.3397899866104126</span>,<span class="num">0.20940999686717987</span>,<span class="num">0.46347999572753906</span>,-<span class="num">0.6479200124740</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p><a href="SentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings">SentenceEmbeddings</a> to combine embeddings into a sentence-level representation</p></span><span class="cmt"><p><a href="https://nlp.johnsnowlabs.com/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsReader" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="WordEmbeddingsReaderextendsStorageReader[Array[Float]]withReadsFromBytes"></a><a id="WordEmbeddingsReader:WordEmbeddingsReader"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/WordEmbeddingsReader.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="" href="WordEmbeddingsReader.html"><span class="name">WordEmbeddingsReader</span></a><span class="result"> extends <a href="../../storage/StorageReader.html" class="extype" name="com.johnsnowlabs.storage.StorageReader">StorageReader</a>[<span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Float">Float</span>]] with <a href="ReadsFromBytes.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadsFromBytes">ReadsFromBytes</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsWriter" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="WordEmbeddingsWriterextendsStorageBatchWriter[Array[Float]]withReadsFromBytes"></a><a id="WordEmbeddingsWriter:WordEmbeddingsWriter"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/WordEmbeddingsWriter.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="" href="WordEmbeddingsWriter.html"><span class="name">WordEmbeddingsWriter</span></a><span class="result"> extends <a href="../../storage/StorageBatchWriter.html" class="extype" name="com.johnsnowlabs.storage.StorageBatchWriter">StorageBatchWriter</a>[<span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Float">Float</span>]] with <a href="ReadsFromBytes.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadsFromBytes">ReadsFromBytes</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="XlmRoBertaEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings]withWriteTensorflowModelwithWriteSentencePieceModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitiveProperties"></a><a id="XlmRoBertaEmbeddings:XlmRoBertaEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/XlmRoBertaEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="The XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov." href="XlmRoBertaEmbeddings.html"><span class="name">XlmRoBertaEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="XlmRoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings">XlmRoBertaEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="XlmRoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings">XlmRoBertaEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/WriteSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.WriteSentencePieceModel">WriteSentencePieceModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a></span>
      </span>
      
      <p class="shortcomment cmt">The XLM-RoBERTa model was proposed in <a href="https://arxiv.org/abs/1911.02116" target="_blank">Unsupervised Cross-lingual Representation Learning at Scale</a>
by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.</p><div class="fullcomment"><div class="comment cmt"><p>The XLM-RoBERTa model was proposed in <a href="https://arxiv.org/abs/1911.02116" target="_blank">Unsupervised Cross-lingual Representation Learning at Scale</a>
by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is based on Facebook's
RoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl
data.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = XlmRoBertaEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)</pre><p>The default model is <code>&quot;xlm_roberta_base&quot;</code>, default language is <code>&quot;xx&quot;</code> (meaning multi-lingual), if no values are provided.
For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20XLM-RoBERTa.ipynb" target="_blank">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/XlmRoBertaEmbeddingsTestSpec.scala" target="_blank">XlmRoBertaEmbeddingsTestSpec</a>.
Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. The Spark NLP Workshop
example shows how to import them <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a>.</p><p><b>Paper Abstract:</b></p><p><i>This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a
wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred
languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly
outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on
XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on
low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We
also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource
languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing
per-language performance; XLM-Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We
will make XLM-R code, data, and models publicly available.</i></p><p><b>Tips:</b></p><ul><li>XLM-RoBERTa is a multilingual model trained on 100 different languages. Unlike some XLM multilingual models, it does
    not require <b>lang</b> parameter to understand which language is used, and should be able to determine the correct
    language from the input ids.</li><li>This implementation is the same as RoBERTa. Refer to the <a href="RoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings">RoBertaEmbeddings</a> for usage examples
    as well as the information relative to the inputs and outputs.</li></ul><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = XlmRoBertaEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)
  .setCaseSensitive(<span class="kw">true</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    tokenizer,
    embeddings,
    embeddingsFinisher
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[-<span class="num">0.05969233065843582</span>,-<span class="num">0.030789051204919815</span>,<span class="num">0.04443822056055069</span>,<span class="num">0.09564960747</span>...|
|[-<span class="num">0.038839809596538544</span>,<span class="num">0.011712731793522835</span>,<span class="num">0.019954433664679527</span>,<span class="num">0.0667808502</span>...|
|[-<span class="num">0.03952755779027939</span>,-<span class="num">0.03455188870429993</span>,<span class="num">0.019103847444057465</span>,<span class="num">0.04311436787</span>...|
|[-<span class="num">0.09579929709434509</span>,<span class="num">0.02494969218969345</span>,-<span class="num">0.014753809198737144</span>,<span class="num">0.10259044915</span>...|
|[<span class="num">0.004710011184215546</span>,-<span class="num">0.022148698568344116</span>,<span class="num">0.011723337695002556</span>,-<span class="num">0.013356896</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p><a href="https://nlp.johnsnowlabs.com/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="XlnetEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings]withWriteTensorflowModelwithWriteSentencePieceModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitiveProperties"></a><a id="XlnetEmbeddings:XlnetEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/XlnetEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="XlnetEmbeddings (XLNet): Generalized Autoregressive Pretraining for Language Understanding" href="XlnetEmbeddings.html"><span class="name">XlnetEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="XlnetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings">XlnetEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="XlnetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings">XlnetEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/WriteSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.WriteSentencePieceModel">WriteSentencePieceModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a></span>
      </span>
      
      <p class="shortcomment cmt">XlnetEmbeddings (XLNet): Generalized Autoregressive Pretraining for Language Understanding</p><div class="fullcomment"><div class="comment cmt"><p>XlnetEmbeddings (XLNet): Generalized Autoregressive Pretraining for Language Understanding</p><p>XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language
modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance
for language tasks involving long context. Overall, XLNet achieves state-of-the-art (SOTA) results on various
downstream language tasks including question answering, natural language inference, sentiment analysis, and document
ranking.</p><p>These word embeddings represent the outputs generated by the XLNet models.</p><p>Note that this is a very computationally expensive module compared to word embedding modules that only perform embedding lookups.
The use of an accelerator is recommended.</p><p><code>&quot;xlnet_large_cased&quot;</code> = <a href="https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip" target="_blank">XLNet-Large</a> | 24-layer, 1024-hidden, 16-heads</p><p><code>&quot;xlnet_base_cased&quot;</code> = <a href="https://storage.googleapis.com/xlnet/released_models/cased_L-12_H-768_A-12.zip" target="_blank">XLNet-Base</a>    |  12-layer, 768-hidden, 12-heads. This model is trained on full data (different from the one in the paper).</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = XlnetEmbeddings.pretrained()
  .setInputCols(<span class="lit">"sentence"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)</pre><p>The default model is <code>&quot;xlnet_base_cased&quot;</code>, if no name is provided.</p><p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/dl-ner/ner_xlnet.ipynb" target="_blank">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/XlnetEmbeddingsTestSpec.scala" target="_blank">XlnetEmbeddingsTestSpec</a>.
Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. The Spark NLP Workshop
example shows how to import them <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a>.</p><p><b>Sources :</b></p><p><a href="https://arxiv.org/abs/1906.08237" target="_blank">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></p><p><a href="https://github.com/zihangdai/xlnet" target="_blank">https://github.com/zihangdai/xlnet</a></p><p><b>Paper abstract: </b></p><p><i>With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves
better performance than pretraining approaches based on autoregressive language modeling. However, relying on
corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune
discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that
(1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the
factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore,
XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically,
under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question
answering, natural language inference, sentiment analysis, and document ranking.</i></p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = XlnetEmbeddings.pretrained()
  .setInputCols(<span class="lit">"token"</span>, <span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  documentAssembler,
  tokenizer,
  embeddings,
  embeddingsFinisher
))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[-<span class="num">0.6287205219268799</span>,-<span class="num">0.4865287244319916</span>,-<span class="num">0.186111718416214</span>,<span class="num">0.234187275171279</span>...|
|[-<span class="num">1.1967450380325317</span>,<span class="num">0.2746637463569641</span>,<span class="num">0.9481253027915955</span>,<span class="num">0.3431355059146881</span>...|
|[-<span class="num">1.0777631998062134</span>,-<span class="num">2.092679977416992</span>,-<span class="num">1.5331977605819702</span>,-<span class="num">1.11190271377563</span>...|
|[-<span class="num">0.8349916934967041</span>,-<span class="num">0.45627787709236145</span>,-<span class="num">0.7890847325325012</span>,-<span class="num">1.028069257736</span>...|
|[-<span class="num">0.134845569729805</span>,-<span class="num">0.11672890186309814</span>,<span class="num">0.4945235550403595</span>,-<span class="num">0.66587203741073</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p><a href="https://nlp.johnsnowlabs.com/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer based embeddings</p></span></dd></dl></div>
    </li></ol>
            </div>

        

        <div class="values members">
              <h3>Value Members</h3>
              <ol>
                <li name="com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="AlbertEmbeddings"></a><a id="AlbertEmbeddings:AlbertEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/AlbertEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of AlbertEmbeddings." href="AlbertEmbeddings$.html"><span class="name">AlbertEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedAlbertModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedAlbertModel">ReadablePretrainedAlbertModel</a> with <a href="ReadAlbertTensorflowModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadAlbertTensorflowModel">ReadAlbertTensorflowModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="AlbertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings">AlbertEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="AlbertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings">AlbertEmbeddings</a>. Please refer to that class for the documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="BertEmbeddings"></a><a id="BertEmbeddings:BertEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/BertEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of BertEmbeddings." href="BertEmbeddings$.html"><span class="name">BertEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedBertModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedBertModel">ReadablePretrainedBertModel</a> with <a href="ReadBertTensorflowModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadBertTensorflowModel">ReadBertTensorflowModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a>. Please refer to that class for the documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="BertSentenceEmbeddings"></a><a id="BertSentenceEmbeddings:BertSentenceEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/BertSentenceEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of BertSentenceEmbeddings." href="BertSentenceEmbeddings$.html"><span class="name">BertSentenceEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedBertSentenceModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedBertSentenceModel">ReadablePretrainedBertSentenceModel</a> with <a href="ReadBertSentenceTensorflowModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadBertSentenceTensorflowModel">ReadBertSentenceTensorflowModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="BertSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings">BertSentenceEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="BertSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings">BertSentenceEmbeddings</a>. Please refer to that class for the documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ChunkEmbeddings"></a><a id="ChunkEmbeddings:ChunkEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ChunkEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of ChunkEmbeddings." href="ChunkEmbeddings$.html"><span class="name">ChunkEmbeddings</span></a><span class="result"> extends <span class="extype" name="org.apache.spark.ml.util.DefaultParamsReadable">DefaultParamsReadable</span>[<a href="ChunkEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings">ChunkEmbeddings</a>] with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="ChunkEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings">ChunkEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="ChunkEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings">ChunkEmbeddings</a>. Please refer to that class for the documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="DistilBertEmbeddings"></a><a id="DistilBertEmbeddings:DistilBertEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/DistilBertEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of DistilBertEmbeddings." href="DistilBertEmbeddings$.html"><span class="name">DistilBertEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedDistilBertModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedDistilBertModel">ReadablePretrainedDistilBertModel</a> with <a href="ReadDistilBertTensorflowModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadDistilBertTensorflowModel">ReadDistilBertTensorflowModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="DistilBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings">DistilBertEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="DistilBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings">DistilBertEmbeddings</a>. Please refer to that class for the documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ElmoEmbeddings"></a><a id="ElmoEmbeddings:ElmoEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ElmoEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of ElmoEmbeddings." href="ElmoEmbeddings$.html"><span class="name">ElmoEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedElmoModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedElmoModel">ReadablePretrainedElmoModel</a> with <a href="ReadElmoTensorflowModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadElmoTensorflowModel">ReadElmoTensorflowModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="ElmoEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings">ElmoEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="ElmoEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings">ElmoEmbeddings</a>. Please refer to that class for the documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.PoolingStrategy" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="PoolingStrategy"></a><a id="PoolingStrategy:PoolingStrategy"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/PoolingStrategy$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="PoolingStrategy$.html"><span class="name">PoolingStrategy</span></a>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="RoBertaEmbeddings"></a><a id="RoBertaEmbeddings:RoBertaEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/RoBertaEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of RoBertaEmbeddings." href="RoBertaEmbeddings$.html"><span class="name">RoBertaEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedRobertaModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedRobertaModel">ReadablePretrainedRobertaModel</a> with <a href="ReadRobertaTensorflowModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadRobertaTensorflowModel">ReadRobertaTensorflowModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="RoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings">RoBertaEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="RoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings">RoBertaEmbeddings</a>. Please refer to that class for the documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="SentenceEmbeddings"></a><a id="SentenceEmbeddings:SentenceEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/SentenceEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of SentenceEmbeddings." href="SentenceEmbeddings$.html"><span class="name">SentenceEmbeddings</span></a><span class="result"> extends <span class="extype" name="org.apache.spark.ml.util.DefaultParamsReadable">DefaultParamsReadable</span>[<a href="SentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings">SentenceEmbeddings</a>] with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="SentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings">SentenceEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="SentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings">SentenceEmbeddings</a>. Please refer to that class for the documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="UniversalSentenceEncoder"></a><a id="UniversalSentenceEncoder:UniversalSentenceEncoder"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/UniversalSentenceEncoder$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of UniversalSentenceEncoder." href="UniversalSentenceEncoder$.html"><span class="name">UniversalSentenceEncoder</span></a><span class="result"> extends <a href="ReadablePretrainedUSEModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedUSEModel">ReadablePretrainedUSEModel</a> with <a href="ReadUSETensorflowModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadUSETensorflowModel">ReadUSETensorflowModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="UniversalSentenceEncoder.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder">UniversalSentenceEncoder</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="UniversalSentenceEncoder.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder">UniversalSentenceEncoder</a>. Please refer to that class for the documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="WordEmbeddings"></a><a id="WordEmbeddings:WordEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/WordEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of WordEmbeddings." href="WordEmbeddings$.html"><span class="name">WordEmbeddings</span></a><span class="result"> extends <span class="extype" name="org.apache.spark.ml.util.DefaultParamsReadable">DefaultParamsReadable</span>[<a href="WordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings">WordEmbeddings</a>] with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="WordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings">WordEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="WordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings">WordEmbeddings</a>. Please refer to that class for the documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsBinaryIndexer" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="WordEmbeddingsBinaryIndexer"></a><a id="WordEmbeddingsBinaryIndexer:WordEmbeddingsBinaryIndexer"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/WordEmbeddingsBinaryIndexer$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="WordEmbeddingsBinaryIndexer$.html"><span class="name">WordEmbeddingsBinaryIndexer</span></a>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="WordEmbeddingsModel"></a><a id="WordEmbeddingsModel:WordEmbeddingsModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/WordEmbeddingsModel$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of WordEmbeddingsModel." href="WordEmbeddingsModel$.html"><span class="name">WordEmbeddingsModel</span></a><span class="result"> extends <a href="ReadablePretrainedWordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedWordEmbeddings">ReadablePretrainedWordEmbeddings</a> with <a href="EmbeddingsCoverage.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.EmbeddingsCoverage">EmbeddingsCoverage</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel</a>. Please refer to that class for the documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsTextIndexer" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="WordEmbeddingsTextIndexer"></a><a id="WordEmbeddingsTextIndexer:WordEmbeddingsTextIndexer"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/WordEmbeddingsTextIndexer$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="WordEmbeddingsTextIndexer$.html"><span class="name">WordEmbeddingsTextIndexer</span></a>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="XlmRoBertaEmbeddings"></a><a id="XlmRoBertaEmbeddings:XlmRoBertaEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/XlmRoBertaEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="XlmRoBertaEmbeddings$.html"><span class="name">XlmRoBertaEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedXlmRobertaModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedXlmRobertaModel">ReadablePretrainedXlmRobertaModel</a> with <a href="ReadXlmRobertaTensorflowModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadXlmRobertaTensorflowModel">ReadXlmRobertaTensorflowModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="XlnetEmbeddings"></a><a id="XlnetEmbeddings:XlnetEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/XlnetEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of XlnetEmbeddings." href="XlnetEmbeddings$.html"><span class="name">XlnetEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedXlnetModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedXlnetModel">ReadablePretrainedXlnetModel</a> with <a href="ReadXlnetTensorflowModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadXlnetTensorflowModel">ReadXlnetTensorflowModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="XlnetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings">XlnetEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="XlnetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings">XlnetEmbeddings</a>. Please refer to that class for the documentation.
</p></div></div>
    </li>
              </ol>
            </div>

        

        
        </div>

        <div id="inheritedMembers">
        
        
        </div>

        <div id="groupedMembers">
        <div class="group" name="Ungrouped">
              <h3>Ungrouped</h3>
              
            </div>
        </div>

      </div>

      <div id="tooltip"></div>

      <div id="footer">  </div>
    </body>
          </div>
        </div>
      </div>
    </body>
      </html>
