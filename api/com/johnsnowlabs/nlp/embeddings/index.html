<!DOCTYPE html >
<html>
        <head>
          <meta http-equiv="X-UA-Compatible" content="IE=edge" />
          <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
          <title>Spark NLP 5.1.2 ScalaDoc  - com.johnsnowlabs.nlp.embeddings</title>
          <meta name="description" content="Spark NLP 5.1.2 ScalaDoc - com.johnsnowlabs.nlp.embeddings" />
          <meta name="keywords" content="Spark NLP 5.1.2 ScalaDoc com.johnsnowlabs.nlp.embeddings" />
          <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
          
      
      <link href="../../../../lib/index.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../../../lib/template.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../../../lib/diagrams.css" media="screen" type="text/css" rel="stylesheet" id="diagrams-css" />
      <script type="text/javascript" src="../../../../lib/jquery.min.js"></script>
      <script type="text/javascript" src="../../../../lib/jquery.panzoom.min.js"></script>
      <script type="text/javascript" src="../../../../lib/jquery.mousewheel.min.js"></script>
      <script type="text/javascript" src="../../../../lib/index.js"></script>
      <script type="text/javascript" src="../../../../index.js"></script>
      <script type="text/javascript" src="../../../../lib/scheduler.js"></script>
      <script type="text/javascript" src="../../../../lib/template.js"></script>
      
      <script type="text/javascript">
        /* this variable can be used by the JS to determine the path to the root document */
        var toRoot = '../../../../';
      </script>
    
        </head>
        <body>
      <div id="search">
        <span id="doc-title">Spark NLP 5.1.2 ScalaDoc<span id="doc-version"></span></span>
        <span class="close-results"><span class="left">&lt;</span> Back</span>
        <div id="textfilter">
          <span class="input">
            <input autocapitalize="none" placeholder="Search" id="index-input" type="text" accesskey="/" />
            <i class="clear material-icons"></i>
            <i id="search-icon" class="material-icons"></i>
          </span>
        </div>
    </div>
      <div id="search-results">
        <div id="search-progress">
          <div id="progress-fill"></div>
        </div>
        <div id="results-content">
          <div id="entity-results"></div>
          <div id="member-results"></div>
        </div>
      </div>
      <div id="content-scroll-container" style="-webkit-overflow-scrolling: touch;">
        <div id="content-container" style="-webkit-overflow-scrolling: touch;">
          <div id="subpackage-spacer">
            <div id="packages">
              <h1>Packages</h1>
              <ul>
                <li name="_root_.root" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="_root_"></a><a id="root:_root_"></a>
      <span class="permalink">
      <a href="../../../../index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../../../../index.html"><span class="name">root</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../../../../index.html" class="extype" name="_root_">root</a></dd></dl></div>
    </li><li name="_root_.com" visbl="pub" class="indented1 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="com"></a><a id="com:com"></a>
      <span class="permalink">
      <a href="../../../../com/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../../../index.html"><span class="name">com</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../../../../index.html" class="extype" name="_root_">root</a></dd></dl></div>
    </li><li name="com.johnsnowlabs" visbl="pub" class="indented2 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="johnsnowlabs"></a><a id="johnsnowlabs:johnsnowlabs"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../../index.html"><span class="name">johnsnowlabs</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../../../index.html" class="extype" name="com">com</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp" visbl="pub" class="indented3 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="nlp"></a><a id="nlp:nlp"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../index.html"><span class="name">nlp</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../../index.html" class="extype" name="com.johnsnowlabs">johnsnowlabs</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators" visbl="pub" class="indented4 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="annotators"></a><a id="annotators:annotators"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/annotators/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../annotators/index.html"><span class="name">annotators</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings" visbl="pub" class="indented4 current" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="embeddings"></a><a id="embeddings:embeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <span class="name">embeddings</span>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></dd></dl></div>
    </li><li class="current-entities indented4">
                        <a class="object" href="AlbertEmbeddings$.html" title="This is the companion object of AlbertEmbeddings."></a>
                        <a class="class" href="AlbertEmbeddings.html" title="ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS - Google Research, Toyota Technological Institute at Chicago"></a>
                        <a href="AlbertEmbeddings.html" title="ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS - Google Research, Toyota Technological Institute at Chicago">AlbertEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="BertEmbeddings$.html" title="This is the companion object of BertEmbeddings."></a>
                        <a class="class" href="BertEmbeddings.html" title="Token-level embeddings using BERT."></a>
                        <a href="BertEmbeddings.html" title="Token-level embeddings using BERT.">BertEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="BertSentenceEmbeddings$.html" title="This is the companion object of BertSentenceEmbeddings."></a>
                        <a class="class" href="BertSentenceEmbeddings.html" title="Sentence-level embeddings using BERT."></a>
                        <a href="BertSentenceEmbeddings.html" title="Sentence-level embeddings using BERT.">BertSentenceEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="CamemBertEmbeddings$.html" title="This is the companion object of CamemBertEmbeddings."></a>
                        <a class="class" href="CamemBertEmbeddings.html" title="The CamemBERT model was proposed in CamemBERT: a Tasty French Language Model by Louis Martin, Benjamin Muller, Pedro Javier Ortiz Suárez, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah, and Benoît Sagot."></a>
                        <a href="CamemBertEmbeddings.html" title="The CamemBERT model was proposed in CamemBERT: a Tasty French Language Model by Louis Martin, Benjamin Muller, Pedro Javier Ortiz Suárez, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah, and Benoît Sagot.">CamemBertEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="ChunkEmbeddings$.html" title="This is the companion object of ChunkEmbeddings."></a>
                        <a class="class" href="ChunkEmbeddings.html" title="This annotator utilizes WordEmbeddings, BertEmbeddings etc."></a>
                        <a href="ChunkEmbeddings.html" title="This annotator utilizes WordEmbeddings, BertEmbeddings etc.">ChunkEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="DeBertaEmbeddings$.html" title="This is the companion object of DeBertaEmbeddings."></a>
                        <a class="class" href="DeBertaEmbeddings.html" title="The DeBERTa model was proposed in DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google’s BERT model released in 2018 and Facebook’s RoBERTa model released in 2019."></a>
                        <a href="DeBertaEmbeddings.html" title="The DeBERTa model was proposed in DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google’s BERT model released in 2018 and Facebook’s RoBERTa model released in 2019.">DeBertaEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="DistilBertEmbeddings$.html" title="This is the companion object of DistilBertEmbeddings."></a>
                        <a class="class" href="DistilBertEmbeddings.html" title="DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base."></a>
                        <a href="DistilBertEmbeddings.html" title="DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base.">DistilBertEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="Doc2VecApproach$.html" title="This is the companion object of Doc2VecApproach."></a>
                        <a class="class" href="Doc2VecApproach.html" title="Trains a Word2Vec model that creates vector representations of words in a text corpus."></a>
                        <a href="Doc2VecApproach.html" title="Trains a Word2Vec model that creates vector representations of words in a text corpus.">Doc2VecApproach</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="Doc2VecModel$.html" title="This is the companion object of Doc2VecModel."></a>
                        <a class="class" href="Doc2VecModel.html" title="Word2Vec model that creates vector representations of words in a text corpus."></a>
                        <a href="Doc2VecModel.html" title="Word2Vec model that creates vector representations of words in a text corpus.">Doc2VecModel</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="E5Embeddings$.html" title="This is the companion object of E5Embeddings."></a>
                        <a class="class" href="E5Embeddings.html" title="Sentence embeddings using E5."></a>
                        <a href="E5Embeddings.html" title="Sentence embeddings using E5.">E5Embeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="ElmoEmbeddings$.html" title="This is the companion object of ElmoEmbeddings."></a>
                        <a class="class" href="ElmoEmbeddings.html" title="Word embeddings from ELMo (Embeddings from Language Models), a language model trained on the 1 Billion Word Benchmark."></a>
                        <a href="ElmoEmbeddings.html" title="Word embeddings from ELMo (Embeddings from Language Models), a language model trained on the 1 Billion Word Benchmark.">ElmoEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="EmbeddingsCoverage.html" title=""></a>
                        <a href="EmbeddingsCoverage.html" title="">EmbeddingsCoverage</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="HasEmbeddingsProperties.html" title=""></a>
                        <a href="HasEmbeddingsProperties.html" title="">HasEmbeddingsProperties</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="InstructorEmbeddings$.html" title="This is the companion object of InstructorEmbeddings."></a>
                        <a class="class" href="InstructorEmbeddings.html" title="Sentence embeddings using INSTRUCTOR."></a>
                        <a href="InstructorEmbeddings.html" title="Sentence embeddings using INSTRUCTOR.">InstructorEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="LongformerEmbeddings$.html" title="This is the companion object of LongformerEmbeddings."></a>
                        <a class="class" href="LongformerEmbeddings.html" title="Longformer is a transformer model for long documents."></a>
                        <a href="LongformerEmbeddings.html" title="Longformer is a transformer model for long documents.">LongformerEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="MPNetEmbeddings$.html" title="This is the companion object of MPNetEmbeddings."></a>
                        <a class="class" href="MPNetEmbeddings.html" title="Sentence embeddings using MPNet."></a>
                        <a href="MPNetEmbeddings.html" title="Sentence embeddings using MPNet.">MPNetEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="object" href="PoolingStrategy$.html" title=""></a>
                        <a href="PoolingStrategy$.html" title="">PoolingStrategy</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadAlbertDLModel.html" title=""></a>
                        <a href="ReadAlbertDLModel.html" title="">ReadAlbertDLModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadBertDLModel.html" title=""></a>
                        <a href="ReadBertDLModel.html" title="">ReadBertDLModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadBertSentenceDLModel.html" title=""></a>
                        <a href="ReadBertSentenceDLModel.html" title="">ReadBertSentenceDLModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadCamemBertDLModel.html" title=""></a>
                        <a href="ReadCamemBertDLModel.html" title="">ReadCamemBertDLModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadDeBertaDLModel.html" title=""></a>
                        <a href="ReadDeBertaDLModel.html" title="">ReadDeBertaDLModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadDistilBertDLModel.html" title=""></a>
                        <a href="ReadDistilBertDLModel.html" title="">ReadDistilBertDLModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadE5DLModel.html" title=""></a>
                        <a href="ReadE5DLModel.html" title="">ReadE5DLModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadElmoDLModel.html" title=""></a>
                        <a href="ReadElmoDLModel.html" title="">ReadElmoDLModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadInstructorDLModel.html" title=""></a>
                        <a href="ReadInstructorDLModel.html" title="">ReadInstructorDLModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadLongformerDLModel.html" title=""></a>
                        <a href="ReadLongformerDLModel.html" title="">ReadLongformerDLModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadMPNetDLModel.html" title=""></a>
                        <a href="ReadMPNetDLModel.html" title="">ReadMPNetDLModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadRobertaDLModel.html" title=""></a>
                        <a href="ReadRobertaDLModel.html" title="">ReadRobertaDLModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadRobertaSentenceDLModel.html" title=""></a>
                        <a href="ReadRobertaSentenceDLModel.html" title="">ReadRobertaSentenceDLModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadUSEDLModel.html" title=""></a>
                        <a href="ReadUSEDLModel.html" title="">ReadUSEDLModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadXlmRobertaDLModel.html" title=""></a>
                        <a href="ReadXlmRobertaDLModel.html" title="">ReadXlmRobertaDLModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadXlmRobertaSentenceDLModel.html" title=""></a>
                        <a href="ReadXlmRobertaSentenceDLModel.html" title="">ReadXlmRobertaSentenceDLModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadXlnetDLModel.html" title=""></a>
                        <a href="ReadXlnetDLModel.html" title="">ReadXlnetDLModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedAlbertModel.html" title=""></a>
                        <a href="ReadablePretrainedAlbertModel.html" title="">ReadablePretrainedAlbertModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedBertModel.html" title=""></a>
                        <a href="ReadablePretrainedBertModel.html" title="">ReadablePretrainedBertModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedBertSentenceModel.html" title=""></a>
                        <a href="ReadablePretrainedBertSentenceModel.html" title="">ReadablePretrainedBertSentenceModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedCamemBertModel.html" title=""></a>
                        <a href="ReadablePretrainedCamemBertModel.html" title="">ReadablePretrainedCamemBertModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedDeBertaModel.html" title=""></a>
                        <a href="ReadablePretrainedDeBertaModel.html" title="">ReadablePretrainedDeBertaModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedDistilBertModel.html" title=""></a>
                        <a href="ReadablePretrainedDistilBertModel.html" title="">ReadablePretrainedDistilBertModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedDoc2Vec.html" title=""></a>
                        <a href="ReadablePretrainedDoc2Vec.html" title="">ReadablePretrainedDoc2Vec</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedE5Model.html" title=""></a>
                        <a href="ReadablePretrainedE5Model.html" title="">ReadablePretrainedE5Model</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedElmoModel.html" title=""></a>
                        <a href="ReadablePretrainedElmoModel.html" title="">ReadablePretrainedElmoModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedInstructorModel.html" title=""></a>
                        <a href="ReadablePretrainedInstructorModel.html" title="">ReadablePretrainedInstructorModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedLongformerModel.html" title=""></a>
                        <a href="ReadablePretrainedLongformerModel.html" title="">ReadablePretrainedLongformerModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedMPNetModel.html" title=""></a>
                        <a href="ReadablePretrainedMPNetModel.html" title="">ReadablePretrainedMPNetModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedRobertaModel.html" title=""></a>
                        <a href="ReadablePretrainedRobertaModel.html" title="">ReadablePretrainedRobertaModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedRobertaSentenceModel.html" title=""></a>
                        <a href="ReadablePretrainedRobertaSentenceModel.html" title="">ReadablePretrainedRobertaSentenceModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedUSEModel.html" title=""></a>
                        <a href="ReadablePretrainedUSEModel.html" title="">ReadablePretrainedUSEModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedWord2Vec.html" title=""></a>
                        <a href="ReadablePretrainedWord2Vec.html" title="">ReadablePretrainedWord2Vec</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedWordEmbeddings.html" title=""></a>
                        <a href="ReadablePretrainedWordEmbeddings.html" title="">ReadablePretrainedWordEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedXlmRobertaModel.html" title=""></a>
                        <a href="ReadablePretrainedXlmRobertaModel.html" title="">ReadablePretrainedXlmRobertaModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedXlmRobertaSentenceModel.html" title=""></a>
                        <a href="ReadablePretrainedXlmRobertaSentenceModel.html" title="">ReadablePretrainedXlmRobertaSentenceModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedXlnetModel.html" title=""></a>
                        <a href="ReadablePretrainedXlnetModel.html" title="">ReadablePretrainedXlnetModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="trait" href="ReadsFromBytes.html" title=""></a>
                        <a href="ReadsFromBytes.html" title="">ReadsFromBytes</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="RoBertaEmbeddings$.html" title="This is the companion object of RoBertaEmbeddings."></a>
                        <a class="class" href="RoBertaEmbeddings.html" title="The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov."></a>
                        <a href="RoBertaEmbeddings.html" title="The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.">RoBertaEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="RoBertaSentenceEmbeddings$.html" title="This is the companion object of RoBertaSentenceEmbeddings."></a>
                        <a class="class" href="RoBertaSentenceEmbeddings.html" title="Sentence-level embeddings using RoBERTa."></a>
                        <a href="RoBertaSentenceEmbeddings.html" title="Sentence-level embeddings using RoBERTa.">RoBertaSentenceEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="SentenceEmbeddings$.html" title="This is the companion object of SentenceEmbeddings."></a>
                        <a class="class" href="SentenceEmbeddings.html" title="Converts the results from WordEmbeddings, BertEmbeddings, or ElmoEmbeddings into sentence or document embeddings by either summing up or averaging all the word embeddings in a sentence or a document (depending on the inputCols)."></a>
                        <a href="SentenceEmbeddings.html" title="Converts the results from WordEmbeddings, BertEmbeddings, or ElmoEmbeddings into sentence or document embeddings by either summing up or averaging all the word embeddings in a sentence or a document (depending on the inputCols).">SentenceEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="UniversalSentenceEncoder$.html" title="This is the companion object of UniversalSentenceEncoder."></a>
                        <a class="class" href="UniversalSentenceEncoder.html" title="The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks."></a>
                        <a href="UniversalSentenceEncoder.html" title="The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.">UniversalSentenceEncoder</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="Word2VecApproach$.html" title="This is the companion object of Word2VecApproach."></a>
                        <a class="class" href="Word2VecApproach.html" title="Trains a Word2Vec model that creates vector representations of words in a text corpus."></a>
                        <a href="Word2VecApproach.html" title="Trains a Word2Vec model that creates vector representations of words in a text corpus.">Word2VecApproach</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="Word2VecModel$.html" title="This is the companion object of Word2VecModel."></a>
                        <a class="class" href="Word2VecModel.html" title="Word2Vec model that creates vector representations of words in a text corpus."></a>
                        <a href="Word2VecModel.html" title="Word2Vec model that creates vector representations of words in a text corpus.">Word2VecModel</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="WordEmbeddings$.html" title="This is the companion object of WordEmbeddings."></a>
                        <a class="class" href="WordEmbeddings.html" title="Word Embeddings lookup annotator that maps tokens to vectors."></a>
                        <a href="WordEmbeddings.html" title="Word Embeddings lookup annotator that maps tokens to vectors.">WordEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="object" href="WordEmbeddingsBinaryIndexer$.html" title=""></a>
                        <a href="WordEmbeddingsBinaryIndexer$.html" title="">WordEmbeddingsBinaryIndexer</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="WordEmbeddingsModel$.html" title="This is the companion object of WordEmbeddingsModel."></a>
                        <a class="class" href="WordEmbeddingsModel.html" title="Word Embeddings lookup annotator that maps tokens to vectors"></a>
                        <a href="WordEmbeddingsModel.html" title="Word Embeddings lookup annotator that maps tokens to vectors">WordEmbeddingsModel</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="class" href="WordEmbeddingsReader.html" title=""></a>
                        <a href="WordEmbeddingsReader.html" title="">WordEmbeddingsReader</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="object" href="WordEmbeddingsTextIndexer$.html" title=""></a>
                        <a href="WordEmbeddingsTextIndexer$.html" title="">WordEmbeddingsTextIndexer</a>
                      </li><li class="current-entities indented4">
                        <span class="separator"></span>
                        <a class="class" href="WordEmbeddingsWriter.html" title=""></a>
                        <a href="WordEmbeddingsWriter.html" title="">WordEmbeddingsWriter</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="XlmRoBertaEmbeddings$.html" title=""></a>
                        <a class="class" href="XlmRoBertaEmbeddings.html" title="The XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov."></a>
                        <a href="XlmRoBertaEmbeddings.html" title="The XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.">XlmRoBertaEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="XlmRoBertaSentenceEmbeddings$.html" title=""></a>
                        <a class="class" href="XlmRoBertaSentenceEmbeddings.html" title="Sentence-level embeddings using XLM-RoBERTa."></a>
                        <a href="XlmRoBertaSentenceEmbeddings.html" title="Sentence-level embeddings using XLM-RoBERTa.">XlmRoBertaSentenceEmbeddings</a>
                      </li><li class="current-entities indented4">
                        <a class="object" href="XlnetEmbeddings$.html" title="This is the companion object of XlnetEmbeddings."></a>
                        <a class="class" href="XlnetEmbeddings.html" title="XlnetEmbeddings (XLNet): Generalized Autoregressive Pretraining for Language Understanding"></a>
                        <a href="XlnetEmbeddings.html" title="XlnetEmbeddings (XLNet): Generalized Autoregressive Pretraining for Language Understanding">XlnetEmbeddings</a>
                      </li><li name="com.johnsnowlabs.nlp.finisher" visbl="pub" class="indented4 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="finisher"></a><a id="finisher:finisher"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/finisher/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../finisher/index.html"><span class="name">finisher</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.pretrained" visbl="pub" class="indented4 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="pretrained"></a><a id="pretrained:pretrained"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/pretrained/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../pretrained/index.html"><span class="name">pretrained</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.recursive" visbl="pub" class="indented4 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="recursive"></a><a id="recursive:recursive"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/recursive/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../recursive/index.html"><span class="name">recursive</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.serialization" visbl="pub" class="indented4 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="serialization"></a><a id="serialization:serialization"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/serialization/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../serialization/index.html"><span class="name">serialization</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.training" visbl="pub" class="indented4 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="training"></a><a id="training:training"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/training/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../training/index.html"><span class="name">training</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.util" visbl="pub" class="indented4 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="util"></a><a id="util:util"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/util/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../util/index.html"><span class="name">util</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></dd></dl></div>
    </li>
              </ul>
            </div>
          </div>
          <div id="content">
            <body class="package value">
      <div id="definition">
        <div class="big-circle package">p</div>
        <p id="owner"><a href="../../../index.html" class="extype" name="com">com</a>.<a href="../../index.html" class="extype" name="com.johnsnowlabs">johnsnowlabs</a>.<a href="../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></p>
        <h1>embeddings<span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span></h1>
        
      </div>

      <h4 id="signature" class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <span class="name">embeddings</span>
      </span>
      </h4>

      
          <div id="comment" class="fullcommenttop"></div>
        

      <div id="mbrsel">
        <div class="toggle"></div>
        <div id="memberfilter">
          <i class="material-icons arrow"></i>
          <span class="input">
            <input id="mbrsel-input" placeholder="Filter all members" type="text" accesskey="/" />
          </span>
          <i class="clear material-icons"></i>
        </div>
        <div id="filterby">
          <div id="order">
            <span class="filtertype">Ordering</span>
            <ol>
              
              <li class="alpha in"><span>Alphabetic</span></li>
              
            </ol>
          </div>
          
          <div id="visbl">
              <span class="filtertype">Visibility</span>
              <ol><li class="public in"><span>Public</span></li><li class="all out"><span>All</span></li></ol>
            </div>
        </div>
      </div>

      <div id="template">
        <div id="allMembers">
        

        <div id="types" class="types members">
              <h3>Type Members</h3>
              <ol><li name="com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="AlbertEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings]withWriteTensorflowModelwithWriteSentencePieceModelwithWriteOnnxModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitivePropertieswithHasEngine"></a><a id="AlbertEmbeddings:AlbertEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/AlbertEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS - Google Research, Toyota Technological Institute at Chicago" href="AlbertEmbeddings.html"><span class="name">AlbertEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="AlbertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings">AlbertEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="AlbertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings">AlbertEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/WriteSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.WriteSentencePieceModel">WriteSentencePieceModel</a> with <a href="../../ml/onnx/WriteOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.WriteOnnxModel">WriteOnnxModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a> with <a href="../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS - Google
Research, Toyota Technological Institute at Chicago</p><div class="fullcomment"><div class="comment cmt"><p>ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS - Google
Research, Toyota Technological Institute at Chicago</p><p>These word embeddings represent the outputs generated by the Albert model. All official Albert
releases by google in TF-HUB are supported with this Albert Wrapper:</p><p><b>Ported TF-Hub Models:</b></p><p><code>&quot;albert_base_uncased&quot;</code> | <a href="https://tfhub.dev/google/albert_base/3" target="_blank">albert_base</a> |
768-embed-dim, 12-layer, 12-heads, 12M parameters</p><p><code>&quot;albert_large_uncased&quot;</code> | <a href="https://tfhub.dev/google/albert_large/3" target="_blank">albert_large</a> |
1024-embed-dim, 24-layer, 16-heads, 18M parameters</p><p><code>&quot;albert_xlarge_uncased&quot;</code> | <a href="https://tfhub.dev/google/albert_xlarge/3" target="_blank">albert_xlarge</a> |
2048-embed-dim, 24-layer, 32-heads, 60M parameters</p><p><code>&quot;albert_xxlarge_uncased&quot;</code> | <a href="https://tfhub.dev/google/albert_xxlarge/3" target="_blank">albert_xxlarge</a> |
4096-embed-dim, 12-layer, 64-heads, 235M parameters</p><p>This model requires input tokenization with SentencePiece model, which is provided by
Spark-NLP (See tokenizers package).</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = AlbertEmbeddings.pretrained()
 .setInputCols(<span class="lit">"sentence"</span>, <span class="lit">"token"</span>)
 .setOutputCol(<span class="lit">"embeddings"</span>)</pre><p>The default model is <code>&quot;albert_base_uncased&quot;</code>, if no name is provided.</p><p>For extended examples of usage, see the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/dl-ner/ner_albert.ipynb" target="_blank">Examples</a>
and the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/AlbertEmbeddingsTestSpec.scala" target="_blank">AlbertEmbeddingsTestSpec</a>.
To see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a>.</p><p><b>References:</b></p><p><a href="https://arxiv.org/pdf/1909.11942.pdf" target="_blank">ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS</a></p><p><a href="https://github.com/google-research/ALBERT" target="_blank">https://github.com/google-research/ALBERT</a></p><p><a href="https://tfhub.dev/s?q=albert" target="_blank">https://tfhub.dev/s?q=albert</a></p><p><b>Paper abstract:</b></p><p><i>Increasing model size when pretraining natural language representations often results in
improved performance on downstream tasks. However, at some point further model increases
become harder due to GPU/TPU memory limitations and longer training times. To address these
problems, we present two parameter reduction techniques to lower memory consumption and
increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence
shows that our proposed methods lead to models that scale much better compared to the original
BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence,
and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our
best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks
while having fewer parameters compared to BERT-large.</i></p><p><b>Tips:</b> ALBERT uses repeating layers which results in a small memory footprint, however
the computational cost remains similar to a BERT-like architecture with the same number of
hidden layers as it has to iterate through the same number of (repeating) layers.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = AlbertEmbeddings.pretrained()
  .setInputCols(<span class="lit">"token"</span>, <span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  documentAssembler,
  tokenizer,
  embeddings,
  embeddingsFinisher
))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[<span class="num">1.1342473030090332</span>,-<span class="num">1.3855540752410889</span>,<span class="num">0.9818322062492371</span>,-<span class="num">0.784737348556518</span>...|
|[<span class="num">0.847029983997345</span>,-<span class="num">1.047153353691101</span>,-<span class="num">0.1520637571811676</span>,-<span class="num">0.6245765686035156</span>...|
|[-<span class="num">0.009860038757324219</span>,-<span class="num">0.13450059294700623</span>,<span class="num">2.707749128341675</span>,<span class="num">1.2916892766952</span>...|
|[-<span class="num">0.04192575812339783</span>,-<span class="num">0.5764210224151611</span>,-<span class="num">0.3196685314178467</span>,-<span class="num">0.527840495109</span>...|
|[<span class="num">0.15583214163780212</span>,-<span class="num">0.1614152491092682</span>,-<span class="num">0.28423872590065</span>,-<span class="num">0.135491415858268</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="../annotators/classifier/dl/AlbertForTokenClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.classifier.dl.AlbertForTokenClassification">AlbertForTokenClassification</a>
  for AlbertEmbeddings with a token classification layer on top</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="BertEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.BertEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.BertEmbeddings]withWriteTensorflowModelwithWriteOnnxModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitivePropertieswithHasEngine"></a><a id="BertEmbeddings:BertEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/BertEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Token-level embeddings using BERT." href="BertEmbeddings.html"><span class="name">BertEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../ml/onnx/WriteOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.WriteOnnxModel">WriteOnnxModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a> with <a href="../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">Token-level embeddings using BERT.</p><div class="fullcomment"><div class="comment cmt"><p>Token-level embeddings using BERT. BERT (Bidirectional Encoder Representations from
Transformers) provides dense vector representations for natural language by using a deep,
pre-trained neural network with the Transformer architecture.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = BertEmbeddings.pretrained()
  .setInputCols(<span class="lit">"token"</span>, <span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"bert_embeddings"</span>)</pre><p>The default model is <code>&quot;small_bert_L2_768&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>For extended examples of usage, see the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/dl-ner/ner_bert.ipynb" target="_blank">Examples</a>
and the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/BertEmbeddingsTestSpec.scala" target="_blank">BertEmbeddingsTestSpec</a>.
To see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a>.</p><p><b>Sources</b> :</p><p><a href="https://arxiv.org/abs/1810.04805" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p><p><a href="https://github.com/google-research/bert" target="_blank">https://github.com/google-research/bert</a></p><p><b> Paper abstract </b></p><p><i>We introduce a new language representation model called BERT, which stands for Bidirectional
Encoder Representations from Transformers. Unlike recent language representation models, BERT
is designed to pre-train deep bidirectional representations from unlabeled text by jointly
conditioning on both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create state-of-the-art
models for a wide range of tasks, such as question answering and language inference, without
substantial task-specific architecture modifications. BERT is conceptually simple and
empirically powerful. It obtains new state-of-the-art results on eleven natural language
processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement),
MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1
to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute
improvement).</i></p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.BertEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = BertEmbeddings.pretrained(<span class="lit">"small_bert_L2_128"</span>, <span class="lit">"en"</span>)
  .setInputCols(<span class="lit">"token"</span>, <span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"bert_embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"bert_embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  documentAssembler,
  tokenizer,
  embeddings,
  embeddingsFinisher
))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[-<span class="num">2.3497989177703857</span>,<span class="num">0.480538547039032</span>,-<span class="num">0.3238905668258667</span>,-<span class="num">1.612930893898010</span>...|
|[-<span class="num">2.1357314586639404</span>,<span class="num">0.32984697818756104</span>,-<span class="num">0.6032363176345825</span>,-<span class="num">1.6791689395904</span>...|
|[-<span class="num">1.8244884014129639</span>,-<span class="num">0.27088963985443115</span>,-<span class="num">1.059438943862915</span>,-<span class="num">0.9817547798156</span>...|
|[-<span class="num">1.1648050546646118</span>,-<span class="num">0.4725411534309387</span>,-<span class="num">0.5938255786895752</span>,-<span class="num">1.5780693292617</span>...|
|[-<span class="num">0.9125322699546814</span>,<span class="num">0.4563939869403839</span>,-<span class="num">0.3975459933280945</span>,-<span class="num">1.81611204147338</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="BertSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings">BertSentenceEmbeddings</a> for sentence-level embeddings</p></span><span class="cmt"><p>
  <a href="../annotators/classifier/dl/BertForTokenClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.classifier.dl.BertForTokenClassification">BertForTokenClassification</a>
  For BertEmbeddings with a token classification layer on top</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="BertSentenceEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings]withWriteTensorflowModelwithWriteOnnxModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitivePropertieswithHasEnginewithHasProtectedParams"></a><a id="BertSentenceEmbeddings:BertSentenceEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/BertSentenceEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Sentence-level embeddings using BERT." href="BertSentenceEmbeddings.html"><span class="name">BertSentenceEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="BertSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings">BertSentenceEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="BertSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings">BertSentenceEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../ml/onnx/WriteOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.WriteOnnxModel">WriteOnnxModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a> with <a href="../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a> with <a href="../HasProtectedParams.html" class="extype" name="com.johnsnowlabs.nlp.HasProtectedParams">HasProtectedParams</a></span>
      </span>
      
      <p class="shortcomment cmt">Sentence-level embeddings using BERT.</p><div class="fullcomment"><div class="comment cmt"><p>Sentence-level embeddings using BERT. BERT (Bidirectional Encoder Representations from
Transformers) provides dense vector representations for natural language by using a deep,
pre-trained neural network with the Transformer architecture.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = BertSentenceEmbeddings.pretrained()
  .setInputCols(<span class="lit">"sentence"</span>)
  .setOutputCol(<span class="lit">"sentence_bert_embeddings"</span>)</pre><p>The default model is <code>&quot;sent_small_bert_L2_768&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>For extended examples of usage, see the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BERT%20Sentence.ipynb" target="_blank">Examples</a>
and the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/BertSentenceEmbeddingsTestSpec.scala" target="_blank">BertSentenceEmbeddingsTestSpec</a>.</p><p><b>Sources</b> :</p><p><a href="https://arxiv.org/abs/1810.04805" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p><p><a href="https://github.com/google-research/bert" target="_blank">https://github.com/google-research/bert</a></p><p><b> Paper abstract </b></p><p><i>We introduce a new language representation model called BERT, which stands for Bidirectional
Encoder Representations from Transformers. Unlike recent language representation models, BERT
is designed to pre-train deep bidirectional representations from unlabeled text by jointly
conditioning on both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create state-of-the-art
models for a wide range of tasks, such as question answering and language inference, without
substantial task-specific architecture modifications. BERT is conceptually simple and
empirically powerful. It obtains new state-of-the-art results on eleven natural language
processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement),
MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1
to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute
improvement).</i></p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator.SentenceDetector
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> sentence = <span class="kw">new</span> SentenceDetector()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"sentence"</span>)

<span class="kw">val</span> embeddings = BertSentenceEmbeddings.pretrained(<span class="lit">"sent_small_bert_L2_128"</span>)
  .setInputCols(<span class="lit">"sentence"</span>)
  .setOutputCol(<span class="lit">"sentence_bert_embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"sentence_bert_embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  documentAssembler,
  sentence,
  embeddings,
  embeddingsFinisher
))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"John loves apples. Mary loves oranges. John loves Mary."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[-<span class="num">0.8951074481010437</span>,<span class="num">0.13753940165042877</span>,<span class="num">0.3108254075050354</span>,-<span class="num">1.65693199634552</span>...|
|[-<span class="num">0.6180210709571838</span>,-<span class="num">0.12179657071828842</span>,-<span class="num">0.191165953874588</span>,-<span class="num">1.4497021436691</span>...|
|[-<span class="num">0.822715163230896</span>,<span class="num">0.7568016648292542</span>,-<span class="num">0.1165061742067337</span>,-<span class="num">1.59048593044281</span>,...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="BertSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings">BertSentenceEmbeddings</a> for sentence-level embeddings</p></span><span class="cmt"><p>
  <a href="../annotators/classifier/dl/BertForSequenceClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.classifier.dl.BertForSequenceClassification">BertForSequenceClassification</a>
  for embeddings with a sequence classification layer on top</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.CamemBertEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="CamemBertEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.CamemBertEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.CamemBertEmbeddings]withWriteTensorflowModelwithWriteSentencePieceModelwithWriteOnnxModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitivePropertieswithHasEngine"></a><a id="CamemBertEmbeddings:CamemBertEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/CamemBertEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="The CamemBERT model was proposed in CamemBERT: a Tasty French Language Model by Louis Martin, Benjamin Muller, Pedro Javier Ortiz Suárez, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah, and Benoît Sagot." href="CamemBertEmbeddings.html"><span class="name">CamemBertEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="CamemBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.CamemBertEmbeddings">CamemBertEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="CamemBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.CamemBertEmbeddings">CamemBertEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/WriteSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.WriteSentencePieceModel">WriteSentencePieceModel</a> with <a href="../../ml/onnx/WriteOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.WriteOnnxModel">WriteOnnxModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a> with <a href="../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">The CamemBERT model was proposed in CamemBERT: a Tasty French Language Model by Louis Martin,
Benjamin Muller, Pedro Javier Ortiz Suárez, Yoann Dupont, Laurent Romary, Éric Villemonte de
la Clergerie, Djamé Seddah, and Benoît Sagot.</p><div class="fullcomment"><div class="comment cmt"><p>The CamemBERT model was proposed in CamemBERT: a Tasty French Language Model by Louis Martin,
Benjamin Muller, Pedro Javier Ortiz Suárez, Yoann Dupont, Laurent Romary, Éric Villemonte de
la Clergerie, Djamé Seddah, and Benoît Sagot. It is based on Facebook’s RoBERTa model released
in 2019. It is a model trained on 138GB of French text.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = CamemBertEmbeddings.pretrained()
  .setInputCols(<span class="lit">"token"</span>, <span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"camembert_embeddings"</span>)</pre><p>The default model is <code>&quot;camembert_base&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>For extended examples of usage, see the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/dl-ner/ner_bert.ipynb" target="_blank">Examples</a>
and the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/CamemBertEmbeddingsTestSpec.scala" target="_blank">CamemBertEmbeddingsTestSpec</a>.
To see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a>.</p><p><b>Sources</b> :</p><p><a href="https://arxiv.org/abs/1911.03894" target="_blank">CamemBERT: a Tasty French Language Model</a></p><p><a href="https://huggingface.co/camembert" target="_blank">https://huggingface.co/camembert</a></p><p><b> Paper abstract </b></p><p><i>Pretrained language models are now ubiquitous in Natural Language Processing. Despite their
success, most available models have either been trained on English data or on the
concatenation of data in multiple languages. This makes practical use of such models --in all
languages except English-- very limited. In this paper, we investigate the feasibility of
training monolingual Transformer-based language models for other languages, taking French as
an example and evaluating our language models on part-of-speech tagging, dependency parsing,
named entity recognition and natural language inference tasks. We show that the use of web
crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a
relatively small web crawled dataset (4GB) leads to results that are as good as those obtained
using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the
state of the art in all four downstream tasks.</i></p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.CamemBertEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = CamemBertEmbeddings.pretrained()
  .setInputCols(<span class="lit">"token"</span>, <span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"camembert_embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"camembert_embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  documentAssembler,
  tokenizer,
  embeddings,
  embeddingsFinisher
))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"C'est une phrase."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[<span class="num">0.08442357927560806</span>,-<span class="num">0.12863239645957947</span>,-<span class="num">0.03835778683423996</span>,<span class="num">0.200479581952</span>...|
|[<span class="num">0.048462312668561935</span>,<span class="num">0.12637358903884888</span>,-<span class="num">0.27429091930389404</span>,-<span class="num">0.07516729831</span>...|
|[<span class="num">0.02690504491329193</span>,<span class="num">0.12104076147079468</span>,<span class="num">0.012526623904705048</span>,-<span class="num">0.031543646007</span>...|
|[<span class="num">0.05877285450696945</span>,-<span class="num">0.08773420006036758</span>,-<span class="num">0.06381352990865707</span>,<span class="num">0.122621834278</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ChunkEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings]withHasSimpleAnnotate[com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings]"></a><a id="ChunkEmbeddings:ChunkEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ChunkEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="This annotator utilizes WordEmbeddings, BertEmbeddings etc." href="ChunkEmbeddings.html"><span class="name">ChunkEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="ChunkEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings">ChunkEmbeddings</a>] with <a href="../HasSimpleAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasSimpleAnnotate">HasSimpleAnnotate</a>[<a href="ChunkEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings">ChunkEmbeddings</a>]</span>
      </span>
      
      <p class="shortcomment cmt">This annotator utilizes <a href="WordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings">WordEmbeddings</a>, <a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a> etc.</p><div class="fullcomment"><div class="comment cmt"><p>This annotator utilizes <a href="WordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings">WordEmbeddings</a>, <a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a> etc. to generate chunk
embeddings from either <a href="../annotators/Chunker.html" class="extype" name="com.johnsnowlabs.nlp.annotators.Chunker">Chunker</a>,
<a href="../annotators/NGramGenerator.html" class="extype" name="com.johnsnowlabs.nlp.annotators.NGramGenerator">NGramGenerator</a>, or
<a href="../annotators/ner/NerConverter.html" class="extype" name="com.johnsnowlabs.nlp.annotators.ner.NerConverter">NerConverter</a> outputs.</p><p>For extended examples of usage, see the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/embeddings/ChunkEmbeddings.ipynb" target="_blank">Examples</a>
and the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/ChunkEmbeddingsTestSpec.scala" target="_blank">ChunkEmbeddingsTestSpec</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.{NGramGenerator, Tokenizer}
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="cmt">// Extract the Embeddings from the NGrams</span>
<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> sentence = <span class="kw">new</span> SentenceDetector()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"sentence"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"sentence"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> nGrams = <span class="kw">new</span> NGramGenerator()
  .setInputCols(<span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"chunk"</span>)
  .setN(<span class="num">2</span>)

<span class="kw">val</span> embeddings = WordEmbeddingsModel.pretrained()
  .setInputCols(<span class="lit">"sentence"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)
  .setCaseSensitive(<span class="kw">false</span>)

<span class="cmt">// Convert the NGram chunks into Word Embeddings</span>
<span class="kw">val</span> chunkEmbeddings = <span class="kw">new</span> ChunkEmbeddings()
  .setInputCols(<span class="lit">"chunk"</span>, <span class="lit">"embeddings"</span>)
  .setOutputCol(<span class="lit">"chunk_embeddings"</span>)
  .setPoolingStrategy(<span class="lit">"AVERAGE"</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    sentence,
    tokenizer,
    nGrams,
    embeddings,
    chunkEmbeddings
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(chunk_embeddings) as result"</span>)
  .select(<span class="lit">"result.annotatorType"</span>, <span class="lit">"result.result"</span>, <span class="lit">"result.embeddings"</span>)
  .show(<span class="num">5</span>, <span class="num">80</span>)
+---------------+----------+--------------------------------------------------------------------------------+
|  annotatorType|    result|                                                                      embeddings|
+---------------+----------+--------------------------------------------------------------------------------+
|word_embeddings|   This is|[-<span class="num">0.55661</span>, <span class="num">0.42829502</span>, <span class="num">0.86661</span>, -<span class="num">0.409785</span>, <span class="num">0.06316501</span>, <span class="num">0.120775</span>, -<span class="num">0.0732005</span>, ...|
|word_embeddings|      is a|[-<span class="num">0.40674996</span>, <span class="num">0.22938299</span>, <span class="num">0.50597</span>, -<span class="num">0.288195</span>, <span class="num">0.555655</span>, <span class="num">0.465145</span>, <span class="num">0.140118</span>, <span class="num">0.</span>..|
|word_embeddings|a sentence|[<span class="num">0.17417</span>, <span class="num">0.095253006</span>, -<span class="num">0.0530925</span>, -<span class="num">0.218465</span>, <span class="num">0.714395</span>, <span class="num">0.79860497</span>, <span class="num">0.0129999</span>...|
|word_embeddings|sentence .|[<span class="num">0.139705</span>, <span class="num">0.177955</span>, <span class="num">0.1887775</span>, -<span class="num">0.45545</span>, <span class="num">0.20030999</span>, <span class="num">0.461557</span>, -<span class="num">0.07891501</span>, ...|
+---------------+----------+--------------------------------------------------------------------------------+</pre></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.DeBertaEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="DeBertaEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.DeBertaEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.DeBertaEmbeddings]withWriteTensorflowModelwithWriteOnnxModelwithWriteSentencePieceModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitivePropertieswithHasEngine"></a><a id="DeBertaEmbeddings:DeBertaEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/DeBertaEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="The DeBERTa model was proposed in DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google’s BERT model released in 2018 and Facebook’s RoBERTa model released in 2019." href="DeBertaEmbeddings.html"><span class="name">DeBertaEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="DeBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DeBertaEmbeddings">DeBertaEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="DeBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DeBertaEmbeddings">DeBertaEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../ml/onnx/WriteOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.WriteOnnxModel">WriteOnnxModel</a> with <a href="../../ml/tensorflow/sentencepiece/WriteSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.WriteSentencePieceModel">WriteSentencePieceModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a> with <a href="../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">The DeBERTa model was proposed in
<a href="https://arxiv.org/abs/2006.03654" target="_blank">DeBERTa: Decoding-enhanced BERT with Disentangled Attention</a>
by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google’s BERT model
released in 2018 and Facebook’s RoBERTa model released in 2019.</p><div class="fullcomment"><div class="comment cmt"><p>The DeBERTa model was proposed in
<a href="https://arxiv.org/abs/2006.03654" target="_blank">DeBERTa: Decoding-enhanced BERT with Disentangled Attention</a>
by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google’s BERT model
released in 2018 and Facebook’s RoBERTa model released in 2019.</p><p>This model requires input tokenization with SentencePiece model, which is provided by Spark
NLP (See tokenizers package).</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = DeBertaEmbeddings.pretrained()
 .setInputCols(<span class="lit">"sentence"</span>, <span class="lit">"token"</span>)
 .setOutputCol(<span class="lit">"embeddings"</span>)</pre><p>The default model is <code>&quot;deberta_v3_base&quot;</code>, if no name is provided.</p><p>For extended examples see
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/DeBertaEmbeddingsTestSpec.scala" target="_blank">DeBertaEmbeddingsTestSpec</a>.
To see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a>.</p><p>It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half
of the data used in RoBERTa.</p><p><b>References:</b></p><p><a href="https://github.com/microsoft/DeBERTa" target="_blank">https://github.com/microsoft/DeBERTa</a></p><p><a href="https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/" target="_blank">https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/</a></p><p><b>Paper abstract:</b></p><p><i>Recent progress in pre-trained neural language models has significantly improved the
performance of many natural language processing (NLP) tasks. In this paper we propose a new
model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves
the BERT and RoBERTa models using two novel techniques. The first is the disentangled
attention mechanism, where each word is represented using two vectors that encode its content
and position, respectively, and the attention weights among words are computed using
disentangled matrices on their contents and relative positions. Second, an enhanced mask
decoder is used to replace the output softmax layer to predict the masked tokens for model
pretraining. We show that these two techniques significantly improve the efficiency of model
pretraining and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model
trained on half of the training data performs consistently better on a wide range of NLP
tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3%
(88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code and pre-trained models
will be made publicly available at https://github.com/microsoft/DeBERTa.</i></p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.DeBertaEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = DeBertaEmbeddings.pretrained()
  .setInputCols(<span class="lit">"token"</span>, <span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  documentAssembler,
  tokenizer,
  embeddings,
  embeddingsFinisher
))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[<span class="num">1.1342473030090332</span>,-<span class="num">1.3855540752410889</span>,<span class="num">0.9818322062492371</span>,-<span class="num">0.784737348556518</span>...|
|[<span class="num">0.847029983997345</span>,-<span class="num">1.047153353691101</span>,-<span class="num">0.1520637571811676</span>,-<span class="num">0.6245765686035156</span>...|
|[-<span class="num">0.009860038757324219</span>,-<span class="num">0.13450059294700623</span>,<span class="num">2.707749128341675</span>,<span class="num">1.2916892766952</span>...|
|[-<span class="num">0.04192575812339783</span>,-<span class="num">0.5764210224151611</span>,-<span class="num">0.3196685314178467</span>,-<span class="num">0.527840495109</span>...|
|[<span class="num">0.15583214163780212</span>,-<span class="num">0.1614152491092682</span>,-<span class="num">0.28423872590065</span>,-<span class="num">0.135491415858268</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="DistilBertEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings]withWriteTensorflowModelwithWriteOnnxModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitivePropertieswithHasEngine"></a><a id="DistilBertEmbeddings:DistilBertEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/DistilBertEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base." href="DistilBertEmbeddings.html"><span class="name">DistilBertEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="DistilBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings">DistilBertEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="DistilBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings">DistilBertEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../ml/onnx/WriteOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.WriteOnnxModel">WriteOnnxModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a> with <a href="../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT
base.</p><div class="fullcomment"><div class="comment cmt"><p>DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT
base. It has 40% less parameters than <code>bert-base-uncased</code>, runs 60% faster while preserving
over 95% of BERT's performances as measured on the GLUE language understanding benchmark.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = DistilBertEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)</pre><p>The default model is <code>&quot;distilbert_base_cased&quot;</code>, if no name is provided. For available
pretrained models please see the <a href="https://sparknlp.org/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>For extended examples of usage, see the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/HuggingFace%20in%20Spark%20NLP%20-%20DistilBERT.ipynb" target="_blank">Examples</a>
and the
<a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/DistilBertEmbeddingsTestSpec.scala" target="_blank">DistilBertEmbeddingsTestSpec</a>.
To see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a>.</p><p>The DistilBERT model was proposed in the paper
<a href="https://arxiv.org/abs/1910.01108" target="_blank">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a>.</p><p><b>Paper Abstract:</b></p><p><i>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural
Language Processing (NLP), operating these large models in on-the-edge and/or under
constrained computational training or inference budgets remains challenging. In this work, we
propose a method to pre-train a smaller general-purpose language representation model, called
DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like
its larger counterparts. While most prior work investigated the use of distillation for
building task-specific models, we leverage knowledge distillation during the pretraining phase
and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of
its language understanding capabilities and being 60% faster. To leverage the inductive biases
learned by larger models during pretraining, we introduce a triple loss combining language
modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is
cheaper to pre-train and we demonstrate its capabilities for on-device computations in a
proof-of-concept experiment and a comparative on-device study.</i></p><p>Tips:</p><ul><li>DistilBERT doesn't have <code>:obj:token_type_ids</code>, you don't need to indicate which token
    belongs to which segment. Just separate your segments with the separation token
    <code>:obj:tokenizer.sep_token</code> (or <code>:obj:[SEP]</code>).</li><li>DistilBERT doesn't have options to select the input positions (<code>:obj:position_ids</code> input).
    This could be added if necessary though, just let us know if you need this option.</li></ul><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = DistilBertEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)
  .setCaseSensitive(<span class="kw">true</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    tokenizer,
    embeddings,
    embeddingsFinisher
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[<span class="num">0.1127224713563919</span>,-<span class="num">0.1982710212469101</span>,<span class="num">0.5360898375511169</span>,-<span class="num">0.272536993026733</span>...|
|[<span class="num">0.35534414649009705</span>,<span class="num">0.13215228915214539</span>,<span class="num">0.40981462597846985</span>,<span class="num">0.14036104083061</span>...|
|[<span class="num">0.328085333108902</span>,-<span class="num">0.06269335001707077</span>,-<span class="num">0.017595693469047546</span>,-<span class="num">0.024373905733</span>...|
|[<span class="num">0.15617232024669647</span>,<span class="num">0.2967822253704071</span>,<span class="num">0.22324979305267334</span>,-<span class="num">0.04568954557180</span>...|
|[<span class="num">0.45411425828933716</span>,<span class="num">0.01173491682857275</span>,<span class="num">0.190129816532135</span>,<span class="num">0.1178255230188369</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="../annotators/classifier/dl/DistilBertForTokenClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.classifier.dl.DistilBertForTokenClassification">DistilBertForTokenClassification</a>
  for DistilBertEmbeddings with a token classification layer on top</p></span><span class="cmt"><p>
  <a href="../annotators/classifier/dl/DistilBertForSequenceClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.classifier.dl.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a>
  for DistilBertEmbeddings with a sequence classification layer on top</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.Doc2VecApproach" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="Doc2VecApproachextendsAnnotatorApproach[com.johnsnowlabs.nlp.embeddings.Doc2VecModel]withHasStorageRefwithHasEnableCachingPropertieswithHasProtectedParams"></a><a id="Doc2VecApproach:Doc2VecApproach"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/Doc2VecApproach.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Trains a Word2Vec model that creates vector representations of words in a text corpus." href="Doc2VecApproach.html"><span class="name">Doc2VecApproach</span></a><span class="result"> extends <a href="../AnnotatorApproach.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorApproach">AnnotatorApproach</a>[<a href="Doc2VecModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Doc2VecModel">Doc2VecModel</a>] with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasEnableCachingProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasEnableCachingProperties">HasEnableCachingProperties</a> with <a href="../HasProtectedParams.html" class="extype" name="com.johnsnowlabs.nlp.HasProtectedParams">HasProtectedParams</a></span>
      </span>
      
      <p class="shortcomment cmt">Trains a Word2Vec model that creates vector representations of words in a text corpus.</p><div class="fullcomment"><div class="comment cmt"><p>Trains a Word2Vec model that creates vector representations of words in a text corpus.</p><p>The algorithm first constructs a vocabulary from the corpus and then learns vector
representation of words in the vocabulary. The vector representation can be used as features
in natural language processing and machine learning algorithms.</p><p>We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a
hierarchical softmax method to train the model. The variable names in the implementation match
the original C implementation.</p><p>For instantiated/pretrained models, see <a href="Doc2VecModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Doc2VecModel">Doc2VecModel</a>.</p><p><b>Sources</b> :</p><p>For the original C implementation, see https://code.google.com/p/word2vec/</p><p>For the research paper, see
<a href="https://arxiv.org/abs/1301.3781" target="_blank">Efficient Estimation of Word Representations in Vector Space</a>
and
<a href="https://arxiv.org/pdf/1310.4546v1.pdf" target="_blank">Distributed Representations of Words and Phrases and their Compositionality</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator.{Tokenizer, Doc2VecApproach}
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = <span class="kw">new</span> Doc2VecApproach()
  .setInputCols(<span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    tokenizer,
    embeddings
  ))

<span class="kw">val</span> path = <span class="lit">"src/test/resources/spell/sherlockholmes.txt"</span>
<span class="kw">val</span> dataset = spark.sparkContext.textFile(path)
  .toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> pipelineModel = pipeline.fit(dataset)</pre></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.Doc2VecModel" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="Doc2VecModelextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.Doc2VecModel]withHasSimpleAnnotate[com.johnsnowlabs.nlp.embeddings.Doc2VecModel]withHasStorageRefwithHasEmbeddingsPropertieswithParamsAndFeaturesWritable"></a><a id="Doc2VecModel:Doc2VecModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/Doc2VecModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Word2Vec model that creates vector representations of words in a text corpus." href="Doc2VecModel.html"><span class="name">Doc2VecModel</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="Doc2VecModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Doc2VecModel">Doc2VecModel</a>] with <a href="../HasSimpleAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasSimpleAnnotate">HasSimpleAnnotate</a>[<a href="Doc2VecModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Doc2VecModel">Doc2VecModel</a>] with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../ParamsAndFeaturesWritable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesWritable">ParamsAndFeaturesWritable</a></span>
      </span>
      
      <p class="shortcomment cmt">Word2Vec model that creates vector representations of words in a text corpus.</p><div class="fullcomment"><div class="comment cmt"><p>Word2Vec model that creates vector representations of words in a text corpus.</p><p>The algorithm first constructs a vocabulary from the corpus and then learns vector
representation of words in the vocabulary. The vector representation can be used as features
in natural language processing and machine learning algorithms.</p><p>We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a
hierarchical softmax method to train the model. The variable names in the implementation match
the original C implementation.</p><p>This is the instantiated model of the <a href="Doc2VecApproach.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Doc2VecApproach">Doc2VecApproach</a>. For training your own model, please
see the documentation of that class.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = Doc2VecModel.pretrained()
  .setInputCols(<span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)</pre><p>The default model is <code>&quot;doc2vec_gigaword_300&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the <a href="https://sparknlp.org/models" target="_blank">Models Hub</a>.</p><p><b>Sources</b> :</p><p>For the original C implementation, see https://code.google.com/p/word2vec/</p><p>For the research paper, see
<a href="https://arxiv.org/abs/1301.3781" target="_blank">Efficient Estimation of Word Representations in Vector Space</a>
and
<a href="https://arxiv.org/pdf/1310.4546v1.pdf" target="_blank">Distributed Representations of Words and Phrases and their Compositionality</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator.{Tokenizer, Doc2VecModel}
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher

<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = Doc2VecModel.pretrained()
  .setInputCols(<span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  documentAssembler,
  tokenizer,
  embeddings,
  embeddingsFinisher
))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">1</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[<span class="num">0.06222493574023247</span>,<span class="num">0.011579325422644615</span>,<span class="num">0.009919632226228714</span>,<span class="num">0.109361454844</span>...|
+--------------------------------------------------------------------------------+</pre></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.E5Embeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="E5EmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.E5Embeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.E5Embeddings]withWriteTensorflowModelwithWriteOnnxModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitivePropertieswithHasEngine"></a><a id="E5Embeddings:E5Embeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/E5Embeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Sentence embeddings using E5." href="E5Embeddings.html"><span class="name">E5Embeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="E5Embeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.E5Embeddings">E5Embeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="E5Embeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.E5Embeddings">E5Embeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../ml/onnx/WriteOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.WriteOnnxModel">WriteOnnxModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a> with <a href="../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">Sentence embeddings using E5.</p><div class="fullcomment"><div class="comment cmt"><p>Sentence embeddings using E5.</p><p>E5, an instruction-finetuned text embedding model that can generate text embeddings tailored
to any task (e.g., classification, retrieval, clustering, text evaluation, etc.)</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = E5Embeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"e5_embeddings"</span>)</pre><p>The default model is <code>&quot;e5_small&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?q=E5" target="_blank">Models Hub</a>.</p><p>For extended examples of usage, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/E5EmbeddingsTestSpec.scala" target="_blank">E5EmbeddingsTestSpec</a>.</p><p><b>Sources</b> :</p><p><a href="https://arxiv.org/pdf/2212.03533" target="_blank">Text Embeddings by Weakly-Supervised Contrastive Pre-training</a></p><p><a href="https://github.com/microsoft/unilm/tree/master/e5" target="_blank">E5 Github Repository</a></p><p><b> Paper abstract </b></p><p><i>This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a
wide range of tasks. The model is trained in a contrastive manner with weak supervision
signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily
used as a general-purpose embedding model for any tasks requiring a single-vector
representation of texts such as retrieval, clustering, and classification, achieving strong
performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56
datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that
outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled
data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing
embedding models with 40× more parameters.</i></p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.E5Embeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> embeddings = E5Embeddings.pretrained(<span class="lit">"e5_small"</span>, <span class="lit">"en"</span>)
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"e5_embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"e5_embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  documentAssembler,
  embeddings,
  embeddingsFinisher
))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"query: how much protein should a female eat"</span>,
<span class="lit">"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day."</span> +
But, as you can see from <span class="kw">this</span> chart, you'll need to increase that <span class="kw">if</span> you're expecting or training <span class="kw">for</span> a<span class="lit">" +
marathon. Check out the chart below to see how much protein you should be eating each day."</span>

).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">1</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[[<span class="num">8.0190285E-4</span>, -<span class="num">0.005974853</span>, -<span class="num">0.072875895</span>, <span class="num">0.007944068</span>, <span class="num">0.026059335</span>, -<span class="num">0.0080</span>...|
[[<span class="num">0.050514214</span>, <span class="num">0.010061974</span>, -<span class="num">0.04340176</span>, -<span class="num">0.020937217</span>, <span class="num">0.05170225</span>, <span class="num">0.01157857</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ElmoEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings]withHasSimpleAnnotate[com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings]withWriteTensorflowModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitivePropertieswithHasEngine"></a><a id="ElmoEmbeddings:ElmoEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ElmoEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Word embeddings from ELMo (Embeddings from Language Models), a language model trained on the 1 Billion Word Benchmark." href="ElmoEmbeddings.html"><span class="name">ElmoEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="ElmoEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings">ElmoEmbeddings</a>] with <a href="../HasSimpleAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasSimpleAnnotate">HasSimpleAnnotate</a>[<a href="ElmoEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings">ElmoEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a> with <a href="../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">Word embeddings from ELMo (Embeddings from Language Models), a language model trained on the 1
Billion Word Benchmark.</p><div class="fullcomment"><div class="comment cmt"><p>Word embeddings from ELMo (Embeddings from Language Models), a language model trained on the 1
Billion Word Benchmark.</p><p>Note that this is a very computationally expensive module compared to word embedding modules
that only perform embedding lookups. The use of an accelerator is recommended.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = ElmoEmbeddings.pretrained()
  .setInputCols(<span class="lit">"sentence"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"elmo_embeddings"</span>)</pre><p>The default model is <code>&quot;elmo&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>The pooling layer can be set with <code>setPoolingLayer</code> to the following values:</p><ul><li><code>&quot;word_emb&quot;</code>: the character-based word representations with shape <code>[batch_size,
    max_length, 512]</code>.</li><li><code>&quot;lstm_outputs1&quot;</code>: the first LSTM hidden state with shape <code>[batch_size, max_length,
    1024]</code>.</li><li><code>&quot;lstm_outputs2&quot;</code>: the second LSTM hidden state with shape <code>[batch_size, max_length,
    1024]</code>.</li><li><code>&quot;elmo&quot;</code>: the weighted sum of the 3 layers, where the weights are trainable. This tensor
    has shape <code>[batch_size, max_length, 1024]</code>.</li></ul><p>For extended examples of usage, see the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/dl-ner/ner_elmo.ipynb" target="_blank">Examples</a>
and the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/ElmoEmbeddingsTestSpec.scala" target="_blank">ElmoEmbeddingsTestSpec</a>.</p><p><b>References:</b></p><p><a href="https://tfhub.dev/google/elmo/3" target="_blank">https://tfhub.dev/google/elmo/3</a></p><p><a href="https://arxiv.org/abs/1802.05365" target="_blank">Deep contextualized word representations</a></p><p><b> Paper abstract:</b></p><p><i>We introduce a new type of deep contextualized word representation that models both (1)
complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary
across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions
of the internal states of a deep bidirectional language model (biLM), which is pre-trained on
a large text corpus. We show that these representations can be easily added to existing models
and significantly improve the state of the art across six challenging NLP problems, including
question answering, textual entailment and sentiment analysis. We also present an analysis
showing that exposing the deep internals of the pre-trained network is crucial, allowing
downstream models to mix different types of semi-supervision signals.</i></p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = ElmoEmbeddings.pretrained()
  .setPoolingLayer(<span class="lit">"word_emb"</span>)
  .setInputCols(<span class="lit">"token"</span>, <span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  documentAssembler,
  tokenizer,
  embeddings,
  embeddingsFinisher
))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[<span class="num">6.662458181381226E-4</span>,-<span class="num">0.2541114091873169</span>,-<span class="num">0.6275503039360046</span>,<span class="num">0.5787073969841</span>...|
|[<span class="num">0.19154725968837738</span>,<span class="num">0.22998669743537903</span>,-<span class="num">0.2894386649131775</span>,<span class="num">0.21524395048618</span>...|
|[<span class="num">0.10400570929050446</span>,<span class="num">0.12288510054349899</span>,-<span class="num">0.07056470215320587</span>,-<span class="num">0.246389418840</span>...|
|[<span class="num">0.49932169914245605</span>,-<span class="num">0.12706467509269714</span>,<span class="num">0.30969417095184326</span>,<span class="num">0.2643227577209</span>...|
|[-<span class="num">0.8871506452560425</span>,-<span class="num">0.20039963722229004</span>,-<span class="num">1.0601330995559692</span>,<span class="num">0.0348707810044</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of other
  transformer based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.EmbeddingsCoverage" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="EmbeddingsCoverageextendsAnyRef"></a><a id="EmbeddingsCoverage:EmbeddingsCoverage"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/EmbeddingsCoverage.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="EmbeddingsCoverage.html"><span class="name">EmbeddingsCoverage</span></a><span class="result"> extends <a href="../../../../scala/index.html#AnyRef=Object" class="extmbr" name="scala.AnyRef">AnyRef</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="HasEmbeddingsPropertiesextendsParamswithHasProtectedParams"></a><a id="HasEmbeddingsProperties:HasEmbeddingsProperties"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/HasEmbeddingsProperties.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="HasEmbeddingsProperties.html"><span class="name">HasEmbeddingsProperties</span></a><span class="result"> extends <span class="extype" name="org.apache.spark.ml.param.Params">Params</span> with <a href="../HasProtectedParams.html" class="extype" name="com.johnsnowlabs.nlp.HasProtectedParams">HasProtectedParams</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.InstructorEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="InstructorEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.InstructorEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.InstructorEmbeddings]withWriteTensorflowModelwithHasEmbeddingsPropertieswithHasStorageRefwithWriteSentencePieceModelwithHasCaseSensitivePropertieswithHasEngine"></a><a id="InstructorEmbeddings:InstructorEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/InstructorEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Sentence embeddings using INSTRUCTOR." href="InstructorEmbeddings.html"><span class="name">InstructorEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="InstructorEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.InstructorEmbeddings">InstructorEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="InstructorEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.InstructorEmbeddings">InstructorEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../../ml/tensorflow/sentencepiece/WriteSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.WriteSentencePieceModel">WriteSentencePieceModel</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a> with <a href="../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">Sentence embeddings using INSTRUCTOR.</p><div class="fullcomment"><div class="comment cmt"><p>Sentence embeddings using INSTRUCTOR.</p><p>Instructor👨‍🏫, an instruction-finetuned text embedding model that can generate text
embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation,
etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction,
without any finetuning. Instructor👨‍ achieves sota on 70 diverse embedding tasks!</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = InstructorEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"instructor_embeddings"</span>)</pre><p>The default model is <code>&quot;instructor_base&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?q=Instructor" target="_blank">Models Hub</a>.</p><p>For extended examples of usage, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/InstructorEmbeddingsTestSpec.scala" target="_blank">InstructorEmbeddingsTestSpec</a>.</p><p><b>Sources</b> :</p><p><a href="https://arxiv.org/abs/2212.09741" target="_blank">One Embedder, Any Task: Instruction-Finetuned Text Embeddings</a></p><p><a href="https://github.com/HKUNLP/instructor-embedding/" target="_blank">INSTRUCTOR Github Repository</a></p><p><b> Paper abstract </b></p><p><i>We introduce INSTRUCTOR, a new method for computing text embeddings given task instructions:
every text input is embedded together with instructions explaining the use case (e.g., task
and domain descriptions). Unlike encoders from prior work that are more specialized,
INSTRUCTOR is a single embedder that can generate text embeddings tailored to different
downstream tasks and domains, without any further training. We first annotate instructions for
330 diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive loss. We
evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are unseen during training),
ranging from classification and information retrieval to semantic textual similarity and text
generation evaluation. INSTRUCTOR, while having an order of magnitude fewer parameters than
the previous best model, achieves state-of-the-art performance, with an average improvement of
3.4% compared to the previous best results on the 70 diverse datasets. Our analysis suggests
that INSTRUCTOR is robust to changes in instructions, and that instruction finetuning
mitigates the challenge of training a single model on diverse datasets. Our model, code, and
data are available at this https URL. <a href="https://instructor-embedding.github.io/" target="_blank">https://instructor-embedding.github.io/</a> </i></p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.InstructorEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> embeddings = InstructorEmbeddings.pretrained(<span class="lit">"instructor_base"</span>, <span class="lit">"en"</span>)
  .setInputCols(<span class="lit">"document"</span>)
  .setInstruction(<span class="lit">"Represent the Medicine sentence for clustering: "</span>)
  .setOutputCol(<span class="lit">"instructor_embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"instructor_embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  documentAssembler,
  embeddings,
  embeddingsFinisher
))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"Dynamical Scalar Degree of Freedom in Horava-Lifshitz Gravity"</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">1</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[-<span class="num">2.3497989177703857</span>,<span class="num">0.480538547039032</span>,-<span class="num">0.3238905668258667</span>,-<span class="num">1.612930893898010</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.LongformerEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="LongformerEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.LongformerEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.LongformerEmbeddings]withWriteTensorflowModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitivePropertieswithHasEngine"></a><a id="LongformerEmbeddings:LongformerEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/LongformerEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Longformer is a transformer model for long documents." href="LongformerEmbeddings.html"><span class="name">LongformerEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="LongformerEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.LongformerEmbeddings">LongformerEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="LongformerEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.LongformerEmbeddings">LongformerEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a> with <a href="../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">Longformer is a transformer model for long documents.</p><div class="fullcomment"><div class="comment cmt"><p>Longformer is a transformer model for long documents. The Longformer model was presented in
<a href="https://arxiv.org/pdf/2004.05150.pdf" target="_blank">Longformer: The Long-Document Transformer</a> by Iz
Beltagy, Matthew E. Peters, Arman Cohan. longformer-base-4096 is a BERT-like model started
from the RoBERTa checkpoint and pretrained for MLM on long documents. It supports sequences of
length up to 4,096.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = LongformerEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)</pre><p>The default model is <code>&quot;longformer_base_4096&quot;</code>, if no name is provided. For available
pretrained models please see the <a href="https://sparknlp.org/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>For some examples of usage, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/LongformerEmbeddingsTestSpec.scala" target="_blank">LongformerEmbeddingsTestSpec</a>.
To see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a>.</p><p><b>Paper Abstract:</b></p><p><i>Transformer-based models are unable to process long sequences due to their self-attention
operation, which scales quadratically with the sequence length. To address this limitation, we
introduce the Longformer with an attention mechanism that scales linearly with sequence
length, making it easy to process documents of thousands of tokens or longer. Longformer's
attention mechanism is a drop-in replacement for the standard self-attention and combines a
local windowed attention with a task motivated global attention. Following prior work on
long-sequence transformers, we evaluate Longformer on character-level language modeling and
achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also
pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained
Longformer consistently outperforms RoBERTa on long document tasks and sets new
state-of-the-art results on WikiHop and TriviaQA. We finally introduce the
Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative
sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization
dataset.</i></p><p>The original code can be found <code><code><code>here</code></code></code> <a href="https://github.com/allenai/longformer" target="_blank">https://github.com/allenai/longformer</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base._
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator._
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = LongformerEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)
  .setCaseSensitive(<span class="kw">true</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    tokenizer,
    embeddings,
    embeddingsFinisher
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[<span class="num">0.18792399764060974</span>,-<span class="num">0.14591649174690247</span>,<span class="num">0.20547787845134735</span>,<span class="num">0.1468472778797</span>...|
|[<span class="num">0.22845706343650818</span>,<span class="num">0.18073144555091858</span>,<span class="num">0.09725798666477203</span>,-<span class="num">0.0417917296290</span>...|
|[<span class="num">0.07037967443466187</span>,-<span class="num">0.14801117777824402</span>,-<span class="num">0.03603338822722435</span>,-<span class="num">0.17893412709</span>...|
|[-<span class="num">0.08734266459941864</span>,<span class="num">0.2486150562763214</span>,-<span class="num">0.009067727252840996</span>,-<span class="num">0.24408400058</span>...|
|[<span class="num">0.22409197688102722</span>,-<span class="num">0.4312366545200348</span>,<span class="num">0.1401449590921402</span>,<span class="num">0.356410235166549</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="../annotators/classifier/dl/LongformerForTokenClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.classifier.dl.LongformerForTokenClassification">LongformerForTokenClassification</a>
  for Longformer embeddings with a token classification layer on top</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.MPNetEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="MPNetEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.MPNetEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.MPNetEmbeddings]withWriteTensorflowModelwithWriteOnnxModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitivePropertieswithHasEngine"></a><a id="MPNetEmbeddings:MPNetEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/MPNetEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Sentence embeddings using MPNet." href="MPNetEmbeddings.html"><span class="name">MPNetEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="MPNetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.MPNetEmbeddings">MPNetEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="MPNetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.MPNetEmbeddings">MPNetEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../ml/onnx/WriteOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.WriteOnnxModel">WriteOnnxModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a> with <a href="../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">Sentence embeddings using MPNet.</p><div class="fullcomment"><div class="comment cmt"><p>Sentence embeddings using MPNet.</p><p>The MPNet model was proposed in MPNet: Masked and Permuted Pre-training for Language
Understanding by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu. MPNet adopts a novel
pre-training method, named masked and permuted language modeling, to inherit the advantages of
masked language modeling and permuted language modeling for natural language understanding.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = MPNetEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"mpnet_embeddings"</span>)</pre><p>The default model is <code>&quot;all_mpnet_base_v2&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?q=MPNet" target="_blank">Models Hub</a>.</p><p>For extended examples of usage, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/MPNetEmbeddingsTestSpec.scala" target="_blank">MPNetEmbeddingsTestSpec</a>.</p><p><b>Sources</b> :</p><p><a href="https://arxiv.org/abs/2004.09297" target="_blank">MPNet: Masked and Permuted Pre-training for Language Understanding</a></p><p><a href="https://github.com/microsoft/MPNet" target="_blank">MPNet Github Repository</a></p><p><b> Paper abstract </b></p><p><i>BERT adopts masked language modeling (MLM) for pre-training and is one of the most
successful pre-training models. Since BERT neglects dependency among predicted tokens, XLNet
introduces permuted language modeling (PLM) for pre-training to address this problem. However,
XLNet does not leverage the full position information of a sentence and thus suffers from
position discrepancy between pre-training and fine-tuning. In this paper, we propose MPNet, a
novel pre-training method that inherits the advantages of BERT and XLNet and avoids their
limitations. MPNet leverages the dependency among predicted tokens through permuted language
modeling (vs. MLM in BERT), and takes auxiliary position information as input to make the
model see a full sentence and thus reducing the position discrepancy (vs. PLM in XLNet). We
pre-train MPNet on a large-scale dataset (over 160GB text corpora) and fine-tune on a variety
of down-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet outperforms
MLM and PLM by a large margin, and achieves better results on these tasks compared with
previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same
model setting.</i></p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.MPNetEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> embeddings = MPNetEmbeddings.pretrained(<span class="lit">"all_mpnet_base_v2"</span>, <span class="lit">"en"</span>)
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"mpnet_embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"mpnet_embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  documentAssembler,
  embeddings,
  embeddingsFinisher
))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is an example sentence"</span>, <span class="lit">"Each sentence is converted"</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">1</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[[<span class="num">0.022502584</span>, -<span class="num">0.078291744</span>, -<span class="num">0.023030775</span>, -<span class="num">0.0051000593</span>, -<span class="num">0.080340415</span>, <span class="num">0.039</span>...|
|[[<span class="num">0.041702367</span>, <span class="num">0.0010974605</span>, -<span class="num">0.015534201</span>, <span class="num">0.07092203</span>, -<span class="num">0.0017729357</span>, <span class="num">0.04661</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadAlbertDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadAlbertDLModelextendsReadTensorflowModelwithReadSentencePieceModelwithReadOnnxModel"></a><a id="ReadAlbertDLModel:ReadAlbertDLModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadAlbertDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadAlbertDLModel.html"><span class="name">ReadAlbertDLModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/ReadSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.ReadSentencePieceModel">ReadSentencePieceModel</a> with <a href="../../ml/onnx/ReadOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.ReadOnnxModel">ReadOnnxModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadBertDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadBertDLModelextendsReadTensorflowModelwithReadOnnxModel"></a><a id="ReadBertDLModel:ReadBertDLModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadBertDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadBertDLModel.html"><span class="name">ReadBertDLModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../ml/onnx/ReadOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.ReadOnnxModel">ReadOnnxModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadBertSentenceDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadBertSentenceDLModelextendsReadTensorflowModelwithReadOnnxModel"></a><a id="ReadBertSentenceDLModel:ReadBertSentenceDLModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadBertSentenceDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadBertSentenceDLModel.html"><span class="name">ReadBertSentenceDLModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../ml/onnx/ReadOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.ReadOnnxModel">ReadOnnxModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadCamemBertDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadCamemBertDLModelextendsReadTensorflowModelwithReadSentencePieceModelwithReadOnnxModel"></a><a id="ReadCamemBertDLModel:ReadCamemBertDLModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadCamemBertDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadCamemBertDLModel.html"><span class="name">ReadCamemBertDLModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/ReadSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.ReadSentencePieceModel">ReadSentencePieceModel</a> with <a href="../../ml/onnx/ReadOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.ReadOnnxModel">ReadOnnxModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadDeBertaDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadDeBertaDLModelextendsReadTensorflowModelwithReadSentencePieceModelwithReadOnnxModel"></a><a id="ReadDeBertaDLModel:ReadDeBertaDLModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadDeBertaDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadDeBertaDLModel.html"><span class="name">ReadDeBertaDLModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/ReadSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.ReadSentencePieceModel">ReadSentencePieceModel</a> with <a href="../../ml/onnx/ReadOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.ReadOnnxModel">ReadOnnxModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadDistilBertDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadDistilBertDLModelextendsReadTensorflowModelwithReadOnnxModel"></a><a id="ReadDistilBertDLModel:ReadDistilBertDLModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadDistilBertDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadDistilBertDLModel.html"><span class="name">ReadDistilBertDLModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../ml/onnx/ReadOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.ReadOnnxModel">ReadOnnxModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadE5DLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadE5DLModelextendsReadTensorflowModelwithReadOnnxModel"></a><a id="ReadE5DLModel:ReadE5DLModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadE5DLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadE5DLModel.html"><span class="name">ReadE5DLModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../ml/onnx/ReadOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.ReadOnnxModel">ReadOnnxModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadElmoDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadElmoDLModelextendsReadTensorflowModel"></a><a id="ReadElmoDLModel:ReadElmoDLModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadElmoDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadElmoDLModel.html"><span class="name">ReadElmoDLModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadInstructorDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadInstructorDLModelextendsReadTensorflowModelwithReadSentencePieceModel"></a><a id="ReadInstructorDLModel:ReadInstructorDLModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadInstructorDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadInstructorDLModel.html"><span class="name">ReadInstructorDLModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/ReadSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.ReadSentencePieceModel">ReadSentencePieceModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadLongformerDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadLongformerDLModelextendsReadTensorflowModel"></a><a id="ReadLongformerDLModel:ReadLongformerDLModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadLongformerDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadLongformerDLModel.html"><span class="name">ReadLongformerDLModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadMPNetDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadMPNetDLModelextendsReadTensorflowModelwithReadOnnxModel"></a><a id="ReadMPNetDLModel:ReadMPNetDLModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadMPNetDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadMPNetDLModel.html"><span class="name">ReadMPNetDLModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../ml/onnx/ReadOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.ReadOnnxModel">ReadOnnxModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadRobertaDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadRobertaDLModelextendsReadTensorflowModelwithReadOnnxModel"></a><a id="ReadRobertaDLModel:ReadRobertaDLModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadRobertaDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadRobertaDLModel.html"><span class="name">ReadRobertaDLModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../ml/onnx/ReadOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.ReadOnnxModel">ReadOnnxModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadRobertaSentenceDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadRobertaSentenceDLModelextendsReadTensorflowModel"></a><a id="ReadRobertaSentenceDLModel:ReadRobertaSentenceDLModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadRobertaSentenceDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadRobertaSentenceDLModel.html"><span class="name">ReadRobertaSentenceDLModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadUSEDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadUSEDLModelextendsReadTensorflowModel"></a><a id="ReadUSEDLModel:ReadUSEDLModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadUSEDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadUSEDLModel.html"><span class="name">ReadUSEDLModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadXlmRobertaDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadXlmRobertaDLModelextendsReadTensorflowModelwithReadSentencePieceModelwithReadOnnxModel"></a><a id="ReadXlmRobertaDLModel:ReadXlmRobertaDLModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadXlmRobertaDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadXlmRobertaDLModel.html"><span class="name">ReadXlmRobertaDLModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/ReadSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.ReadSentencePieceModel">ReadSentencePieceModel</a> with <a href="../../ml/onnx/ReadOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.ReadOnnxModel">ReadOnnxModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadXlmRobertaSentenceDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadXlmRobertaSentenceDLModelextendsReadTensorflowModelwithReadSentencePieceModel"></a><a id="ReadXlmRobertaSentenceDLModel:ReadXlmRobertaSentenceDLModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadXlmRobertaSentenceDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadXlmRobertaSentenceDLModel.html"><span class="name">ReadXlmRobertaSentenceDLModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/ReadSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.ReadSentencePieceModel">ReadSentencePieceModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadXlnetDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadXlnetDLModelextendsReadTensorflowModelwithReadSentencePieceModel"></a><a id="ReadXlnetDLModel:ReadXlnetDLModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadXlnetDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadXlnetDLModel.html"><span class="name">ReadXlnetDLModel</span></a><span class="result"> extends <a href="../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/ReadSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.ReadSentencePieceModel">ReadSentencePieceModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedAlbertModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedAlbertModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings]"></a><a id="ReadablePretrainedAlbertModel:ReadablePretrainedAlbertModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedAlbertModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedAlbertModel.html"><span class="name">ReadablePretrainedAlbertModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="AlbertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings">AlbertEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="AlbertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings">AlbertEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedBertModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedBertModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.BertEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.BertEmbeddings]"></a><a id="ReadablePretrainedBertModel:ReadablePretrainedBertModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedBertModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedBertModel.html"><span class="name">ReadablePretrainedBertModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedBertSentenceModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedBertSentenceModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings]"></a><a id="ReadablePretrainedBertSentenceModel:ReadablePretrainedBertSentenceModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedBertSentenceModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedBertSentenceModel.html"><span class="name">ReadablePretrainedBertSentenceModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="BertSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings">BertSentenceEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="BertSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings">BertSentenceEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedCamemBertModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedCamemBertModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.CamemBertEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.CamemBertEmbeddings]"></a><a id="ReadablePretrainedCamemBertModel:ReadablePretrainedCamemBertModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedCamemBertModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedCamemBertModel.html"><span class="name">ReadablePretrainedCamemBertModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="CamemBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.CamemBertEmbeddings">CamemBertEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="CamemBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.CamemBertEmbeddings">CamemBertEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedDeBertaModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedDeBertaModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.DeBertaEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.DeBertaEmbeddings]"></a><a id="ReadablePretrainedDeBertaModel:ReadablePretrainedDeBertaModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedDeBertaModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedDeBertaModel.html"><span class="name">ReadablePretrainedDeBertaModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="DeBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DeBertaEmbeddings">DeBertaEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="DeBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DeBertaEmbeddings">DeBertaEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedDistilBertModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedDistilBertModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings]"></a><a id="ReadablePretrainedDistilBertModel:ReadablePretrainedDistilBertModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedDistilBertModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedDistilBertModel.html"><span class="name">ReadablePretrainedDistilBertModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="DistilBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings">DistilBertEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="DistilBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings">DistilBertEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedDoc2Vec" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedDoc2VecextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.Doc2VecModel]withHasPretrained[com.johnsnowlabs.nlp.embeddings.Doc2VecModel]"></a><a id="ReadablePretrainedDoc2Vec:ReadablePretrainedDoc2Vec"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedDoc2Vec.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedDoc2Vec.html"><span class="name">ReadablePretrainedDoc2Vec</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="Doc2VecModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Doc2VecModel">Doc2VecModel</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="Doc2VecModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Doc2VecModel">Doc2VecModel</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedE5Model" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedE5ModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.E5Embeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.E5Embeddings]"></a><a id="ReadablePretrainedE5Model:ReadablePretrainedE5Model"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedE5Model.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedE5Model.html"><span class="name">ReadablePretrainedE5Model</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="E5Embeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.E5Embeddings">E5Embeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="E5Embeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.E5Embeddings">E5Embeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedElmoModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedElmoModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings]"></a><a id="ReadablePretrainedElmoModel:ReadablePretrainedElmoModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedElmoModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedElmoModel.html"><span class="name">ReadablePretrainedElmoModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="ElmoEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings">ElmoEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="ElmoEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings">ElmoEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedInstructorModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedInstructorModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.InstructorEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.InstructorEmbeddings]"></a><a id="ReadablePretrainedInstructorModel:ReadablePretrainedInstructorModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedInstructorModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedInstructorModel.html"><span class="name">ReadablePretrainedInstructorModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="InstructorEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.InstructorEmbeddings">InstructorEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="InstructorEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.InstructorEmbeddings">InstructorEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedLongformerModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedLongformerModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.LongformerEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.LongformerEmbeddings]"></a><a id="ReadablePretrainedLongformerModel:ReadablePretrainedLongformerModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedLongformerModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedLongformerModel.html"><span class="name">ReadablePretrainedLongformerModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="LongformerEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.LongformerEmbeddings">LongformerEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="LongformerEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.LongformerEmbeddings">LongformerEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedMPNetModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedMPNetModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.MPNetEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.MPNetEmbeddings]"></a><a id="ReadablePretrainedMPNetModel:ReadablePretrainedMPNetModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedMPNetModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedMPNetModel.html"><span class="name">ReadablePretrainedMPNetModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="MPNetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.MPNetEmbeddings">MPNetEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="MPNetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.MPNetEmbeddings">MPNetEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedRobertaModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedRobertaModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings]"></a><a id="ReadablePretrainedRobertaModel:ReadablePretrainedRobertaModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedRobertaModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedRobertaModel.html"><span class="name">ReadablePretrainedRobertaModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="RoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings">RoBertaEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="RoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings">RoBertaEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedRobertaSentenceModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedRobertaSentenceModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.RoBertaSentenceEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.RoBertaSentenceEmbeddings]"></a><a id="ReadablePretrainedRobertaSentenceModel:ReadablePretrainedRobertaSentenceModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedRobertaSentenceModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedRobertaSentenceModel.html"><span class="name">ReadablePretrainedRobertaSentenceModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="RoBertaSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaSentenceEmbeddings">RoBertaSentenceEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="RoBertaSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaSentenceEmbeddings">RoBertaSentenceEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedUSEModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedUSEModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder]withHasPretrained[com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder]"></a><a id="ReadablePretrainedUSEModel:ReadablePretrainedUSEModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedUSEModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedUSEModel.html"><span class="name">ReadablePretrainedUSEModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="UniversalSentenceEncoder.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder">UniversalSentenceEncoder</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="UniversalSentenceEncoder.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder">UniversalSentenceEncoder</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedWord2Vec" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedWord2VecextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.Word2VecModel]withHasPretrained[com.johnsnowlabs.nlp.embeddings.Word2VecModel]"></a><a id="ReadablePretrainedWord2Vec:ReadablePretrainedWord2Vec"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedWord2Vec.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedWord2Vec.html"><span class="name">ReadablePretrainedWord2Vec</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="Word2VecModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Word2VecModel">Word2VecModel</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="Word2VecModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Word2VecModel">Word2VecModel</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedWordEmbeddings" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedWordEmbeddingsextendsStorageReadable[com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel]withHasPretrained[com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel]"></a><a id="ReadablePretrainedWordEmbeddings:ReadablePretrainedWordEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedWordEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedWordEmbeddings.html"><span class="name">ReadablePretrainedWordEmbeddings</span></a><span class="result"> extends <a href="../../storage/StorageReadable.html" class="extype" name="com.johnsnowlabs.storage.StorageReadable">StorageReadable</a>[<a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedXlmRobertaModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedXlmRobertaModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings]"></a><a id="ReadablePretrainedXlmRobertaModel:ReadablePretrainedXlmRobertaModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedXlmRobertaModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedXlmRobertaModel.html"><span class="name">ReadablePretrainedXlmRobertaModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="XlmRoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings">XlmRoBertaEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="XlmRoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings">XlmRoBertaEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedXlmRobertaSentenceModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedXlmRobertaSentenceModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.XlmRoBertaSentenceEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.XlmRoBertaSentenceEmbeddings]"></a><a id="ReadablePretrainedXlmRobertaSentenceModel:ReadablePretrainedXlmRobertaSentenceModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedXlmRobertaSentenceModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedXlmRobertaSentenceModel.html"><span class="name">ReadablePretrainedXlmRobertaSentenceModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="XlmRoBertaSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaSentenceEmbeddings">XlmRoBertaSentenceEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="XlmRoBertaSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaSentenceEmbeddings">XlmRoBertaSentenceEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedXlnetModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedXlnetModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings]withHasPretrained[com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings]"></a><a id="ReadablePretrainedXlnetModel:ReadablePretrainedXlnetModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadablePretrainedXlnetModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedXlnetModel.html"><span class="name">ReadablePretrainedXlnetModel</span></a><span class="result"> extends <a href="../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="XlnetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings">XlnetEmbeddings</a>] with <a href="../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="XlnetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings">XlnetEmbeddings</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.ReadsFromBytes" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadsFromBytesextendsAnyRef"></a><a id="ReadsFromBytes:ReadsFromBytes"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ReadsFromBytes.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadsFromBytes.html"><span class="name">ReadsFromBytes</span></a><span class="result"> extends <a href="../../../../scala/index.html#AnyRef=Object" class="extmbr" name="scala.AnyRef">AnyRef</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="RoBertaEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings]withWriteTensorflowModelwithWriteOnnxModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitivePropertieswithHasEngine"></a><a id="RoBertaEmbeddings:RoBertaEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/RoBertaEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov." href="RoBertaEmbeddings.html"><span class="name">RoBertaEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="RoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings">RoBertaEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="RoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings">RoBertaEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../ml/onnx/WriteOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.WriteOnnxModel">WriteOnnxModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a> with <a href="../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">The RoBERTa model was proposed in
<a href="https://arxiv.org/abs/1907.11692" target="_blank">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>
by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, Veselin Stoyanov.</p><div class="fullcomment"><div class="comment cmt"><p>The RoBERTa model was proposed in
<a href="https://arxiv.org/abs/1907.11692" target="_blank">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>
by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, Veselin Stoyanov. It is based on Google's BERT model released in
2018.</p><p>It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining
objective and training with much larger mini-batches and learning rates.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = RoBertaEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)</pre><p>The default model is <code>&quot;roberta_base&quot;</code>, if no name is provided. For available pretrained models
please see the <a href="https://sparknlp.org/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>For extended examples of usage, see the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/HuggingFace%20in%20Spark%20NLP%20-%20RoBERTa.ipynb" target="_blank">Examples</a>
and the
<a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/RoBertaEmbeddingsTestSpec.scala" target="_blank">RoBertaEmbeddingsTestSpec</a>.
To see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a>.</p><p><b>Paper Abstract:</b></p><p><i>Language model pretraining has led to significant performance gains but careful comparison
between different approaches is challenging. Training is computationally expensive, often done
on private datasets of different sizes, and, as we will show, hyperparameter choices have
significant impact on the final results. We present a replication study of BERT pretraining
(Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and
training data size. We find that BERT was significantly undertrained, and can match or exceed
the performance of every model published after it. Our best model achieves state-of-the-art
results on GLUE, RACE and SQuAD. These results highlight the importance of previously
overlooked design choices, and raise questions about the source of recently reported
improvements. We release our models and code.</i></p><p>Tips:</p><ul><li>RoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same
    as GPT-2) and uses a different pretraining scheme.</li><li>RoBERTa doesn't have :obj:<code>token_type_ids</code>, you don't need to indicate which token belongs
    to which segment. Just separate your segments with the separation token
    :obj:<code>tokenizer.sep_token</code> (or :obj:<code>&lt;/s&gt;</code>)</li></ul><p>The original code can be found <code><code><code>here</code></code></code>
<a href="https://github.com/pytorch/fairseq/tree/master/examples/roberta" target="_blank">https://github.com/pytorch/fairseq/tree/master/examples/roberta</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = RoBertaEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)
  .setCaseSensitive(<span class="kw">true</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    tokenizer,
    embeddings,
    embeddingsFinisher
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[<span class="num">0.18792399764060974</span>,-<span class="num">0.14591649174690247</span>,<span class="num">0.20547787845134735</span>,<span class="num">0.1468472778797</span>...|
|[<span class="num">0.22845706343650818</span>,<span class="num">0.18073144555091858</span>,<span class="num">0.09725798666477203</span>,-<span class="num">0.0417917296290</span>...|
|[<span class="num">0.07037967443466187</span>,-<span class="num">0.14801117777824402</span>,-<span class="num">0.03603338822722435</span>,-<span class="num">0.17893412709</span>...|
|[-<span class="num">0.08734266459941864</span>,<span class="num">0.2486150562763214</span>,-<span class="num">0.009067727252840996</span>,-<span class="num">0.24408400058</span>...|
|[<span class="num">0.22409197688102722</span>,-<span class="num">0.4312366545200348</span>,<span class="num">0.1401449590921402</span>,<span class="num">0.356410235166549</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="RoBertaSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaSentenceEmbeddings">RoBertaSentenceEmbeddings</a> for sentence-level embeddings</p></span><span class="cmt"><p>
  <a href="../annotators/classifier/dl/RoBertaForTokenClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.classifier.dl.RoBertaForTokenClassification">RoBertaForTokenClassification</a>
  For RoBerta embeddings with a token classification layer on top</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.RoBertaSentenceEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="RoBertaSentenceEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.RoBertaSentenceEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.RoBertaSentenceEmbeddings]withWriteTensorflowModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitivePropertieswithHasEngine"></a><a id="RoBertaSentenceEmbeddings:RoBertaSentenceEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/RoBertaSentenceEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Sentence-level embeddings using RoBERTa." href="RoBertaSentenceEmbeddings.html"><span class="name">RoBertaSentenceEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="RoBertaSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaSentenceEmbeddings">RoBertaSentenceEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="RoBertaSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaSentenceEmbeddings">RoBertaSentenceEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a> with <a href="../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">Sentence-level embeddings using RoBERTa.</p><div class="fullcomment"><div class="comment cmt"><p>Sentence-level embeddings using RoBERTa. The RoBERTa model was proposed in
<a href="https://arxiv.org/abs/1907.11692" target="_blank">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>
by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, Veselin Stoyanov. It is based on Google's BERT model released in
2018.</p><p>It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining
objective and training with much larger mini-batches and learning rates.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = RoBertaSentenceEmbeddings.pretrained()
  .setInputCols(<span class="lit">"sentence"</span>)
  .setOutputCol(<span class="lit">"sentence_embeddings"</span>)</pre><p>The default model is <code>&quot;sent_roberta_base&quot;</code>, if no name is provided. For available pretrained
models please see the <a href="https://sparknlp.org/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>To see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a> and to see more extended
examples, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/RoBertaEmbeddingsTestSpec.scala" target="_blank">RoBertaEmbeddingsTestSpec</a>.</p><p><b>Paper Abstract:</b></p><p><i>Language model pretraining has led to significant performance gains but careful comparison
between different approaches is challenging. Training is computationally expensive, often done
on private datasets of different sizes, and, as we will show, hyperparameter choices have
significant impact on the final results. We present a replication study of BERT pretraining
(Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and
training data size. We find that BERT was significantly undertrained, and can match or exceed
the performance of every model published after it. Our best model achieves state-of-the-art
results on GLUE, RACE and SQuAD. These results highlight the importance of previously
overlooked design choices, and raise questions about the source of recently reported
improvements. We release our models and code.</i></p><p>Tips:</p><ul><li>RoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same
    as GPT-2) and uses a different pretraining scheme.</li><li>RoBERTa doesn't have :obj:<code>token_type_ids</code>, you don't need to indicate which token belongs
    to which segment. Just separate your segments with the separation token
    :obj:<code>tokenizer.sep_token</code> (or :obj:<code>&lt;/s&gt;</code>)</li></ul><p>The original code can be found <code><code><code>here</code></code></code>
<a href="https://github.com/pytorch/fairseq/tree/master/examples/roberta" target="_blank">https://github.com/pytorch/fairseq/tree/master/examples/roberta</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base._
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator._
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> sentenceEmbeddings = RoBertaSentenceEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"sentence_embeddings"</span>)
  .setCaseSensitive(<span class="kw">true</span>)

<span class="cmt">// you can either use the output to train ClassifierDL, SentimentDL, or MultiClassifierDL</span>
<span class="cmt">// or you can use EmbeddingsFinisher to prepare the results for Spark ML functions</span>

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"sentence_embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    tokenizer,
    sentenceEmbeddings,
    embeddingsFinisher
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[<span class="num">0.18792399764060974</span>,-<span class="num">0.14591649174690247</span>,<span class="num">0.20547787845134735</span>,<span class="num">0.1468472778797</span>...|
|[<span class="num">0.22845706343650818</span>,<span class="num">0.18073144555091858</span>,<span class="num">0.09725798666477203</span>,-<span class="num">0.0417917296290</span>...|
|[<span class="num">0.07037967443466187</span>,-<span class="num">0.14801117777824402</span>,-<span class="num">0.03603338822722435</span>,-<span class="num">0.17893412709</span>...|
|[-<span class="num">0.08734266459941864</span>,<span class="num">0.2486150562763214</span>,-<span class="num">0.009067727252840996</span>,-<span class="num">0.24408400058</span>...|
|[<span class="num">0.22409197688102722</span>,-<span class="num">0.4312366545200348</span>,<span class="num">0.1401449590921402</span>,<span class="num">0.356410235166549</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="RoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings">RoBertaEmbeddings</a> for token-level embeddings</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="SentenceEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings]withHasSimpleAnnotate[com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings]withHasEmbeddingsPropertieswithHasStorageRef"></a><a id="SentenceEmbeddings:SentenceEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/SentenceEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Converts the results from WordEmbeddings, BertEmbeddings, or ElmoEmbeddings into sentence or document embeddings by either summing up or averaging all the word embeddings in a sentence or a document (depending on the inputCols)." href="SentenceEmbeddings.html"><span class="name">SentenceEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="SentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings">SentenceEmbeddings</a>] with <a href="../HasSimpleAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasSimpleAnnotate">HasSimpleAnnotate</a>[<a href="SentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings">SentenceEmbeddings</a>] with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a></span>
      </span>
      
      <p class="shortcomment cmt">Converts the results from <a href="WordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings">WordEmbeddings</a>, <a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a>, or <a href="ElmoEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings">ElmoEmbeddings</a> into
sentence or document embeddings by either summing up or averaging all the word embeddings in a
sentence or a document (depending on the inputCols).</p><div class="fullcomment"><div class="comment cmt"><p>Converts the results from <a href="WordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings">WordEmbeddings</a>, <a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a>, or <a href="ElmoEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings">ElmoEmbeddings</a> into
sentence or document embeddings by either summing up or averaging all the word embeddings in a
sentence or a document (depending on the inputCols).</p><p>This can be configured with <code>setPoolingStrategy</code>, which either be <code>&quot;AVERAGE&quot;</code> or <code>&quot;SUM&quot;</code>.</p><p>For more extended examples see the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/text-similarity/Spark_NLP_Spark_ML_Text_Similarity.ipynb" target="_blank">Examples</a>.
and the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/SentenceEmbeddingsTestSpec.scala" target="_blank">SentenceEmbeddingsTestSpec</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = WordEmbeddingsModel.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)

<span class="kw">val</span> embeddingsSentence = <span class="kw">new</span> SentenceEmbeddings()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>, <span class="lit">"embeddings"</span>))
  .setOutputCol(<span class="lit">"sentence_embeddings"</span>)
  .setPoolingStrategy(<span class="lit">"AVERAGE"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"sentence_embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    tokenizer,
    embeddings,
    embeddingsSentence,
    embeddingsFinisher
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[-<span class="num">0.22093398869037628</span>,<span class="num">0.25130119919776917</span>,<span class="num">0.41810303926467896</span>,-<span class="num">0.380883991718</span>...|
+--------------------------------------------------------------------------------+</pre></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="UniversalSentenceEncoderextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder]withHasEmbeddingsPropertieswithHasStorageRefwithWriteTensorflowModelwithHasEngine"></a><a id="UniversalSentenceEncoder:UniversalSentenceEncoder"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/UniversalSentenceEncoder.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks." href="UniversalSentenceEncoder.html"><span class="name">UniversalSentenceEncoder</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="UniversalSentenceEncoder.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder">UniversalSentenceEncoder</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="UniversalSentenceEncoder.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder">UniversalSentenceEncoder</a>] with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for
text classification, semantic similarity, clustering and other natural language tasks.</p><div class="fullcomment"><div class="comment cmt"><p>The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for
text classification, semantic similarity, clustering and other natural language tasks.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> useEmbeddings = UniversalSentenceEncoder.pretrained()
  .setInputCols(<span class="lit">"sentence"</span>)
  .setOutputCol(<span class="lit">"sentence_embeddings"</span>)</pre><p>The default model is <code>&quot;tfhub_use&quot;</code>, if no name is provided. For available pretrained models
please see the <a href="https://sparknlp.org/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>For extended examples of usage, see the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/classification/ClassifierDL_Train_multi_class_news_category_classifier.ipynb" target="_blank">Examples</a>
and the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/UniversalSentenceEncoderTestSpec.scala" target="_blank">UniversalSentenceEncoderTestSpec</a>.</p><p><b>References:</b></p><p><a href="https://arxiv.org/abs/1803.11175" target="_blank">Universal Sentence Encoder</a></p><p><a href="https://tfhub.dev/google/universal-sentence-encoder/2" target="_blank">https://tfhub.dev/google/universal-sentence-encoder/2</a></p><p><b>Paper abstract:</b></p><p><i>We present models for encoding sentences into embedding vectors that specifically target
transfer learning to other NLP tasks. The models are efficient and result in accurate
performance on diverse transfer tasks. Two variants of the encoding models allow for
trade-offs between accuracy and compute resources. For both variants, we investigate and
report the relationship between model complexity, resource consumption, the availability of
transfer task training data, and task performance. Comparisons are made with baselines that
use word level transfer learning via pretrained word embeddings as well as baselines do not
use any transfer learning. We find that transfer learning using sentence embeddings tends to
outperform word level transfer. With transfer learning via sentence embeddings, we observe
surprisingly good performance with minimal amounts of supervised training data for a transfer
task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at
detecting model bias. Our pre-trained sentence encoding models are made freely available for
download and on TF Hub.</i></p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator.SentenceDetector
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> sentence = <span class="kw">new</span> SentenceDetector()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"sentence"</span>)

<span class="kw">val</span> embeddings = UniversalSentenceEncoder.pretrained()
  .setInputCols(<span class="lit">"sentence"</span>)
  .setOutputCol(<span class="lit">"sentence_embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"sentence_embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    sentence,
    embeddings,
    embeddingsFinisher
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[<span class="num">0.04616805538535118</span>,<span class="num">0.022307956591248512</span>,-<span class="num">0.044395286589860916</span>,-<span class="num">0.0016493503</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.Word2VecApproach" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="Word2VecApproachextendsAnnotatorApproach[com.johnsnowlabs.nlp.embeddings.Word2VecModel]withHasStorageRefwithHasEnableCachingPropertieswithHasProtectedParams"></a><a id="Word2VecApproach:Word2VecApproach"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/Word2VecApproach.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Trains a Word2Vec model that creates vector representations of words in a text corpus." href="Word2VecApproach.html"><span class="name">Word2VecApproach</span></a><span class="result"> extends <a href="../AnnotatorApproach.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorApproach">AnnotatorApproach</a>[<a href="Word2VecModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Word2VecModel">Word2VecModel</a>] with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasEnableCachingProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasEnableCachingProperties">HasEnableCachingProperties</a> with <a href="../HasProtectedParams.html" class="extype" name="com.johnsnowlabs.nlp.HasProtectedParams">HasProtectedParams</a></span>
      </span>
      
      <p class="shortcomment cmt">Trains a Word2Vec model that creates vector representations of words in a text corpus.</p><div class="fullcomment"><div class="comment cmt"><p>Trains a Word2Vec model that creates vector representations of words in a text corpus.</p><p>The algorithm first constructs a vocabulary from the corpus and then learns vector
representation of words in the vocabulary. The vector representation can be used as features
in natural language processing and machine learning algorithms.</p><p>We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a
hierarchical softmax method to train the model. The variable names in the implementation match
the original C implementation.</p><p>For instantiated/pretrained models, see <a href="Word2VecModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Word2VecModel">Word2VecModel</a>.</p><p><b>Sources</b> :</p><p>For the original C implementation, see https://code.google.com/p/word2vec/</p><p>For the research paper, see
<a href="https://arxiv.org/abs/1301.3781" target="_blank">Efficient Estimation of Word Representations in Vector Space</a>
and
<a href="https://arxiv.org/pdf/1310.4546v1.pdf" target="_blank">Distributed Representations of Words and Phrases and their Compositionality</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator.{Tokenizer, Word2VecApproach}
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = <span class="kw">new</span> Word2VecApproach()
  .setInputCols(<span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    tokenizer,
    embeddings
  ))

<span class="kw">val</span> path = <span class="lit">"src/test/resources/spell/sherlockholmes.txt"</span>
<span class="kw">val</span> dataset = spark.sparkContext.textFile(path)
  .toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> pipelineModel = pipeline.fit(dataset)</pre></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.Word2VecModel" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="Word2VecModelextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.Word2VecModel]withHasSimpleAnnotate[com.johnsnowlabs.nlp.embeddings.Word2VecModel]withHasStorageRefwithHasEmbeddingsPropertieswithParamsAndFeaturesWritable"></a><a id="Word2VecModel:Word2VecModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/Word2VecModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Word2Vec model that creates vector representations of words in a text corpus." href="Word2VecModel.html"><span class="name">Word2VecModel</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="Word2VecModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Word2VecModel">Word2VecModel</a>] with <a href="../HasSimpleAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasSimpleAnnotate">HasSimpleAnnotate</a>[<a href="Word2VecModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Word2VecModel">Word2VecModel</a>] with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../ParamsAndFeaturesWritable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesWritable">ParamsAndFeaturesWritable</a></span>
      </span>
      
      <p class="shortcomment cmt">Word2Vec model that creates vector representations of words in a text corpus.</p><div class="fullcomment"><div class="comment cmt"><p>Word2Vec model that creates vector representations of words in a text corpus.</p><p>The algorithm first constructs a vocabulary from the corpus and then learns vector
representation of words in the vocabulary. The vector representation can be used as features
in natural language processing and machine learning algorithms.</p><p>We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a
hierarchical softmax method to train the model. The variable names in the implementation match
the original C implementation.</p><p>This is the instantiated model of the <a href="Word2VecApproach.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Word2VecApproach">Word2VecApproach</a>. For training your own model,
please see the documentation of that class.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = Word2VecModel.pretrained()
  .setInputCols(<span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)</pre><p>The default model is <code>&quot;word2vec_gigaword_300&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the <a href="https://sparknlp.org/models" target="_blank">Models Hub</a>.</p><p><b>Sources</b> :</p><p>For the original C implementation, see https://code.google.com/p/word2vec/</p><p>For the research paper, see
<a href="https://arxiv.org/abs/1301.3781" target="_blank">Efficient Estimation of Word Representations in Vector Space</a>
and
<a href="https://arxiv.org/pdf/1310.4546v1.pdf" target="_blank">Distributed Representations of Words and Phrases and their Compositionality</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator.{Tokenizer, Word2VecModel}
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher

<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = Word2VecModel.pretrained()
  .setInputCols(<span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  documentAssembler,
  tokenizer,
  embeddings,
  embeddingsFinisher
))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">1</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[<span class="num">0.06222493574023247</span>,<span class="num">0.011579325422644615</span>,<span class="num">0.009919632226228714</span>,<span class="num">0.109361454844</span>...|
+--------------------------------------------------------------------------------+</pre></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="WordEmbeddingsextendsAnnotatorApproach[com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel]withHasStoragewithHasEmbeddingsProperties"></a><a id="WordEmbeddings:WordEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/WordEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Word Embeddings lookup annotator that maps tokens to vectors." href="WordEmbeddings.html"><span class="name">WordEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorApproach.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorApproach">AnnotatorApproach</a>[<a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel</a>] with <a href="../../storage/HasStorage.html" class="extype" name="com.johnsnowlabs.storage.HasStorage">HasStorage</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a></span>
      </span>
      
      <p class="shortcomment cmt">Word Embeddings lookup annotator that maps tokens to vectors.</p><div class="fullcomment"><div class="comment cmt"><p>Word Embeddings lookup annotator that maps tokens to vectors.</p><p>For instantiated/pretrained models, see <a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel</a>.</p><p>A custom token lookup dictionary for embeddings can be set with <code>setStoragePath</code>. Each line of
the provided file needs to have a token, followed by their vector representation, delimited by
a spaces.</p><pre>...
are <span class="num">0.39658191506190343</span> <span class="num">0.630968081620067</span> <span class="num">0.5393722253731201</span> <span class="num">0.8428180123359783</span>
were <span class="num">0.7535235923631415</span> <span class="num">0.9699218875629833</span> <span class="num">0.10397182122983872</span> <span class="num">0.11833962569383116</span>
stress <span class="num">0.0492683418305907</span> <span class="num">0.9415954572751959</span> <span class="num">0.47624463167525755</span> <span class="num">0.16790967216778263</span>
induced <span class="num">0.1535748762292387</span> <span class="num">0.33498936903209897</span> <span class="num">0.9235178224122094</span> <span class="num">0.1158772920395934</span>
...</pre><p>If a token is not found in the dictionary, then the result will be a zero vector of the same
dimension. Statistics about the rate of converted tokens, can be retrieved with
<a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel.withCoverageColumn</a> and
<a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel.overallCoverage</a>.</p><p>For extended examples of usage, see the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/scala/training/NerDL/win/customNerDlPipeline/CustomForNerDLPipeline.java" target="_blank">Examples</a>
and the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/WordEmbeddingsTestSpec.scala" target="_blank">WordEmbeddingsTestSpec</a>.</p><h4>Example</h4><p>In this example, the file <code>random_embeddings_dim4.txt</code> has the form of the content above.</p><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.WordEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.util.io.ReadAs
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = <span class="kw">new</span> WordEmbeddings()
  .setStoragePath(<span class="lit">"src/test/resources/random_embeddings_dim4.txt"</span>, ReadAs.TEXT)
  .setStorageRef(<span class="lit">"glove_4d"</span>)
  .setDimension(<span class="num">4</span>)
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    tokenizer,
    embeddings,
    embeddingsFinisher
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"The patient was diagnosed with diabetes."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="kw">false</span>)
+----------------------------------------------------------------------------------+
|result                                                                            |
+----------------------------------------------------------------------------------+
|[<span class="num">0.9439099431037903</span>,<span class="num">0.4707513153553009</span>,<span class="num">0.806300163269043</span>,<span class="num">0.16176554560661316</span>]     |
|[<span class="num">0.7966810464859009</span>,<span class="num">0.5551124811172485</span>,<span class="num">0.8861005902290344</span>,<span class="num">0.28284206986427307</span>]    |
|[<span class="num">0.025029370561242104</span>,<span class="num">0.35177749395370483</span>,<span class="num">0.052506182342767715</span>,<span class="num">0.1887107789516449</span>]|
|[<span class="num">0.08617766946554184</span>,<span class="num">0.8399239182472229</span>,<span class="num">0.5395117998123169</span>,<span class="num">0.7864698767662048</span>]    |
|[<span class="num">0.6599600911140442</span>,<span class="num">0.16109347343444824</span>,<span class="num">0.6041093468666077</span>,<span class="num">0.8913561105728149</span>]    |
|[<span class="num">0.5955275893211365</span>,<span class="num">0.01899011991918087</span>,<span class="num">0.4397728443145752</span>,<span class="num">0.8911281824111938</span>]    |
|[<span class="num">0.9840458631515503</span>,<span class="num">0.7599489092826843</span>,<span class="num">0.9417727589607239</span>,<span class="num">0.8624503016471863</span>]     |
+----------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="SentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings">SentenceEmbeddings</a> to combine embeddings into a sentence-level representation</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="WordEmbeddingsModelextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel]withHasSimpleAnnotate[com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel]withHasEmbeddingsPropertieswithHasStorageModelwithParamsAndFeaturesWritablewithReadsFromBytes"></a><a id="WordEmbeddingsModel:WordEmbeddingsModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/WordEmbeddingsModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Word Embeddings lookup annotator that maps tokens to vectors" href="WordEmbeddingsModel.html"><span class="name">WordEmbeddingsModel</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel</a>] with <a href="../HasSimpleAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasSimpleAnnotate">HasSimpleAnnotate</a>[<a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel</a>] with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageModel.html" class="extype" name="com.johnsnowlabs.storage.HasStorageModel">HasStorageModel</a> with <a href="../ParamsAndFeaturesWritable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesWritable">ParamsAndFeaturesWritable</a> with <a href="ReadsFromBytes.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadsFromBytes">ReadsFromBytes</a></span>
      </span>
      
      <p class="shortcomment cmt">Word Embeddings lookup annotator that maps tokens to vectors</p><div class="fullcomment"><div class="comment cmt"><p>Word Embeddings lookup annotator that maps tokens to vectors</p><p>This is the instantiated model of <a href="WordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings">WordEmbeddings</a>.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = WordEmbeddingsModel.pretrained()
    .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
    .setOutputCol(<span class="lit">"embeddings"</span>)</pre><p>The default model is <code>&quot;glove_100d&quot;</code>, if no name is provided. For available pretrained models
please see the <a href="https://sparknlp.org/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>There are also two convenient functions to retrieve the embeddings coverage with respect to
the transformed dataset:</p><ul><li><code>withCoverageColumn(dataset, embeddingsCol, outputCol)</code>: Adds a custom column with word
    coverage stats for the embedded field: (<code>coveredWords</code>, <code>totalWords</code>,
    <code>coveragePercentage</code>). This creates a new column with statistics for each row.</li></ul><pre><span class="kw">val</span> wordsCoverage = WordEmbeddingsModel.withCoverageColumn(resultDF, <span class="lit">"embeddings"</span>, <span class="lit">"cov_embeddings"</span>)
wordsCoverage.select(<span class="lit">"text"</span>,<span class="lit">"cov_embeddings"</span>).show(<span class="kw">false</span>)
+-------------------+--------------+
|text               |cov_embeddings|
+-------------------+--------------+
|This is a sentence.|[<span class="num">5</span>, <span class="num">5</span>, <span class="num">1.0</span>]   |
+-------------------+--------------+</pre><ul><li><code>overallCoverage(dataset, embeddingsCol)</code>: Calculates overall word coverage for the whole
    data in the embedded field. This returns a single coverage object considering all rows in
    the field.</li></ul><pre><span class="kw">val</span> wordsOverallCoverage = WordEmbeddingsModel.overallCoverage(wordsCoverage,<span class="lit">"embeddings"</span>).percentage
<span class="num">1.0</span></pre><p>For extended examples of usage, see the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/quick_start_offline.ipynb" target="_blank">Examples</a>
and the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/WordEmbeddingsTestSpec.scala" target="_blank">WordEmbeddingsTestSpec</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = WordEmbeddingsModel.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    tokenizer,
    embeddings,
    embeddingsFinisher
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[-<span class="num">0.570580005645752</span>,<span class="num">0.44183000922203064</span>,<span class="num">0.7010200023651123</span>,-<span class="num">0.417129993438720</span>...|
|[-<span class="num">0.542639970779419</span>,<span class="num">0.4147599935531616</span>,<span class="num">1.0321999788284302</span>,-<span class="num">0.4024400115013122</span>...|
|[-<span class="num">0.2708599865436554</span>,<span class="num">0.04400600120425224</span>,-<span class="num">0.020260000601410866</span>,-<span class="num">0.17395000159</span>...|
|[<span class="num">0.6191999912261963</span>,<span class="num">0.14650000631809235</span>,-<span class="num">0.08592499792575836</span>,-<span class="num">0.2629800140857</span>...|
|[-<span class="num">0.3397899866104126</span>,<span class="num">0.20940999686717987</span>,<span class="num">0.46347999572753906</span>,-<span class="num">0.6479200124740</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="SentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings">SentenceEmbeddings</a> to combine embeddings into a sentence-level representation</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsReader" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="WordEmbeddingsReaderextendsStorageReader[Array[Float]]withReadsFromBytes"></a><a id="WordEmbeddingsReader:WordEmbeddingsReader"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/WordEmbeddingsReader.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="" href="WordEmbeddingsReader.html"><span class="name">WordEmbeddingsReader</span></a><span class="result"> extends <a href="../../storage/StorageReader.html" class="extype" name="com.johnsnowlabs.storage.StorageReader">StorageReader</a>[<span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Float">Float</span>]] with <a href="ReadsFromBytes.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadsFromBytes">ReadsFromBytes</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsWriter" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="WordEmbeddingsWriterextendsStorageBatchWriter[Array[Float]]withReadsFromBytes"></a><a id="WordEmbeddingsWriter:WordEmbeddingsWriter"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/WordEmbeddingsWriter.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="" href="WordEmbeddingsWriter.html"><span class="name">WordEmbeddingsWriter</span></a><span class="result"> extends <a href="../../storage/StorageBatchWriter.html" class="extype" name="com.johnsnowlabs.storage.StorageBatchWriter">StorageBatchWriter</a>[<span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Float">Float</span>]] with <a href="ReadsFromBytes.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadsFromBytes">ReadsFromBytes</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="XlmRoBertaEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings]withWriteTensorflowModelwithWriteSentencePieceModelwithWriteOnnxModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitivePropertieswithHasEngine"></a><a id="XlmRoBertaEmbeddings:XlmRoBertaEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/XlmRoBertaEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="The XLM-RoBERTa model was proposed in Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov." href="XlmRoBertaEmbeddings.html"><span class="name">XlmRoBertaEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="XlmRoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings">XlmRoBertaEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="XlmRoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings">XlmRoBertaEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/WriteSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.WriteSentencePieceModel">WriteSentencePieceModel</a> with <a href="../../ml/onnx/WriteOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.WriteOnnxModel">WriteOnnxModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a> with <a href="../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">The XLM-RoBERTa model was proposed in
<a href="https://arxiv.org/abs/1911.02116" target="_blank">Unsupervised Cross-lingual Representation Learning at Scale</a>
by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,
Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.</p><div class="fullcomment"><div class="comment cmt"><p>The XLM-RoBERTa model was proposed in
<a href="https://arxiv.org/abs/1911.02116" target="_blank">Unsupervised Cross-lingual Representation Learning at Scale</a>
by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,
Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is based
on Facebook's RoBERTa model released in 2019. It is a large multi-lingual language model,
trained on 2.5TB of filtered CommonCrawl data.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = XlmRoBertaEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)</pre><p>The default model is <code>&quot;xlm_roberta_base&quot;</code>, default language is <code>&quot;xx&quot;</code> (meaning multi-lingual),
if no values are provided. For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>For extended examples of usage, see the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/HuggingFace%20in%20Spark%20NLP%20-%20XLM-RoBERTa.ipynb" target="_blank">Examples</a>
and the
<a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/XlmRoBertaEmbeddingsTestSpec.scala" target="_blank">XlmRoBertaEmbeddingsTestSpec</a>.
To see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a>.</p><p><b>Paper Abstract:</b></p><p><i>This paper shows that pretraining multilingual language models at scale leads to significant
performance gains for a wide range of cross-lingual transfer tasks. We train a
Transformer-based masked language model on one hundred languages, using more than two
terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms
multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average
accuracy on XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R
performs particularly well on low-resource languages, improving 11.8% in XNLI accuracy for
Swahili and 9.2% for Urdu over the previous XLM model. We also present a detailed empirical
evaluation of the key factors that are required to achieve these gains, including the
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high
and low resource languages at scale. Finally, we show, for the first time, the possibility of
multilingual modeling without sacrificing per-language performance; XLM-Ris very competitive
with strong monolingual models on the GLUE and XNLI benchmarks. We will make XLM-R code, data,
and models publicly available.</i></p><p><b>Tips:</b></p><ul><li>XLM-RoBERTa is a multilingual model trained on 100 different languages. Unlike some XLM
    multilingual models, it does not require <b>lang</b> parameter to understand which language
    is used, and should be able to determine the correct language from the input ids.</li><li>This implementation is the same as RoBERTa. Refer to the <a href="RoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings">RoBertaEmbeddings</a> for usage
    examples as well as the information relative to the inputs and outputs.</li></ul><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = XlmRoBertaEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)
  .setCaseSensitive(<span class="kw">true</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    tokenizer,
    embeddings,
    embeddingsFinisher
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[-<span class="num">0.05969233065843582</span>,-<span class="num">0.030789051204919815</span>,<span class="num">0.04443822056055069</span>,<span class="num">0.09564960747</span>...|
|[-<span class="num">0.038839809596538544</span>,<span class="num">0.011712731793522835</span>,<span class="num">0.019954433664679527</span>,<span class="num">0.0667808502</span>...|
|[-<span class="num">0.03952755779027939</span>,-<span class="num">0.03455188870429993</span>,<span class="num">0.019103847444057465</span>,<span class="num">0.04311436787</span>...|
|[-<span class="num">0.09579929709434509</span>,<span class="num">0.02494969218969345</span>,-<span class="num">0.014753809198737144</span>,<span class="num">0.10259044915</span>...|
|[<span class="num">0.004710011184215546</span>,-<span class="num">0.022148698568344116</span>,<span class="num">0.011723337695002556</span>,-<span class="num">0.013356896</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="XlmRoBertaSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaSentenceEmbeddings">XlmRoBertaSentenceEmbeddings</a> for sentence-level embeddings</p></span><span class="cmt"><p>
  <a href="../annotators/classifier/dl/XlmRoBertaForTokenClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.classifier.dl.XlmRoBertaForTokenClassification">XlmRoBertaForTokenClassification</a>
  For XlmRoBerta embeddings with a token classification layer on top</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaSentenceEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="XlmRoBertaSentenceEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.XlmRoBertaSentenceEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.XlmRoBertaSentenceEmbeddings]withWriteTensorflowModelwithWriteSentencePieceModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitivePropertieswithHasEngine"></a><a id="XlmRoBertaSentenceEmbeddings:XlmRoBertaSentenceEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/XlmRoBertaSentenceEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Sentence-level embeddings using XLM-RoBERTa." href="XlmRoBertaSentenceEmbeddings.html"><span class="name">XlmRoBertaSentenceEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="XlmRoBertaSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaSentenceEmbeddings">XlmRoBertaSentenceEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="XlmRoBertaSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaSentenceEmbeddings">XlmRoBertaSentenceEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/WriteSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.WriteSentencePieceModel">WriteSentencePieceModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a> with <a href="../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">Sentence-level embeddings using XLM-RoBERTa.</p><div class="fullcomment"><div class="comment cmt"><p>Sentence-level embeddings using XLM-RoBERTa. The XLM-RoBERTa model was proposed in
<a href="https://arxiv.org/abs/1911.02116" target="_blank">Unsupervised Cross-lingual Representation Learning at Scale</a>
by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,
Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is based
on Facebook's RoBERTa model released in 2019. It is a large multi-lingual language model,
trained on 2.5TB of filtered CommonCrawl data.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = XlmRoBertaSentenceEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"sentence_embeddings"</span>)</pre><p>The default model is <code>&quot;sent_xlm_roberta_base&quot;</code>, default language is <code>&quot;xx&quot;</code> (meaning
multi-lingual), if no values are provided. For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Embeddings" target="_blank">Models Hub</a>.</p><p>To see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a> and to see more extended
examples, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/XlmRoBertaSentenceEmbeddingsTestSpec.scala" target="_blank">XlmRoBertaSentenceEmbeddingsTestSpec</a>.</p><p><b>Paper Abstract:</b></p><p><i>This paper shows that pretraining multilingual language models at scale leads to significant
performance gains for a wide range of cross-lingual transfer tasks. We train a
Transformer-based masked language model on one hundred languages, using more than two
terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms
multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average
accuracy on XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R
performs particularly well on low-resource languages, improving 11.8% in XNLI accuracy for
Swahili and 9.2% for Urdu over the previous XLM model. We also present a detailed empirical
evaluation of the key factors that are required to achieve these gains, including the
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high
and low resource languages at scale. Finally, we show, for the first time, the possibility of
multilingual modeling without sacrificing per-language performance; XLM-Ris very competitive
with strong monolingual models on the GLUE and XNLI benchmarks. We will make XLM-R code, data,
and models publicly available.</i></p><p><b>Tips:</b></p><ul><li>XLM-RoBERTa is a multilingual model trained on 100 different languages. Unlike some XLM
    multilingual models, it does not require <b>lang</b> parameter to understand which language
    is used, and should be able to determine the correct language from the input ids.</li><li>This implementation is the same as RoBERTa. Refer to the <a href="RoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings">RoBertaEmbeddings</a> for usage
    examples as well as the information relative to the inputs and outputs.</li></ul><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base._
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator._
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="std">Array</span>(<span class="lit">"document"</span>))
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> sentenceEmbeddings = XlmRoBertaSentenceEmbeddings.pretrained()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"sentence_embeddings"</span>)
  .setCaseSensitive(<span class="kw">true</span>)

<span class="cmt">// you can either use the output to train ClassifierDL, SentimentDL, or MultiClassifierDL</span>
<span class="cmt">// or you can use EmbeddingsFinisher to prepare the results for Spark ML functions</span>

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"sentence_embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline()
  .setStages(<span class="std">Array</span>(
    documentAssembler,
    tokenizer,
    sentenceEmbeddings,
    embeddingsFinisher
  ))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[-<span class="num">0.05969233065843582</span>,-<span class="num">0.030789051204919815</span>,<span class="num">0.04443822056055069</span>,<span class="num">0.09564960747</span>...|
|[-<span class="num">0.038839809596538544</span>,<span class="num">0.011712731793522835</span>,<span class="num">0.019954433664679527</span>,<span class="num">0.0667808502</span>...|
|[-<span class="num">0.03952755779027939</span>,-<span class="num">0.03455188870429993</span>,<span class="num">0.019103847444057465</span>,<span class="num">0.04311436787</span>...|
|[-<span class="num">0.09579929709434509</span>,<span class="num">0.02494969218969345</span>,-<span class="num">0.014753809198737144</span>,<span class="num">0.10259044915</span>...|
|[<span class="num">0.004710011184215546</span>,-<span class="num">0.022148698568344116</span>,<span class="num">0.011723337695002556</span>,-<span class="num">0.013356896</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="XlmRoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings">XlmRoBertaEmbeddings</a> for token-level embeddings</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="XlnetEmbeddingsextendsAnnotatorModel[com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings]withHasBatchedAnnotate[com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings]withWriteTensorflowModelwithWriteSentencePieceModelwithHasEmbeddingsPropertieswithHasStorageRefwithHasCaseSensitivePropertieswithHasEngine"></a><a id="XlnetEmbeddings:XlnetEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/XlnetEmbeddings.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="XlnetEmbeddings (XLNet): Generalized Autoregressive Pretraining for Language Understanding" href="XlnetEmbeddings.html"><span class="name">XlnetEmbeddings</span></a><span class="result"> extends <a href="../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="XlnetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings">XlnetEmbeddings</a>] with <a href="../HasBatchedAnnotate.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotate">HasBatchedAnnotate</a>[<a href="XlnetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings">XlnetEmbeddings</a>] with <a href="../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../ml/tensorflow/sentencepiece/WriteSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.WriteSentencePieceModel">WriteSentencePieceModel</a> with <a href="HasEmbeddingsProperties.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.HasEmbeddingsProperties">HasEmbeddingsProperties</a> with <a href="../../storage/HasStorageRef.html" class="extype" name="com.johnsnowlabs.storage.HasStorageRef">HasStorageRef</a> with <a href="../HasCaseSensitiveProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasCaseSensitiveProperties">HasCaseSensitiveProperties</a> with <a href="../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">XlnetEmbeddings (XLNet): Generalized Autoregressive Pretraining for Language Understanding</p><div class="fullcomment"><div class="comment cmt"><p>XlnetEmbeddings (XLNet): Generalized Autoregressive Pretraining for Language Understanding</p><p>XLNet is a new unsupervised language representation learning method based on a novel
generalized permutation language modeling objective. Additionally, XLNet employs
Transformer-XL as the backbone model, exhibiting excellent performance for language tasks
involving long context. Overall, XLNet achieves state-of-the-art (SOTA) results on various
downstream language tasks including question answering, natural language inference, sentiment
analysis, and document ranking.</p><p>These word embeddings represent the outputs generated by the XLNet models.</p><p>Note that this is a very computationally expensive module compared to word embedding modules
that only perform embedding lookups. The use of an accelerator is recommended.</p><p><code>&quot;xlnet_large_cased&quot;</code> =
<a href="https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip" target="_blank">XLNet-Large</a>
\| 24-layer, 1024-hidden, 16-heads</p><p><code>&quot;xlnet_base_cased&quot;</code> =
<a href="https://storage.googleapis.com/xlnet/released_models/cased_L-12_H-768_A-12.zip" target="_blank">XLNet-Base</a>
\| 12-layer, 768-hidden, 12-heads. This model is trained on full data (different from the one
in the paper).</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> embeddings = XlnetEmbeddings.pretrained()
  .setInputCols(<span class="lit">"sentence"</span>, <span class="lit">"token"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)</pre><p>The default model is <code>&quot;xlnet_base_cased&quot;</code>, if no name is provided.</p><p>For extended examples of usage, see the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/dl-ner/ner_xlnet.ipynb" target="_blank">Examples</a>
and the
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/XlnetEmbeddingsTestSpec.scala" target="_blank">XlnetEmbeddingsTestSpec</a>.
To see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a>.</p><p><b>Sources :</b></p><p><a href="https://arxiv.org/abs/1906.08237" target="_blank">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></p><p><a href="https://github.com/zihangdai/xlnet" target="_blank">https://github.com/zihangdai/xlnet</a></p><p><b>Paper abstract: </b></p><p><i>With the capability of modeling bidirectional contexts, denoising autoencoding based
pretraining like BERT achieves better performance than pretraining approaches based on
autoregressive language modeling. However, relying on corrupting the input with masks, BERT
neglects dependency between the masked positions and suffers from a pretrain-finetune
discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive
pretraining method that (1) enables learning bidirectional contexts by maximizing the expected
likelihood over all permutations of the factorization order and (2) overcomes the limitations
of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from
Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically,
under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large
margin, including question answering, natural language inference, sentiment analysis, and
document ranking.</i></p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base.DocumentAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotators.Tokenizer
<span class="kw">import</span> com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings
<span class="kw">import</span> com.johnsnowlabs.nlp.EmbeddingsFinisher
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> documentAssembler = <span class="kw">new</span> DocumentAssembler()
  .setInputCol(<span class="lit">"text"</span>)
  .setOutputCol(<span class="lit">"document"</span>)

<span class="kw">val</span> tokenizer = <span class="kw">new</span> Tokenizer()
  .setInputCols(<span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"token"</span>)

<span class="kw">val</span> embeddings = XlnetEmbeddings.pretrained()
  .setInputCols(<span class="lit">"token"</span>, <span class="lit">"document"</span>)
  .setOutputCol(<span class="lit">"embeddings"</span>)

<span class="kw">val</span> embeddingsFinisher = <span class="kw">new</span> EmbeddingsFinisher()
  .setInputCols(<span class="lit">"embeddings"</span>)
  .setOutputCols(<span class="lit">"finished_embeddings"</span>)
  .setOutputAsVector(<span class="kw">true</span>)
  .setCleanAnnotations(<span class="kw">false</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  documentAssembler,
  tokenizer,
  embeddings,
  embeddingsFinisher
))

<span class="kw">val</span> data = <span class="std">Seq</span>(<span class="lit">"This is a sentence."</span>).toDF(<span class="lit">"text"</span>)
<span class="kw">val</span> result = pipeline.fit(data).transform(data)

result.selectExpr(<span class="lit">"explode(finished_embeddings) as result"</span>).show(<span class="num">5</span>, <span class="num">80</span>)
+--------------------------------------------------------------------------------+
|                                                                          result|
+--------------------------------------------------------------------------------+
|[-<span class="num">0.6287205219268799</span>,-<span class="num">0.4865287244319916</span>,-<span class="num">0.186111718416214</span>,<span class="num">0.234187275171279</span>...|
|[-<span class="num">1.1967450380325317</span>,<span class="num">0.2746637463569641</span>,<span class="num">0.9481253027915955</span>,<span class="num">0.3431355059146881</span>...|
|[-<span class="num">1.0777631998062134</span>,-<span class="num">2.092679977416992</span>,-<span class="num">1.5331977605819702</span>,-<span class="num">1.11190271377563</span>...|
|[-<span class="num">0.8349916934967041</span>,-<span class="num">0.45627787709236145</span>,-<span class="num">0.7890847325325012</span>,-<span class="num">1.028069257736</span>...|
|[-<span class="num">0.134845569729805</span>,-<span class="num">0.11672890186309814</span>,<span class="num">0.4945235550403595</span>,-<span class="num">0.66587203741073</span>...|
+--------------------------------------------------------------------------------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="../annotators/classifier/dl/XlnetForTokenClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.classifier.dl.XlnetForTokenClassification">XlnetForTokenClassification</a>
  For Xlnet embeddings with a token classification layer on top</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based embeddings</p></span></dd></dl></div>
    </li></ol>
            </div>

        

        <div class="values members">
              <h3>Value Members</h3>
              <ol>
                <li name="com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="AlbertEmbeddings"></a><a id="AlbertEmbeddings:AlbertEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/AlbertEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of AlbertEmbeddings." href="AlbertEmbeddings$.html"><span class="name">AlbertEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedAlbertModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedAlbertModel">ReadablePretrainedAlbertModel</a> with <a href="ReadAlbertDLModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadAlbertDLModel">ReadAlbertDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="AlbertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings">AlbertEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="AlbertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings">AlbertEmbeddings</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="BertEmbeddings"></a><a id="BertEmbeddings:BertEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/BertEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of BertEmbeddings." href="BertEmbeddings$.html"><span class="name">BertEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedBertModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedBertModel">ReadablePretrainedBertModel</a> with <a href="ReadBertDLModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadBertDLModel">ReadBertDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="BertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertEmbeddings">BertEmbeddings</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="BertSentenceEmbeddings"></a><a id="BertSentenceEmbeddings:BertSentenceEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/BertSentenceEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of BertSentenceEmbeddings." href="BertSentenceEmbeddings$.html"><span class="name">BertSentenceEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedBertSentenceModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedBertSentenceModel">ReadablePretrainedBertSentenceModel</a> with <a href="ReadBertSentenceDLModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadBertSentenceDLModel">ReadBertSentenceDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="BertSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings">BertSentenceEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="BertSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings">BertSentenceEmbeddings</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.CamemBertEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="CamemBertEmbeddings"></a><a id="CamemBertEmbeddings:CamemBertEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/CamemBertEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of CamemBertEmbeddings." href="CamemBertEmbeddings$.html"><span class="name">CamemBertEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedCamemBertModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedCamemBertModel">ReadablePretrainedCamemBertModel</a> with <a href="ReadCamemBertDLModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadCamemBertDLModel">ReadCamemBertDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="CamemBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.CamemBertEmbeddings">CamemBertEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="CamemBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.CamemBertEmbeddings">CamemBertEmbeddings</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ChunkEmbeddings"></a><a id="ChunkEmbeddings:ChunkEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ChunkEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of ChunkEmbeddings." href="ChunkEmbeddings$.html"><span class="name">ChunkEmbeddings</span></a><span class="result"> extends <span class="extype" name="org.apache.spark.ml.util.DefaultParamsReadable">DefaultParamsReadable</span>[<a href="ChunkEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings">ChunkEmbeddings</a>] with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="ChunkEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings">ChunkEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="ChunkEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings">ChunkEmbeddings</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.DeBertaEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="DeBertaEmbeddings"></a><a id="DeBertaEmbeddings:DeBertaEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/DeBertaEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of DeBertaEmbeddings." href="DeBertaEmbeddings$.html"><span class="name">DeBertaEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedDeBertaModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedDeBertaModel">ReadablePretrainedDeBertaModel</a> with <a href="ReadDeBertaDLModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadDeBertaDLModel">ReadDeBertaDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="DeBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DeBertaEmbeddings">DeBertaEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="DeBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DeBertaEmbeddings">DeBertaEmbeddings</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="DistilBertEmbeddings"></a><a id="DistilBertEmbeddings:DistilBertEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/DistilBertEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of DistilBertEmbeddings." href="DistilBertEmbeddings$.html"><span class="name">DistilBertEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedDistilBertModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedDistilBertModel">ReadablePretrainedDistilBertModel</a> with <a href="ReadDistilBertDLModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadDistilBertDLModel">ReadDistilBertDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="DistilBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings">DistilBertEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="DistilBertEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings">DistilBertEmbeddings</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.Doc2VecApproach" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="Doc2VecApproach"></a><a id="Doc2VecApproach:Doc2VecApproach"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/Doc2VecApproach$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of Doc2VecApproach." href="Doc2VecApproach$.html"><span class="name">Doc2VecApproach</span></a><span class="result"> extends <span class="extype" name="org.apache.spark.ml.util.DefaultParamsReadable">DefaultParamsReadable</span>[<a href="Doc2VecApproach.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Doc2VecApproach">Doc2VecApproach</a>] with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="Doc2VecApproach.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Doc2VecApproach">Doc2VecApproach</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="Doc2VecApproach.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Doc2VecApproach">Doc2VecApproach</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.Doc2VecModel" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="Doc2VecModel"></a><a id="Doc2VecModel:Doc2VecModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/Doc2VecModel$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of Doc2VecModel." href="Doc2VecModel$.html"><span class="name">Doc2VecModel</span></a><span class="result"> extends <a href="ReadablePretrainedDoc2Vec.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedDoc2Vec">ReadablePretrainedDoc2Vec</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="Doc2VecModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Doc2VecModel">Doc2VecModel</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="Doc2VecModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Doc2VecModel">Doc2VecModel</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.E5Embeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="E5Embeddings"></a><a id="E5Embeddings:E5Embeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/E5Embeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of E5Embeddings." href="E5Embeddings$.html"><span class="name">E5Embeddings</span></a><span class="result"> extends <a href="ReadablePretrainedE5Model.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedE5Model">ReadablePretrainedE5Model</a> with <a href="ReadE5DLModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadE5DLModel">ReadE5DLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="E5Embeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.E5Embeddings">E5Embeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="E5Embeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.E5Embeddings">E5Embeddings</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ElmoEmbeddings"></a><a id="ElmoEmbeddings:ElmoEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/ElmoEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of ElmoEmbeddings." href="ElmoEmbeddings$.html"><span class="name">ElmoEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedElmoModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedElmoModel">ReadablePretrainedElmoModel</a> with <a href="ReadElmoDLModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadElmoDLModel">ReadElmoDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="ElmoEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings">ElmoEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="ElmoEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings">ElmoEmbeddings</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.InstructorEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="InstructorEmbeddings"></a><a id="InstructorEmbeddings:InstructorEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/InstructorEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of InstructorEmbeddings." href="InstructorEmbeddings$.html"><span class="name">InstructorEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedInstructorModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedInstructorModel">ReadablePretrainedInstructorModel</a> with <a href="ReadInstructorDLModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadInstructorDLModel">ReadInstructorDLModel</a> with <a href="../../ml/tensorflow/sentencepiece/ReadSentencePieceModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.sentencepiece.ReadSentencePieceModel">ReadSentencePieceModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="InstructorEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.InstructorEmbeddings">InstructorEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="InstructorEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.InstructorEmbeddings">InstructorEmbeddings</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.LongformerEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="LongformerEmbeddings"></a><a id="LongformerEmbeddings:LongformerEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/LongformerEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of LongformerEmbeddings." href="LongformerEmbeddings$.html"><span class="name">LongformerEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedLongformerModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedLongformerModel">ReadablePretrainedLongformerModel</a> with <a href="ReadLongformerDLModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadLongformerDLModel">ReadLongformerDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="LongformerEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.LongformerEmbeddings">LongformerEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="LongformerEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.LongformerEmbeddings">LongformerEmbeddings</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.MPNetEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="MPNetEmbeddings"></a><a id="MPNetEmbeddings:MPNetEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/MPNetEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of MPNetEmbeddings." href="MPNetEmbeddings$.html"><span class="name">MPNetEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedMPNetModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedMPNetModel">ReadablePretrainedMPNetModel</a> with <a href="ReadMPNetDLModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadMPNetDLModel">ReadMPNetDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="MPNetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.MPNetEmbeddings">MPNetEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="MPNetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.MPNetEmbeddings">MPNetEmbeddings</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.PoolingStrategy" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="PoolingStrategy"></a><a id="PoolingStrategy:PoolingStrategy"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/PoolingStrategy$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="PoolingStrategy$.html"><span class="name">PoolingStrategy</span></a>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="RoBertaEmbeddings"></a><a id="RoBertaEmbeddings:RoBertaEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/RoBertaEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of RoBertaEmbeddings." href="RoBertaEmbeddings$.html"><span class="name">RoBertaEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedRobertaModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedRobertaModel">ReadablePretrainedRobertaModel</a> with <a href="ReadRobertaDLModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadRobertaDLModel">ReadRobertaDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="RoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings">RoBertaEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="RoBertaEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings">RoBertaEmbeddings</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.RoBertaSentenceEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="RoBertaSentenceEmbeddings"></a><a id="RoBertaSentenceEmbeddings:RoBertaSentenceEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/RoBertaSentenceEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of RoBertaSentenceEmbeddings." href="RoBertaSentenceEmbeddings$.html"><span class="name">RoBertaSentenceEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedRobertaSentenceModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedRobertaSentenceModel">ReadablePretrainedRobertaSentenceModel</a> with <a href="ReadRobertaSentenceDLModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadRobertaSentenceDLModel">ReadRobertaSentenceDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="RoBertaSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaSentenceEmbeddings">RoBertaSentenceEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="RoBertaSentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.RoBertaSentenceEmbeddings">RoBertaSentenceEmbeddings</a>. Please refer to that class for
the documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="SentenceEmbeddings"></a><a id="SentenceEmbeddings:SentenceEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/SentenceEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of SentenceEmbeddings." href="SentenceEmbeddings$.html"><span class="name">SentenceEmbeddings</span></a><span class="result"> extends <span class="extype" name="org.apache.spark.ml.util.DefaultParamsReadable">DefaultParamsReadable</span>[<a href="SentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings">SentenceEmbeddings</a>] with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="SentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings">SentenceEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="SentenceEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings">SentenceEmbeddings</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="UniversalSentenceEncoder"></a><a id="UniversalSentenceEncoder:UniversalSentenceEncoder"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/UniversalSentenceEncoder$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of UniversalSentenceEncoder." href="UniversalSentenceEncoder$.html"><span class="name">UniversalSentenceEncoder</span></a><span class="result"> extends <a href="ReadablePretrainedUSEModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedUSEModel">ReadablePretrainedUSEModel</a> with <a href="ReadUSEDLModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadUSEDLModel">ReadUSEDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="UniversalSentenceEncoder.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder">UniversalSentenceEncoder</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="UniversalSentenceEncoder.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder">UniversalSentenceEncoder</a>. Please refer to that class for
the documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.Word2VecApproach" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="Word2VecApproach"></a><a id="Word2VecApproach:Word2VecApproach"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/Word2VecApproach$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of Word2VecApproach." href="Word2VecApproach$.html"><span class="name">Word2VecApproach</span></a><span class="result"> extends <span class="extype" name="org.apache.spark.ml.util.DefaultParamsReadable">DefaultParamsReadable</span>[<a href="Word2VecApproach.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Word2VecApproach">Word2VecApproach</a>] with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="Word2VecApproach.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Word2VecApproach">Word2VecApproach</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="Word2VecApproach.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Word2VecApproach">Word2VecApproach</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.Word2VecModel" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="Word2VecModel"></a><a id="Word2VecModel:Word2VecModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/Word2VecModel$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of Word2VecModel." href="Word2VecModel$.html"><span class="name">Word2VecModel</span></a><span class="result"> extends <a href="ReadablePretrainedWord2Vec.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedWord2Vec">ReadablePretrainedWord2Vec</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="Word2VecModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Word2VecModel">Word2VecModel</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="Word2VecModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.Word2VecModel">Word2VecModel</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="WordEmbeddings"></a><a id="WordEmbeddings:WordEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/WordEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of WordEmbeddings." href="WordEmbeddings$.html"><span class="name">WordEmbeddings</span></a><span class="result"> extends <span class="extype" name="org.apache.spark.ml.util.DefaultParamsReadable">DefaultParamsReadable</span>[<a href="WordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings">WordEmbeddings</a>] with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="WordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings">WordEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="WordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddings">WordEmbeddings</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsBinaryIndexer" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="WordEmbeddingsBinaryIndexer"></a><a id="WordEmbeddingsBinaryIndexer:WordEmbeddingsBinaryIndexer"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/WordEmbeddingsBinaryIndexer$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="WordEmbeddingsBinaryIndexer$.html"><span class="name">WordEmbeddingsBinaryIndexer</span></a>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="WordEmbeddingsModel"></a><a id="WordEmbeddingsModel:WordEmbeddingsModel"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/WordEmbeddingsModel$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of WordEmbeddingsModel." href="WordEmbeddingsModel$.html"><span class="name">WordEmbeddingsModel</span></a><span class="result"> extends <a href="ReadablePretrainedWordEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedWordEmbeddings">ReadablePretrainedWordEmbeddings</a> with <a href="EmbeddingsCoverage.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.EmbeddingsCoverage">EmbeddingsCoverage</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="WordEmbeddingsModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel">WordEmbeddingsModel</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.embeddings.WordEmbeddingsTextIndexer" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="WordEmbeddingsTextIndexer"></a><a id="WordEmbeddingsTextIndexer:WordEmbeddingsTextIndexer"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/WordEmbeddingsTextIndexer$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="WordEmbeddingsTextIndexer$.html"><span class="name">WordEmbeddingsTextIndexer</span></a>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="XlmRoBertaEmbeddings"></a><a id="XlmRoBertaEmbeddings:XlmRoBertaEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/XlmRoBertaEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="XlmRoBertaEmbeddings$.html"><span class="name">XlmRoBertaEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedXlmRobertaModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedXlmRobertaModel">ReadablePretrainedXlmRobertaModel</a> with <a href="ReadXlmRobertaDLModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadXlmRobertaDLModel">ReadXlmRobertaDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.XlmRoBertaSentenceEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="XlmRoBertaSentenceEmbeddings"></a><a id="XlmRoBertaSentenceEmbeddings:XlmRoBertaSentenceEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/XlmRoBertaSentenceEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="XlmRoBertaSentenceEmbeddings$.html"><span class="name">XlmRoBertaSentenceEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedXlmRobertaSentenceModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedXlmRobertaSentenceModel">ReadablePretrainedXlmRobertaSentenceModel</a> with <a href="ReadXlmRobertaSentenceDLModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadXlmRobertaSentenceDLModel">ReadXlmRobertaSentenceDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="XlnetEmbeddings"></a><a id="XlnetEmbeddings:XlnetEmbeddings"></a>
      <span class="permalink">
      <a href="../../../../com/johnsnowlabs/nlp/embeddings/XlnetEmbeddings$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of XlnetEmbeddings." href="XlnetEmbeddings$.html"><span class="name">XlnetEmbeddings</span></a><span class="result"> extends <a href="ReadablePretrainedXlnetModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadablePretrainedXlnetModel">ReadablePretrainedXlnetModel</a> with <a href="ReadXlnetDLModel.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.ReadXlnetDLModel">ReadXlnetDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="XlnetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings">XlnetEmbeddings</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="XlnetEmbeddings.html" class="extype" name="com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings">XlnetEmbeddings</a>. Please refer to that class for the
documentation.
</p></div></div>
    </li>
              </ol>
            </div>

        

        
        </div>

        <div id="inheritedMembers">
        
        
        </div>

        <div id="groupedMembers">
        <div class="group" name="Ungrouped">
              <h3>Ungrouped</h3>
              
            </div>
        </div>

      </div>

      <div id="tooltip"></div>

      <div id="footer">  </div>
    </body>
          </div>
        </div>
      </div>
    </body>
      </html>
