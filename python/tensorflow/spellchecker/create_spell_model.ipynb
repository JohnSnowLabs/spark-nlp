{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSbNzQEyonvD"
   },
   "source": [
    "## Creates TensorFlow Graphs for Spark NLP ContextSpellChecker\n",
    "TensorFlow: `1.15.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uClGG_-goqzk"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_9adVefvbY0"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/feature/create_tf_graph_spellchecker/python/tensorflow/spellchecker/create_tf_models_colab.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMo3XYrZpS1r",
    "outputId": "2cc32fb9-87f5-4750-d123-2adf2f8041a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTcTXEw-onvH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class RNNLM(object):\n",
    "\n",
    "    def persist_graph(self, filename):\n",
    "        # Add ops to save and restore all the variables (not used here but we need it in the graph).\n",
    "        tf.train.Saver()\n",
    "        tf.train.write_graph(self.sess.graph, './', filename, False)\n",
    "\n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 num_epochs,\n",
    "                 check_point_step,\n",
    "                 num_layers,\n",
    "                 num_hidden_units,\n",
    "                 max_gradient_norm,\n",
    "                 max_num_classes=1902,\n",
    "                 max_word_ids=890,\n",
    "                 vocab_size=34800,\n",
    "                 initial_learning_rate=1,\n",
    "                 final_learning_rate=0.001,\n",
    "                 test_batch_size=36,\n",
    "                 max_seq_len=350\n",
    "                 ):\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # these two parameters depend on the factorization of the language model\n",
    "        self.max_num_classes = max_num_classes\n",
    "        self.max_word_ids = max_word_ids\n",
    "\n",
    "        # this is the batch for training\n",
    "        self.batch_size = batch_size\n",
    "        # this is the batch for testing\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.check_point_step = check_point_step\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden_units = num_hidden_units\n",
    "        self.max_gradient_norm = max_gradient_norm\n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"train/global_step\")\n",
    "\n",
    "        # these are inputs to the graph\n",
    "        self.wordIds = \"batches:0\"\n",
    "        self.contextIds = \"batches:1\"\n",
    "        self.contextWordIds = \"batches:2\"\n",
    "\n",
    "        # dynamic learning rate, decay every 1500 batches\n",
    "        self.initial_learning_rate = tf.placeholder(tf.float32, name=\"train/initial_learning_rate\")\n",
    "        self.final_learning_rate = tf.placeholder(tf.float32, name=\"train/final_learning_rate\")\n",
    "        self.learning_rate = tf.train.exponential_decay(self.initial_learning_rate, self.global_step, 1500, 0.96, staircase=True)\n",
    "        self.learning_rate = tf.cond(tf.less(self.learning_rate, self.final_learning_rate),\n",
    "                                     lambda: tf.constant(final_learning_rate), lambda: self.learning_rate, name=\"train/learning_rate\")\n",
    "\n",
    "        self.dropout_rate = tf.placeholder(tf.float32, name=\"dropout_rate\")\n",
    "\n",
    "        self.file_name_train = tf.placeholder(tf.string)\n",
    "        self.file_name_validation = tf.placeholder(tf.string)\n",
    "        self.file_name_test = tf.placeholder(tf.string, name='file_name')\n",
    "\n",
    "        # this tensor holds in-memory data for testing, dimensions:\n",
    "        # {batch_size, sentence_len, (wordid, classid, class_wid)}\n",
    "        self.in_memory_test = tf.placeholder(tf.int32, shape=[None, None, None], name='in-memory-input')\n",
    "\n",
    "        # the input batch(ids), the class ids and word ids for the output batch\n",
    "        self.input_batch = tf.placeholder(tf.int32, shape=[None, None], name='input_batch')\n",
    "        self.output_batch_cids = tf.placeholder(tf.int32, shape=[None, None], name='output_batch_cids')\n",
    "        self.output_batch_wids = tf.placeholder(tf.int32, shape=[None, None], name='output_batch_wids')\n",
    "        self.batch_lengths = tf.placeholder(tf.int32, shape=[None], name='input_batch_lengths')\n",
    "\n",
    "        # Input embedding mat\n",
    "        self.input_embedding_mat = tf.get_variable(\"input_embedding_mat\",\n",
    "                                                   [self.vocab_size, self.num_hidden_units],\n",
    "                                                   dtype=tf.float32)\n",
    "\n",
    "        self.input_embedded = tf.nn.embedding_lookup(self.input_embedding_mat, self.input_batch)\n",
    "\n",
    "        # LSTM cell\n",
    "        rnn_layers = []\n",
    "        for _ in range(self.num_layers):\n",
    "            cell = tf.contrib.rnn.LSTMCell(self.num_hidden_units, state_is_tuple=True)\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=self.dropout_rate)\n",
    "            rnn_layers.append(cell)\n",
    "\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells=rnn_layers, state_is_tuple=True)\n",
    "        self.cell = cell\n",
    "\n",
    "        # Output embedding - classes\n",
    "        self.output_class_embedding_mat = tf.get_variable(\"output_class_embedding_mat\",\n",
    "                                                          [self.max_num_classes, self.num_hidden_units],\n",
    "                                                          dtype=tf.float32)\n",
    "\n",
    "        self.output_class_embedding_bias = tf.get_variable(\"output_class_embedding_bias\",\n",
    "                                                           [self.max_num_classes],\n",
    "                                                           dtype=tf.float32)\n",
    "\n",
    "        # Output embedding - word ids\n",
    "        self.output_wordid_embedding_mat = tf.get_variable(\"output_wordid_embedding_mat\",\n",
    "                                                           [self.max_word_ids, self.num_hidden_units],\n",
    "                                                           dtype=tf.float32)\n",
    "\n",
    "        self.output_wordid_embedding_bias = tf.get_variable(\"output_wordid_embedding_bias\",\n",
    "                                                            [self.max_word_ids],\n",
    "                                                            dtype=tf.float32)\n",
    "\n",
    "        # The shape of outputs is [batch_size, max_length, num_hidden_units]\n",
    "        outputs, _ = tf.nn.dynamic_rnn(\n",
    "            cell=self.cell,\n",
    "            inputs=self.input_embedded,\n",
    "            sequence_length=self.batch_lengths,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        def output_class_embedding(current_output):\n",
    "            return tf.add(\n",
    "                tf.matmul(current_output, tf.transpose(self.output_class_embedding_mat)), self.output_class_embedding_bias)\n",
    "\n",
    "        def output_wordid_embedding(current_output):\n",
    "            return tf.add(\n",
    "                tf.matmul(current_output, tf.transpose(self.output_wordid_embedding_mat)), self.output_wordid_embedding_bias)\n",
    "\n",
    "        # To compute the logits - classes\n",
    "        class_logits = tf.map_fn(output_class_embedding, outputs)\n",
    "        class_logits = tf.reshape(class_logits, [-1, self.max_num_classes], name='cl') #(total_word_cnt, n_classes)\n",
    "\n",
    "        class_loss = tf.nn.sparse_softmax_cross_entropy_with_logits \\\n",
    "            (labels=tf.reshape(self.output_batch_cids, [-1]), logits=class_logits)\n",
    "\n",
    "\n",
    "        # To compute the logits - word ids\n",
    "        wordid_logits = tf.map_fn(output_wordid_embedding, outputs)\n",
    "        # dim(batch_size, n_words)\n",
    "        wordid_logits = tf.reshape(wordid_logits, [-1, self.max_word_ids])\n",
    "        wordid_loss = tf.nn.sparse_softmax_cross_entropy_with_logits \\\n",
    "            (labels=tf.reshape(self.output_batch_wids, [-1]), logits=wordid_logits)\n",
    "\n",
    "        # Train\n",
    "        params = tf.trainable_variables()\n",
    "        opt = tf.train.AdagradOptimizer(self.learning_rate)\n",
    "\n",
    "        # Global loss, we can add here, because we're handling log probabilities\n",
    "        self.loss = tf.add(class_loss, wordid_loss)\n",
    "\n",
    "        gradients = tf.gradients(self.loss, params, colocate_gradients_with_ops=True)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n",
    "        self.updates = opt.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step, name=\"train/updates\")\n",
    "\n",
    "        # Add another loss, used for evaluation - more efficient - evaluate multiple candidates words at the same time\n",
    "        self.candidate_word_ids = tf.placeholder(tf.int32, shape=[1, None], name='test_wids')\n",
    "        self.candidate_class_ids = tf.placeholder(tf.int32, shape=[1, None], name='test_cids')\n",
    "\n",
    "        cand_cnt = tf.shape(self.candidate_class_ids)\n",
    "\n",
    "        # broadcasted class logits - take only the last element, and repeat it\n",
    "        bc_cl_logits = tf.tile(class_logits[-1:, :], tf.reverse(cand_cnt, axis=tf.constant([0])), name='bccl')\n",
    "        classid_losses = tf.nn.sparse_softmax_cross_entropy_with_logits \\\n",
    "            (labels=tf.reshape(self.candidate_class_ids, [-1]), logits=bc_cl_logits, name='cidlosses')\n",
    "\n",
    "        bc_id_logits = tf.tile(wordid_logits[-1:, :], tf.reverse(cand_cnt, axis=tf.constant([0])))\n",
    "        wordid_losses = tf.nn.sparse_softmax_cross_entropy_with_logits \\\n",
    "            (labels=tf.reshape(self.candidate_word_ids, [-1]), logits=bc_id_logits, name='widlosses')\n",
    "\n",
    "        self.losses = tf.add(wordid_losses, classid_losses, 'test_losses')\n",
    "        self.sess = tf.Session()\n",
    "        init = tf.global_variables_initializer()\n",
    "        print(init.name)\n",
    "        self.sess.run(init)\n",
    "\n",
    "\n",
    "    def load_classes(self, file_path):\n",
    "        class_word = dict()\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                chunks = line.split('|')\n",
    "                try:\n",
    "                    class_word[int(chunks[0])] = (int(chunks[1]), int(chunks[2]))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        self.class_word = class_word\n",
    "        return class_word\n",
    "\n",
    "    def load_vocab(self, file_path):\n",
    "        word_id = dict()\n",
    "        with open(file_path, 'r') as f:\n",
    "            for i, line in enumerate(f.readlines()):\n",
    "                chunks = line.split('|')\n",
    "                word_id[chunks[0]] = i\n",
    "\n",
    "        self.word_ids = word_id\n",
    "        return word_id\n",
    "\n",
    "    def dataset_generator(self, batch_size, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            idx = 0\n",
    "            while idx < len(lines):\n",
    "                ids = [[int(k) for k in line.split()] for line in lines[idx:idx + batch_size]]\n",
    "                cids = [[self.class_word[i][0] for i in line][1:] for line in ids]\n",
    "                wids = [[self.class_word[i][1] for i in line][1:] for line in ids]\n",
    "                ids = [idlist[:-1] for idlist in ids]\n",
    "                lens = [len(line) for line in ids]\n",
    "\n",
    "                # pad to fixed size\n",
    "                ids = [idlist + [0] * (self.max_seq_len - len(idlist)) for idlist in ids]\n",
    "                cids = [idlist + [0] * (self.max_seq_len - len(idlist)) for idlist in cids]\n",
    "                wids = [idlist + [0] * (self.max_seq_len - len(idlist)) for idlist in wids]\n",
    "                idx += batch_size\n",
    "\n",
    "                # or truncate\n",
    "                ids = [line[:self.max_seq_len] for line in ids]\n",
    "                cids = [line[:self.max_seq_len] for line in cids]\n",
    "                wids = [line[:self.max_seq_len] for line in wids]\n",
    "\n",
    "                # yield a batch\n",
    "                if len(ids) == self.batch_size:\n",
    "                    yield (ids, cids, wids, lens)\n",
    "\n",
    "\n",
    "    def save(self, path, sess):\n",
    "        dropout_rate_info = tf.saved_model.utils.build_tensor_info(self.dropout_rate)\n",
    "        loss_info = tf.saved_model.utils.build_tensor_info(self.loss)\n",
    "\n",
    "        model_xy_sig = tf.saved_model.signature_def_utils.build_signature_def(\n",
    "            inputs={'dropout_rate': dropout_rate_info},  outputs={'ppl': loss_info}, method_name='predict')\n",
    "\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(path)\n",
    "        builder.add_meta_graph_and_variables(sess,\n",
    "                                             ['our-graph'],\n",
    "                                             signature_def_map={'sig_def':model_xy_sig})\n",
    "\n",
    "        builder.save()\n",
    "\n",
    "    def sum_losses(self, losses, lens):\n",
    "        starts = list(range(0, self.max_seq_len * self.batch_size, self.max_seq_len))\n",
    "        ends = lens\n",
    "        return sum([sum(losses[start:start + shift]) for start, shift in zip(starts, ends)])\n",
    "\n",
    "    def memory_train(self, sess, dataset, epochs=10):\n",
    "        ''' train from data in memory '''\n",
    "\n",
    "        best_score = np.inf\n",
    "        patience = 15\n",
    "        epoch = 0\n",
    "\n",
    "        while epoch < epochs:\n",
    "            print('epoch %d' % epoch)\n",
    "            train_loss = 0.0\n",
    "            train_valid_words = 0\n",
    "\n",
    "            for (input_batch, output_cids, output_wids, lens) in dataset:\n",
    "\n",
    "                _loss, global_step, current_learning_rate, _ = sess.run(\n",
    "                    [self.loss, self.global_step, self.learning_rate, self.updates],\n",
    "                    {self.input_batch: input_batch, self.output_batch_cids: output_cids,\n",
    "                     self.output_batch_wids: output_wids, self.dropout_rate: 0.65,\n",
    "                     self.batch_lengths: lens, self.initial_learning_rate: 1.0, self.final_learning_rate: .5})\n",
    "            epoch += 1\n",
    "\n",
    "    # this is no longer supported Python side, training works Scala only\n",
    "    def batch_train(self, sess, saver, train_path, valid_path):\n",
    "\n",
    "        best_score = np.inf\n",
    "        patience = 15\n",
    "        epoch = 0\n",
    "        self.train_path = train_path\n",
    "        self.valid_path = valid_path\n",
    "\n",
    "        while epoch < self.num_epochs:\n",
    "            print('epoch %d' % epoch)\n",
    "            train_loss = 0.0\n",
    "            train_valid_words = 0\n",
    "\n",
    "            for (input_batch, output_cids, output_wids, lens) in self.dataset_generator(self.batch_size, self.train_path):\n",
    "\n",
    "                _loss, global_step, current_learning_rate, _ = sess.run(\n",
    "                    [self.loss, self.global_step, self.learning_rate, self.updates],\n",
    "                    {self.input_batch: input_batch, self.output_batch_cids: output_cids,\n",
    "                     self.output_batch_wids: output_wids, self.dropout_rate: 0.65,\n",
    "                     self.batch_lengths: lens})\n",
    "\n",
    "                train_loss += self.sum_losses(_loss, lens)\n",
    "                train_valid_words += sum(lens) #_valid_words\n",
    "\n",
    "                if global_step % self.check_point_step == 0:\n",
    "                    import gc\n",
    "                    gc.collect()\n",
    "                    train_loss /= train_valid_words\n",
    "                    train_ppl = math.exp(train_loss)\n",
    "                    print (\"Training Step: {}, LR: {}\".format(global_step, current_learning_rate))\n",
    "                    print (\"    Training PPL: {}\".format(train_ppl))\n",
    "                    train_loss = 0.0\n",
    "                    train_valid_words = 0\n",
    "\n",
    "            # The end of one epoch\n",
    "            # run validation\n",
    "            dev_loss = 0.0\n",
    "            dev_valid_words = 0\n",
    "\n",
    "            for (input_batch, output_cids, output_wids, lens) in self.dataset_generator(self.batch_size, self.valid_path):\n",
    "                _dev_loss = sess.run(\n",
    "                    [self.loss],\n",
    "                    {self.input_batch: input_batch, self.output_batch_cids: output_cids,\n",
    "                     self.output_batch_wids: output_wids, self.dropout_rate: 0.65,\n",
    "                     self.batch_lengths: lens})\n",
    "\n",
    "                # problem here!\n",
    "                dev_loss += self.sum_losses(_dev_loss[0], lens)\n",
    "                dev_valid_words += sum(lens)\n",
    "\n",
    "            dev_loss /= dev_valid_words\n",
    "            dev_ppl = math.exp(dev_loss)\n",
    "            print(\"Validation PPL: {}\".format(dev_ppl))\n",
    "            if dev_ppl < best_score:\n",
    "                saver.save(sess, \"model/best_model.ckpt\")\n",
    "                best_score = dev_ppl\n",
    "            epoch += 1\n",
    "\n",
    "    def predict(self, sess, raw_sentences, verbose=False):\n",
    "        '''\n",
    "           this version of predict() should be deprecated\n",
    "        '''\n",
    "\n",
    "        global_dev_loss = 0.0\n",
    "        global_dev_valid_words = 0\n",
    "\n",
    "        for raw_line in raw_sentences:\n",
    "\n",
    "            splits = raw_line.split()\n",
    "            wids = [self.word_ids[token] for token in splits]\n",
    "\n",
    "            cids = [self.class_word[i][0] for i in wids]\n",
    "            wcids = [self.class_word[i][1] for i in wids]\n",
    "\n",
    "            # graph access is split into a. initialization and\n",
    "            sess.run(self.test_init_op, {self.in_memory_test: np.array([[wids, cids, wcids]])})\n",
    "\n",
    "            raw_line = raw_line.strip()\n",
    "\n",
    "            # b. actually feeding the data to the nodes.\n",
    "            # don't do this split access in production!\n",
    "            cl = tf.get_default_graph().get_tensor_by_name(\"cl:0\")\n",
    "            _dev_loss, _dev_valid_words, input_line, mask_, cl_ = sess.run(\n",
    "                [self.loss, self.valid_words, self.input_batch, self.mask, cl],\n",
    "                {self.dropout_rate: 1.0})\n",
    "\n",
    "            dev_loss = np.sum(_dev_loss)\n",
    "            dev_valid_words = _dev_valid_words\n",
    "\n",
    "            global_dev_loss += dev_loss\n",
    "            global_dev_valid_words += dev_valid_words\n",
    "\n",
    "            if verbose:\n",
    "                dev_loss /= dev_valid_words\n",
    "                dev_ppl = math.exp(dev_loss)\n",
    "                print(raw_line + \"    Test PPL: {}\".format(dev_ppl))\n",
    "\n",
    "        global_dev_loss /= global_dev_valid_words\n",
    "        global_dev_ppl = math.exp(global_dev_loss)\n",
    "        #print(\"Global Test PPL: {}\".format(global_dev_ppl))\n",
    "\n",
    "    def predict_(self, sess, candidates, verbose=False):\n",
    "\n",
    "        sent = 'she came to me in an unexpected unexpected'\n",
    "        splits = sent.split()\n",
    "        #splits.reverse()\n",
    "\n",
    "        # sentence input\n",
    "        wids = [self.word_ids[token] for token in splits]\n",
    "        cids = [self.class_word[i][0] for i in wids]\n",
    "        wcids = [self.class_word[i][1] for i in wids]\n",
    "\n",
    "        # candidate inputs\n",
    "        can_wids = [self.word_ids[token] for token in candidates]\n",
    "        can_cids = [[self.class_word[i][0] for i in can_wids]]\n",
    "        can_wcids = [[self.class_word[i][1] for i in can_wids]]\n",
    "\n",
    "        # these two are for debugging\n",
    "        #cl = tf.get_default_graph().get_tensor_by_name(\"cl:0\")\n",
    "        #bccl = tf.get_default_graph().get_tensor_by_name(\"bccl:0\")\n",
    "        losses = sess.run(\n",
    "            [self.losses],\n",
    "            {self.dropout_rate: 1.0,\n",
    "             self.wordIds: np.array([wids[:-1]]),\n",
    "             self.contextIds: np.array([cids[1:]]),\n",
    "             self.contextWordIds: np.array([wcids[1:]]),\n",
    "             self.candidate_word_ids: np.array(can_wcids),\n",
    "             self.candidate_class_ids: np.array(can_cids)\n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwstAPQvrQyj"
   },
   "outputs": [],
   "source": [
    "def create_graph(hunits, num_layers, classes, vocab_size):\n",
    "    model = RNNLM(batch_size=24,\n",
    "                  num_epochs=5,\n",
    "                  check_point_step=5000,\n",
    "                  num_layers=num_layers,\n",
    "                  num_hidden_units=hunits,\n",
    "                  max_gradient_norm=5.0,\n",
    "                  max_num_classes=classes,\n",
    "                  max_word_ids=classes,\n",
    "                  vocab_size=vocab_size,\n",
    "                  initial_learning_rate=.7,\n",
    "                  final_learning_rate=0.0005)\n",
    "\n",
    "    # Persist graph\n",
    "    model.persist_graph('nlm_%d_%d_%d_%d.pb' % (hunits, num_layers, classes, vocab_size))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X1R99MUwonvJ",
    "outputId": "3d960b95-5ad4-45d1-d4fd-d5d8f5945b2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-9cf6939b3aff>:84: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-4-9cf6939b3aff>:88: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-4-9cf6939b3aff>:114: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "init\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RNNLM at 0x7f37ccd51d10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_graph(300, 2, 2000, 56650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4S9TwwkonvK",
    "outputId": "3f3e567b-fe18-4188-cef4-745933b162cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlm_300_2_2000_56650.pb  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Create tf models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
